[
  {
    "id": 1,
    "question": "Quelle est la principale difficulté lors de la correspondance entre qubits logiques et qubits physiques sur du matériel NISQ ?",
    "A": "La topologie de connectivité limitée de l'agencement des qubits physiques forme typiquement un graphe sparse qui ne peut pas directement accommoder toutes les interactions de portes à deux qubits requises et spécifiées dans le circuit logique, nécessitant l'insertion d'opérations SWAP supplémentaires pour acheminer l'information quantique entre qubits non adjacents. Cette surcharge de routage augmente substantiellement la profondeur du circuit et amplifie les effets de décohérence.",
    "B": "Les fidélités de porte hétérogènes entre différentes paires de qubits physiques dans la topologie de couplage créent des objectifs d'optimisation conflictuels lors de l'allocation, où minimiser la profondeur du circuit par un compactage dense des qubits peut forcer des opérations d'intrication critiques sur des liaisons de faible fidélité, tandis que des allocations éparses qui privilégient les connexions de haute fidélité nécessitent un routage SWAP supplémentaire. Le mappeur doit équilibrer ces contraintes concurrentes sans connaissance complète des taux d'erreur à l'exécution, car les fidélités de porte fluctuent avec la dérive de calibration et les motifs de diaphonie qui dépendent de l'ordonnancement spécifique des portes produit par les décisions d'allocation, créant des dépendances circulaires où le mappage optimal nécessite de connaître l'ordonnancement final, mais l'ordonnancement optimal dépend du mappage choisi.",
    "C": "Les temps de cohérence T1 et T2 différentiels à travers le réseau de qubits physiques introduisent des contraintes temporelles qui entrent en conflit avec les contraintes spatiales imposées par la connectivité limitée, exigeant que le compilateur optimise simultanément l'assignation des qubits et l'ordonnancement des portes pour faire correspondre les qubits de courte durée de vie avec les opérations survenant tôt dans le circuit tout en réservant les qubits de haute cohérence pour les opérations ultérieures. Ce problème d'optimisation couplé devient intraitable pour les circuits dépassant une taille modeste car chaque mappage candidat induit un chemin critique différent à travers la topologie du circuit qui détermine quels qubits subissent les périodes d'inactivité les plus longues, forçant l'allocateur à résoudre des problèmes d'ordonnancement sous contraintes de ressources NP-difficiles de manière itérative pour chaque configuration de mappage d'essai avant d'identifier la solution globalement optimale.",
    "D": "L'asymétrie bidirectionnelle des implémentations natives CNOT sur de nombreuses plateformes NISQ restreint quel qubit dans chaque paire physique peut servir de contrôle versus cible, créant des contraintes d'arêtes dirigées dans le graphe de couplage qui limitent les mappages logiques-vers-physiques réalisables par rapport aux hypothèses de connectivité non dirigée. Lorsque le circuit logique nécessite des opérations CNOT dans les deux directions entre deux qubits logiques mappés sur une paire physiquement connectée, le compilateur doit insérer des portes SWAP supplémentaires ou décomposer une direction CNOT dans la direction disponible en utilisant la conjugaison de Hadamard, augmentant le nombre de portes par des facteurs qui se composent à travers de multiples conflits de ce type, particulièrement problématique pour les circuits avec des motifs d'intrication bidirectionnels denses qui ne peuvent être satisfaits par aucun mappage respectant les contraintes directionnelles.",
    "solution": "A"
  },
  {
    "id": 2,
    "question": "Quelle propriété de la mécanique quantique permet aux ordinateurs quantiques d'effectuer certains calculs plus rapidement que les ordinateurs classiques ?",
    "A": "Le déterminisme classique encodé dans les systèmes quantiques à travers des évolutions unitaires soigneusement conçues qui préservent les relations déterministes entre états d'entrée et de sortie — en mappant les opérations logiques classiques sur des portes quantiques réversibles tout en maintenant une causalité stricte, les ordinateurs quantiques peuvent exploiter l'évolution prévisible des systèmes quantiques fermés pour obtenir des accélérations computationnelles.",
    "B": "Les distributions de probabilité absolues qui restent constantes tout au long du calcul quantique, fournissant des poids statistiques stables pour chaque état de base computationnel — contrairement aux algorithmes probabilistes classiques où les distributions de probabilité évoluent de manière imprévisible, la mécanique quantique garantit que les probabilités de la règle de Born sont des quantités conservées.",
    "C": "La superposition permet aux ordinateurs quantiques d'exister simultanément dans plusieurs états computationnels, leur permettant d'explorer un nombre exponentiellement grand de chemins de solution en parallèle à travers une seule évolution cohérente — lorsqu'elle est combinée avec des effets d'interférence qui amplifient les amplitudes des réponses correctes tout en annulant celles incorrectes, et l'intrication qui crée des corrélations entre qubits qui n'ont pas d'analogue classique, la superposition forme le fondement des accélérations quantiques en permettant aux algorithmes comme ceux de Shor et Grover de traiter de vastes espaces de solutions en utilisant des ressources quantiques polynomiales là où les ordinateurs classiques nécessiteraient un temps exponentiel. La clé est que la mesure fait s'effondrer cette superposition pour extraire le résultat computationnel, mais pendant l'évolution, tous les états de base contribuent simultanément à la dynamique.",
    "D": "Les états computationnels fixes que les systèmes quantiques maintiennent naturellement en raison des principes de minimisation d'énergie — les ordinateurs quantiques exploitent le fait que les qubits restent préférentiellement dans leurs états de base initialisés à moins d'être explicitement perturbés.",
    "solution": "C"
  },
  {
    "id": 3,
    "question": "Quelle est l'idée clé derrière les Réseaux Antagonistes Génératifs Quantiques (QGANs) ?",
    "A": "Les circuits quantiques servent à la fois de générateur et de discriminateur, formant une boucle d'entraînement compétitive où le générateur prépare des états quantiques paramétrés par des circuits ansatz variationnels tandis que le discriminateur, également implémenté comme un circuit quantique paramétré, effectue des mesures pour distinguer les données d'entraînement réelles des échantillons générés. Cette architecture antagoniste quantique-à-quantique permet le flux de gradient à travers les canaux quantiques et atteint potentiellement une accélération quadratique dans la tâche discriminative comparée aux discriminateurs de réseaux neuronaux classiques.",
    "B": "Ils exploitent l'intrication quantique pour modéliser des corrélations complexes de haute dimension exponentiellement que les GANs classiques ont du mal à capturer efficacement, tout en tirant simultanément parti de la superposition pour explorer l'espace d'échantillonnage beaucoup plus efficacement que les méthodes d'échantillonnage classiques. En encodant les corrélations dans des états intriqués plutôt que dans des paramètres explicites, les QGANs peuvent représenter des distributions de probabilité conjointes qui nécessiteraient un nombre exponentiellement grand de paramètres classiques, fournissant un avantage exponentiel potentiel dans certaines tâches de modélisation générative où les données présentent des dépendances statistiques de type quantique à longue portée.",
    "C": "La superposition génère plusieurs échantillons à la fois, créant essentiellement une distribution de probabilité entière simultanément plutôt que d'échantillonner séquentiellement comme les GANs classiques.",
    "D": "Tous ces mécanismes travaillant ensemble forment le fondement de la façon dont les QGANs atteignent leurs avantages computationnels par rapport aux modèles génératifs classiques",
    "solution": "D"
  },
  {
    "id": 4,
    "question": "Quelle est la fonction principale d'un répéteur quantique dans l'Internet Quantique ?",
    "A": "Effectuer la correction d'erreurs quantiques sur les qubits transmis aux nœuds intermédiaires en encodant les qubits logiques dans des codes multi-qubits comme le code de Steane ou le code de surface, permettant aux erreurs accumulées pendant la transmission d'être détectées et corrigées avant transmission. Cela permet la communication quantique à longue distance en rafraîchissant continuellement l'information quantique par correction d'erreurs active à chaque station de répétition, contournant la décohérence sans violer le non-clonage puisque les opérations de correction d'erreurs préservent l'état logique encodé tout en rejetant les syndromes d'erreur.",
    "B": "Étendre la distribution d'intrication au-delà des limites de transmission directe à travers des protocoles d'échange et de purification d'intrication, qui permettent l'établissement de paires intriquées de haute fidélité entre nœuds distants malgré la perte de photons et la décohérence. En divisant les longues distances en segments plus courts et en effectuant des mesures de Bell aux nœuds intermédiaires, les répéteurs quantiques contournent la décroissance exponentielle de l'intrication avec la distance.",
    "C": "Implémenter la génération d'intrication déterministe entre segments de réseau adjacents en stockant les qubits photoniques dans des mémoires quantiques basées sur la matière et en effectuant la création d'intrication annoncée par interférence à deux photons sur des séparateurs de faisceau. Ce processus en deux étapes établit d'abord l'intrication de manière probabiliste par interférence de Hong-Ou-Mandel, puis utilise des mémoires quantiques pour synchroniser les événements d'intrication réussis à travers plusieurs liaisons, étendant effectivement les portées de transmission d'état quantique tout en évitant le besoin de purification lorsque les temps de cohérence de mémoire dépassent le taux de génération d'intrication.",
    "D": "Amplifier la fidélité d'intrication dégradée par application séquentielle de protocoles de distillation d'intrication comme les schémas BBPSSW ou DEJMPS, qui consomment plusieurs paires intriquées bruitées pour produire moins de paires de haute fidélité à travers des opérations locales et communication classique aux nœuds intermédiaires. En filtrant itérativement les erreurs par mesures bilatérales et post-sélection, les répéteurs quantiques restaurent la qualité d'intrication qui s'est dégradée pendant la transmission, permettant la communication quantique à longue distance sans nécessiter de codes de correction d'erreurs quantiques ou de mesures d'état de Bell entre parties distantes.",
    "solution": "B"
  },
  {
    "id": 5,
    "question": "Pourquoi l'ordonnancement entrelacé des termes réduit-il l'erreur de Trotter-Suzuki en chimie quantique ?",
    "A": "En alternant les composantes diagonales et non-diagonales du hamiltonien à chaque pas de Trotter, les termes diagonaux suppriment effectivement le transfert de population induit par les transitions non-diagonales, puisque l'évolution de phase accumulée des opérateurs diagonaux crée des motifs d'interférence destructive qui annulent les amplitudes de transition indésirables avant qu'elles ne puissent s'accumuler à des niveaux significatifs sur plusieurs tranches temporelles",
    "B": "Lorsque les termes du hamiltonien sont entrelacés plutôt que regroupés par type, le spectre de Fourier de l'évolution de Trotter résultante devient plus uniformément distribué à travers les composantes de fréquence, aplatissant effectivement les oscillations haute fréquence qui autrement se composeraient à travers les pas de temps successifs. Cet aplatissement spectral réduit l'amplitude des termes d'erreur dans l'expansion de Baker-Campbell-Hausdorff, conduisant à des annulations dans les contributions de commutateur de second ordre",
    "C": "Les sous-termes regroupés par espace propre minimisent l'accumulation d'erreur de phase entre exponentielles",
    "D": "L'entrelacement alterne entre différents types de termes hamiltoniens tout au long de la séquence de Trotter plutôt que de regrouper tous les termes d'un type ensemble, ce qui réduit l'accumulation systématique d'erreurs de commutateur. Lorsque les termes non-commutatifs sont entremêlés, les exponentielles consécutives contiennent plus fréquemment des opérateurs qui annulent partiellement les erreurs les uns des autres à travers les termes d'expansion de Baker-Campbell-Hausdorff, conduisant à une précision globale améliorée comparée à un ordonnancement par blocs où les erreurs se composent unidirectionnellement.",
    "solution": "D"
  },
  {
    "id": 6,
    "question": "Quelle méthodologie d'attaque avancée peut compromettre la sécurité des cryptomonnaies quantiques ?",
    "A": "Double dépense par attaque en superposition, où un adversaire prépare une transaction malveillante dans une superposition cohérente de plusieurs états contradictoires, lui permettant de diffuser simultanément des opérations de dépense incompatibles vers différents nœuds du réseau. Lors de la mesure par le mécanisme de consensus du réseau, l'attaquant peut sélectivement effondrer la superposition vers la branche donnant le résultat le plus favorable, dépensant effectivement le même jeton quantique plusieurs fois avant que les limites de décohérence ne soient atteintes.",
    "B": "Création de fork de blockchain quantique, qui exploite le théorème de non-clonage à l'envers en utilisant l'échange d'intrication pour générer des historiques de blockchain parallèles causalement cohérents qui apparaissent chacun valides selon les protocoles de vérification standard.",
    "C": "Récupération de clés accélérée par Shor, qui applique l'algorithme de Shor pour factoriser les grands nombres composites sous-jacents aux primitives cryptographiques à clé publique utilisées dans les protocoles de cryptomonnaie quantique. En calculant efficacement les logarithmes discrets ou en factorisant les modules RSA en temps polynomial, un adversaire disposant d'un ordinateur quantique tolérant aux fautes peut dériver les clés privées à partir d'adresses publiquement diffusées, permettant la signature non autorisée de transactions et la compromission complète de la sécurité des portefeuilles sur l'ensemble du réseau.",
    "D": "Avantage de l'algorithme de minage quantique, par lequel un adversaire ayant accès à un ordinateur quantique tolérant aux fautes peut exploiter l'algorithme de Grover pour obtenir une accélération quadratique dans le processus de résolution du puzzle de preuve de travail par rapport aux mineurs classiques. Cette accélération se compose exponentiellement sur plusieurs blocs, permettant au mineur quantique de dominer la création de blocs et de contrôler l'ordonnancement des transactions, centralisant effectivement ce qui devrait être un mécanisme de consensus distribué et permettant la censure ou la manipulation rétrospective des transactions.",
    "solution": "C"
  },
  {
    "id": 7,
    "question": "Dans le contexte des implémentations pratiques d'apprentissage automatique quantique, les chercheurs ont exploré diverses approches pour rendre les Machines à Vecteurs de Support Quantiques (QSVM) viables sur des dispositifs à court terme malgré d'importantes limitations matérielles. Considérez un scénario où vous implémentez un QSVM sur un processeur supraconducteur de 50 qubits avec des temps T1 d'environ 100 microsecondes et des fidélités de portes à deux qubits de 99%. Qu'est-ce qui est essentiel pour que les QSVM réduisent le bruit et les erreurs de calcul dans ces contraintes réalistes ?",
    "A": "En maximisant l'intrication multipartite à travers les 50 qubits par application agressive de chaînes CNOT et de portes à phase contrôlée, l'état quantique devient de plus en plus robuste à la décohérence locale grâce à la nature distribuée de l'encodage de l'information quantique, ce qui permet aux erreurs sur les qubits individuels d'être diluées à travers le système intriqué plutôt que de corrompre des points de données spécifiques, fournissant ainsi une forme inhérente de redondance qui stabilise le calcul du noyau sans codes de correction d'erreur explicites.",
    "B": "Des pipelines de pré-traitement classiques peuvent être conçus pour identifier et filtrer les échantillons d'entraînement qui nécessiteraient des circuits quantiques profonds dépassant la fenêtre de cohérence, sélectionnant efficacement un ensemble de données résistant au bruit dont les entrées de la matrice de noyau peuvent être estimées avec des circuits peu profonds.",
    "C": "Opérer avec un budget de qubits restreint réduit considérablement les taux d'erreur cumulatifs en raccourcissant la largeur du circuit.",
    "D": "Des techniques robustes de correction d'erreurs et des architectures quantiques évolutives capables de gérer l'accumulation d'erreurs à travers plusieurs couches de portes tout en maintenant une profondeur de circuit suffisante pour une évaluation significative du noyau, combinées à des stratégies d'atténuation d'erreurs comme l'extrapolation à bruit zéro et l'annulation d'erreurs probabiliste qui compensent les portes imparfaites sans la surcharge complète des codes tolérants aux fautes.",
    "solution": "D"
  },
  {
    "id": 8,
    "question": "Quelle est la relation entre la capacité d'intrication d'un circuit quantique et son expressivité ?",
    "A": "Inversement proportionnelles de sorte qu'augmenter la capacité d'intrication d'un circuit quantique diminue nécessairement son expressivité, car les états hautement intriqués forment un sous-ensemble de mesure nulle de l'espace de Hilbert total et les circuits optimisés pour générer une intrication maximale deviennent spécialisés vers ces états atypiques.",
    "B": "La capacité d'intrication détermine la borne supérieure de l'expressivité dans le sens où un circuit ne peut jamais atteindre des valeurs d'expressivité dépassant sa capacité normalisée de génération d'intrication, puisque les états qui ne sont pas suffisamment intriqués n'occupent qu'un sous-espace limité de l'espace de Hilbert total.",
    "C": "Ce sont des propriétés non liées car la capacité d'intrication mesure uniquement les corrélations bipartites entre sous-systèmes tandis que l'expressivité quantifie à quel point un circuit paramétré peut échantillonner uniformément l'espace d'états complet.",
    "D": "Les circuits avec une capacité d'intrication plus élevée tendent à avoir une expressivité plus élevée, car la capacité à générer des états intriqués à travers plusieurs qubits permet au circuit d'accéder à une distribution plus large et plus uniforme sur l'espace de Hilbert, ce qui est directement corrélé à la capacité du circuit à représenter divers états quantiques nécessaires pour les algorithmes variationnels et les tâches d'apprentissage automatique quantique.",
    "solution": "D"
  },
  {
    "id": 9,
    "question": "Pourquoi les hamiltoniens effectifs non-hermitiens sont-ils utiles pour modéliser les systèmes quantiques ouverts ?",
    "A": "Les hamiltoniens non-hermitiens permettent de représenter la dynamique de systèmes fermés avec des conditions aux limites dépendantes du temps grâce à des valeurs propres d'énergie complexes dont les composantes imaginaires encodent le taux de flux d'information quantique à travers les frontières du système. Ce formalisme est particulièrement utile lors de la modélisation de systèmes couplés à des réservoirs markoviens car la composante anti-hermitienne capture la dissipation tout en préservant la structure symplectique requise pour générer l'évolution temporelle physique via des équations de Schrödinger modifiées qui tiennent compte du couplage environnemental.",
    "B": "Les termes d'énergie imaginaires encodent les processus de décroissance et de dissipation — cela permet d'utiliser les équations d'évolution de Schrödinger standard tout en capturant naturellement la dynamique non-unitaire des systèmes couplés à des environnements externes, fournissant un cadre computationnellement efficace pour la modélisation phénoménologique de la décohérence.",
    "C": "Le formalisme non-hermitien fournit une description naturelle de la dynamique conditionnelle où l'évolution du système dépend de résultats de mesure nuls — aucun saut quantique détecté. Les valeurs propres complexes encodent à la fois l'évolution cohérente et les canaux de décroissance, les parties imaginaires représentant les taux de perte qui renormalisent la norme restante du système. Cette approche devient exacte dans la limite de mesure continue où les opérateurs de saut sont échelonnés de manière appropriée, la rendant particulièrement utile pour modéliser des systèmes sous surveillance faible continue.",
    "D": "Les hamiltoniens effectifs non-hermitiens capturent la dynamique de post-sélection où certains résultats de mesure sont rejetés, la composante anti-hermitienne encodant le courant de probabilité pour les trajectoires qui ne sont pas post-sélectionnées. Le spectre complexe permet un calcul efficace de l'évolution conditionnelle pour les systèmes présentant des effets Zénon quantiques, où des mesures projectives fréquentes altèrent les taux de décroissance effectifs. Ce formalisme préserve la structure analytique de l'évolution de Schrödinger tout en incorporant l'effondrement non-unitaire associé au conditionnement de mesure via des contributions d'énergie imaginaires.",
    "solution": "B"
  },
  {
    "id": 10,
    "question": "Lors de l'exécution d'un CNOT logique par chirurgie de réseau entre des patchs de code de surface planaire, quel compromis de ressources influence le plus la latence globale du circuit ?",
    "A": "Pendant la phase de fusion de la chirurgie de réseau, les vacances (qubits physiques manquants) à l'intérieur du volume de l'un ou l'autre patch créent des régions où les mesures de stabilisateurs ne peuvent être effectuées, réduisant localement la distance de code effective du patch fusionné. Si la densité de vacances dans le volume dépasse un seuil critique — typiquement autour de 5-10% selon la performance du décodeur — le patch fusionné ne peut maintenir son taux d'erreur logique nominal car les erreurs peuvent proliférer à travers les régions adjacentes aux vacances plus rapidement que le décodeur ne peut les corriger.",
    "B": "Dans les architectures de code de surface multi-patchs, chaque qubit physique servant d'ancille pour l'extraction de syndrome fonctionne à une fréquence de résonance spécifique pour permettre l'adressage sélectif. Lors de l'exécution d'une chirurgie de réseau entre patchs, tous les qubits ancilles le long de la frontière partagée doivent rester désaccordés de leurs fréquences d'inactivité pendant l'opération de fusion pour empêcher la participation non désirée aux mesures de syndrome des patchs voisins.",
    "C": "Les patchs de code de surface sont définis par des plaquettes alternées de mesures de stabilisateurs de type X et Z, la couleur de la frontière (X ou Z) déterminant quel opérateur de Pauli logique a une représentation de faible poids le long de ce bord. Lors de la fusion de patchs pour la chirurgie de réseau, choisir la mauvaise couleur de frontière pour l'opération logique prévue signifie que la chirurgie ne peut pas directement implémenter la porte désirée, nécessitant plutôt l'injection d'un état magique suivi d'une téléportation de porte.",
    "D": "La largeur de la frontière partagée entre les patchs fusionnés par rapport au nombre de cycles séquentiels de fusion-séparation requis pour compléter l'opération logique — des frontières plus larges permettent une stabilisation plus rapide du syndrome après fusion mais consomment plus de qubits physiques et augmentent le temps d'inactivité pour les patchs non impliqués, tandis que des frontières plus étroites réduisent la surcharge en qubits mais prolongent la durée de chaque cycle de chirurgie en raison de temps de convergence de correction d'erreurs plus longs, forçant un compromis direct entre ressources spatiales et coût d'exécution temporel qui domine la latence totale du circuit.",
    "solution": "D"
  },
  {
    "id": 11,
    "question": "Supposons qu'un état quantique |φ⟩ soit téléporté d'un qubit de données dans le processeur A vers un qubit de communication dans le processeur B en utilisant TeleData. L'état réside maintenant entièrement dans le processeur B, ce qui signifie que le processeur A ne détient plus aucune information quantique sur |φ⟩—le qubit de données original a été mesuré et s'est effondré. Si vous souhaitez effectuer d'autres opérations impliquant des qubits de données dans le processeur A qui dépendent de l'état |φ⟩, vous faites face à une contrainte fondamentale : l'information quantique ne peut pas être copiée (théorème de non-clonage), et elle se trouve maintenant dans un processeur différent. Laquelle des affirmations suivantes est nécessairement vraie pour permettre des opérations futures impliquant des qubits de données dans le processeur A qui nécessitent un accès à |φ⟩?",
    "A": "L'état doit être téléporté de nouveau vers le processeur A en utilisant un nouveau cycle de téléportation, ce qui nécessite d'établir une nouvelle intrication entre les processeurs et d'effectuer une autre mesure de Bell suivie d'unitaires correctifs. Ce transfert inverse déplace physiquement l'information quantique là où elle est nécessaire pour les calculs suivants.",
    "B": "L'état doit être téléporté de nouveau vers le processeur A en utilisant le même canal quantique, ce qui nécessite d'inverser les résultats de mesure du protocole original et d'appliquer les corrections de Pauli inverses dans l'ordre opposé. Ce transfert rétrograde reconstruit l'information quantique à son emplacement original en exploitant la symétrie de renversement temporel du protocole de téléportation et en réutilisant la structure de corrélation établie par la paire de Bell originale avant qu'elle ne soit consommée par la mesure.",
    "C": "L'état doit être téléporté de nouveau vers le processeur A en utilisant uniquement la communication classique, en transmettant les deux bits classiques de la mesure de Bell originale ainsi qu'un troisième bit de syndrome qui encode les coordonnées de la sphère de Bloch. Ce transfert théorique de l'information permet au processeur A de reconstruire |φ⟩ par des unitaires locaux guidés par les données classiques reçues, contournant le besoin d'une nouvelle intrication tout en respectant le théorème de non-clonage grâce à l'irréversibilité du processus de mesure original.",
    "D": "L'état doit être téléporté de nouveau vers le processeur A en utilisant l'échange d'intrication sur la paire de Bell originale, ce qui convertit l'intrication consommée en une nouvelle ressource reliant le qubit de communication du processeur B au qubit de données du processeur A. Ce protocole bidirectionnel exploite la corrélation quantique résiduelle préservée dans l'enregistrement de mesure, permettant à l'état d'être reconstruit à l'emplacement original par des opérations à choix retardé conditionnées par les résultats classiques des deux processeurs sans nécessiter un deuxième cycle de distribution d'intrication.",
    "solution": "A"
  },
  {
    "id": 12,
    "question": "Dans les circuits quantiques, la porte CNOT est également désignée par lequel des noms suivants?",
    "A": "La nomenclature iSWAP est parfois utilisée car CNOT peut être décomposée en portes iSWAP combinées avec des rotations de qubit unique, et dans les architectures supraconductrices où les portes natives à deux qubits implémentent des interactions iSWAP, les praticiens se réfèrent souvent à l'opération contrôlée synthétisée comme iSWAP. Cette équivalence à des unitaires locaux près rend les termes fonctionnellement interchangeables dans les contextes d'optimisation de circuits.",
    "B": "La désignation porte XOR, héritée de l'informatique classique où CNOT implémente l'opération XOR logique sur le qubit cible contrôlé par le qubit source. Cette correspondance classique-quantique fait de XOR la terminologie naturelle lors de la description de l'action de CNOT sur les états de base computationnelle, et le nom reste répandu dans les frameworks de programmation quantique qui mettent l'accent sur les implémentations de logique classique réversible.",
    "C": "L'étiquette CP (phase contrôlée) est standard car les portes CNOT et CZ sont équivalentes à une rotation de base près—l'application de Hadamards avant et après CZ produit CNOT—et puisque CZ applique un basculement de phase plutôt qu'un basculement de bit, la terminologie généralisée CP capture les deux opérations comme portes de Pauli contrôlées. De nombreux frameworks quantiques traitent CNOT et CP comme synonymes étant donné leur équivalence unitaire locale.",
    "D": "CX, qui signifie opération X contrôlée, puisque la porte CNOT applique une opération Pauli-X (basculement de bit) au qubit cible lorsque le qubit de contrôle est dans l'état |1⟩, faisant de CX un raccourci naturel et largement adopté dans les diagrammes de circuits quantiques et les frameworks de programmation.",
    "solution": "D"
  },
  {
    "id": 13,
    "question": "Lors de l'utilisation de la marche quantique pour détecter si un groupe est commutatif, l'algorithme exploite la dynamique de marche sur le graphe de Cayley pour explorer efficacement la structure du groupe. Dans les implémentations pratiques sur des dispositifs à court terme, la surcharge de ressources limite souvent la taille de groupe accessible. Quelle tâche computationnelle plus petite l'algorithme résout-il réellement comme sous-routine principale avant de conclure quoi que ce soit sur la commutativité globale?",
    "A": "L'algorithme recherche une seule paire de générateurs non commutatifs (g,h) satisfaisant gh ≠ hg par marche quantique sur le graphe de Cayley, mais de manière critique, il doit vérifier que cette paire génère un sous-groupe non abélien ⟨g,h⟩ d'ordre au moins 6, car détecter la simple non-commutativité gh ≠ hg seule est insuffisant—la paire pourrait satisfaire (gh)² = (hg)² ou des relations de commutation d'ordre supérieur qui restaurent une commutativité effective dans la structure quotient, donc l'algorithme résout en réalité le problème de certification de non-abélianité du sous-groupe nécessitant la vérification de |⟨g,h⟩/Z(⟨g,h⟩)| > 1.",
    "B": "L'algorithme recherche parmi les paires de générateurs de groupe en utilisant une marche quantique sur le graphe de Cayley pour détecter toute instance unique où deux générateurs ne commutent pas, c'est-à-dire pour trouver une paire de générateurs non commutatifs (g,h) telle que gh ≠ hg, ce qui certifie immédiatement que le groupe entier est non abélien sans nécessiter une énumération exhaustive de tous les éléments du groupe ou le calcul de la structure complète des commutateurs, puisque l'existence d'une seule telle paire suffit à prouver la non-commutativité et la marche quantique fournit une accélération quadratique par rapport à l'échantillonnage aléatoire classique lors de la localisation de ce témoin parmi les O(|S|²) paires possibles de générateurs dans un groupe avec ensemble générateur S.",
    "C": "L'algorithme exécute une marche quantique qui échantillonne des éléments de groupe aléatoires g,h en composant des générateurs et évalue le commutateur [g,h] = ghg⁻¹h⁻¹, cherchant à détecter si [g,h] = e pour toutes les paires échantillonnées, mais la sous-routine principale qu'il résout réellement est le problème d'égalité de groupe : étant donné deux éléments de groupe représentés comme produits de générateurs, déterminer s'ils représentent le même élément—cela nécessite de résoudre le problème du mot dans la présentation du groupe, et l'algorithme obtient un avantage en utilisant l'amplification d'amplitude quantique pour détecter toute instance où [g,h] ≠ e parmi les paires échantillonnées aléatoirement.",
    "D": "L'algorithme effectue une estimation de phase quantique sur la représentation unitaire de l'opérateur de marche quantique sur le graphe de Cayley pour extraire le spectre des valeurs propres, qui encode la commutativité par des motifs de dégénérescence spectrale : les groupes abéliens présentent des phases propres uniformément distribuées e^(2πik/|G|) tandis que les groupes non abéliens montrent un regroupement déterminé par la table des caractères, donc la sous-routine principale résout le problème d'estimation du gap spectral, mesurant si l'espacement minimal des valeurs propres dépasse 2π/|G|² ce qui indiquerait l'effondrement du théorème de distribution de phase abélienne pour les marches quantiques.",
    "solution": "B"
  },
  {
    "id": 14,
    "question": "Dans le contexte du calcul quantique photonique expérimental, considérons une configuration généralisée d'échantillonnage bosonique où des photons thermiques sont introduits dans les modes d'entrée d'un interféromètre optique linéaire. La dureté computationnelle de l'échantillonnage à partir de la distribution de sortie est connue pour présenter une transition de phase lorsque la température environnementale augmente. La dureté de l'échantillonnage bosonique généralisé avec des états thermiques montre une transition à une température critique parce que :",
    "A": "Au-dessus de cette température, les photons se comportent davantage comme des particules discernables—l'occupation thermique brouille les motifs d'interférence bosonique qui rendent le problème classiquement difficile, détruisant essentiellement les corrélations quantiques nécessaires à la complexité computationnelle. Le paramètre de discernabilité augmente avec la température jusqu'à ce que le permanent perde ses propriétés d'anti-concentration.",
    "B": "Les statistiques de photons thermiques passent de distributions sous-poissoniennes à super-poissoniennes, amenant la fonction permanent à échantillonner à partir d'une classe de complexité computationnelle différente. En dessous de la température critique, les amplitudes d'état de Fock restent approximativement distribuées selon une gaussienne, préservant la dureté #P, mais les excitations thermiques au-dessus de kT≈ℏω déplacent la distribution vers un échantillonnage aléatoire de Haar classique qui admet des algorithmes d'approximation efficaces en temps polynomial.",
    "C": "La visibilité d'interférence Hong-Ou-Mandel subit une transition de percolation à la température critique, où le déphasage thermique fait dépasser au taux de coïncidence à deux photons le seuil classique de 50%. Au-dessus de ce point, la probabilité de regroupement bosonique devient distinguable de l'anti-regroupement fermionique, permettant une simulation classique via des déterminants signés plutôt que des permanents, effondrant ainsi la complexité computationnelle de #P-complet à temps polynomial.",
    "D": "L'occupation thermique induit des canaux effectifs de perte de photons qui augmentent linéairement avec la température, et lorsque le coefficient de transmission η(T) tombe en dessous d'une valeur critique ηc≈0,73, la distribution de sortie peut être échantillonnée classiquement de manière efficace en utilisant des algorithmes Metropolis-Hastings sur la fonction Torontonienne. Cette frontière de phase sépare le régime où les algorithmes classiques de falsification en temps polynomial échouent de celui où ils réussissent avec une forte probabilité, reliant directement la thermalisation à l'effondrement de l'avantage computationnel quantique.",
    "solution": "A"
  },
  {
    "id": 15,
    "question": "Quelle approche montre le plus de promesses pour résoudre le problème du sous-groupe caché non abélien?",
    "A": "La stratégie la plus prometteuse implique de décomposer le problème du sous-groupe caché non abélien en une séquence hiérarchique de sous-problèmes abéliens en exploitant la structure des séries de composition en théorie des groupes. Cette réduction exploite le fait que tout groupe fini admet une chaîne de sous-groupes normaux où chaque quotient est abélien, permettant aux techniques standard d'échantillonnage de Fourier de résoudre chaque couche indépendamment. L'algorithme quantique procède en identifiant d'abord les classes latérales par rapport au sous-groupe normal abélien maximal, puis en appliquant récursivement des solveurs HSP abéliens aux groupes quotients jusqu'à ce que le sous-groupe caché complet soit reconstruit par composition algébrique des solutions partielles.",
    "B": "Remplacer le cadre traditionnel d'échantillonnage de Fourier quantique par des algorithmes de marche quantique qui explorent la structure du graphe de Cayley du groupe fournit une accélération exponentielle pour les problèmes de sous-groupe caché non abélien. Ces marches quantiques atteignent des temps de mélange qui évoluent avec le diamètre du groupe plutôt qu'avec ses propriétés de théorie des représentations, contournant efficacement le problème de mesure qui afflige les approches par états de classes latérales. En encodant le sous-groupe caché comme conditions aux limites sur la marche et en utilisant l'estimation de phase pour détecter les périodicités dans la dynamique de marche, cette méthode peut identifier les sous-groupes non abéliens sans nécessiter de mesures intriquées sur plusieurs sorties de transformée de Fourier quantique.",
    "C": "La mesure assez bonne (pretty good measurement) sur plusieurs copies de l'état de classe latérale, qui extrait l'information du sous-groupe par des mesures collectives.",
    "D": "La technique fonctionne en effectuant d'abord des transformées de Fourier quantiques standard pour créer des états de classes latérales, puis en appliquant des protocoles d'amplification d'amplitude soigneusement conçus qui améliorent les signaux de mesure faibles correspondant à la structure du sous-groupe. Cette stratégie d'amplification est nécessaire car les représentations non abéliennes répartissent l'information du sous-groupe caché sur des composantes irréductibles de haute dimension où elle n'apparaît que sous forme de corrélations subtiles.",
    "solution": "C"
  },
  {
    "id": 16,
    "question": "Quelle stratégie quantique peut être utilisée pour récupérer la disponibilité des qubits après l'extraction de caractéristiques ?",
    "A": "Exécuter l'unitaire inversé temporellement pour désintriqquer les qubits de caractéristiques du reste du système, les restaurant à un état produit séparable qui peut être réinitialisé en toute sécurité sans perturber les autres qubits. En appliquant l'inverse des portes d'extraction de caractéristiques, on inverse de manière cohérente le processus de génération d'intrication, décalculant effectivement l'encodage des caractéristiques et ramenant ces qubits à leur état initial pour une réutilisation dans les couches de circuit suivantes.",
    "B": "Mettre en œuvre un transfert d'état quantique en utilisant des portes iSWAP ou √SWAP pour déplacer de manière cohérente les amplitudes des qubits de caractéristiques vers un registre auxiliaire de qubits ancilla frais initialisés à |0⟩, laissant les qubits de caractéristiques originaux dans un état maximalement mélangé. Après extraction des résultats de mesure classiques des états transférés dans le registre ancilla, les qubits de caractéristiques maintenant décorrélés peuvent être réinitialisés et réutilisés. Cette stratégie préserve la cohérence quantique pendant le transfert tout en rendant les qubits originaux disponibles sans nécessiter de décalcul.",
    "C": "Appliquer une rétroaction basée sur la mesure : mesurer les qubits de caractéristiques dans la base de calcul pour extraire des résultats de chaînes de bits classiques, puis conditionner les rotations à un qubit suivantes sur ces résultats de mesure pour préparer les qubits non mesurés restants dans un état qui factorise l'information des caractéristiques. Ce protocole de réinitialisation conditionnelle utilise les données de mesure classiques comme table de consultation pour des unitaires correctifs qui tracent effectivement le sous-système mesuré hors de l'état quantique joint, restaurant la séparabilité sans appliquer de portes inverses et permettant la réutilisation des qubits dans les étapes de circuit ultérieures.",
    "D": "Utiliser des protocoles de mesure différée pour reporter l'effondrement des fonctions d'onde des qubits de caractéristiques jusqu'à la couche de circuit finale, maintenant une cohérence quantique complète en appliquant conditionnellement des opérations sur les qubits en aval qui dépendent des états des qubits de caractéristiques via des portes contrôlées. En traitant les observables d'extraction de caractéristiques comme des mesures virtuelles encodées dans l'intrication plutôt que comme des effondrements projectifs, on préserve la superposition des qubits tout au long du circuit tout en extrayant l'information des caractéristiques par des mesures conjointes finales, réutilisant effectivement les qubits sans les réinitialiser physiquement aux étapes intermédiaires.",
    "solution": "A"
  },
  {
    "id": 17,
    "question": "L'échantillonnage de circuits aléatoires diffère de l'échantillonnage bosonique principalement en ce que l'échantillonnage de circuits aléatoires :",
    "A": "Emploie des mesures adaptatives où les bases de mesure ultérieures dépendent des résultats antérieurs, utilisant un calcul classique en anticipation pour diriger l'évolution quantique, tandis que l'échantillonnage bosonique fixe tous les opérateurs de mesure dans la base du nombre de photons avant la préparation de l'état. Cette adaptivité permet à l'échantillonnage de circuits aléatoires de vérifier l'échantillonnage correct de la distribution par benchmarking d'entropie croisée contre la simulation classique de circuits peu profonds.",
    "B": "Utilise des portes de qubits discrètes appliquées en séquences stratifiées plutôt que des transformations optiques linéaires continues agissant sur des modes photoniques, en faisant fondamentalement un modèle computationnel basé sur des portes où l'évolution unitaire procède par des opérations séquentielles à deux qubits au lieu de réseaux passifs de séparateurs de faisceau qui implémentent des matrices de diffusion fixes.",
    "C": "Génère des distributions de sortie en mesurant des états stabilisateurs après application de portes de Clifford aléatoires suivies d'une couche finale non-Clifford, tandis que l'échantillonnage bosonique mesure des états de Fock après évolution optique linéaire d'entrées à photon unique. La difficulté de l'échantillonnage de circuits aléatoires repose sur la propriété d'anticoncentration des distributions de sortie, qui découle des statistiques de Porter-Thomas des unitaires aléatoires de Haar appliqués aux états de la base de calcul.",
    "D": "Exploite la difficulté computationnelle de l'échantillonnage de la distribution de sortie de circuits classiques réversibles augmentés de portes de phase aléatoires à un qubit, où chaque couche applique un unitaire diagonal uniformément aléatoire à chaque qubit avant qu'une couche de permutation fixe mélange les états de la base de calcul. La difficulté dérive de la #P-complétude du calcul des amplitudes dans ces réseaux phase-permutation, tandis que la difficulté de l'échantillonnage bosonique découle du calcul des permanents de sous-matrices tirées de la matrice de diffusion complète.",
    "solution": "B"
  },
  {
    "id": 18,
    "question": "Dans le contexte des algorithmes d'apprentissage automatique quantique qui revendiquent des accélérations exponentielles, quelle propriété fondamentale de la mécanique quantique est le plus souvent citée comme source principale d'avantage computationnel, et sous quelles conditions spécifiques cet avantage se manifeste-t-il en pratique ? Considérez à la fois les modèles théoriques et les limitations expérimentales actuelles lors de la formulation de votre réponse.",
    "A": "La puissance computationnelle du parallélisme quantique émerge de la capacité à préparer une superposition uniforme sur tous les 2^n états de base en utilisant seulement n portes Hadamard, créant effectivement un ensemble exponentiellement grand d'entrées en temps polynomial. Ce parallélisme devient pratiquement utile lorsque le problème présente une structure globale qui peut être exploitée par interférence, comme dans la recherche de Grover où l'interférence destructive amplifie l'état cible. Cependant, dans les contextes d'apprentissage automatique, extraire des informations utiles sur toutes les évaluations parallèles simultanément reste un défi fondamental, puisque la mesure effondre la superposition en un seul résultat, nécessitant soit des schémas d'amplification d'amplitude soigneux, soit d'accepter que nous ne pouvons accéder qu'à des propriétés agrégées plutôt qu'à des résultats individuels.",
    "B": "L'intrication multipartite génère des structures de corrélation exponentiellement complexes qui permettent aux systèmes quantiques d'encoder des dépendances entre variables de manières qui résistent à la factorisation classique ou à la décomposition en réseaux de tenseurs. Lorsque les données d'entraînement ou l'architecture du modèle présentent naturellement ces corrélations intriquées—comme dans les simulations de chimie quantique ou de physique à plusieurs corps—le système quantique peut représenter et manipuler ces relations avec des ressources polynomiales tandis que les approches classiques nécessiteraient une mémoire exponentielle. La contrainte critique est que le matériel quantique actuel souffre de décroissance de l'intrication via des canaux de décohérence, avec des temps de cohérence typiques limitant à des circuits de profondeur de 100-1000 portes avant que la qualité de l'intrication ne se dégrade en dessous de seuils utiles.",
    "C": "En encodant l'information comme amplitudes de probabilité dans un vecteur d'état quantique de dimension 2^n, les systèmes quantiques réalisent une expansion exponentielle de la capacité représentationnelle comparée aux n bits classiques ou qubits physiquement présents. Cette densité représentationnelle permet aux réseaux neuronaux quantiques de modéliser théoriquement des fonctions avec des espaces de paramètres exponentiellement grands en utilisant seulement un nombre polynomial de qubits physiques. La difficulté fondamentale est que bien que le vecteur d'état contienne exponentiellement d'amplitudes, extraire une amplitude spécifique nécessite soit une tomographie complète (exponentiellement de mesures) soit des techniques d'interférence spécialisées qui fonctionnent seulement pour des requêtes structurées, ce qui signifie que la représentation exponentielle ne se traduit pas en avantage computationnel exponentiel à moins que le problème ne permette l'interférence constructive et les statistiques de mesure globales.",
    "D": "Tout ce qui précède contribue de manière synergique, puisque la superposition permet l'exploration parallèle, l'intrication capture des corrélations complexes, et la représentation d'espace d'états exponentielle fournit le substrat computationnel sous-jacent. L'avantage pratique dépend de la structure du problème et de la qualité du matériel.",
    "solution": "D"
  },
  {
    "id": 19,
    "question": "Quelle est la limitation principale des implémentations quantiques du clustering k-means ?",
    "A": "Extraire les centres de clusters de l'état quantique nécessite d'effectuer une tomographie sur un espace de Hilbert exponentiellement grand, qui évolue en O(2^n) mesures pour n qubits. Bien que les centroïdes puissent être encodés efficacement comme amplitudes quantiques, reconstruire leurs coordonnées classiques nécessite soit une tomographie d'état complète soit des protocoles de tomographie par ombre, qui introduisent tous deux une surcharge de mesure pouvant dominer le temps d'exécution et potentiellement éliminer tout avantage quantique gagné pendant la phase de calcul de distance.",
    "B": "L'architecture de circuit quantique pour calculer les distances euclidiennes entre les points de données et les centroïdes nécessite fondamentalement des opérations contrôlées dont la profondeur évolue polynomialement avec la dimension des caractéristiques, créant des opportunités significatives de décohérence sur les dispositifs NISQ. De plus, implémenter le test de swap ou d'autres techniques d'estimation de distance exige des qubits ancilla et des calibrations de portes précises, rendant l'oracle de distance quantique substantiellement plus gourmand en ressources que le calcul classique O(nd) par itération, où n est le nombre de points et d est la dimensionnalité.",
    "C": "Tout ce qui précède",
    "D": "Les calculs de distance dans les circuits quantiques sont contraints par la nécessité d'encoder les vecteurs de caractéristiques classiques en amplitudes quantiques via l'encodage d'amplitude, qui nécessite lui-même O(d) opérations par point de données où d est la dimension des caractéristiques. De plus, calculer toutes les distances par paires simultanément nécessiterait un nombre de qubits évoluant linéairement avec à la fois la taille de l'ensemble de données et l'espace des caractéristiques, rendant la profondeur du circuit quantique prohibitivement grande même pour des ensembles de données de taille modérée, annulant ainsi l'accélération théorique du parallélisme quantique.",
    "solution": "C"
  },
  {
    "id": 20,
    "question": "Que se passe-t-il dans l'algorithme de Shor si la période trouvée est impaire ?",
    "A": "Les implémentations supraconductrices modernes incorporent des boucles de rétroaction adaptatives où le processeur quantique surveille la parité de la période mesurée en temps réel ; si r mod 2 = 1 est détecté pendant la lecture de la transformée de Fourier quantique inverse, le système de contrôle réinitialise immédiatement le registre ancilla et sélectionne une nouvelle base aléatoire a' sans retourner le contrôle à l'hôte classique.",
    "B": "L'algorithme procède en calculant pgcd(a^(r/2) ± 1, N) en utilisant l'exposant fractionnaire r/2, ce qui donne un facteur non trivial dans environ la moitié des cas parce que la période impaire satisfait toujours le critère d'Euler pour les résidus quadratiques modulo N. Cette approche exploite l'expansion en fractions continues de la phase mesurée pour interpoler entre les puissances entières, récupérant effectivement les facteurs même lorsque le post-traitement classique rejetterait autrement le résultat, bien qu'au prix de taux d'erreur plus élevés en pratique.",
    "C": "Une période impaire r signale que N doit être exprimable comme b^k pour une certaine base entière b et un exposant k ≥ 2, parce que l'ordre de tout élément dans le groupe multiplicatif Z*_N divise φ(N), et φ(b^k) est toujours pair sauf si k=1 et b=2. L'algorithme de Shor détecte cette structure dans l'étape de prétraitement classique initiale en vérifiant si N est une puissance parfaite avant d'invoquer la sous-routine quantique, donc rencontrer une période impaire pendant la phase quantique indique une incohérence logique qui termine toute la tentative de factorisation plutôt que de simplement redémarrer avec une nouvelle base.",
    "D": "Cette exécution particulière de la sous-routine quantique est infructueuse, et la logique de contrôle classique sélectionne une nouvelle base aléatoire a' copremière avec N avant de redémarrer toute la procédure de recherche de période, parce qu'une période impaire ne peut pas être utilisée pour calculer les facteurs via la formule pgcd(a^(r/2) ± 1, N) sans rencontrer des exposants non entiers.",
    "solution": "D"
  },
  {
    "id": 21,
    "question": "Quel principe fondamental rend la correction d'erreurs quantiques plus difficile que la correction d'erreurs classiques ?",
    "A": "Alors que les mesures classiques détruisent inévitablement les superpositions et provoquent l'effondrement des états quantiques, le processus de mesure dans les systèmes quantiques peut en réalité renforcer l'intrication entre le qubit mesuré et l'appareil de mesure par le biais de la rétroaction de l'observation. Cette amplification des corrélations signifie que lors de l'exécution des mesures de syndrome dans les codes de correction d'erreurs quantiques, chaque événement de mesure augmente l'entropie d'intrication entre le bloc de code et les registres ancilla, construisant progressivement des corrélations quantiques qui doivent être soigneusement gérées — sinon, ces structures intriquées croissantes introduisent des erreurs corrélées qui se propagent à travers les cycles de correction ultérieurs, rendant le protocole de correction d'erreurs plus fragile que les schémas classiques où les mesures extraient simplement l'information sans modifier les structures de corrélation.",
    "B": "Les ressources de calcul nécessaires pour simuler classiquement les processus d'erreur quantiques évoluent exponentiellement avec le nombre de qubits dans le système, ce qui crée un goulot d'étranglement fondamental lors de la conception et de la vérification des codes de correction d'erreurs quantiques. Pour un système avec n qubits, la matrice densité contient 2^(2n) entrées, ce qui signifie que même tester si un code de correction d'erreurs proposé fonctionne correctement pour des systèmes de 50 qubits nécessiterait de suivre environ 10^30 nombres complexes.",
    "C": "Le théorème de non-clonage empêche la copie de qubits pour des vérifications par redondance, rendant impossible la vérification de l'information quantique par simple duplication et comparaison comme dans les codes de répétition classiques",
    "D": "L'information quantique réside fondamentalement dans des états propres discrets correspondant à des quantités observables, chaque qubit existant soit dans l'état de base computationnel |0⟩ soit |1⟩ à tout instant donné. Cette nature discrète signifie que les erreurs ne peuvent que basculer les qubits entre ces configurations classiques bien définies, similaires aux erreurs de basculement de bits dans les systèmes classiques, mais la correction d'erreurs quantiques doit en plus gérer le fait que la mesure force cet effondrement discret depuis n'importe quelle superposition — ainsi, le défi ne provient pas d'erreurs continues, mais de la gestion des résultats de mesure discrets tout en empêchant le processus de détection lui-même de projeter par inadvertance le qubit logique encodé dans un état propre incorrect des stabilisateurs du code.",
    "solution": "C"
  },
  {
    "id": 22,
    "question": "Dans les systèmes de distribution quantique de clés basés sur l'intrication, il existe une vulnérabilité fondamentale liée à la source quantique elle-même qui peut être exploitée sans mesurer directement les photons transmis. Cette faille de sécurité apparaît lorsqu'un adversaire peut subtilement manipuler ou distinguer entre différentes émissions de la source d'une manière qui révèle des informations partielles sur la clé secrète. Considérez un scénario où la source de paires de photons intriqués ne produit pas des états quantiques parfaitement identiques pour chaque événement d'émission, permettant à un espion d'obtenir des connaissances sur les bases ou résultats de mesure. Quel est le principe fondamental derrière cette classe d'attaques ?",
    "A": "La vulnérabilité se concentre sur la manipulation sélective de l'efficacité du détecteur d'annonce dans les sources de conversion paramétrique descendante spontanée, où un espion peut ajuster dynamiquement les seuils de détection pour annoncer préférentiellement certains états de paires de photons plutôt que d'autres en fonction de leurs caractéristiques de polarisation ou de timing. En biaisant quelles émissions sont annoncées et donc utilisées pour la génération de clé, l'adversaire crée une distribution non uniforme sur les résultats de mesure des parties légitimes sans introduire d'erreurs détectables.",
    "B": "Méthodes d'interception par échange d'intrication où l'adversaire effectue des mesures d'état de Bell sur les photons interceptés et crée de nouvelles paires intriquées à transmettre aux parties légitimes, maintenant les statistiques de corrélation tout en extrayant l'information de clé à travers les résultats de mesure obtenus pendant le processus d'échange. En choisissant soigneusement quand effectuer l'opération d'échange en fonction des annonces sur le canal public, l'espion peut obtenir sélectivement des informations sur les bits de clé.",
    "C": "La stratégie d'attaque implique de rediriger un photon de chaque paire intriquée à travers un appareil de mesure de Bell contrôlé avant qu'il n'atteigne le récepteur légitime, puis d'utiliser le résultat de mesure pour déterminer quel état de base computationnel préparer et transmettre au destinataire prévu. Cette technique de redirection de mesure de Bell permet à l'adversaire d'effondrer l'intrication d'une manière qui semble statistiquement cohérente avec une transmission directe.",
    "D": "L'attaque exploite les variations dans les caractéristiques d'émission de la source quantique qui permettent à un espion de distinguer entre différentes émissions de paires intriquées, obtenant ainsi des informations sur la clé sans effectuer de mesures qui perturberaient les états quantiques de manière détectable. Ceci est particulièrement dangereux car les preuves de sécurité standard supposent des émissions de source identiques et indépendamment distribuées.",
    "solution": "D"
  },
  {
    "id": 23,
    "question": "En informatique quantique distribuée, supposons que vous avez trois nœuds A, B et C disposés en ligne, où A et B partagent une paire intriquée, et B et C partagent une paire intriquée séparée. Vous voulez que A et C partagent directement une intrication, mais ils n'ont aucun canal quantique physique les reliant. Quel est le rôle principal de l'échange d'intrication dans ce scénario, et quel principe fondamental permet son fonctionnement malgré l'absence d'interaction directe entre les nœuds distants ?",
    "A": "Le nœud B effectue une mesure de Bell sur ses deux qubits provenant des paires A-B et B-C, projetant A et C dans un état intriqué sans interaction directe. Cela exploite les corrélations quantiques et l'effondrement induit par la mesure.",
    "B": "L'échange d'intrication transfère des corrélations quantiques entre des nœuds non adjacents en faisant effectuer par B des mesures de parité qui comparent les phases entre ses deux qubits des paires séparées, conditionnant A et C dans un état de Bell partagé. Cela fonctionne grâce au principe de rétroaction de mesure, où les mesures projectives locales en B établissent rétroactivement des corrélations entre A et C qui se manifestent comme des violations des inégalités de Bell malgré le fait que A et C ne partagent jamais un cône de lumière commun, reposant fondamentalement sur la non-localité quantique plutôt que sur le transfert d'information.",
    "C": "Le protocole permet la distribution d'intrication à distance en faisant exécuter au nœud B une opération SWAP contrôlée entre ses moitiés des paires A-B et B-C, qui échange l'information quantique entre les paires de Bell initialement indépendantes sans effondrer leurs superpositions. Cela préserve l'intrication par évolution unitaire plutôt que par mesure, exploitant la réversibilité des opérations quantiques pour rediriger les corrélations du nœud intermédiaire vers les points terminaux tout en maintenant la cohérence, bien que la fidélité finale de l'état A-C dépende de la minimisation de la décohérence pendant l'implémentation de la porte SWAP.",
    "D": "L'échange établit l'intrication A-C en utilisant B pour implémenter un relais quantique où la téléportation séquentielle transfère l'état d'un qubit à travers la chaîne tout en consommant les deux paires initiales comme ressource de canal quantique. Le principe fondamental est le transfert d'état quantique par communication assistée par intrication, où la mesure de Bell de B sur une paire génère les bits classiques nécessaires pour compléter la téléportation, puis ces mêmes bits sont utilisés avec la seconde paire pour étendre le transfert vers C, convertissant effectivement deux liens intriqués à courte portée en un lien à longue portée par des opérations de mesure et de correction avec anticipation.",
    "solution": "A"
  },
  {
    "id": 24,
    "question": "Quelle technique précise offre la meilleure atténuation contre les attaques par canaux auxiliaires dans les modules de sécurité matériels post-quantiques ?",
    "A": "Implémentations à temps constant avec transformations algorithmiques d'aveuglement",
    "B": "Schémas de masquage d'ordre supérieur combinés avec des contre-mesures de mélange",
    "C": "Exécution à puissance normalisée avec primitives d'insertion de délai aléatoire",
    "D": "Environnements d'exécution isolés avec génération de nombres aléatoires quantiques",
    "solution": "D"
  },
  {
    "id": 25,
    "question": "Dans le contexte de l'informatique quantique basée sur la mesure, supposons que vous avez un état cluster 2D sur un réseau carré où certains qubits ont été mesurés dans des bases qui dépendent des résultats de mesure précédents (mesures adaptatives). Les qubits restants non mesurés forment un sous-graphe connexe. Quel est le rôle des états non-gaussiens dans l'apprentissage automatique quantique avec variables continues ?",
    "A": "Les opérations non-gaussiennes poussent les systèmes à variables continues au-delà du calcul quantique gaussien vers le territoire du calcul quantique universel. Sans ressources non-gaussiennes, les systèmes CV restent efficacement simulables classiquement par le théorème de Gottesman-Knill à variables continues, vous restreignant aux opérations sur les états gaussiens qui peuvent être suivis via des matrices de covariance. Dans l'apprentissage automatique quantique, les états non-gaussiens permettent les cartes de caractéristiques non linéaires et les distributions de probabilité complexes essentielles pour l'avantage quantique, allant au-delà de la structure quadratique de l'espace des phases qui limite les états gaussiens.",
    "B": "Ils sont essentiellement l'équivalent quantique des fonctions d'activation dans les réseaux de neurones, similaires à ReLU ou sigmoïde dans les architectures classiques.",
    "C": "Les états non-gaussiens vous permettent d'encoder des caractéristiques non linéaires dans l'état quantique lui-même, ce que les états gaussiens ne peuvent fondamentalement pas faire en raison de leur structure d'espace des phases limitée. Puisque les états gaussiens n'occupent que des régions ellipsoïdales convexes de l'espace des phases et évoluent sous des transformations symplectiques qui préservent la convexité, ils ne peuvent représenter que des caractéristiques polynomiales jusqu'au second ordre, alors que les états non-gaussiens avec négativité de Wigner peuvent encoder des noyaux non linéaires arbitraires et des corrélations de moments d'ordre supérieur qui sont essentiels pour les tâches d'apprentissage automatique comme la classification avec des frontières de décision courbes et la régression non linéaire.",
    "D": "Tous ces aspects capturent des éléments importants : l'universalité au-delà des opérations gaussiennes, les capacités d'encodage de caractéristiques non linéaires, et la connexion aux fonctions d'activation dans les réseaux de neurones quantiques",
    "solution": "D"
  },
  {
    "id": 26,
    "question": "Quelle est la fonction principale d'un processeur de marche quantique programmable ?",
    "A": "Implémenter des opérateurs de pièce adaptatifs qui s'ajustent dynamiquement en fonction du retour de mesure instantané provenant des qubits ancillaires, permettant à la marche de réagir aux distributions de probabilité intermédiaires. En conditionnant les opérations de déplacement subséquentes sur ces résultats de mesure, le processeur peut diriger le marcheur vers les régions de haute probabilité tout en maintenant la superposition quantique à travers les états de base computationnels non mesurés, créant effectivement un protocole de navigation hybride classique-quantique.",
    "B": "Implémente des algorithmes basés sur les marches pour diverses entrées de graphes en reconfigurant les opérateurs de pièce et de déplacement pour correspondre aux différentes topologies de graphes et structures d'adjacence, permettant l'exécution flexible d'algorithmes de recherche, d'échantillonnage et d'optimisation sans nécessiter une refonte matérielle.",
    "C": "Générer des états intriqués multi-marcheurs où des marcheurs quantiques distincts partagent une cohérence de phase à travers des structures de graphes séparées, permettant des protocoles de recherche distribués. En initialisant les marcheurs dans des configurations de type paires de Bell et en appliquant des opérateurs de pièce corrélés, le processeur exploite les effets d'interférence non locaux entre des régions de graphes spatialement séparées, ce qui améliore l'accélération quadratique caractéristique des marches quantiques lorsque plusieurs graphes interconnectés doivent être explorés simultanément.",
    "D": "Synthétiser des hamiltoniens dépendant du temps à partir de la décomposition spectrale de l'unitaire de marche, permettant la simulation de marches quantiques en temps continu par trotterisation du laplacien du graphe. En discrétisant l'opérateur d'évolution en pas de temps suffisamment petits et en appliquant les formules de Suzuki-Trotter, le processeur approxime la dynamique continue tout en maintenant la structure pièce-déplacement, ce qui préserve les propriétés de localité nécessaires pour une implémentation efficace sur des architectures de plus proches voisins.",
    "solution": "B"
  },
  {
    "id": 27,
    "question": "Dans le contexte de l'informatique quantique, AWG est couramment utilisé pour définir la forme des impulsions de contrôle. Que signifie AWG ?",
    "A": "Adaptive Waveform Generator, une plateforme matérielle incorporant des boucles de rétroaction en temps réel qui modifient les caractéristiques des impulsions en cours de séquence en fonction des résultats de mesure ou des conditions d'erreur détectées pendant l'exécution du circuit quantique. Ces dispositifs emploient des modèles prédictifs de la dynamique des qubits pour ajuster de manière préemptive les amplitudes, durées et phases des impulsions afin de compenser la dérive de calibration ou les effets de diaphonie observés dans les cycles précédents.",
    "B": "Amplitude Waveform Generator, un dispositif spécialisé conçu exclusivement pour générer des impulsions de contrôle avec des enveloppes d'amplitude programmables tout en maintenant des relations de fréquence et de phase fixes.",
    "C": "Automated Waveform Generation, faisant référence au système de contrôle en boucle fermée qui synthétise automatiquement des formes d'impulsions optimales en intégrant la rétroaction de tomographie d'état de qubit en temps réel avec des algorithmes d'apprentissage automatique qui convergent vers les paramètres de forme d'onde produisant la fidélité de porte maximale. Cette terminologie met l'accent sur l'aspect d'automatisation où le processus de conception d'impulsion est retiré du réglage manuel et s'appuie plutôt sur des routines d'optimisation algorithmique telles que GRAPE (Gradient Ascent Pulse Engineering) basé sur le gradient ou des algorithmes génétiques fonctionnant sur des contrôleurs FPGA.",
    "D": "Arbitrary Waveform Generator, un instrument électronique programmable qui synthétise des signaux de tension définis par l'utilisateur avec un contrôle temporel précis sur les caractéristiques d'amplitude, de fréquence et de phase. Ces dispositifs permettent aux expérimentateurs de créer des enveloppes d'impulsions personnalisées optimisées pour des opérations de portes quantiques spécifiques, supportant à la fois des formes standard comme les gaussiennes et des formes d'onde modulées complexes requises pour un contrôle de haute fidélité.",
    "solution": "D"
  },
  {
    "id": 28,
    "question": "Quelle vulnérabilité sophistiquée existe dans l'implémentation des protocoles de calcul quantique aveugle ?",
    "A": "La structure de corrélation de la base de mesure crée un canal auxiliaire théorique de l'information — lorsque le serveur observe les dépendances temporelles dans les séquences de choix de base du client à travers plusieurs tours, il peut effectuer une inférence statistique pour reconstruire partiellement la topologie du circuit sous-jacent avec une probabilité non négligeable. Bien que les résultats de mesure individuels restent parfaitement randomisés par le chiffrement à masque jetable, les probabilités conditionnelles entre les sélections de base successives laissent fuir des informations structurelles sur le graphe de calcul. Spécifiquement, la distribution de fréquence des mesures consécutives de base X versus Y sur des qubits adjacents révèle le motif de portes intriquantes, permettant à un serveur avec suffisamment d'échantillons de distinguer entre familles d'algorithmes par des tests de rapport de vraisemblance qui atteignent une précision de classification meilleure que le hasard.",
    "B": "Les circuits pièges peuvent être statistiquement distingués du calcul réel — si le serveur apprend quels tours sont des pièges de vérification versus des portes déléguées réelles, la confidentialité s'effondre. Le serveur peut analyser des propriétés statistiques comme l'entropie des résultats de mesure, les motifs de choix de base, ou la densité des opérations non-Clifford pour identifier les tours pièges avec une précision meilleure que le hasard. Une fois que l'identification des pièges réussit même partiellement, la garantie de vérifiabilité s'effondre car le serveur peut se comporter honnêtement sur les pièges détectés tout en déviant stratégiquement sur les tours de calcul réels, compromettant à la fois la confidentialité et la correction sans déclencher les conditions d'abandon côté client.",
    "C": "La granularité de l'angle de rotation imposée par l'électronique de contrôle à précision finie crée une vulnérabilité d'empreinte digitale — lorsque le client demande des rotations de qubit unique compilées à partir de l'ensemble de portes universel du protocole, les erreurs d'approximation discrètes s'accumulent différemment selon l'algorithme cible exécuté. Un serveur malveillant avec des mesures de fidélité de porte calibrées peut effectuer une analyse en composantes principales sur les motifs d'erreur de phase résiduelle à travers plusieurs qubits pour extraire des signatures spécifiques à l'algorithme. Cela fonctionne car différentes tâches computationnelles induisent des distributions caractéristiques d'angles de rotation qui laissent des traces statistiquement distinguables dans les opérations de portes réalisées versus demandées, permettant au serveur de classifier le type de calcul par apprentissage supervisé sur les statistiques de syndromes d'erreur.",
    "D": "La surcharge d'authentification dans le calcul aveugle vérifié introduit un canal caché par modulation de probabilité d'abandon — lorsque des serveurs malveillants injectent des quantités contrôlées de décohérence qui restent sous le seuil de détection, ils biaisent les taux d'échec des circuits pièges de manière à encoder les informations extraites sur le calcul réel. En corrompant stratégiquement les qubits non-pièges avec des taux d'erreur précisément calibrés qui maintiennent la fidélité globale dans des limites acceptables, le serveur peut manipuler quels circuits pièges spécifiques échouent à la vérification, créant un canal de communication binaire qui laisse fuir des résultats computationnels partiels à travers le motif d'exécutions de protocole abandonnées versus complétées sans dépasser les limites de distinguabilité statistique du client pour un comportement honnête versus malveillant.",
    "solution": "B"
  },
  {
    "id": 29,
    "question": "Quelle hypothèse permet à ECDQC de spécialiser son optimisation aux circuits de type QAOA et QFT ?",
    "A": "Mesure en étape finale avec rétroaction classique différée — l'extraction de syndrome et la lecture d'observable se produisent exclusivement à la couche terminale du circuit, mais l'optimisation clé provient du report de toutes les décisions de contrôle classique jusqu'après l'achèvement des opérations quantiques. Ce choix architectural met en pipeline les portes quantiques à travers un modèle de propagation avant où les résultats de mesure informent le post-traitement classique subséquent plutôt que les corrections en mi-circuit, permettant à ECDQC d'exploiter les motifs de mesure réguliers et la surcharge de lecture prévisible caractéristiques à la fois de l'estimation d'énergie QAOA et des étapes de lecture de phase de QFT.",
    "B": "Ces circuits présentent des motifs d'interaction réguliers et prévisibles avec des portes intriquantes globales creuses et des répétitions de couches structurées, permettant à ECDQC d'exploiter les symétries inhérentes et la localité dans les hamiltoniens de mélangeur et les opérateurs de phase, permettant des stratégies de compilation optimisées qui tirent parti de la structure périodique et des exigences de connectivité limitées caractéristiques à la fois de l'évolution de fonction de coût QAOA et de l'échelle de phase contrôlée de QFT.",
    "C": "Décompositions de Pauli structurées avec poids borné — les hamiltoniens de mélangeur QAOA et les opérateurs de phase de QFT admettent des décompositions creuses en chaînes de Pauli avec un poids évoluant logarithmiquement avec le nombre de qubits, ce qui signifie que chaque terme dans l'hamiltonien agit de manière non triviale sur au plus O(log n) qubits. ECDQC exploite cette parcimonie en routant uniquement entre les qubits apparaissant dans la même chaîne de Pauli, contournant les exigences de connectivité complète tous-à-tous et optimisant l'allocation des ressources d'intrication basée sur la structure de support des termes plutôt que sur la topologie de couplage globale.",
    "D": "Unitaires diagonaux dans la base computationnelle — l'opérateur de fonction de coût de QAOA et les rotations de phase contrôlée de QFT implémentent exclusivement des portes diagonales qui commutent avec les mesures de base computationnelle, ce qui signifie qu'elles peuvent être compilées en opérations classiques de recul de phase sans véritables portes intriquantes multi-qubits au-delà de la préparation d'état initiale. ECDQC traite ces couches diagonales comme des modulations classiques de probabilités de mesure, optimisant les calendriers d'accumulation de phase et tirant parti de la commutativité pour réordonner les portes librement sans suivre la génération d'intrication ou la propagation de décohérence.",
    "solution": "B"
  },
  {
    "id": 30,
    "question": "Dans le calcul quantique numérique-analogique, vous alternez entre de courtes impulsions de portes numériques et des blocs d'évolution analogique plus longs pour simuler un hamiltonien cible. Supposons que vous discrétisez l'évolution analogique à des intervalles de temps fixes Δt. Pourquoi l'aliasing peut-il dégrader la fidélité de simulation, même lorsque chaque bloc analogique est parfaitement implémenté ?",
    "A": "Le pas de temps fini Δt introduit une coupure dans la représentation temporelle de l'évolution hamiltonienne, et par le principe d'incertitude cette coupure dans la variable temporelle correspond à une incertitude en énergie qui élargit les caractéristiques spectrales de H — lorsque les états propres à haute énergie de l'hamiltonien cible ont des espacements de valeurs propres dépassant π/Δt, cet élargissement provoque un chevauchement spectral qui corrompt la dynamique.",
    "B": "Les protocoles numérique-analogiques reposent sur la décomposition de Trotter où les blocs analogiques approximent e^(-iHΔt) mais avec une petite erreur systématique O(Δt²) — lorsque l'hamiltonien cible contient des termes oscillants à haute fréquence avec des périodes plus courtes que 2Δt, l'erreur de Trotter du second ordre échoue à moyenner correctement ces oscillations et les amplifie plutôt par accumulation résonnante, produisant une perte de fidélité qui évolue super-linéairement avec le temps d'évolution.",
    "C": "Le découpage temporel discret échantillonne effectivement l'évolution hamiltonienne continue à des intervalles Δt, et si les composantes haute fréquence dans l'hamiltonien cible dépassent la fréquence de Nyquist π/Δt, ces fréquences se replient comme des répliques spectrales qui corrompent la dynamique basse fréquence prévue.",
    "D": "Les blocs analogiques parfaitement implémentés évoluent sous l'hamiltonien natif exp(-iH_native Δt) exactement, mais l'hamiltonien cible H_target diffère généralement de H_native par des termes oscillants à haute fréquence générés par les transformations de référentiel tournant — lorsque ces corrections oscillantes ont des composantes de Fourier dépassant la fréquence de Nyquist π/Δt fixée par la discrétisation, elles se replient dans le secteur basse fréquence où elles interfèrent constructivement avec la dynamique désirée et introduisent des erreurs de phase systématiques.",
    "solution": "C"
  },
  {
    "id": 31,
    "question": "Pour maintenir une taille d'état de marcheur gérable, les algorithmes de marche quantique pour la distinction d'éléments choisissent une taille de sous-ensemble égale à :",
    "A": "Approximativement la racine cubique de la longueur de la liste, équilibrant la dimension de l'état contre la probabilité de collision.",
    "B": "Proportionnelle à la racine carrée de n pour correspondre au seuil du paradoxe des anniversaires, garantissant que la probabilité de collision dans chaque sous-ensemble atteint un ordre constant tout en maintenant le diamètre du graphe de Johnson à O(n^(1/2)).",
    "C": "La racine quatrième de n, optimisant le compromis entre le coût de la phase de préparation et le nombre d'opérations de mise à jour à travers tous les cycles de détection de collision.",
    "D": "Inversement proportionnelle à l'écart spectral, typiquement n^(2/5), garantissant que le temps de mélange de la marche quantique reste sous-linéaire tout en contenant suffisamment d'éléments pour la détection de collision.",
    "solution": "A"
  },
  {
    "id": 32,
    "question": "Dans la planification temporelle pour les circuits quantiques, quelle contrainte doit être imposée pour les opérations de portes parallèles ?",
    "A": "Les portes planifiées pour une exécution parallèle doivent agir sur des ensembles disjoints de qubits pour éviter les conflits de ressources, puisque chaque qubit physique ne peut participer qu'à une seule opération de porte à un instant donné, garantissant qu'aucun qubit n'est simultanément ciblé par plusieurs opérations chevauchantes qui violeraient le principe fondamental de l'évolution unitaire.",
    "B": "Les portes planifiées en parallèle doivent agir sur des ensembles de qubits disjoints pour prévenir les conflits de base de mesure, car des opérations simultanées sur des qubits chevauchants nécessiteraient que l'état quantique s'effondre dans les états propres d'observables non commutatives pendant la même fenêtre de mesure, violant le principe d'incertitude pour les variables conjuguées et créant des résultats de syndrome ambigus dans les protocoles de correction d'erreurs.",
    "C": "Les portes s'exécutant en parallèle doivent opérer sur des qubits séparés pour éviter de violer le théorème de non-clonage, car l'application de deux unitaires distincts au même qubit simultanément nécessiterait de dupliquer de manière cohérente l'état quantique du qubit à travers plusieurs branches computationnelles avant de les recombiner, ce qui est interdit par la linéarité de la mécanique quantique pour des états inconnus arbitraires.",
    "D": "Les portes planifiées dans la même couche temporelle doivent cibler des registres de qubits disjoints pour préserver la causalité dans le graphe de dépendances du circuit, car l'utilisation chevauchante de qubits créerait des dépendances de données cycliques où les sorties de portes se réinjectent dans leurs propres entrées au sein d'un seul cycle d'horloge, violant la structure acyclique requise pour la compilation déterministe des programmes quantiques.",
    "solution": "A"
  },
  {
    "id": 33,
    "question": "Quelle est la signification de l'approche Quantum Wasserstein Generative Adversarial Network (QWGAN) ?",
    "A": "En tirant parti de la formulation duale de la métrique de Wasserstein, QWGAN stabilise la dynamique d'entraînement adversariale qui souffrirait autrement d'effondrement de modes et de gradients évanescents dans le régime quantique, particulièrement lorsque la distribution de sortie du générateur est supportée sur des variétés d'états quantiques de faible dimension difficiles à distinguer avec des mesures de divergence standard.",
    "B": "Par une procédure variationnelle itérative, QWGAN entraîne des circuits quantiques paramétrés pour générer des états quantiques dont les statistiques de mesure approximent étroitement les distributions de probabilité cibles, même lorsque ces distributions proviennent de systèmes quantiques à N corps complexes qui sont classiquement insolubles à simuler, permettant ainsi un échantillonnage efficace à partir de distributions quantiques de haute dimension en utilisant uniquement des circuits de profondeur polynomiale.",
    "C": "Tout ce qui précède",
    "D": "Le cadre emploie des discriminateurs quantiques contraints par Lipschitz combinés à la théorie du transport optimal pour garantir la convergence des mises à jour de paramètres du générateur, fournissant des bornes rigoureuses sur l'erreur d'approximation et la complexité d'échantillonnage qui évoluent polynomialement plutôt qu'exponentiellement avec la taille du système, contrairement aux GAN classiques ou aux modèles génératifs quantiques standard qui manquent de tels fondements théoriques.",
    "solution": "C"
  },
  {
    "id": 34,
    "question": "Quel est un défi majeur dans la gestion de l'exécution de calculs quantiques distribués ?",
    "A": "Le calcul quantique distribué nécessite d'établir une distribution directe d'intrication entre toutes les paires de processeurs avant le début de l'exécution du circuit, chaque processeur maintenant des copies complètes de l'état quantique global par des protocoles continus de transfert d'état quantique. Cette approche garantit la tolérance aux pannes en permettant à tout processeur de vérifier indépendamment les résultats du calcul par des mesures locales sans surcharge de communication classique, bien qu'elle exige des ressources d'intrication évoluant exponentiellement avec le nombre de processeurs et limite les implémentations pratiques à de petits réseaux.",
    "B": "Partitionner efficacement le circuit et planifier intelligemment des sous-ensembles computationnels sur différents processeurs tout en minimisant la surcharge de préparation d'état distribué, de distribution d'intrication et de communication inter-processeurs, tous ces éléments pouvant augmenter considérablement le nombre total d'opérations requises.",
    "C": "Le défi principal implique le maintien de la cohérence de phase entre des processeurs quantiques spatialement séparés par des références d'oscillateurs locaux synchronisés, exigeant que chaque processeur exécute des séquences de portes identiques en coordination temporelle stricte. Bien que les stratégies de partitionnement de circuit puissent distribuer la charge computationnelle, le goulot d'étranglement fondamental reste l'accumulation de dérive de phase relative entre nœuds, qui croît quadratiquement avec la distance inter-processeurs et nécessite des protocoles fréquents de recalibration de verrouillage de phase qui dominent la surcharge totale du temps d'exécution.",
    "D": "La gestion de l'exécution quantique distribuée nécessite fondamentalement de convertir toutes les portes multi-qubits en rotations de qubits uniques combinées à des opérations classiques de propagation directe, car les corrélations quantiques ne peuvent être maintenues entre des processeurs spatialement séparés sans effondrer les états de superposition. Cette contrainte force les compilateurs de circuits à décomposer les opérations d'intrication non locales en séquences de mesures locales suivies de corrections conditionnées classiquement, bien que des avancées récentes dans les modèles basés sur la mesure atténuent partiellement cette limitation par des techniques de préparation d'états en grappe.",
    "solution": "B"
  },
  {
    "id": 35,
    "question": "Un groupe de recherche soupçonne que leur fournisseur quantique cloud est compromis. Ils observent qu'un plug-in de compilation malveillant a inséré des portes SWAP cachées entre leurs qubits de données et des qubits ancillaires de réserve dans la disposition du dispositif, suivies immédiatement d'opérations de réinitialisation sur ces ancillas. Le circuit du groupe effectue des routines d'optimisation propriétaires sur des données financières sensibles. En supposant que l'adversaire contrôle les mesures de qubits de réserve après réinitialisation, quelle brèche de confidentialité spécifique ce vecteur d'attaque permet-il ?",
    "A": "Exfiltration d'état — l'adversaire mesure passivement les données qui ont été échangées dans le registre ancillaire avant la réinitialisation, fuyant l'information quantique hors du calcul prévu. En lisant les qubits ancillaires avant qu'ils ne soient réinitialisés, l'attaquant capture l'information d'état quantique qui a été temporairement transférée depuis le circuit propriétaire, permettant la reconstruction de résultats computationnels intermédiaires et exposant potentiellement les données financières sensibles encodées dans les amplitudes quantiques.",
    "B": "Extraction d'oracle paramétrique — en corrélant systématiquement le timing et le placement des opérations SWAP insérées avec les fluctuations de temps d'exécution observables dans le circuit compilé, l'adversaire fait de l'ingénierie inverse de la séquence de portes propriétaire par analyse de canal auxiliaire. Les portes SWAP agissent comme marqueurs temporels qui révèlent quels qubits computationnels portent des résultats intermédiaires de haute valeur à chaque couche du circuit, permettant la reconstruction de la structure d'arbre de décision de l'algorithme et l'importance relative de différents chemins computationnels par analyse de temps d'exécution différentiel à travers plusieurs soumissions de tâches.",
    "C": "Manipulation des résultats de mesure — après avoir échangé les qubits de données dans le registre ancillaire et les avoir mesurés extérieurement, l'adversaire injecte des états de remplacement soigneusement construits dans ces positions ancillaires avant de les réintégrer dans le registre computationnel lors de l'opération de réinitialisation subséquente. Cela crée un canal bidirectionnel où l'attaquant non seulement extrait l'information d'état quantique mais injecte également des perturbations ciblées qui biaisent les statistiques de mesure finales vers des résultats prédéterminés, permettant la manipulation des résultats d'optimisation financière en faveur d'intérêts adverses.",
    "D": "Empreinte d'intrication — les opérations SWAP créent un canal quantique caché en établissant des paires de Bell entre les qubits de données et les ancillas contrôlés par l'adversaire avant la réinitialisation. Bien que les ancillas soient réinitialisés à |0⟩, l'historique d'intrication antérieur laisse des corrélations de phase détectables dans les couches computationnelles suivantes qui encodent une signature unique identifiant quelles valeurs de données spécifiques ont été traitées. L'adversaire extrait ces empreintes en mesurant des stabilisateurs multi-qubits sur le registre ancillaire à travers des exécutions de tâches séquentielles, reconstruisant les données financières par analyse de corrélation tomographique.",
    "solution": "A"
  },
  {
    "id": 36,
    "question": "Quelle vulnérabilité sophistiquée existe dans l'implémentation de la cryptographie quantique indépendante des dispositifs ?",
    "A": "Failles de détection avec post-sélection sélective durant les fenêtres temporelles de coïncidence",
    "B": "La sécurité des protocoles indépendants des dispositifs repose fondamentalement sur l'hypothèse que les bases de mesure sont sélectionnées à l'aide de générateurs de nombres véritablement aléatoires, indépendants de tous les événements quantiques antérieurs et des facteurs environnementaux. Cependant, si le générateur de nombres aléatoires utilisé pour choisir les paramètres de mesure présente même des corrélations microscopiques avec le dispositif de préparation d'états quantiques — peut-être par le biais de fluctuations d'alimentation électrique partagées, de couplage thermique ou d'interférences électromagnétiques — un espion pourrait exploiter ces dépendances subtiles pour prédire les choix de mesure à venir. Cette prévisibilité des bases permet une préparation stratégique d'états qui imite les violations des inégalités de Bell tout en ne fournissant en réalité aucune sécurité, sapant complètement la garantie d'indépendance des dispositifs sans nécessiter d'accès direct aux dispositifs quantiques eux-mêmes",
    "C": "La sécurité indépendante des dispositifs nécessite une vérification précise que les systèmes quantiques mesurés opèrent effectivement dans la dimension d'espace de Hilbert revendiquée, ce qui est généralement validé par des protocoles de témoins de dimension. Un adversaire peut exploiter les faiblesses de ces procédures de vérification en concevant soigneusement des dispositifs quantiques qui semblent violer les témoins de dimension pour des systèmes de faible dimension (tels que les qubits) lorsqu'ils sont testés avec des opérateurs témoins standard, mais qui opèrent en réalité dans des espaces de dimension supérieure où les corrélations classiques peuvent simuler le comportement quantique attendu. Cette attaque par usurpation de dimension réussit parce que la plupart des témoins de dimension pratiques sont dérivés de mesures tomographiques incomplètes qui ne peuvent pas distinguer les véritables systèmes à deux niveaux des systèmes de dimension supérieure qui ont été soigneusement préparés pour se comporter comme des qubits uniquement pour l'ensemble spécifique de mesures de test incluses dans le protocole de vérification",
    "D": "Les failles d'indépendance des mesures dans les tests de Bell surviennent lorsque les générateurs de nombres aléatoires sélectionnant les bases de mesure ne sont pas véritablement indépendants des dispositifs quantiques testés, permettant des corrélations subtiles via des facteurs environnementaux partagés comme les interférences électromagnétiques ou les fluctuations d'alimentation. Ces corrélations permettent aux adversaires de prédire les choix de mesure et de préparer des états quantiques qui imitent les violations de Bell sans fournir de sécurité véritable.",
    "solution": "D"
  },
  {
    "id": 37,
    "question": "Dans un protocole de distribution quantique de clés (QKD) multi-tours fonctionnant sur un canal à fibre optique avec pertes de longueur L et coefficient d'atténuation α, qu'est-ce qui limite fondamentalement le taux de génération de clé secrète R en fonction de la distance, et en quoi cette contrainte diffère-t-elle des protocoles d'échange de clés classiques opérant sur le même canal physique ? Considérez à la fois la borne PLOB pour les protocoles sans répéteur et l'impact des effets de taille finie sur l'amplification de confidentialité lorsque le nombre de signaux transmis N n'est pas asymptotiquement grand.",
    "A": "Le taux de génération de clé secrète R évolue exponentiellement avec la distance selon R ~ exp(-αL/2) en raison de la perte de photons, fondamentalement différent des canaux classiques où l'amplification du signal peut restaurer les débits binaires. La borne PLOB établit que le QKD sans répéteur ne peut excéder -log₂(1-η) bits par utilisation du canal où η est la transmissivité, tandis que les effets de taille finie introduisent des pénalités supplémentaires proportionnelles à O(1/√N) dans l'étape d'amplification de confidentialité, nécessitant des longueurs de blocs plus importantes pour approcher le taux asymptotique.",
    "B": "Le taux de clé subit une décroissance exponentielle R ~ exp(-αL) gouvernée par l'atténuation de Beer-Lambert, mais cela correspond à la communication optique classique où les amplificateurs à fibre dopée à l'erbium restaurent la force du signal tous les 80 km. La distinction fondamentale réside dans la borne PLOB limitant les taux sans répéteur à approximativement η log₂(η) bits par mode plutôt que la capacité de Shannon classique C = log₂(1 + SNR). Les corrections de taille finie évoluent selon O(log N/N) plutôt que O(1/√N), provenant de l'estimation de la min-entropie lisse dans le cadre de sécurité de Renner, rendant le QKD plus vulnérable aux fluctuations statistiques que les protocoles classiques avec correction d'erreur à décision dure.",
    "C": "La perte de photons impose une évolution R ~ exp(-αL/2) en raison des exigences de transmission à photon unique, tandis que la communication classique à états cohérents atteint une évolution R ~ exp(-αL/4) grâce à la détection homodyne qui accède aux deux quadratures. La borne PLOB prouve que sans répéteurs quantiques, la capacité ne peut excéder la capacité de compression mono-mode du canal, approximativement -log₂(1-η²) bits par utilisation. Les pénalités de taille finie contribuent des corrections O(√(log N)/N) dues au hachage résiduel dans la sécurité universellement composable, nécessitant N > 10⁸ pour atteindre 1% des taux asymptotiques, contrairement aux codes classiques nécessitant seulement N > 10⁵.",
    "D": "La décroissance exponentielle induite par les pertes R ~ exp(-αL) limite fondamentalement de manière identique les canaux quantiques et classiques puisque les deux transmettent des photons à travers la même fibre. La distinction clé est que le QKD nécessite une authentification bilatérale consommant log₂N bits par tour, créant un surcoût qui devient prohibitif lorsque N < 10⁶, tandis que le Diffie-Hellman classique se termine en communication constante. La borne PLOB fait en réalité référence au budget optique de la couche physique plutôt qu'à la capacité théorique de l'information, et les effets de taille finie se manifestent par un taux d'erreur quantique de bits (QBER) accru lorsque les tailles d'échantillon descendent en dessous des seuils du régime gaussien autour de N = 10⁴.",
    "solution": "A"
  },
  {
    "id": 38,
    "question": "Pourquoi la distribution quantique de clés (QKD) est-elle généralement utilisée conjointement avec le chiffrement symétrique ?",
    "A": "Le QKD sert exclusivement de mécanisme de génération et de distribution sécurisée de clés, établissant des clés secrètes partagées prouvablement sécurisées entre les parties par le biais de principes de mécanique quantique, mais il ne gère pas le chiffrement réel de données volumineuses. Les algorithmes de chiffrement symétrique comme AES doivent ensuite utiliser ces clés distribuées quantiquement pour chiffrer et déchiffrer efficacement de grands volumes de données à des vitesses pratiques, puisque le QKD lui-même fonctionne à des débits beaucoup plus faibles limités par la transmission et la détection de photons.",
    "B": "Le QKD établit des clés partagées inconditionnellement sécurisées via des canaux quantiques, mais le matériel de clé réel n'existe que comme résultats de mesure d'états quantiques effondrés plutôt que comme clés cryptographiques persistantes. Le chiffrement symétrique est donc requis pour transformer ces résultats de mesure quantique éphémères en matériel de clé stable et réutilisable qui peut être stocké dans la mémoire classique et appliqué de manière répétée sur plusieurs sessions de chiffrement sans dégrader les garanties de sécurité théorique de l'information fournies par le théorème de non-clonage.",
    "C": "Bien que le QKD fournisse une sécurité théorique de l'information pour l'établissement de clés, le protocole révèle intrinsèquement des informations de temporisation et des modèles de communication via le canal de réconciliation classique utilisé pour la correction d'erreurs et l'amplification de confidentialité. Le chiffrement symétrique est nécessaire pour chiffrer cette communication par canal auxiliaire classique, prévenant les attaques d'analyse de trafic qui pourraient exploiter les corrélations statistiques entre les canaux quantiques et classiques pour inférer des propriétés du matériel de clé distribué sans observer directement les états quantiques.",
    "D": "Les protocoles QKD génèrent des clés partagées à des taux fondamentalement limités par la perte de canal et l'efficacité du détecteur, atteignant généralement seulement 1-10 kbps sur des distances pratiques en raison des contraintes de transmission de photons. Cependant, les technologies modernes de mémoire quantique ne peuvent préserver la cohérence de ces bits de clé distribués quantiquement que pendant des millisecondes avant que la décohérence ne détruise les garanties de sécurité. Le chiffrement symétrique fournit une couche de stockage classique qui convertit les clés quantiques en chaînes de bits classiques corrigées d'erreurs immédiatement après la mesure, prolongeant la durée de vie effective du matériel de clé de microsecondes à des années tout en maintenant les propriétés de sécurité établies durant la phase quantique.",
    "solution": "A"
  },
  {
    "id": 39,
    "question": "Quel est le principal défi dans l'implémentation de versions quantiques de la rétropropagation ?",
    "A": "Les trois problèmes se combinent pour créer une incompatibilité fondamentale entre les circuits quantiques et les paradigmes d'optimisation basés sur le gradient empruntés à l'apprentissage profond classique.",
    "B": "Les portes quantiques ne sont pas différentiables au sens habituel car elles représentent des transformations unitaires discrètes plutôt que des fonctions lisses, donc la règle de la chaîne ne s'applique pas sans les mapper d'abord dans un espace de paramètres. Bien que les règles de décalage de paramètre puissent calculer des dérivées en évaluant le circuit à des valeurs de paramètres décalées, cette approche nécessite plusieurs exécutions de circuit par paramètre et ne se compose pas naturellement à travers des architectures profondes comme le fait la rétropropagation classique.",
    "C": "Les mesures effondrent le système exactement quand vous avez besoin de cohérence pour calculer les gradients, détruisant l'information quantique même nécessaire pour évaluer comment les erreurs au niveau de la couche de sortie devraient influencer les paramètres de circuit antérieurs. Chaque mesure échantillonne d'une distribution de probabilité plutôt que de retourner une valeur de gradient définie, vous forçant à répéter l'ensemble de la passe avant des milliers de fois pour estimer les dérivées avec une variance acceptable, ce qui annule une grande partie de l'accélération quantique potentielle.",
    "D": "Le théorème de non-clonage empêche la mise en cache des états quantiques intermédiaires pendant la passe avant, sur laquelle la rétropropagation classique s'appuie pour stocker les activations en vue de leur réutilisation pendant le calcul du gradient. Sans copies des états intermédiaires, vous ne pouvez pas effectuer l'étape de rétropropagation d'erreur qui compare les sorties désirées versus réelles à chaque couche, forçant des stratégies alternatives d'estimation de gradient qui nécessitent plusieurs exécutions de circuit par mise à jour de paramètre.",
    "solution": "D"
  },
  {
    "id": 40,
    "question": "Dans le contexte de la modélisation générative et de l'apprentissage non supervisé, les machines de Boltzmann quantiques ont été proposées comme extension naturelle de leurs homologues classiques. Quelles sont les principales applications des machines de Boltzmann quantiques en apprentissage automatique et analyse de données, particulièrement dans les scénarios où les ressources quantiques pourraient offrir des avantages computationnels par rapport aux modèles graphiques probabilistes classiques ?",
    "A": "Les QBM ciblent les tâches d'apprentissage non supervisé incluant la détection d'anomalies dans les données de capteurs de haute dimension, la modélisation générative de conformations moléculaires pour la découverte de médicaments, et l'apprentissage de représentations latentes pour la tomographie d'états quantiques compressée. Leur avantage quantique est conjecturé émerger de l'échantillonnage de Gibbs cohérent permis par l'évolution en temps imaginaire sur les recuits quantiques, contournant potentiellement les temps de mélange exponentiels qui affligent les chaînes de Markov classiques dans les distributions multimodales, bien que les démonstrations expérimentales restent limitées à de petits systèmes de preuve de concept avec moins de 50 paramètres effectifs.",
    "B": "Les QBM trouvent leur utilité principale dans les tâches d'apprentissage non supervisé telles que le regroupement de données de haute dimension, l'apprentissage de représentations de caractéristiques hiérarchiques à partir d'ensembles de données non étiquetées, et la réduction de dimensionnalité — essentiellement des problèmes de reconnaissance de motifs où l'échantillonnage quantique à partir de distributions de Boltzmann pourrait théoriquement accélérer la phase d'entraînement. Leur avantage quantique proposé réside dans une équilibration plus rapide vers les distributions thermiques et un échantillonnage efficace de paysages de probabilité complexes qui défient les méthodes classiques de Monte-Carlo par chaînes de Markov.",
    "C": "Les machines de Boltzmann quantiques sont principalement appliquées aux scénarios d'apprentissage supervisé où les données d'entraînement étiquetées pilotent l'optimisation basée sur le gradient d'hamiltoniens d'Ising à champ transverse encodant la tâche de classification. En représentant les étiquettes de classe comme conditions aux limites sur le réseau de qubits du QBM et en exploitant l'effet tunnel quantique pour échapper aux minima locaux pendant la rétropropagation, ces modèles atteignent une convergence plus rapide que les réseaux profonds classiques sur les tâches de vision. L'avantage quantique se manifeste par une complexité d'échantillon exponentiellement réduite lors de l'apprentissage de frontières de décision de rang faible.",
    "D": "Les QBM se spécialisent dans les architectures d'apprentissage semi-supervisé où les unités visibles quantiques encodent les données d'entraînement classiques tandis que les unités cachées quantiques représentent la structure latente, permettant une inférence hybride qui combine l'estimation du maximum de vraisemblance classique avec l'amplification d'amplitude quantique. Les applications incluent l'entraînement adversarial génératif où le réseau discriminateur est implémenté comme un circuit quantique effectuant l'estimation de densité par interférence destructive, et les auto-encodeurs variationnels où l'espace latent est un état quantique à variables continues permettant un encodage exponentiellement compact des corrélations par rapport aux variables latentes classiques discrètes.",
    "solution": "B"
  },
  {
    "id": 41,
    "question": "Qu'est-ce qui motive l'inclusion de portes à deux qubits paramétrées comme XY(θ) dans les ansätze des algorithmes quantiques variationnels (VQE) ?",
    "A": "Force d'intrication ajustable comme paramètre variationnel : La porte XY(θ) fournit une opération d'intrication ajustable où θ devient un degré de liberté variationnel supplémentaire, permettant potentiellement à l'ansatz d'approximer les états fondamentaux cibles avec moins de couches de circuit que ce qui serait nécessaire en utilisant uniquement des portes à deux qubits fixes comme CNOT ou CZ.",
    "B": "L'ajustabilité continue du couplage d'échange permet à XY(θ) d'interpoler entre des états produits et des états maximalement intriqués, offrant des avantages d'optimisation basée sur le gradient par rapport aux portes fixes discrètes. Cependant, la surcharge de calibration matérielle pour les portes paramétrées augmente généralement les erreurs systématiques proportionnellement au nombre de valeurs distinctes de θ requises pendant l'optimisation, les rendant moins favorables que les portes natives fixes lorsque la profondeur du circuit dépasse le seuil limité par la cohérence d'environ 50-100 opérations à deux qubits sur les dispositifs supraconducteurs actuels.",
    "C": "Les portes paramétrées permettent l'encodage direct de symétries spécifiques au problème en mappant directement les paramètres physiques comme les angles de liaison dans les molécules sur les angles de porte, réduisant ainsi la dimension variationnelle. Bien que XY(θ) puisse représenter certains hamiltoniens de spin plus naturellement que les décompositions basées sur CNOT, cette approche adaptée aux symétries nécessite une refonte de l'ansatz dépendante du problème et sacrifie l'applicabilité universelle que les ansätze hardware-efficient à portes fixes offrent sur des paysages d'optimisation arbitraires.",
    "D": "La porte XY(θ) implémente des opérations SWAP partielles avec une amplitude contrôlable, permettant un transfert de population fractionnel entre les états de base de calcul qui peut être optimisé pour correspondre au profil exact d'entropie d'intrication des états propres cibles. Ce contrôle fin permet une meilleure approximation des fonctions de corrélation dans les systèmes fortement interagissants, bien que les paramètres θ supplémentaires étendent la dimensionnalité du paysage d'optimisation d'un facteur égal au nombre de portes à deux qubits, introduisant potentiellement plus de minima locaux que les architectures à angle fixe.",
    "solution": "A"
  },
  {
    "id": 42,
    "question": "Quelle vulnérabilité sophistiquée existe dans le processus de distillation de clé de la distribution quantique de clés ?",
    "A": "La résistance quantique de la fonction de hachage devient critique lorsque le post-traitement classique utilise des fonctions de hachage cryptographiques pour vérifier la parité pendant la correction d'erreurs, car les futurs ordinateurs quantiques exécutant l'algorithme de Grover pourraient inverser ces hachages pour reconstruire les bits de clé bruts. Si la fonction de hachage manque de résistance quantique suffisante, un espion pourrait exploiter les collisions de hachage.",
    "B": "La synchronisation de trame de réconciliation d'information échoue lorsque les canaux classiques utilisés pour échanger les syndromes de correction d'erreurs subissent une gigue temporelle ou une perte de paquets, amenant Alice et Bob à appliquer des vérifications de parité à des fenêtres de bits mal alignées. Cette désynchronisation est particulièrement exploitable car un espion peut retarder ou réordonner sélectivement les messages classiques.",
    "C": "L'estimation d'entropie de l'amplification de confidentialité devient vulnérable lorsque la min-entropie de la clé tamisée après correction d'erreurs est surestimée, conduisant à l'extraction d'une clé finale plus longue que le caractère aléatoire secret réellement disponible. Si l'estimation d'entropie suppose une efficacité idéalisée du détecteur ou sous-estime l'information d'Eve provenant des fuites de réconciliation de base, le taux de compression de l'amplification de confidentialité peut être insuffisant. Cela résulte en une clé finale où certains bits sont partiellement corrélés avec les connaissances de l'espion, violant les garanties de sécurité théorique de l'information. La vulnérabilité est particulièrement aiguë lorsque les effets de taille finie ne sont pas correctement pris en compte dans l'application du Leftover Hash Lemma, ou lorsque les informations de canal latéral provenant de variations temporelles dans la communication classique révèlent des bits supplémentaires au-delà des calculs du taux d'erreur quantique de bits.",
    "D": "Le calcul de fuite de correction d'erreurs devient vulnérable lorsque la quantité d'information classique échangée pendant la correction d'erreurs basée sur les syndromes est sous-estimée, permettant à un espion d'obtenir plus de connaissances sur la clé tamisée que ce qui est pris en compte dans l'étape d'amplification de confidentialité. Si la borne de fuite suppose des codes LDPC idéalisés mais révèle des informations de canal latéral par le timing ou les longueurs de messages, le taux de clé final peut être surestimé.",
    "solution": "C"
  },
  {
    "id": 43,
    "question": "Quelle est la différence principale entre le problème du sous-groupe caché pour les groupes abéliens et non abéliens ?",
    "A": "La distinction critique réside dans la complexité de mesure requise après la transformée de Fourier quantique (QFT) : pour les groupes abéliens, un seul état de coset |ψ_H⟩ = (1/√|H|)Σ_{h∈H}|gh⟩ soumis à QFT produit des résultats de mesure qui se projettent directement sur les étiquettes de représentation irréductible (irrep), chacune étant unidimensionnelle et correspondant à un caractère de groupe χ_ρ: G → ℂ*, permettant un post-traitement classique en temps polynomial pour reconstruire le sous-groupe caché H à partir de O(log|G|) échantillons de mesure indépendants via l'algèbre linéaire sur la table des caractères. En revanche, les groupes non abéliens possèdent des irreps de dimension d_ρ > 1, souvent à l'échelle de √|G| pour les groupes symétriques S_n, ce qui signifie que la QFT mappe les états de coset en superpositions sur les entrées matricielles dans ces espaces de représentation de haute dimension : la mesure donne à la fois l'étiquette irrep ρ et un indice d'élément matriciel (i,j) ∈ [d_ρ]×[d_ρ], mais l'information du sous-groupe caché devient encodée dans des motifs de corrélation subtils à travers ces distributions d'éléments matriciels. Extraire H de ces statistiques de mesure irrep de haute dimension nécessite génériquement soit un nombre exponentiel de mesures quantiques pour effectuer une tomographie complète de l'espace de représentation, soit des mesures polynomiales combinées avec un calcul classique exponentiel pour résoudre le système résultant de contraintes non linéaires, créant une barrière informationnelle fondamentale absente dans le cas abélien où les dimensions irrep restent uniformément égales à un.",
    "B": "La différence structurelle se manifeste dans la stratégie d'échantillonnage de Fourier : les groupes abéliens satisfont le théorème fondamental des groupes abéliens finiment engendrés, qui garantit une décomposition G ≅ ℤ_{n₁} ⊕ ℤ_{n₂} ⊕ ... ⊕ ℤ_{n_k} en une somme directe de groupes cycliques, permettant au problème du sous-groupe caché d'être factorisé en k problèmes indépendants unidimensionnels qui peuvent être résolus via la recherche de période quantique standard sur chaque facteur cyclique en utilisant O(k·log|G|) requêtes. La transformée de Fourier quantique sur G se décompose en conséquence comme QFT_G = QFT_{n₁} ⊗ QFT_{n₂} ⊗ ... ⊗ QFT_{n_k}, et mesurer dans cette base de Fourier produit tensoriel révèle la structure du sous-groupe caché composante par composante avec une efficacité polynomiale. À l'inverse, les groupes non abéliens manquent de telles décompositions canoniques : des groupes comme le groupe symétrique S_n ou les groupes diédraux D_n ne peuvent pas être écrits comme des produits directs de sous-groupes plus simples d'une manière qui respecte la structure du sous-groupe caché, forçant les algorithmes à travailler avec la théorie complète des représentations non abéliennes où la QFT devient un changement de base en forme bloc-diagonale avec des blocs correspondant à des irreps de dimensions variables d_ρ ≤ √|G|. L'absence de structure produit tensoriel signifie que l'information du sous-groupe caché ne peut pas être isolée en facteurs indépendants de faible dimension, nécessitant une résolution simultanée de corrélations à travers plusieurs secteurs irrep de haute dimension — une tâche qui exige des ressources exponentielles dans le cas général malgré une complexité de requête quantique polynomiale.",
    "C": "Pour les groupes abéliens, la transformée de Fourier quantique opère sur une structure où toutes les représentations irréductibles sont unidimensionnelles, ce qui signifie que les résultats de mesure de la QFT révèlent directement la périodicité du sous-groupe caché par une simple arithmétique modulaire sur les fréquences observées. La base de Fourier diagonalise l'opération de groupe de manière nette, et un post-traitement polynomial suffit pour extraire les générateurs du sous-groupe. En contraste frappant, les groupes non abéliens possèdent des représentations irréductibles de dimension supérieure à un — croissant souvent comme √|G| ou plus — ce qui signifie que la QFT sur de tels groupes produit des résultats de mesure qui tombent dans des espaces de représentation de haute dimension où l'information du sous-groupe caché devient encodée dans des motifs de corrélation complexes à travers les entrées matricielles plutôt que de simples pics de fréquence. Extraire le sous-groupe de ces coefficients irrep multidimensionnels nécessite généralement un nombre exponentiel de mesures ou des mesures polynomiales suivies d'un post-traitement classique exponentiel, créant une barrière computationnelle fondamentale absente dans le cas abélien.",
    "D": "La division fondamentale provient de la façon dont la transformée de Fourier quantique interagit avec la théorie des représentations du groupe : dans les groupes abéliens G, le lemme de Schur combiné avec la commutativité [g₁,g₂]=0 ∀g₁,g₂∈G force chaque représentation irréductible à être unidimensionnelle (d_ρ=1 pour tout ρ), ce qui signifie que la QFT décompose l'algèbre de groupe ℂ[G] en une somme directe d'espaces propres unidimensionnels étiquetés par des caractères ρ: G→U(1), et mesurer un état de coset transformé par QFT |ψ_H⟩=(1/√|H|)Σ_{h∈H}|gh⟩ s'effondre à l'état de base |ρ⟩ avec une probabilité déterminée par si ρ s'annule sur le sous-groupe caché H (c'est-à-dire si ρ(h)=1 ∀h∈H). Collecter O(log|G|) tels échantillons ρ₁,...,ρ_k et résoudre le système linéaire ρ_i(h)=1 via des logarithmes discrets sur le groupe de caractères Ĝ≅G reconstruit H en temps polynomial. Pour les groupes non abéliens, les irreps ont des dimensions d_ρ>1 s'échelonnant jusqu'à Θ(√|G|), donc la QFT mappe les états de coset en superpositions sur des triplets (ρ,i,j) où i,j∈[d_ρ] indexent les entrées matricielles dans l'irrep ρ. La distribution sur ces indices matriciels encode H à travers des coefficients de Fourier de théorie des représentations qui satisfont des règles de somme non triviales, mais contrairement au cas abélien, aucun algorithme efficace n'est connu pour extraire H à partir d'un nombre polynomial de tels échantillons sans résoudre des problèmes classiquement difficiles comme l'isomorphisme de graphes ou la réduction de réseau intégrés dans la structure de représentation.",
    "solution": "C"
  },
  {
    "id": 44,
    "question": "Dans les architectures de réseau quantique, vous concevez un protocole pour distribuer un état quantique |ψ⟩ d'une source centrale à cinq laboratoires géographiquement séparés, chacun devant effectuer des mesures locales sur des copies identiques. Votre étudiant diplômé propose d'utiliser un simple circuit de clonage par diffusion. Pourquoi les protocoles de multidiffusion quantique diffèrent-ils fondamentalement de la multidiffusion classique dans ce scénario ?",
    "A": "Le non-clonage empêche la copie, donc vous devez soit distribuer des paires intriquées distinctes à chaque destination, soit utiliser un état intriqué multi-qubits à partir duquel chaque partie peut extraire des informations corrélées par des opérations locales et de la communication classique. Le clonage parfait violerait la linéarité de la mécanique quantique, forçant les protocoles à partager l'intrication plutôt que de dupliquer les états.",
    "B": "La multidiffusion quantique nécessite l'établissement d'états GHZ ou d'états de graphe comme substrat de distribution, tandis que la multidiffusion classique fonctionne en dupliquant des chaînes de bits aux routeurs intermédiaires. La source prépare un état intriqué à (n+1) qubits où un qubit reste à la source et n qubits sont distribués aux destinataires ; la matrice de densité réduite de chaque destinataire approxime l'état désiré |ψ⟩ jusqu'à des corrections unitaires locales déterminées par l'information de syndrome classique diffusée après les mesures de téléportation, avec une fidélité se dégradant comme 1-O(1/n) plutôt que d'obtenir une reproduction parfaite.",
    "C": "Le théorème de non-clonage interdit la distribution déterministe 1→n pour des états quantiques inconnus, forçant les protocoles de multidiffusion soit à accepter un succès probabiliste nécessitant une coordination classique pour vérifier la réception, soit à distribuer des clones approximatifs avec une fidélité bornée par F ≤ (n+1)/(n+2) selon le clonage optimal de Buzek-Hillery, soit à préétablir une intrication partagée qui téléporte effectivement l'état à travers des mesures de Bell post-sélectionnées dont les résultats doivent être diffusés classiquement à tous les destinataires avant qu'ils puissent reconstruire des copies locales, changeant fondamentalement la structure du protocole par rapport à la simple duplication de paquets classique.",
    "D": "Les canaux quantiques présentent une accumulation de phase dépendante du chemin qui crée des interférences destructives lors de la division d'un état quantique à travers plusieurs routes spatiales, tandis que les bits classiques se propagent indépendamment à travers chaque branche de l'arbre de multidiffusion. Spécifiquement, lorsqu'un qubit photonique traverse un réseau de diviseurs optiques avec n sorties, l'amplitude de la fonction d'onde se divise comme 1/√n à travers tous les chemins, mais les différences de longueur de chemin optique relative ΔL introduisent des déphasages φ = 2πΔL/λ qui provoquent l'évolution de l'état distribué en un état mixte avec une pureté (1+cos φ)/2, nécessitant une stabilisation de phase active avec une précision λ/n pour maintenir la cohérence à travers tous les destinataires.",
    "solution": "A"
  },
  {
    "id": 45,
    "question": "Laquelle des approches suivantes N'est PAS une approche courante pour concevoir des ansätze de circuits quantiques ?",
    "A": "Structures hardware-efficient utilisant des ensembles de portes natives — Ces ansätze sont soigneusement construits pour minimiser la profondeur du circuit en employant exclusivement des portes qui peuvent être exécutées nativement sur le processeur quantique cible sans nécessiter de décomposition en opérations plus primitives, réduisant à la fois la surcharge de compilation et les erreurs de porte accumulées, particulièrement pour les algorithmes variationnels.",
    "B": "Conceptions inspirées du problème qui reflètent les symétries du hamiltonien — Lorsque le problème cible présente des propriétés de symétrie connues telles que la conservation du nombre de particules, la parité de spin ou la périodicité spatiale, l'ansatz peut être conçu pour préserver ces symétries par construction grâce à une sélection minutieuse de rotations paramétrées et de motifs d'intrication, réduisant considérablement la dimension de l'espace de recherche variationnel.",
    "C": "Circuits générés aléatoirement avec une intrication fixe qui ignorent la structure du problème — Cette approche construit des ansätze en échantillonnant aléatoirement des séquences de portes et des motifs d'intrication sans considération des symétries du problème sous-jacent, de la structure hamiltonienne ou des contraintes matérielles, rejetant délibérément les informations spécifiques au domaine qui pourraient améliorer la convergence et l'expressivité en faveur d'une exploration non structurée de l'espace de Hilbert complet.",
    "D": "Motifs de réseau tensoriel comme MPS ou MERA — En organisant les portes paramétriques selon des architectures de réseau tensoriel bien étudiées telles que les états de produit matriciel ou le renormalisation d'intrication multi-échelle, ces ansätze exploitent la structure de corrélation hiérarchique caractéristique des systèmes quantiques à plusieurs corps.",
    "solution": "C"
  },
  {
    "id": 46,
    "question": "Dans les réseaux quantiques distribués, les Accords de Niveau de Service Quantiques (QSLAs) doivent gérer des métriques sans équivalent classique. Au-delà des garanties traditionnelles de disponibilité et de latence, un QSLA doit spécifier des critères de performance propres à la communication quantique. Quel défi cela introduit-il que les SLA classiques évitent complètement ?",
    "A": "Définir des garanties contractuelles pour les seuils de fidélité d'intrication, les taux de génération de paires intriquées et les fenêtres temporelles de décohérence — des métriques de performance sans équivalents classiques qui ne peuvent être mesurées sans consumer la ressource quantique elle-même. Contrairement à la perte de paquets ou à la bande passante classiques qui peuvent être surveillées passivement, vérifier la qualité de l'intrication nécessite des mesures destructives d'états de Bell qui détruisent la ressource même garantie. Les QSLAs doivent spécifier des plages acceptables pour la concurrence, la négativité ou la fidélité aux états maximalement intriqués, ainsi que des taux de génération mesurés en ebits par seconde et des durées de cohérence garanties, créant des contrats exécutoires autour de phénomènes quantiques que les SLA classiques n'abordent jamais puisque les bits classiques ne décohèrent pas et n'exhibent pas de corrélations non-locales.",
    "B": "Établir des garanties contractuelles pour la fidélité de préparation d'états quantiques distribués, les taux de distribution d'intrication multipartite et les fenêtres de capacité de canal quantique — des métriques de performance propres aux réseaux quantiques qui nécessitent une vérification par des protocoles de reconstruction tomographique. Contrairement au débit ou à la gigue classiques qui peuvent être surveillés en continu par échantillonnage de paquets, certifier la performance d'un réseau quantique nécessite une tomographie de processus complète qui demande exponentiellement 4^n mesures pour n qubits, rendant la vérification intraitable computationnellement pour les grands systèmes. Les QSLAs doivent spécifier des plages acceptables pour la pureté d'état, l'intrication de formation ou la fidélité de canal aux canaux quantiques idéaux, ainsi que des taux de distribution mesurés en paires de Bell par seconde, créant des contrats exécutoires autour de ressources quantiques dont la vérification nécessite fondamentalement un calcul classique exponentiel que la surveillance SLA classique évite complètement.",
    "C": "Spécifier des garanties contractuelles pour les taux de distribution de clés quantiques, les probabilités de succès d'échange d'intrication et les durées de stockage en mémoire quantique — des métriques de performance sans équivalents classiques qui ne peuvent être vérifiées sans perturber l'information quantique elle-même. Contrairement aux taux d'erreur ou à la latence classiques qui peuvent être mesurés via des canaux de surveillance redondants, évaluer la qualité de communication quantique nécessite d'effectuer des mesures de syndrome qui collapsent partiellement les états de superposition, extrayant uniquement l'information de syndrome tout en préservant les qubits logiques. Les QSLAs doivent spécifier des plages acceptables pour l'intrication distillable, l'information mutuelle quantique ou la fidélité aux états GHZ, ainsi que des taux de distribution mesurés en bits de clé secrète par seconde et des temps de stockage garantis, créant des contrats exécutoires autour de phénomènes quantiques que les SLA classiques évitent puisque les signaux classiques peuvent être copiés pour une surveillance non-invasive.",
    "D": "Négocier des garanties contractuelles pour les temps de préservation de la cohérence quantique, la brillance des paires de photons intriqués et la fidélité de transmission d'états quantiques — des métriques de performance absentes des réseaux classiques qui ne peuvent être surveillées en continu sans introduire une rétroaction de mesure qui perturbe le canal quantique lui-même. Contrairement au rapport signal/bruit ou à la bande passante classiques qui peuvent être mesurés par des wattmètres en ligne, la caractérisation d'un canal quantique nécessite une tomographie d'état périodique qui interrompt temporairement le service pour injecter des états sondes et effectuer des mesures projectives. Les QSLAs doivent spécifier des plages acceptables pour la fidélité du processus de canal, le rendement d'intrication par impulsion de pompe ou les temps de cohérence T₂ de la mémoire, ainsi que des taux de répétition mesurés en paires annoncées par seconde et des facteurs de qualité d'isolation environnementale, créant des contrats exécutoires autour de ressources quantiques dont la caractérisation perturbe intrinsèquement la prestation de service que les SLA classiques peuvent surveiller de manière transparente sans impact sur le service.",
    "solution": "A"
  },
  {
    "id": 47,
    "question": "Les tests de swap utilisés dans les fonctions de coût basées sur la fidélité sont limités sur les circuits profonds principalement parce que :",
    "A": "L'estimation parfaite du recouvrement ne fonctionne que sur du matériel sans bruit car la mesure de fidélité du test de swap repose sur l'interférence quantique destructive dans les statistiques de mesure de l'ancilla, où la probabilité de mesurer |0⟩ égale (1 + |⟨ψ|φ⟩|²)/2, mais tout bruit de dépolarisation ou infidélité de porte introduit de la décohérence qui biaise cette probabilité vers le bas de façon non-linéaire.",
    "B": "Ils convertissent des observables locales en opérateurs non-locaux nécessitant une connectivité à longue portée que la plupart des architectures actuelles ne peuvent implémenter efficacement, car l'opération controlled-SWAP du test de swap entre deux états quantiques exige physiquement que le qubit ancilla interagisse simultanément avec des qubits de données spatialement séparés.",
    "C": "Mesurer à mi-parcours supprime les contraintes de cohérence car le protocole de test de swap nécessite une mesure projective du qubit ancilla après application des portes controlled-SWAP et Hadamard, ce qui collapse l'état quantique et détruit toute structure d'intrication restante entre les registres de données, empêchant la propagation des corrélations quantiques vers l'avant à travers les couches de circuit suivantes.",
    "D": "Les qubits ancilla augmentent l'exposition aux erreurs en introduisant des ressources quantiques supplémentaires qui doivent être initialisées, manipulées via des opérations contrôlées et mesurées, chaque étape accumulant décohérence et erreurs de portes qui se composent multiplicativement à travers le protocole de test de swap, particulièrement problématique dans les circuits profonds fonctionnant déjà près des limites de temps de cohérence.",
    "solution": "D"
  },
  {
    "id": 48,
    "question": "Pourquoi les optimisations basées sur la parcimonie des simulations de vecteurs d'état ne peuvent-elles pas être utilisées directement ?",
    "A": "Les matrices de densité représentant des états quantiques mixtes sont intrinsèquement déficientes en rang lorsque le système présente un degré de pureté inférieur à un, mais cette propriété structurelle ne se traduit pas en motifs de parcimonie utiles dans la base computationnelle. Les termes de cohérence hors-diagonale qui encodent les corrélations quantiques sont distribués dans toute la matrice d'une manière qui dépend du choix spécifique de base, et puisque les processus de bruit physiques comme l'amortissement d'amplitude et le déphasage affectent différents éléments matriciels de façon non-uniforme, il n'existe aucune structure parcimonieuse naturelle qui persiste à travers les opérations de portes — toute tentative d'exploiter une parcimonie dépendante de la base nécessiterait des transformations de base constantes qui élimineraient les gains computationnels.",
    "B": "Même lorsque le vecteur d'état initial contient de nombreuses amplitudes nulles permettant des représentations parcimonieuses, les portes quantiques elles-mêmes sont implémentées comme des matrices unitaires denses qui couplent ensemble tous les états de la base computationnelle. Cela signifie qu'appliquer même une rotation à un qubit unique à un état parcimonieux produit généralement une sortie dense, et les portes d'intrication multi-qubits mélangent davantage les amplitudes à travers tout l'espace de Hilbert, détruisant tout motif de parcimonie qui aurait pu exister dans la configuration d'entrée.",
    "C": "Les GPU manquent de support natif de matrices parcimonieuses pour les représentations d'opérateurs de densité, et le surcoût de conversion entre formats annule tout avantage computationnel. Bien que les architectures GPU modernes fournissent des bibliothèques comme cuSPARSE pour gérer l'algèbre linéaire parcimonieuse, le problème fondamental est que les matrices de densité de systèmes bruités nécessitent une conversion de format continue entre le stockage compressé par lignes (CSR) et les représentations denses lors de chaque application de porte, ce qui introduit des goulots d'étranglement de transfert mémoire qui submergent complètement l'accélération théorique de la réduction des opérations en virgule flottante, particulièrement lors du traitement d'opérateurs de dimension 2^n × 2^n pour des systèmes au-delà de 15-20 qubits.",
    "D": "Le bruit introduit des entrées non-nulles partout dans la représentation matricielle de densité, détruisant toute structure de parcimonie qui pourrait exister dans les vecteurs d'état sans bruit. Les canaux quantiques modélisant les processus de décohérence comme le bruit de dépolarisation ou l'amortissement d'amplitude font acquérir à chaque élément matriciel des valeurs non-nulles via la somme d'opérateurs de Kraus, et cette structure dense persiste tout au long du calcul indépendamment des propriétés de l'état initial.",
    "solution": "D"
  },
  {
    "id": 49,
    "question": "Quel rôle jouent les Nœuds de Confiance Simplifiés dans une chaîne de distribution de clés quantiques ?",
    "A": "Ils relayent la distribution d'intrication à travers les segments du réseau en effectuant un échange d'intrication via des mesures d'états de Bell sur les paires de photons entrantes, puis transmettent les signaux de succès annoncés et l'information de base aux nœuds adjacents sans exécuter de protocoles complets de distillation de clés. Cette architecture permet la distribution d'intrication à longue distance en décomposant la perte exponentielle en segments linéaires gérables, où chaque nœud effectue uniquement la mesure quantique et la communication classique nécessaires pour établir des corrélations brutes, reportant l'amplification de confidentialité et la réconciliation d'erreurs computationnellement intensives jusqu'à ce que l'état intriqué de bout en bout atteigne les utilisateurs terminaux aux extrémités du réseau.",
    "B": "Ils relayent la parité des résultats de mesure sans exécuter le post-traitement complet, effectuant essentiellement un transfert classique des résultats de détection bruts entre segments QKD adjacents. Cela permet au réseau de s'étendre au-delà des distances à un seul saut en décomposant le lien en sections gérables où chaque nœud transmet simplement les valeurs de bits et les choix de base sans exécuter d'algorithmes d'amplification de confidentialité ou de correction d'erreurs computationnellement intensifs, permettant un établissement rapide de clés à l'échelle de réseaux métropolitains tout en maintenant les hypothèses de confiance aux points de relais intermédiaires.",
    "C": "Ils effectuent des opérations QKD de préparation-et-mesure indépendamment sur chaque segment de lien adjacent, générant des clés brutes séparées avec les nœuds voisins, puis exécutent un protocole de combinaison de clés de confiance où des opérations XOR bit à bit fusionnent les clés par segment en un secret de bout en bout partagé entre utilisateurs terminaux. Chaque nœud effectue son propre tamisage et correction d'erreurs avec ses voisins immédiats, produisant des sous-clés sécurisées qui sont combinées via un chiffrement classique à masque jetable, permettant au réseau de s'étendre linéairement avec la distance tout en nécessitant une confiance totale dans la capacité de chaque nœud intermédiaire à protéger le matériel de clé combiné contre toute compromission.",
    "D": "Ils implémentent une modulation adaptative de l'intensité d'états leurres en surveillant les statistiques de perte de canal en temps réel à travers les segments de fibre adjacents et en ajustant dynamiquement la distribution du nombre de photons des impulsions cohérentes faibles envoyées entre nœuds. Lorsqu'un segment subit une perte élevée suggérant une activité potentielle d'écoute clandestine, le nœud augmente la proportion d'états leurres à vide et à photon unique par rapport aux états signal, puis effectue un test d'hypothèse statistique sur les motifs de détection observés pour déterminer si la déviation de perte est cohérente avec des attaques de division de nombre de photons ou reflète simplement une dégradation environnementale de la fibre.",
    "solution": "B"
  },
  {
    "id": 50,
    "question": "Quelle est la relation entre l'expressivité d'un circuit quantique paramétré et sa capacité d'entraînement ?",
    "A": "Les circuits plus expressifs sont toujours plus faciles à entraîner en raison de l'abondance de chemins d'optimisation qu'ils fournissent dans l'espace des paramètres.",
    "B": "Ce sont des propriétés fondamentalement indépendantes qui corrèlent dans des architectures spécifiques mais ne partagent aucune connexion théorique fondamentale.",
    "C": "Une expressivité élevée crée typiquement des plateaux arides — la fonction de coût devient exponentiellement plate et les gradients s'évanouissent. Lorsqu'un circuit peut accéder uniformément à une grande portion de l'espace de Hilbert (haute expressivité), le paysage de perte devient extrêmement haute dimension et la magnitude moyenne du gradient évolue exponentiellement petite avec la taille du système. Cette tension expressivité-entraînabilité signifie que les circuits maximalement expressifs sont souvent les plus difficiles à optimiser en pratique.",
    "D": "La capacité d'entraînement dépend uniquement de l'optimiseur classique, pas de l'expressivité du circuit, puisque le paysage d'optimisation est déterminé entièrement par la définition de la fonction de perte et les hyperparamètres de l'optimiseur.",
    "solution": "C"
  },
  {
    "id": 51,
    "question": "Qu'est-ce que l'algorithme de Metropolis quantique ?",
    "A": "Version quantique de Metropolis-Hastings pour échantillonner les distributions thermiques dans les systèmes à plusieurs corps, où les circuits quantiques implémentent les transitions de chaîne de Markov par des rotations contrôlées et des mesures projectives qui acceptent ou rejettent les mouvements proposés en fonction des différences d'énergie, permettant une exploration efficace des états d'équilibre.",
    "B": "Protocole d'échantillonnage quantique implémentant la thermalisation par des sous-routines d'estimation de phase qui préparent des états de Gibbs, où l'évolution unitaire contrôlée encode la décomposition spectrale du hamiltonien et l'amplification d'amplitude biaise les résultats de mesure vers des configurations de basse énergie selon les poids de Boltzmann, obtenant une accélération polynomiale par rapport au Monte Carlo par chaîne de Markov classique.",
    "C": "Adaptation de Metropolis-Hastings utilisant la préparation d'état adiabatique combinée à des marches quantiques sur l'espace de configuration, où l'interpolation graduelle du hamiltonien maintient l'équilibre détaillé tandis que le tunneling quantique améliore les temps de mélange, et les mesures projectives d'énergie déterminent les probabilités d'acceptation pour les transitions d'état proposées dans les protocoles d'échantillonnage à l'équilibre thermique.",
    "D": "Variante de recuit quantique qui implémente l'échantillonnage thermique par programmation de champ transverse et rétroaction basée sur la mesure, où les opérateurs de marche quantique de Szegedy encodent les conditions d'équilibre détaillé et la modification d'amplitude de type Grover accélère la convergence vers les distributions de Gibbs en exploitant les interférences quantiques dans les amplitudes de probabilité de transition entre états de configuration.",
    "solution": "A"
  },
  {
    "id": 52,
    "question": "Quel défi doit être relevé lors de l'application des Processus Gaussiens Quantiques à des ensembles de données réelles ?",
    "A": "L'incertitude inhérente aux états quantiques, régie par le principe d'incertitude de Heisenberg, ne peut être modélisée ou propagée à travers les cadres de Processus Gaussiens car la nature probabiliste des mesures quantiques est fondamentalement incompatible avec la structure de covariance déterministe requise par la théorie des GP.",
    "B": "Les Processus Gaussiens Quantiques ne peuvent fondamentalement pas traiter les tâches de régression en raison de la nature continue de l'espace de sortie, qui entre en conflit avec les résultats de mesure discrets requis par la mécanique quantique. Bien que les QGP excellent dans la classification binaire et multiclasse en mappant les évaluations de noyau à des états quantiques discrets, le postulat de projection force toutes les mesures à s'effondrer sur des valeurs propres, rendant impossible l'extraction des valeurs de fonction continues nécessaires pour la régression. Cette limitation nécessite des architectures hybrides classiques-quantiques où le post-traitement classique reconstruit les sorties de régression à partir de multiples mesures quantiques discrètes.",
    "C": "Les Processus Gaussiens Quantiques ne nécessitent absolument aucun ajustement d'hyperparamètres car le noyau quantique est uniquement déterminé par la structure d'espace de Hilbert de la carte de caractéristiques quantique, éliminant le besoin de sélection de bande passante, de paramètres de régularisation ou de choix de noyau.",
    "D": "Les stratégies d'atténuation du bruit et de correction d'erreurs restent critiques pour garantir des prédictions fiables des Processus Gaussiens Quantiques lorsqu'ils sont déployés sur du matériel quantique à court terme, où la décohérence, les erreurs de porte et le bruit de mesure peuvent corrompre les évaluations de noyau et conduire à des distributions a posteriori inexactes. Une gestion efficace du bruit nécessite une calibration soigneuse et des techniques d'atténuation d'erreurs.",
    "solution": "D"
  },
  {
    "id": 53,
    "question": "Quelle vulnérabilité spécifique existe dans l'architecture de connectivité des qubits des processeurs quantiques ?",
    "A": "Les goulots d'étranglement de routage dans les topologies fortement contraintes surviennent lorsque les algorithmes quantiques nécessitent des interactions entre qubits distants dans des architectures à connectivité limitée, forçant le compilateur à insérer de longues séquences de portes SWAP pour déplacer l'information quantique à travers la puce. Un adversaire ayant connaissance du graphe de connectivité et de l'algorithme cible peut analyser ces schémas de routage pour déduire quels qubits détiennent des informations critiques à différents moments du calcul.",
    "B": "Les contraintes de couplage aux plus proches voisins limitent les processeurs quantiques à n'effectuer des portes à deux qubits qu'entre qubits physiquement adjacents, nécessitant une utilisation extensive de réseaux SWAP pour implémenter des opérations multi-qubits arbitraires. La nature déterministe de l'insertion de SWAP crée des états intermédiaires prévisibles pendant l'exécution du circuit, permettant à un attaquant effectuant des mesures par canaux auxiliaires à des points spécifiques de la chaîne SWAP d'intercepter l'information quantique en transit entre qubits non adjacents.",
    "C": "Les dépendances de lignes de contrôle partagées qui permettent la diaphonie entre qubits, activant des interactions non intentionnelles lorsque plusieurs qubits sont pilotés simultanément ou lorsque des signaux de contrôle destinés à un qubit affectent par inadvertance les qubits voisins en raison d'une isolation imparfaite dans l'infrastructure de distribution micro-ondes. Cette contrainte architecturale signifie que les opérations sur un qubit peuvent fuir des informations vers ou devenir corrélées avec des qubits voisins, créant des canaux cachés pour le transfert d'information qui contournent la surveillance de sécurité au niveau des portes logiques.",
    "D": "Les voies de couplage par résonance croisée dans les architectures à fréquence fixe créent des interactions parasites entre qubits qui partagent des lignes de pilotage micro-ondes ou sont couplés via des modes de résonateur communs. Un adversaire capable d'injecter des signaux d'interférence précisément synchronisés peut sélectivement améliorer des termes de résonance croisée spécifiques pour créer des voies de communication cachées permettant à un qubit d'influencer un autre sans exécuter de portes explicites, contournant les mécanismes de sécurité qui ne surveillent que la séquence de portes logiques.",
    "solution": "C"
  },
  {
    "id": 54,
    "question": "L'accélération d'évaluation de formules pour les arbres NAND a inspiré des algorithmes ultérieurs pour évaluer des formules booléennes générales en :",
    "A": "Les chercheurs ont réalisé que la marche quantique utilisée pour traverser les arbres NAND pouvait être simulée classiquement pour des formules booléennes générales en remplaçant l'opérateur de diffusion quantique par une marche aléatoire classique sur l'arbre d'analyse de la formule, où chaque nœud est visité avec une probabilité proportionnelle au carré de l'amplitude de l'état quantique correspondant. Bien que cette approche classique sacrifie l'accélération quadratique, elle atteint un facteur d'approximation logarithmique en échantillonnant O(N^(1/2) log N) chemins à travers la formule et en moyennant les résultats.",
    "B": "Les algorithmes ultérieurs ont étendu le résultat des arbres NAND en développant un réseau de tri quantique qui prétraite les bits d'entrée dans un ordre canonique avant d'interroger la structure de la formule, réduisant la complexité d'oracle de O(√N) à O(log²N) pour les formules de profondeur d. L'étape de tri exploite le parallélisme quantique pour comparer toutes les 2ⁿ affectations d'entrée possibles simultanément via l'amplification d'amplitude.",
    "C": "L'accélération quantique pour les arbres NAND a inspiré une nouvelle classe d'algorithmes qui mappent des formules booléennes arbitraires sur des interféromètres optiques linéaires, où chaque variable est encodée dans la présence ou l'absence d'un photon dans un mode spécifique et les connecteurs logiques (AND, OR, NOT) sont implémentés via des séparateurs de faisceaux avec des transmissivités choisies pour correspondre à l'arbre syntaxique de la formule. En injectant un état de Fock multi-photon dans l'interféromètre et en effectuant un échantillonnage bosonique aux ports de sortie, ces algorithmes obtiennent une accélération quadratique dans l'évaluation de formules car la symétrisation bosonique calcule intrinsèquement des intégrales de chemin sur toutes les affectations de valeurs de vérité possibles en parallèle. Le lien avec les arbres NAND provient du fait que les arbres binaires équilibrés correspondent à des géométries d'interféromètre parfaitement symétriques (cascades de Mach-Zehnder), et la taille du témoin dans les programmes de span se traduit directement par le nombre de photons requis.",
    "D": "Convertir toute formule booléenne en une représentation équivalente de programme de span qui admet un témoin de petite taille, permettant aux algorithmes quantiques d'interroger la structure de la formule avec une complexité proportionnelle à la taille du témoin plutôt qu'à la taille de la formule, généralisant ainsi l'accélération en racine carrée des arbres NAND équilibrés aux formules arbitraires avec une structure déséquilibrée ou irrégulière.",
    "solution": "D"
  },
  {
    "id": 55,
    "question": "Quelle technique d'attaque spécifique peut exploiter les procédures d'initialisation des processeurs quantiques ?",
    "A": "La perturbation de l'équilibre thermique se produit lorsqu'un adversaire manipule l'environnement cryogénique du processeur quantique pour empêcher les qubits de se relaxer complètement vers leur état fondamental pendant la phase d'initialisation. En élevant subtilement la température effective du réfrigérateur à dilution ou en introduisant un chauffage localisé par des impulsions micro-ondes ciblées, l'attaquant peut s'assurer que les qubits conservent des populations d'excitation résiduelle qui s'écartent de l'état |0⟩ prévu, introduisant ainsi un biais systématique dans le calcul qui s'accumule de manière cohérente tout au long de l'algorithme.",
    "B": "L'interférence d'impulsion de réinitialisation — un adversaire injecte des signaux soigneusement synchronisés pendant le protocole de réinitialisation pour biaiser l'état initialisé loin de |0⟩, qui se propage ensuite à travers le calcul",
    "C": "La perturbation de l'état fondamental implique d'exploiter la constante de temps de relaxation finie (T₁) des qubits supraconducteurs en introduisant une interférence contrôlée immédiatement après une opération de réinitialisation nominale. Un attaquant qui a connaissance du calendrier d'impulsions du processeur peut injecter des signaux en opposition de phase qui ré-excitent partiellement le qubit après qu'il a commencé à se relaxer, créant un décalage déterministe dans la matrice de densité de l'état initial. Ce décalage persiste tout au long de l'exécution du circuit car les erreurs d'initialisation ne sont pas corrigées par les techniques standard d'atténuation d'erreurs au niveau des portes.",
    "D": "La surveillance d'excitation résiduelle exploite le fait que les qubits dans un réfrigérateur à dilution sont continuellement surveillés par des résonateurs de lecture, et un adversaire ayant accès à la chaîne de lecture peut injecter des photons parasites dans ces résonateurs pendant la fenêtre d'initialisation. Ces photons induisent des déplacements Stark AC qui modifient dynamiquement la fréquence de transition du qubit, provoquant un désaccord de l'impulsion de réinitialisation par rapport à la fréquence réelle du qubit et laissant le qubit dans un état mixte plutôt que l'état pur |0⟩, qui sert ensuite d'entrée corrompue à l'algorithme quantique.",
    "solution": "B"
  },
  {
    "id": 56,
    "question": "Quelle caractéristique rend la famille de portes FSim(θ,φ) attractive pour les processeurs supraconducteurs de type Google ?",
    "A": "L'ajustement continu de l'angle iSWAP et de la phase conditionnelle permet une compilation efficace des variantes CZ, iSWAP et SWAP sans recalibrage extensif entre différentes opérations de porte.",
    "B": "Les portes FSim implémentent naturellement l'interaction de Mølmer-Sørensen par couplage de flux modulé entre transmons, produisant une opération intriquante dont la fidélité s'améliore avec une durée d'impulsion plus longue grâce à la moyenne motionnelle du bruit de flux—contrairement aux portes résonantes où des impulsions plus longues accumulent plus de déphasage—permettant des compromis ajustables entre vitesse de porte et taux d'erreur limités par la cohérence dans l'espace des paramètres θ-φ.",
    "C": "La famille FSim couvre l'intégralité de la chambre de Weyl à deux qubits avec seulement l'amplitude et la durée de l'impulsion de flux comme paramètres de contrôle, éliminant le besoin de pilotages micro-ondes pendant les opérations intriquantes et évitant ainsi la diaphonie due aux collisions de fréquences entre les tons de pilotage et les qubits spectateurs, qui dans les réseaux denses de transmons avec une anharmonicité <500 MHz peuvent induire des transitions parasites corrompant les qubits voisins non impliqués dans la porte.",
    "D": "La paramétrisation FSim correspond directement à l'évolution hamiltonienne native sous couplage d'échange ajustable, nécessitant seulement une mise en forme adiabatique de l'impulsion de flux plutôt qu'une modulation précise d'amplitude et de phase des pilotages micro-ondes, ce qui réduit la sensibilité à l'atténuation des lignes de contrôle et aux coefficients de réflexion qui varient avec les fluctuations de température dans le réfrigérateur à dilution, atteignant une stabilité de calibrage quotidien 2-3 fois meilleure comparée aux schémas de portes activées par micro-ondes.",
    "solution": "A"
  },
  {
    "id": 57,
    "question": "Quel est le rôle principal des mesures dans un algorithme quantique variationnel ?",
    "A": "La génération de nombres aléatoires pour l'optimisation stochastique exploite la nature intrinsèquement probabiliste des mesures quantiques pour produire une entropie de haute qualité pour les optimiseurs classiques comme SPSA ou Adam qui nécessitent des estimations de gradient stochastique.",
    "B": "Réduire les superpositions en états classiques constitue la fonction primaire de mesure car les algorithmes variationnels opèrent fondamentalement en préparant des états de superposition paramétrés sur la base de calcul puis en extrayant l'information en forçant chaque qubit dans un résultat défini |0⟩ ou |1⟩. La réduction induite par la mesure transforme la distribution de probabilité quantique encodée dans les amplitudes en une chaîne de bits classique que l'optimiseur traite. Sans ce mécanisme de réduction, l'état quantique resterait inaccessible aux systèmes de contrôle classiques, rendant impossible l'évaluation de la performance du circuit ou la mise à jour des paramètres variationnels basée sur les résultats de calcul.",
    "C": "Estimer les valeurs d'espérance pour les fonctions de coût en préparant de manière répétée l'état quantique paramétré et en mesurant des observables dans des bases désignées, puis en utilisant la distribution statistique des résultats de mesure pour approximer l'énergie ou la fonction objectif qui guide l'optimisation classique des paramètres. Le processus de mesure échantillonne la distribution de probabilité encodée dans l'état quantique, fournissant les données empiriques nécessaires pour évaluer l'information de gradient et mettre à jour les paramètres variationnels vers des solutions optimales.",
    "D": "Implémenter des portes par calcul basé sur la mesure devient essentiel dans les cadres variationnels car mesurer des qubits dans des bases désignées suivi d'une propagation classique des résultats de mesure peut exécuter de manière déterministe toute transformation unitaire.",
    "solution": "C"
  },
  {
    "id": 58,
    "question": "Considérez une architecture de mémoire quantique qui utilise des qubits de chat bosoniques, connus pour leurs propriétés de bruit préservant le biais où les erreurs de basculement de bit sont exponentiellement supprimées tandis que les erreurs de basculement de phase se produisent à des taux comparables aux autres schémas d'encodage. Dans les implémentations pratiques de tels systèmes, les expérimentateurs superposent souvent une correction d'erreur supplémentaire au-dessus de l'encodage des qubits de chat. Comment le code de répétition complète-t-il les qubits de chat bosoniques dans la correction d'erreur ?",
    "A": "Implémente un vote majoritaire sur des états de chat spatialement séparés pour détecter les erreurs de phase par des mesures de parité.",
    "B": "Étend l'ingénierie de dissipation à deux photons à des configurations multi-modes qui stabilisent collectivement les deux quadratures d'erreur.",
    "C": "Corrige les erreurs de basculement de phase résiduelles auxquelles les qubits de chat sont sensibles",
    "D": "Exploite la stabilisation sans mesure par rétroaction autonome qui tire parti de la structure de bruit biaisé.",
    "solution": "C"
  },
  {
    "id": 59,
    "question": "Quelle technique sophistiquée fournit la garantie de sécurité la plus forte pour la génération de nombres aléatoires quantiques ?",
    "A": "Les générateurs de nombres aléatoires quantiques auto-testés fournissent les garanties les plus fortes en utilisant des séquences de mesure soigneusement conçues qui permettent au dispositif de vérifier son propre comportement quantique par les corrélations seules, sans nécessiter de confiance dans l'étape de préparation. Ces protocoles atteignent une sécurité comparable aux schémas indépendants du dispositif mais avec une complexité expérimentale considérablement réduite, les rendant pratiques pour le déploiement tout en maintenant une certification de caractère aléatoire prouvable même contre des adversaires sophistiqués qui pourraient contrôler la source.",
    "B": "L'estimation d'entropie avec information quantique auxiliaire traite le dispositif comme une boîte noire mais nécessite encore une caractérisation quantique pour borner la min-entropie disponible pour l'extraction.",
    "C": "Les protocoles d'expansion de caractère aléatoire quantique indépendants du dispositif atteignent les garanties de sécurité les plus fortes en certifiant le caractère aléatoire par des violations d'inégalités de Bell sans faire confiance au fonctionnement interne des dispositifs quantiques.",
    "D": "Les approches à variables continues offrent une sécurité forte car elles opèrent dans des espaces de Hilbert de dimension infinie, les rendant fondamentalement plus résistantes aux attaques par canaux auxiliaires que les systèmes à variables discrètes.",
    "solution": "C"
  },
  {
    "id": 60,
    "question": "Comment le taux d'intrication attendu est-il calculé pour un chemin donné ?",
    "A": "Le taux attendu est calculé en identifiant le taux de génération de lien minimum le long du chemin (le segment goulot d'étranglement) et en multipliant par le produit des probabilités de succès d'échange à chaque nœud intermédiaire, sous l'hypothèse que les échanges sont tentés séquentiellement plutôt que simultanément, ce qui introduit une corrélation temporelle qui réduit le débit effectif comparé aux protocoles d'échange parallèle.",
    "B": "Le taux d'intrication est déterminé par la moyenne harmonique des fidélités de liens individuels pondérées par leurs temps de cohérence respectifs, car les liens de plus faible fidélité contribuent de manière disproportionnée à l'accumulation d'erreur de bout en bout. Ce calcul tient compte du fait que les échanges d'intrication amplifient les erreurs de phase quadratiquement à chaque nœud répéteur, faisant du taux de décohérence du lien le plus faible le facteur dominant limitant la fréquence de distribution globale.",
    "C": "Multiplier les taux de génération de liens et les probabilités de succès d'échange le long du chemin : le taux d'intrication de bout en bout attendu est le produit du taux de génération de paires de Bell de chaque segment et de la probabilité d'échanges d'intrication réussis aux nœuds répéteurs intermédiaires.",
    "D": "Le taux est égal à la somme des temps de génération inverses pour chaque lien plus la somme des durées d'opération d'échange aux nœuds répéteurs, analogue à la résistance en série dans les circuits électriques. Puisque la génération d'intrication et l'échange sont des processus séquentiels qui doivent se terminer avant que la prochaine tentative commence, le temps total par paire distribuée avec succès s'accumule linéairement, faisant de l'inverse de cette somme le taux effectif de bout en bout.",
    "solution": "C"
  },
  {
    "id": 61,
    "question": "À quoi sert l'apprentissage par transfert quantique ?",
    "A": "À éliminer le besoin de données étiquetées en utilisant la superposition quantique pour explorer simultanément toutes les représentations de caractéristiques possibles, permettant l'identification automatique de caractéristiques discriminantes optimales sans étiquettes d'entraînement explicites grâce à l'évaluation parallèle d'un nombre exponentiel de fonctions d'extraction de caractéristiques pendant le pré-entraînement qui découvre la structure intrinsèque de la variété des données, après quoi les tâches en aval peuvent être apprises avec zéro exemple étiqueté car la mesure quantique projette naturellement les données non étiquetées sur les frontières de décision impliquées par la géométrie des caractéristiques découvertes.",
    "B": "À convertir des ensembles de données classiques en représentations purement quantiques sans ingénierie manuelle de caractéristiques en incorporant automatiquement les vecteurs de données classiques dans l'espace de Hilbert par un encodage d'amplitude optimal appris via optimisation variationnelle, où ce processus identifie l'espace d'états quantiques de dimension minimale nécessaire pour capturer le contenu informationnel de l'ensemble de données classique avec des taux de compression exponentiels.",
    "C": "À améliorer l'efficacité d'apprentissage et la généralisation entre tâches quantiques en exploitant des extracteurs de caractéristiques quantiques pré-entraînés ou des circuits paramétrés sur des tâches sources, puis en les affinant pour des tâches cibles avec des données d'entraînement limitées.",
    "D": "À garantir une indépendance statistique complète dans les modèles d'apprentissage automatique quantique qui ne nécessitent jamais d'entraînement préalable en exploitant le théorème de non-clonage pour garantir que chaque instance de modèle apprend de zéro sans hériter de biais d'exécutions précédentes, évitant les effets de transfert négatif qui affligent les approches classiques.",
    "solution": "C"
  },
  {
    "id": 62,
    "question": "La latence de traitement classique en temps réel devient critique pour les décodeurs car des délais supérieurs à quelle échelle temporelle peuvent annuler les bénéfices de la correction d'erreurs ? Ceci est particulièrement important dans les codes de surface où l'extraction de syndrome doit se produire de manière répétée, et tout goulot d'étranglement de traitement peut permettre aux erreurs de se propager plus rapidement qu'elles ne peuvent être corrigées, sapant fondamentalement le seuil de tolérance aux pannes.",
    "A": "Le temps de cycle d'extraction de syndrome entre les mesures successives de stabilisateurs dans le code de correction d'erreurs quantique. Si le décodage classique et la rétroaction prennent plus de temps que l'intervalle entre les cycles de syndrome, le décodeur prend du retard sur le fonctionnement en temps réel et ne peut pas fournir de signaux de correction avant l'arrivée du syndrome suivant, créant un arriéré qui permet aux erreurs de se propager sans contrôle à travers le qubit logique plus rapidement qu'elles ne peuvent être identifiées, défaisant complètement la capacité protectrice du protocole de correction d'erreurs.",
    "B": "Le temps de cohérence T2 des qubits de données entre les cycles successifs d'extraction de syndrome. Si le décodage classique et la rétroaction prennent plus de temps que T2, les erreurs s'accumulent et se propagent à travers le qubit logique plus rapidement que le protocole de correction d'erreurs ne peut les identifier et les corriger, défaisant complètement l'objectif de la correction d'erreurs quantique. Le décodeur doit opérer dans cette fenêtre temporelle pour maintenir la capacité protectrice du code et rester au-dessus du seuil de tolérance aux pannes.",
    "C": "Le temps de déphasage T2* des qubits ancillaires utilisés pour la mesure de syndrome entre les cycles successifs d'extraction. Si le décodage classique et la rétroaction prennent plus de temps que T2*, les erreurs de phase s'accumulent sur les ancillas pendant la latence de décodage et se propagent vers les qubits de données à travers les portes d'intrication dans les cycles de syndrome ultérieurs, corrompant la fidélité de lecture du syndrome plus rapidement que le protocole de correction d'erreurs ne peut compenser, défaisant complètement la capacité protectrice du mécanisme de tolérance aux pannes.",
    "D": "L'échelle de temps de thermalisation régissant la vitesse à laquelle les modes de phonons dans le substrat dissipent l'énergie déposée par les impulsions de contrôle vers l'étage de température de base du réfrigérateur à dilution. Si le décodage classique et la rétroaction prennent plus de temps que ce temps de relaxation thermique, le chauffage résiduel des cycles de syndrome précédents crée un bruit de déphasage sur les qubits de données qui s'accumule plus rapidement que le protocole de correction d'erreurs ne peut suivre, défaisant complètement la capacité protectrice de la mémoire quantique et poussant le système au-dessus du seuil de tolérance aux pannes.",
    "solution": "B"
  },
  {
    "id": 63,
    "question": "Qu'est-ce que le théorème quantique du flot maximum et de la coupe minimale ?",
    "A": "Un principe d'optimisation de réseau quantique stipulant que le flux d'intrication maximal entre deux nœuds égale la coupe de capacité quantique minimale les séparant, mais où « capacité quantique » fait référence à l'information cohérente (la différence entre l'information mutuelle quantique et la capacité classique) plutôt qu'aux mesures d'intrication, applicable aux canaux quantiques bruités régis par l'inégalité de traitement de données quantiques et utilisé dans les protocoles de codage de réseau quantique.",
    "B": "Une généralisation quantique du théorème classique du flot maximum et de la coupe minimale, établissant une dualité entre la capacité maximale de canal quantique pour transmettre l'information quantique ou l'intrication à travers un réseau et la coupe minimale d'intrication ou de capacité qui déconnecte les nœuds source des nœuds cibles, applicable à l'analyse de réseau de communication quantique et aux protocoles de distribution d'intrication.",
    "C": "Une dualité structurelle entre la fidélité maximale réalisable pour la transmission d'états à travers un réseau quantique et la discorde quantique minimale à travers une coupe partitionnant la source de la cible, où la discorde quantique (plutôt que l'intrication) sert de ressource goulot d'étranglement car elle capture toutes les corrélations quantiques y compris celles non accessibles par des opérations locales et communication classique, en faisant la mesure correcte pour les protocoles de routage d'états mixtes généraux dans les réseaux quantiques bruités réalistes.",
    "D": "Le principe selon lequel le taux maximal de distribution de paires EPR à travers un réseau quantique égale le rang de Schmidt minimal à travers toute coupe séparant la source du puits, où le rang de Schmidt (le nombre de coefficients de Schmidt non nuls dans la décomposition bipartite) détermine la dimensionnalité du canal d'intrication, et les coupes sont évaluées sur la base de la structure du produit tensoriel plutôt que des mesures de capacité additives, fournissant une caractérisation dimension-théorique plutôt qu'information-théorique du flux de réseau quantique.",
    "solution": "B"
  },
  {
    "id": 64,
    "question": "Pourquoi les portes de Clifford seules sont-elles insuffisantes pour réaliser le calcul quantique universel ?",
    "A": "Elles transforment les opérateurs de Pauli en opérateurs de Pauli par conjugaison, restant dans le formalisme des stabilisateurs et incapables de générer les phases continues arbitraires requises pour le calcul universel. Des portes non-Clifford comme la porte T ou Toffoli sont nécessaires pour accéder à des états en dehors du groupe de stabilisateurs et atteindre l'espace de transformation SU(2^n) complet.",
    "B": "Elles préservent la structure de phase discrète des états stabilisateurs mais échouent à générer des états avec des relations de phase irrationnelles entre les amplitudes, qui sont requises par le théorème de Solovay-Kitaev pour l'approximation universelle de portes. Bien que les portes de Clifford accèdent à tous les états stabilisateurs—un sous-ensemble dense de la sphère de Bloch pour les qubits uniques—elles ne peuvent pas atteindre des états non-stabilisateurs comme |T⟩ = (|0⟩ + e^(iπ/4)|1⟩)/√2 qui ont des facteurs de phase transcendantaux nécessaires pour compléter un ensemble de portes universel sur SU(2^n).",
    "C": "Elles forment un groupe fini sous composition qui ne peut générer qu'un sous-groupe discret de SU(2^n), spécifiquement le groupe de Pauli généralisé normalisé par la conjugaison de Clifford. Cela signifie que les circuits de Clifford ne peuvent atteindre que des états dont les tableaux de stabilisateurs ont des entrées entières modulo des polynômes cyclotomiques spécifiques, excluant les rotations continues requises par l'universalité. Le théorème de Gottesman-Knill prouve cette restriction : tout circuit de Clifford agissant sur des états stabilisateurs produit des sorties dont les amplitudes n'impliquent que des racines de l'unité de {±1, ±i}, jamais les phases complexes arbitraires nécessaires pour le calcul universel.",
    "D": "Elles ne génèrent que des permutations et des changements de signe de chaînes de Pauli lorsqu'elles agissent sur les générateurs de stabilisateurs, ce qui signifie qu'elles ne peuvent pas implémenter de portes qui font tourner continûment le vecteur de Bloch par des multiples irrationnels de π. Cette limitation survient parce que les portes de Clifford correspondent à des transformations symplectiques sur GF(2), les contraignant à des rotations discrètes de 90 degrés et à des changements de base de type Hadamard. Les portes non-Clifford comme T ou Toffoli introduisent les angles irrationnels nécessaires (phases π/4) qui échappent à la structure du groupe de Clifford fini et permettent une couverture dense de SU(2^n) par composition itérative.",
    "solution": "A"
  },
  {
    "id": 65,
    "question": "Dans l'algorithme quantique pour le problème du sous-groupe caché, vous préparez une superposition uniforme sur le groupe, appliquez l'oracle qui dépend de la structure du sous-groupe caché, puis effectuez une transformée de Fourier quantique avant de mesurer le premier registre. Le résultat de mesure a une interprétation algébrique spécifique qui est centrale pour le fonctionnement de l'algorithme. Quelle structure mathématique cette mesure révèle-t-elle ?",
    "A": "Mesurer le premier registre après la QFT produit un élément uniformément aléatoire d'un des cosets qui partitionnent le groupe selon le sous-groupe caché H. En répétant cette procédure et en collectant plusieurs représentants de cosets, vous pouvez reconstruire la structure du sous-groupe par post-traitement classique qui identifie quels éléments apparaissent toujours ensemble dans le même coset, triangulant effectivement H à partir de ses translatés gauches ou droits.",
    "B": "La mesure produit des candidats générateurs pour le sous-groupe caché en produisant des éléments dont l'ordre divise la structure du sous-groupe, exploitant la périodicité dans le motif de coset de l'oracle.",
    "C": "Un élément de base pour le dual du sous-groupe caché, spécifiquement un caractère irréductible qui s'annule sur tous les cosets sauf le coset identité. La collecte de plusieurs tels caractères orthogonaux par mesures répétées permet au post-traitement classique de reconstruire l'espace annihilateur, dont le dual est précisément le sous-groupe caché H. Cette perspective dans le domaine de Fourier explique pourquoi l'algorithme réussit pour les groupes abéliens.",
    "D": "Chaque mesure produit un membre échantillonné uniformément du sous-groupe caché H lui-même, tiré de la distribution plate sur tous les éléments satisfaisant la propriété de fermeture du sous-groupe. La transformée de Fourier quantique agit comme un opérateur de projection qui filtre les éléments non-sous-groupe, garantissant que seuls les membres valides de H apparaissent dans les statistiques de mesure. Répéter cet échantillonnage construit une image empirique de l'appartenance à H sans avoir besoin de comprendre sa structure algébrique.",
    "solution": "C"
  },
  {
    "id": 66,
    "question": "Dans les implémentations pratiques des protocoles de partage de secret quantique, quelle technique avancée offre la garantie de sécurité la plus forte contre la tricherie interne et l'écoute externe ? Considérez des scénarios où les participants peuvent colluder ou où le bruit du canal pourrait masquer un comportement adversarial. La technique doit gérer à la fois la reconstruction par seuil et maintenir la vérifiabilité tout au long de l'exécution du protocole.",
    "A": "Les schémas de rampe quantiques avec authentification offrent des niveaux de sécurité gradués où la fuite d'information diminue continuellement à mesure que davantage de parts sont combinées. Ces protocoles incorporent des étiquettes d'authentification quantique qui permettent aux participants de vérifier l'intégrité des parts individuelles, détectant certaines formes de tricherie. Cependant, des attaques de collusion sophistiquées peuvent exploiter cela en se coordonnant pour soumettre des parts authentifiées mais collectivement incohérentes.",
    "B": "Les schémas de seuil quantiques avec correction d'erreurs offrent une sécurité robuste en distribuant les parts entre plusieurs parties et en appliquant des codes correcteurs d'erreurs quantiques pour protéger contre le bruit du canal. Bien que ces protocoles excellent à maintenir l'intégrité des données, ils supposent généralement que les participants suivent honnêtement le protocole pendant la reconstruction, et les mécanismes de correction d'erreurs peuvent en fait masquer les comportements de tricherie en traitant les modifications malveillantes comme indiscernables du bruit légitime.",
    "C": "Le partage de secret quantique homomorphe permet le calcul sur des parts chiffrées sans nécessiter de déchiffrement, permettant aux participants d'effectuer des opérations directement sur leurs états quantiques distribués tout en maintenant la confidentialité pendant la phase de calcul. Cependant, la propriété homomorphe elle-même ne renforce pas intrinsèquement la sécurité de la phase initiale de distribution des parts ni ne fournit de mécanismes pour vérifier que les participants exécutent honnêtement les opérations prescrites.",
    "D": "Les protocoles de partage de secret quantique vérifiables qui combinent des systèmes de preuve interactifs avec des codes d'authentification quantique garantissent à la fois l'exactitude et la sécurité contre les participants malveillants. Ces protocoles emploient généralement des tours de vérification d'intrication et des schémas d'engagement classiques pour détecter les tentatives de tricherie tout en maintenant des bornes de sécurité théoriques de l'information même lorsqu'un sous-ensemble de participants collude avec un espion externe. La vérification interactive permet aux parties honnêtes de contester un comportement suspect pendant les phases de distribution et de reconstruction, fournissant des garanties de détection qui restent valides dans des conditions de bruit réalistes tout en gérant le contrôle d'accès basé sur des seuils.",
    "solution": "D"
  },
  {
    "id": 67,
    "question": "Quel est le rôle des tests SWAP dans l'apprentissage automatique quantique ?",
    "A": "Détection d'erreurs matérielles via des mécanismes de parité qui échangent des ancillas avec des qubits de données pour vérifier l'intégrité et détecter les erreurs de bit-flip en comparant les résultats de mesure.",
    "B": "Échanger des informations entre les processeurs quantiques et classiques en échangeant physiquement la représentation de l'état quantique avec son encodage de distribution de probabilité classique, permettant aux algorithmes hybrides de transférer les paramètres appris de manière bidirectionnelle à travers l'interface quantique-classique pendant chaque itération d'entraînement.",
    "C": "Implémenter des opérations d'intrication en échangeant systématiquement les positions des qubits pour créer des motifs d'interférence contrôlés, ce qui génère les corrélations nécessaires pour construire des états intriqués multi-qubits à partir de qubits initialement séparables à travers une séquence de portes SWAP adjacentes.",
    "D": "Mesurer la similarité entre états quantiques en effectuant des opérations SWAP contrôlées suivies de mesures d'interférence, ce qui permet d'estimer probabilistiquement le produit scalaire entre deux états quantiques inconnus à travers des essais répétés, fournissant une accélération quadratique par rapport aux méthodes classiques pour certaines tâches de comparaison d'états.",
    "solution": "D"
  },
  {
    "id": 68,
    "question": "Pourquoi le problème du sous-groupe caché pour les groupes non-abéliens nécessite-t-il souvent des mesures intriquées ?",
    "A": "Les mesures conjointes révèlent des corrélations entre représentants de cosets qui sont encodées à travers plusieurs registres dans l'état quantique transformé de Fourier, et ces corrélations ne deviennent accessibles que par des bases de mesure intriquées qui couplent les registres ensemble.",
    "B": "Les représentations irréductibles des groupes non-abéliens décomposent la sortie de la transformée de Fourier quantique en amplitudes à valeurs matricielles distribuées à travers les sous-espaces de registres, et les mesures séparables sur un seul registre projettent sur les indices de lignes ou colonnes indépendamment, détruisant les cohérences hors-diagonales qui encodent l'information d'appartenance au sous-groupe. Les mesures intriquées couplent ces indices conjointement, extrayant les corrélations d'éléments matriciels qui distinguent différents cosets dans le même espace de représentation irréductible.",
    "C": "La transformée de Fourier quantique sur les groupes non-abéliens produit des superpositions où les phases relatives entre états de base computationnelle encodent la structure de classe de conjugaison plutôt que des éléments de groupe individuels, et les mesures séparables effondrent ces phases indépendamment à travers les registres sans préserver leurs relations mutuelles. Les bases de mesure intriquées s'alignent avec la décomposition de classe de conjugaison en projetant sur des états propres conjoints d'opérateurs de classe, extrayant ainsi les corrélations de phase inter-registres qui révèlent quelles classes de conjugaison appartiennent au sous-groupe caché versus l'espace quotient.",
    "D": "Les représentants de cosets dans les instances de sous-groupe caché non-abéliens apparaissent comme des produits tensoriels d'éléments de groupe distribués à travers plusieurs registres quantiques, et la propriété de fermeture du sous-groupe se manifeste comme une intrication entre ces registres après application de la transformée de Fourier quantique. Les mesures séparables sur des registres individuels marginalisent sur ces corrélations, donnant des distributions uniformes qui ne contiennent aucune information sur le sous-groupe, tandis que les mesures intriquées en base de Bell préservent la structure multiplicative du sous-groupe en révélant quelles paires de registres contiennent des éléments de groupe satisfaisant la relation de fermeture g₁g₂ ∈ H pour le sous-groupe caché H.",
    "solution": "A"
  },
  {
    "id": 69,
    "question": "Quelle technologie répond le mieux au goulot d'étranglement du post-traitement dans les systèmes de distribution quantique de clés à haute vitesse ?",
    "A": "Les clusters de calcul distribué avec interfaces de passage de messages mettent à l'échelle la charge de travail de post-traitement sur plusieurs nœuds, distribuant les calculs de correction d'erreurs en utilisant des bibliothèques MPI pour gérer l'espace de clés en croissance exponentielle. En partitionnant la clé tamisée en segments et en assignant chacun à un nœud de calcul dédié, cette approche atteint théoriquement une accélération linéaire proportionnelle à la taille du cluster, bien que la latence inter-nœuds devienne souvent le facteur limitant en pratique.",
    "B": "L'accélération GPU offre un débit supérieur pour le post-traitement QKD en exploitant des milliers de cœurs CUDA pour paralléliser les étapes de réconciliation d'information et d'amplification de confidentialité. Les capacités massives de calcul en virgule flottante des GPU modernes permettent la correction d'erreurs en temps réel sur des flux de clés brutes multi-Gbps, avec des frameworks comme OpenCL permettant une implémentation efficace du protocole cascade à travers les blocs de threads.",
    "C": "Les processeurs ASIC offrent une efficacité énergétique inégalée pour le post-traitement QKD en câblant les multiplications de matrices de Toeplitz utilisées dans le hachage universel directement dans les portes en silicium, atteignant une latence déterministe inférieure à 10 nanosecondes par bloc. Cependant, les longs cycles de développement et les coûts NRE élevés rendent les ASIC impratiques pour s'adapter aux protocoles de réconciliation en évolution ou supporter simultanément plusieurs standards QKD.",
    "D": "L'implémentation FPGA fournit une accélération matérielle spécialisée pour le post-traitement QKD via des circuits logiques dédiés optimisés pour les opérations au niveau des bits dans la correction d'erreurs et l'amplification de confidentialité. Le tissu reconfigurable permet le traitement parallèle de plusieurs blocs de clés simultanément tout en maintenant une faible latence, avec des FPGA modernes atteignant un débit multi-Gbps grâce à des architectures en pipeline qui exécutent les protocoles de réconciliation en temps réel sans surcharge logicielle.",
    "solution": "D"
  },
  {
    "id": 70,
    "question": "L'approximation du polynôme de Jones d'un entrelacs à la plupart des racines de l'unité est BQP-complète car l'évaluation peut être mappée à quel type de circuit quantique ?",
    "A": "Circuits quantiques topologiques implémentant les statistiques de tressage d'anyons non-abéliens — spécifiquement, le circuit utilise des modèles d'anyons de Fibonacci où le polynôme de Jones à e^(2πi/5) peut être calculé par des opérations de tressage qui correspondent naturellement à des portes quantiques. Chaque croisement dans le diagramme d'entrelacs se mappe à une opération de tressage d'anyon qui agit comme une porte unitaire, et l'opération de trace devient une mesure de la charge topologique. Cependant, cette construction nécessite d'encoder chaque anyon dans plusieurs qubits en utilisant des codes de correction d'erreurs quantiques qui simulent la protection topologique, rendant la surcharge substantielle mais toujours polynomiale.",
    "B": "Simule le mot de tresse en utilisant des portes de phase contrôlées — spécifiquement, le circuit implémente la représentation de l'algèbre de Temperley-Lieb en utilisant des rotations à un qubit et des portes de phase à deux qubits arrangées pour refléter les générateurs du groupe de tresses. Chaque croisement dans le diagramme d'entrelacs correspond à une porte unitaire agissant sur des qubits adjacents, et l'opération de trace nécessaire pour l'évaluation du polynôme est implémentée en mesurant l'état quantique final. La connexion entre ces circuits quantiques et l'évaluation du polynôme de Jones aux racines de l'unité fournit une réduction directe prouvant la BQP-complétude.",
    "C": "Ensembles de portes universelles contenant des portes Hadamard et Toffoli arrangées pour encoder la représentation du groupe de tresses via la matrice de Burau évaluée à la racine de l'unité appropriée. La profondeur du circuit évolue linéairement avec le nombre de croisements, et chaque générateur de tresse σᵢ devient une composition de portes Hadamard sur les qubits i et i+1 suivie d'une porte Toffoli contrôlée sur les deux qubits. L'opération de trace se réduit à mesurer tous les qubits et post-traiter la chaîne de bits classique, bien que cette approche ne fonctionne que pour les entrelacs alternés où la représentation de Burau reste fidèle.",
    "D": "Circuits IQP (instantaneous quantum polynomial-time) constitués de portes diagonales dans la base de Hadamard, où chaque croisement dans le diagramme d'entrelacs devient une porte de rotation ZZ diagonale à deux qubits avec un angle déterminé par la racine de l'unité. La construction fonctionne en initialisant tous les qubits dans des états |+⟩, en appliquant des portes diagonales commutatives correspondant au mot de tresse, puis en mesurant dans la base de Hadamard. La valeur du polynôme de Jones à e^(2πi/k) émerge des statistiques de mesure, spécifiquement du calcul du permanent d'une matrice dont les entrées sont dérivées des résultats de mesure, bien que cela nécessite un post-traitement avec un calcul classique #P-difficile qui rend paradoxalement l'algorithme global inefficace malgré que le circuit quantique soit efficacement implémentable.",
    "solution": "B"
  },
  {
    "id": 71,
    "question": "Dans le contexte de l'exécution distribuée de circuits quantiques, quel est l'objectif de la distinction entre les plans de planification et de réseau ?",
    "A": "Sépare les opérations logiques du circuit du routage d'intrication des qubits physiques, permettant au planificateur d'optimiser le timing d'exécution des portes et l'allocation des ressources en fonction des dépendances du circuit tandis que le plan de réseau gère indépendamment la génération, la distribution et la consommation des liens d'intrication distants. Ce découplage permet à chaque plan de fonctionner de manière asynchrone avec des protocoles spécialisés — le planificateur peut réordonner les portes commutatives pour réduire la profondeur tandis que la couche réseau gère la génération de paires EPR et la fidélité sans nécessiter de synchronisation au niveau des portes.",
    "B": "Cette distinction permet au plan de planification de traiter les liens d'intrication distants comme des ressources abstraites avec des coûts associés en latence et fidélité, permettant aux algorithmes de compilation de circuits d'optimiser l'ordonnancement des portes en fonction des graphes de dépendances sans nécessiter une connaissance en temps réel de la congestion du réseau ou des chemins de routage photonique. Le plan de réseau gère ensuite indépendamment la génération d'intrication en utilisant des protocoles comme l'interférence de photons annoncés ou le couplage déterministe ion-photon, mettant en mémoire tampon les paires EPR dans des mémoires quantiques jusqu'à ce que le planificateur les consomme. En découplant ces fonctions, le système peut mettre en pipeline les opérations de portes distantes : pendant que le planificateur exécute des portes locales sur un module, le plan de réseau pré-génère les liens d'intrication nécessaires pour les opérations distantes futures.",
    "C": "La séparation des plans de planification et de réseau permet à la correction d'erreurs quantiques de fonctionner au niveau du qubit logique indépendamment de la gestion des ressources d'intrication physiques, garantissant que les circuits d'extraction de syndrome s'exécutent sur des calendriers déterministes indépendamment des pannes transitoires de liens réseau ou des retards de génération d'intrication. Le plan de réseau régénère continuellement les paires EPR dégradées en utilisant des protocoles de purification d'intrication, tandis que le planificateur traite les qubits logiques comme toujours disponibles avec une fidélité uniforme, s'appuyant sur la couche réseau pour maintenir un tampon suffisant de paires de Bell à haute fidélité pour satisfaire la cadence de mesure de syndrome requise par le surface code ou d'autres schémas de correction d'erreurs topologiques.",
    "D": "Cette séparation architecturale prend en charge l'exécution parallèle d'opérations distantes non commutatives sur des modules distribués en permettant au plan de planification de suivre quels qubits partagent des ressources d'intrication tandis que le plan de réseau gère les paires de Bell physiques indépendamment. Lorsque deux portes distantes ciblent des sous-ensembles de qubits qui se chevauchent mais opèrent dans différentes bases de mesure (par exemple, des vérifications de stabilisateurs X et Z simultanées sur des paires EPR partagées), le planificateur peut émettre les deux opérations simultanément si le plan de réseau a provisionné des liens d'intrication séparés pour chaque porte. Ce parallélisme est critique car sans séparer l'allocation d'intrication (plan de réseau) de l'analyse des dépendances au niveau des portes (plan de planification), les revendications de ressources conflictuelles forceraient un ordonnancement séquentiel inutile de portes commutatives.",
    "solution": "A"
  },
  {
    "id": 72,
    "question": "Quel est l'avantage principal de l'amplification d'amplitude quantique dans les applications d'apprentissage automatique ? Considérez que de nombreux algorithmes d'apprentissage automatique quantique reposent sur une forme d'optimisation ou de recherche, et l'efficacité de ces sous-routines impacte directement la performance globale et l'évolutivité de l'approche quantique par rapport aux méthodes classiques.",
    "A": "Accélération quadratique dans la recherche d'espaces de solutions non structurés, réduisant les requêtes d'oracle de O(N) à O(√N), mais l'avantage pratique en apprentissage automatique dépend de manière critique du maintien de la cohérence tout au long des itérations d'amplification. Chaque application de l'opérateur de Grover accumule des erreurs de porte, et des analyses récentes montrent que sur des dispositifs NISQ avec des erreurs de portes à deux qubits d'environ 0,1%, le point de croisement où la recherche quantique surpasse l'échantillonnage aléatoire classique se produit uniquement pour des tailles de problème N > 10⁸. En dessous de ce seuil, les erreurs accumulées pendant les √N itérations compensent la réduction des requêtes, rendant l'amplification d'amplitude moins efficace que prétendu pour les applications d'apprentissage automatique à court terme.",
    "B": "Réduction quadratique du nombre d'époques d'entraînement requises pour la convergence dans les réseaux de neurones quantiques, abaissant le nombre d'itérations de O(N) classiquement à O(√N) quantiquement lors de la recherche de configurations optimales de paramètres. Cette accélération s'applique spécifiquement à la boucle d'optimisation externe plutôt qu'aux évaluations de gradient individuelles, car l'amplification d'amplitude peut rechercher efficacement l'espace discret des directions possibles de mise à jour des paramètres. La technique est particulièrement précieuse lorsque le paysage de perte contient de nombreux minima locaux, car le processus d'amplification améliore préférentiellement les amplitudes correspondant aux mises à jour de paramètres qui réduisent la fonction de perte, implémentant effectivement un protocole de descente de gradient amélioré quantiquement.",
    "C": "Accélération quadratique pour identifier des caractéristiques ou des motifs de données optimaux en réduisant la complexité d'échantillonnage de O(N) à O(√N) lors de la recherche sur des espaces de caractéristiques exponentiellement grands. Cet avantage est particulièrement significatif dans les méthodes de noyaux quantiques et les machines à vecteurs de support quantiques, où l'amplification d'amplitude accélère la recherche de vecteurs de support en identifiant efficacement les exemples d'entraînement près de la frontière de décision. L'accélération s'applique même lorsque l'espace de caractéristiques a une structure, car le processus d'amplification concentre adaptivement l'amplitude de probabilité sur les régions de l'espace de Hilbert correspondant à une séparation de marge maximale, le rendant plus puissant que les techniques d'optimisation convexe classiques qui évoluent linéairement avec la taille de l'ensemble de données.",
    "D": "Accélération quadratique dans la recherche de solutions ou d'états particuliers au sein d'un espace de recherche non structuré, réduisant le nombre de requêtes d'oracle de O(N) classiquement à O(√N) quantiquement. Cette amélioration est particulièrement précieuse dans l'optimisation d'apprentissage automatique où trouver de bonnes configurations de paramètres ou identifier des caractéristiques pertinentes nécessite d'évaluer de manière répétée des fonctions objectif coûteuses.",
    "solution": "D"
  },
  {
    "id": 73,
    "question": "Quel est l'objectif des techniques de découpage de circuits quantiques (circuit knitting) ?",
    "A": "Elles partitionnent de grands unitaires en produits tensoriels de blocs de sous-circuits plus petits en exploitant la factorisation approximative de la décomposition de Schmidt de l'opération cible, exécutant chaque facteur indépendamment sur des dispositifs séparés et recombinant les sorties par post-traitement classique des corrélations de mesure pondérées par les coefficients de Schmidt, permettant une exécution distribuée sans intrication entre sous-systèmes pendant la phase d'exécution quantique.",
    "B": "Elles décomposent les circuits dépassant les limites de qubits en fragments chevauchants exécutés séquentiellement avec des réinitialisations en cours de circuit, utilisant un transfert d'état médié par ancilla pour propager les états quantiques partiels entre fragments via des protocoles de couture basés sur la téléportation, reconstruisant le calcul complet par des mesures conditionnelles itérées qui préservent la cohérence à travers les frontières des fragments tout en évitant une surcharge classique exponentielle dans le traitement des résultats de mesure.",
    "C": "Elles partitionnent les calculs dépassant la capacité du dispositif en sous-circuits plus petits exécutés sur le matériel disponible, puis reconstruisent classiquement le résultat complet en combinant les statistiques de mesure de ces fragments en utilisant des décompositions de quasi-probabilité, simulant effectivement des systèmes quantiques plus grands que ceux physiquement accessibles.",
    "D": "Elles expriment de grands circuits quantiques comme des combinaisons linéaires de fragments exécutables plus petits en décomposant les portes à plusieurs qubits en sommes de produits tensoriels implémentables sur des sous-ensembles de qubits disjoints, puis échantillonnent à partir de la distribution de quasi-probabilité résultante sur les résultats de fragments pour reconstruire les valeurs d'espérance, avec une surcharge d'échantillonnage évoluant exponentiellement en fonction de la négativité de la représentation de quasi-probabilité découlant de décompositions de portes non locales.",
    "solution": "C"
  },
  {
    "id": 74,
    "question": "Quel est un défi clé dans la synthèse de circuits efficaces pour la simulation hamiltonienne ?",
    "A": "Trouver des séquences de portes qui approximent avec précision e^{-iHt} tout en minimisant la profondeur du circuit et le nombre total de portes, en particulier lorsque l'hamiltonien contient des termes non commutatifs qui nécessitent des techniques de décomposition sophistiquées comme les formules de Trotter-Suzuki ou des méthodes plus avancées telles que la combinaison linéaire d'unitaires, tout en équilibrant le compromis entre l'erreur d'approximation et la surcharge en ressources",
    "B": "Équilibrer la taille du pas de Trotter Δt contre la non-commutativité des termes hamiltoniens : une discrétisation plus fine réduit l'erreur de commutateur accumulée ||[H_j,H_k]||Δt² mais augmente la profondeur du circuit proportionnellement à T/Δt, tandis que des pas plus grossiers produisent des circuits moins profonds mais amplifient les erreurs systématiques de l'expansion de Baker-Campbell-Hausdorff. L'ordre de décomposition optimal dépend de la structure algébrique de Lie de l'hamiltonien — certains systèmes nécessitent des méthodes du quatrième ordre pour atteindre une précision acceptable, multipliant le nombre de portes par ~5×, tandis que d'autres permettent une séparation du second ordre avec une pénalité d'erreur minimale",
    "C": "Pour les hamiltoniens avec des interactions à longue portée H = Σᵢⱼ Jᵢⱼ σᵢσⱼ où les forces de couplage décroissent algébriquement comme Jᵢⱼ ~ |i-j|⁻ᵅ, l'implémentation du graphe d'interaction complet nécessite O(n²) portes SWAP pour acheminer les paires de qubits non locales vers des positions adjacentes pour l'application de portes à deux qubits. Lorsque α<2, le graphe d'interaction devient non planaire et ne peut pas être intégré efficacement dans les topologies matérielles 2D typiques. Cela crée des compromis fondamentaux profondeur-connectivité : soit accepter une surcharge de profondeur O(n) des chaînes SWAP, soit approximer les termes à longue portée, introduisant des erreurs de troncature contrôlables qui doivent être équilibrées contre les coûts de routage",
    "D": "Construire des séquences de portes qui préservent les symétries hamiltoniennes est essentiel pour maintenir la précision de simulation, puisque les erreurs brisant la symétrie s'accumulent de manière cohérente plutôt que stochastique. Lors de la simulation de systèmes avec des symétries continues comme la conservation de charge U(1) ou la rotation de spin SU(2), même de petites imperfections de porte qui violent ces symétries — telles que des fuites hors du sous-espace computationnel ou des erreurs de calibration dans les angles de rotation — font dériver l'état simulé vers des secteurs non physiques de l'espace de Hilbert. Le défi s'intensifie car les outils de compilation standard optimisent pour le nombre de portes sans tenir compte de la préservation de symétrie, nécessitant des algorithmes de décomposition personnalisés qui imposent explicitement les nombres quantiques conservés tout au long du circuit",
    "solution": "A"
  },
  {
    "id": 75,
    "question": "Que signifie qu'une matrice soit stable dans les solveurs d'équations différentielles quantiques ?",
    "A": "Le déterminant de la matrice doit être égal à un tout au long de l'évolution, garantissant que l'unitarité est préservée à chaque pas de temps et que la probabilité reste conservée sous propagation en temps continu.",
    "B": "La trace de la matrice s'annule identiquement pour tous les systèmes hamiltoniens indépendants du temps, reflétant la conservation de l'énergie et garantissant que l'opérateur d'évolution reste sans trace, ce qui est particulièrement important dans les systèmes quantiques ouverts où la dynamique lindblادienne nécessite que la composante dissipative ait une trace nulle pour une normalisation appropriée de l'évolution de la matrice de densité.",
    "C": "Toutes les valeurs propres ont des parties réelles négatives, garantissant que les erreurs numériques décroissent plutôt que de croître exponentiellement pendant l'évolution temporelle, ce qui est essentiel pour la stabilité à long terme dans les schémas d'intégration d'équations différentielles.",
    "D": "Ses entrées doivent rester bornées par la taille du système, garantissant que la propagation numérique ne produit pas de conditions de débordement même lors de l'évolution d'états sur de longs intervalles de temps ou lors de l'application de méthodes de séparation d'ordre élevé qui impliquent des opérateurs intermédiaires non physiques avec des éléments matriciels potentiellement grands.",
    "solution": "C"
  },
  {
    "id": 76,
    "question": "Comment les désaccords de forme d'onde peuvent-ils être utilisés dans une attaque ?",
    "A": "En introduisant des déviations calibrées d'amplitude ou de phase dans les formes d'onde des impulsions de contrôle qui accumulent systématiquement des erreurs cohérentes à travers les séquences de portes, un attaquant peut biaiser les résultats de calcul vers des distributions de mesure spécifiques tout en maintenant les fidélités des portes individuelles dans des plages acceptables.",
    "B": "Un adversaire peut délibérément modifier les formes d'enveloppe des impulsions de contrôle pour qu'elles dévient des formes d'onde calibrées, induisant ainsi des rotations de qubit non intentionnelles qui diffèrent des opérations de porte cibles tout en restant suffisamment subtiles pour échapper à une détection immédiate.",
    "C": "Les formes d'onde désaccordées altèrent la fréquence de Rabi prévue pendant l'évolution pilotée du qubit, causant des erreurs de sur-rotation ou de sous-rotation qui se composent multiplicativement à travers les couches du circuit, permettant à un adversaire d'ingénier des biais computationnels spécifiques qui échappent à la détection par les protocoles de benchmarking randomisé.",
    "D": "Les désaccords de forme d'onde introduisent des erreurs de phase déterministes qui se propagent à travers les portes intriquantes pour créer des biais contrôlés dans les fidélités des états de Bell, permettant à un attaquant de dégrader sélectivement des chemins computationnels spécifiques tout en maintenant les métriques de performance moyenne des portes dans les tolérances de calibration.",
    "solution": "B"
  },
  {
    "id": 77,
    "question": "Quel risque de sécurité spécifique émerge de la dérive de calibration dans les processeurs quantiques ?",
    "A": "Le décalage de biais de mesure introduit des erreurs systématiques dans la fidélité de lecture qui s'accumulent asymétriquement au fil du temps, causant la migration progressive du seuil de discrimination entre les états |0⟩ et |1⟩ vers un état de base. Cette dérive temporelle du classificateur de lecture crée des fenêtres de vulnérabilité où un adversaire peut prédire les résultats de mesure avec une précision supérieure au hasard en synchronisant ses attaques avec les périodes de biais maximal, brisant effectivement l'uniformité supposée des statistiques de mesure sur laquelle reposent les garanties de sécurité de nombreux protocoles quantiques.",
    "B": "La dégradation de la fidélité des portes au fil du temps crée des vulnérabilités exploitables car les paramètres des impulsions de contrôle deviennent de plus en plus désalignés avec l'hamiltonien du système en évolution, bien que cela se manifeste comme du bruit général plutôt que comme des motifs structurés.",
    "C": "L'instabilité de fréquence des qubits fait fluctuer les valeurs propres d'énergie des qubits individuels en raison du bruit de charge et du bruit de flux dans les circuits supraconducteurs, conduisant à un désaccord des conditions de résonance requises pour des portes quantiques à haute fidélité. Lorsque les fréquences des qubits dérivent de leurs valeurs calibrées, les formes d'impulsion soigneusement conçues qui ont été optimisées lors de la routine de calibration la plus récente deviennent progressivement désalignées avec l'hamiltonien réel du système, réduisant la performance des portes et créant potentiellement des fenêtres temporelles exploitables où un attaquant peut prédire quand les erreurs de porte seront maximisées.",
    "D": "Des motifs d'erreur prévisibles émergent lorsque la dérive de calibration cause des déviations systématiques dans les implémentations de portes qui évoluent de manière déterministe entre les cycles de recalibration, permettant aux adversaires de modéliser les caractéristiques d'erreur dépendantes du temps et d'exploiter les fenêtres temporelles où des opérations spécifiques présentent des modes de défaillance connus. Ces erreurs structurées créent des vulnérabilités car les attaquants peuvent prédire quand et comment les portes dévieront du comportement idéal, permettant des attaques ciblées qui exploitent la corrélation entre le temps écoulé depuis la calibration et les motifs de dégradation de la performance des portes.",
    "solution": "D"
  },
  {
    "id": 78,
    "question": "Pourquoi la « profondeur d'état cluster » est-elle égale à un dans les modèles basés sur la mesure, même pour des algorithmes complexes ?",
    "A": "Toutes les portes CZ sont préparées hors ligne dans l'état cluster initial ; le calcul procède entièrement par des mesures adaptatives de qubits individuels dont les bases dépendent des résultats précédents, de sorte que la profondeur du circuit quantique au sens conventionnel du modèle de portes s'effondre vers l'unique état de ressource intriqué, tandis que la complexité algorithmique se manifeste dans le contrôle classique de rétroaction déterminant les angles de mesure plutôt que dans les couches de portes séquentielles.",
    "B": "Toutes les opérations intriquantes sont préparées hors ligne dans l'état cluster initial ; le calcul procède entièrement par des mesures adaptatives de qubits individuels implémentant des portes virtuelles via téléportation, de sorte que la profondeur du circuit quantique au sens conventionnel s'effondre vers le tour de préparation de l'état de ressource, tandis que la complexité algorithmique se manifeste dans la rétroaction classique déterminant les bases de mesure. Cependant, la profondeur effective du circuit égale la plus longue chaîne de dépendance de mesure, pas la profondeur de l'état cluster, qui compte seulement les couches d'intrication nécessaires pour préparer l'état de graphe avant que les mesures ne commencent.",
    "C": "Toutes les portes unitaires sont encodées dans la géométrie de l'état cluster initial ; le calcul procède par des mesures de qubits individuels qui projettent l'état le long de chemins computationnels prédéterminés par la structure du graphe, de sorte que la profondeur du circuit quantique au sens conventionnel s'effondre vers la préparation unique de l'état de ressource, tandis que la complexité algorithmique se manifeste dans le choix des qubits à mesurer plutôt que dans les angles de mesure. La dimension de liaison de l'état cluster détermine directement la puissance computationnelle—les algorithmes polynomiaux nécessitent une dimension de liaison constante tandis que les accélérations exponentielles nécessitent une dimension de liaison évoluant avec la taille du problème, mais la profondeur reste unitaire car l'ordre de mesure n'affecte pas les résultats finaux.",
    "D": "Toutes les corrélations à deux qubits sont établies hors ligne dans l'état cluster initial ; le calcul procède par des mesures adaptatives de qubits individuels dont les résultats déterminent les bases ultérieures, de sorte que la profondeur du circuit quantique au sens du modèle de portes s'effondre vers l'état de ressource intriqué, tandis que la complexité algorithmique se manifeste dans la topologie du motif de mesure. La profondeur de l'état cluster égale un car elle est définie comme le nombre chromatique du graphe de dépendance de mesure projeté sur le réseau de qubits physiques—même si les couches temporelles de mesure s'étendent sur de nombreux tours, chaque tranche spatiale de mesures simultanées commutant compte comme profondeur un dans le formalisme MBQC.",
    "solution": "A"
  },
  {
    "id": 79,
    "question": "Pourquoi la traduction directe des codes de correction d'erreur classiques (ECC) en informatique quantique est-elle non triviale ?",
    "A": "Alors que les codes classiques reposent sur la redondance par duplication de données, le théorème de non-clonage interdit explicitement la copie d'états quantiques arbitraires, rendant les stratégies naïves de réplication impossibles. De plus, la correction d'erreur classique traite des erreurs discrètes de type unique (retournements de bit), tandis que les systèmes quantiques souffrent de processus d'erreur continus qui se manifestent simultanément comme composantes de retournement de bit et de retournement de phase, nécessitant des protocoles de mesure de syndrome et de correction fondamentalement différents qui préservent la superposition tout au long du cycle de correction d'erreur.",
    "B": "Le principe d'incertitude de Heisenberg établit que toute tentative de mesurer un état quantique à des fins de duplication perturbera inévitablement l'observable complémentaire, détruisant ainsi l'information de phase encodée dans le qubit. Cette contrainte fondamentale signifie que les schémas de redondance classiques, qui dépendent de la création de copies identiques pour le vote majoritaire, ne peuvent pas être directement appliqués à l'information quantique car l'acte de copier effondrerait la superposition, effaçant les propriétés quantiques mêmes que le code vise à protéger.",
    "C": "Contrairement aux bits classiques qui représentent des valeurs binaires discrètes, les ordinateurs quantiques traitent l'information comme des amplitudes de probabilité continues distribuées sur des trajectoires de la sphère de Bloch, nécessitant des mécanismes de correction d'erreur analogiques plutôt que des vérifications de parité numériques.",
    "D": "Le théorème de non-clonage empêche la duplication arbitraire d'états quantiques, éliminant les stratégies de redondance classiques, tandis que les erreurs quantiques se produisent comme des processus continus affectant simultanément les degrés de liberté de retournement de bit et de retournement de phase. De plus, la détection d'erreur basée sur la mesure doit préserver la superposition quantique par extraction de syndrome plutôt que par observation directe des états de qubit, distinguant fondamentalement les codes quantiques de leurs homologues classiques qui peuvent librement mesurer et copier les bits de données.",
    "solution": "D"
  },
  {
    "id": 80,
    "question": "Pourquoi la diaphonie est-elle particulièrement problématique pour les ordinateurs quantiques à grande échelle ?",
    "A": "À mesure que le nombre de qubits augmente linéairement, la diaphonie croît superlinéairement et finit par faire en sorte que tous les qubits du processeur deviennent mutuellement intriqués les uns avec les autres par des couplages hamiltoniens non intentionnels, créant un état intriqué global à N corps qui rend les opérations de portes individuelles incontrôlables. Cette intrication totale émerge parce que les forces de couplage de diaphonie évoluent avec la densité de qubits, produisant un réseau exponentiellement complexe d'interactions parasites qui submerge toute tentative d'adressage sélectif ou de contrôle indépendant des qubits computationnels individuels.",
    "B": "La diaphonie est purement un problème matériel résultant du couplage électromagnétique entre les lignes de contrôle et les modes de résonateur, et elle ne peut pas être atténuée par des techniques logicielles telles que la mise en forme d'impulsion, le découplage dynamique ou la calibration de porte à résonance croisée.",
    "C": "L'augmentation à des centaines ou des milliers de qubits rend la diaphonie complètement indétectable et non mesurable puisque ces interactions parasites se produisent selon des motifs totalement aléatoires qui se moyennent sur de grands ensembles, s'annulant effectivement par symétrie statistique. Cette propriété d'auto-moyennage émerge naturellement dans les systèmes au-delà d'environ 100 qubits, où la loi des grands nombres garantit que les erreurs de phase induites par la diaphonie contribuent un effet net négligeable aux fidélités agrégées des portes, permettant aux dispositifs à grande échelle de fonctionner sans caractérisation explicite de la diaphonie.",
    "D": "Plus de qubits signifie plus d'interactions non désirées entre systèmes quantiques voisins, augmentant les taux d'erreur cumulés à mesure que les couplages parasites s'accumulent. À mesure que le nombre de qubits augmente, le nombre pur de chemins potentiels de diaphonie croît quadratiquement, rendant la calibration et l'atténuation complètes de plus en plus difficiles et finalement impraticables sans changements architecturaux comme une isolation améliorée ou des topologies de connectivité clairsemée.",
    "solution": "D"
  },
  {
    "id": 81,
    "question": "Considérez un algorithme quantique variationnel s'exécutant sur du matériel NISQ actuel où le circuit a été partitionné en plusieurs sous-circuits utilisant une technique de découpage classique. Le partitionnement initial était basé sur des paramètres de bruit estimés à partir de données de calibration du dispositif prises 6 heures avant l'exécution. Pendant l'exécution, une surveillance en temps réel révèle que certains temps de cohérence des qubits se sont significativement dégradés, rendant certains sous-circuits moins favorables que prévu initialement. Pourquoi le redécoupage dynamique pendant l'exécution est-il parfois utilisé dans de tels scénarios ?",
    "A": "Le redécoupage dynamique permet de rééquilibrer la charge classique en déplaçant les sous-circuits intensifs en calcul vers des backends de simulation lorsque la qualité du matériel quantique tombe en dessous de seuils acceptables. Lorsque les temps de cohérence se dégradent de manière inattendue, la stratégie de partitionnement migre les sous-circuits problématiques vers des simulateurs classiques de réseaux de tenseurs qui peuvent maintenir la fidélité grâce à une évolution unitaire exacte. Cette approche hybride en temps réel échange la consommation de ressources quantiques contre une charge de calcul classique, permettant à l'algorithme de se terminer avec succès malgré la dégradation temporelle du matériel en exploitant la précision du simulateur pour les sous-circuits à faible intrication.",
    "B": "Il adapte la décomposition du circuit pour acheminer préférentiellement les portes d'intrication multi-qubits à travers les paires de qubits présentant des fidélités de porte à deux qubits supérieures, mesurées lors de la calibration en temps réel. Lorsque les temps de cohérence se dégradent de manière inattendue, la stratégie de partitionnement peut migrer les sous-circuits riches en CNOT vers des régions de connectivité plus performantes tout en acceptant un surcoût accru en SWAP. Cette optimisation en temps réel redistribue les opérations de porte sur la topologie du dispositif en fonction de la caractérisation actualisée du bruit, échangeant la profondeur de compilation contre la qualité instantanée du matériel pour maintenir la fidélité algorithmique globale malgré les variations temporelles du dispositif.",
    "C": "Le redécoupage dynamique compense la diaphonie en isolant spatialement les exécutions simultanées de sous-circuits dans des régions de qubits non adjacentes lorsque la surveillance du dispositif détecte des taux d'erreur élevés dus à des opérations de porte concurrentes. Le système redistribue les portes pour maximiser la séparation physique entre les qubits actifs, réduisant les erreurs cohérentes dues au couplage parasite. Cette optimisation spatiale en temps réel s'adapte aux signatures de diaphonie variables dans le temps qui émergent de la dérive thermique ou des interférences des lignes de contrôle, maintenant la fidélité de l'algorithme en échangeant le parallélisme d'exécution contre les corrélations d'erreurs qui se développent pendant les sessions expérimentales prolongées.",
    "D": "Il s'adapte aux taux de bruit mesurés et rééquilibre la charge classique en redistribuant les opérations de porte entre les sous-circuits en fonction de la caractérisation actualisée du dispositif. Lorsque les temps de cohérence se dégradent de manière inattendue, la stratégie de partitionnement peut déplacer les opérations gourmandes en profondeur vers des régions de qubits plus performantes tout en acceptant des coûts accrus de post-traitement classique. Cette optimisation en temps réel échange la consommation de ressources quantiques contre une charge de calcul classique pour maintenir la fidélité globale de l'algorithme malgré la qualité variable du matériel dans le temps.",
    "solution": "D"
  },
  {
    "id": 82,
    "question": "Que se passerait-il si l'étape de communication classique était omise après une mesure d'état de Bell dans la téléportation quantique ?",
    "A": "L'intrication de la paire de Bell partagée agit comme un canal quantique pré-établi qui transmet directement les coefficients du vecteur d'état du qubit d'Alice à Bob instantanément lors de la mesure, exploitant les corrélations non locales pour transférer les valeurs α et β sans échange d'information classique. Puisque la paire EPR encode déjà une structure de corrélation parfaite couvrant la séparation spatiale, le qubit de Bob subit une transformation spontanée vers l'état cible lorsqu'Alice effectue sa mesure — l'effondrement de la fonction d'onde se propage superluminalement à travers le lien intriqué. Cela ne viole les théorèmes de non-communication que si nous supposons des contraintes de localité, mais le protocole de téléportation démontre fondamentalement que l'information quantique peut traverser des distances arbitraires par pure intrication, l'étape de communication classique n'étant qu'un mécanisme de vérification redondant plutôt qu'un composant nécessaire au transfert d'état.",
    "B": "Sans les deux bits classiques spécifiant lequel des quatre résultats de mesure de Bell Alice a observé (|Φ+⟩, |Φ-⟩, |Ψ+⟩, ou |Ψ-⟩), Bob ne peut pas appliquer la rotation de Pauli corrective correspondante à son qubit, le laissant dans un état intermédiaire qui ressemble partiellement à l'état cible. Le processus de téléportation réussit toujours à transférer l'information quantique à travers le lien intriqué, mais le qubit de Bob reste encodé dans une base tournée nécessitant la correction manquante — cela entraîne des erreurs systématiques lors de la mesure, avec une fidélité F ≈ 0,75 en moyenne.",
    "C": "Le qubit de Bob se retrouve dans l'un des quatre états possibles avec une probabilité égale, déterminé par le résultat de la base de Bell qu'Alice a mesuré. Sans connaître le résultat de la mesure d'Alice par communication classique, Bob ne peut pas appliquer l'opération de Pauli corrective appropriée, laissant son qubit dans un état effectivement aléatoire qui se moyenne en une matrice de densité maximalement mélangée ρ = I/2 sans information quantique utile préservée.",
    "D": "Le qubit de Bob s'effondre vers |0⟩ ou |1⟩ avec des probabilités correspondant aux coefficients d'amplitude de l'état original |α|² et |β|², préservant les statistiques de population classiques tout en perdant l'information de phase relative. La mesure de Bell détruit la cohérence quantique, convertissant la superposition en un mélange classique qui transmet les probabilités de la base de calcul mais élimine la capacité d'interférence et les éléments hors diagonale de la matrice de densité.",
    "solution": "C"
  },
  {
    "id": 83,
    "question": "Pourquoi la réparation de chemin est-elle critique dans la distribution d'intrication à longue distance ?",
    "A": "La dégradation des liens peut se produire pendant les opérations multi-sauts nécessitant un réacheminement. Lorsque des segments de fibre intermédiaires subissent des taux de perte élevés dus à des perturbations physiques ou lorsque des nœuds répéteurs présentent des fidélités d'échange dégradées dues à des erreurs matérielles transitoires, le protocole doit reconfigurer dynamiquement le chemin d'intrication à travers des liens réseau alternatifs pour contourner les sections compromises et maintenir la connectivité de bout en bout sans redémarrer toute la séquence de génération depuis le début.",
    "B": "L'échange d'intrication est non déterministe avec une probabilité de succès P_swap < 1 à chaque nœud, donc lorsqu'une mesure d'état de Bell échoue (indiquée par la détection d'un résultat non maximalement intriqué), ce segment doit être régénéré tandis que les liens voisins déjà établis restent stockés en mémoire quantique. La réparation de chemin identifie quel échange spécifique a échoué via la communication classique des résultats de mesure, puis retente sélectivement uniquement le segment non réussi plutôt que de jeter toute la chaîne, exploitant le fait que les mémoires quantiques peuvent préserver les paires de génération antérieure pour des durées dépassant les temps de nouvelle tentative d'échange unique.",
    "C": "La perte de photons dans la fibre évolue exponentiellement avec la distance selon e^(-αL), faisant que les taux de génération d'intrication entre nœuds distants deviennent infimes, donc les tentatives directes de bout en bout prennent un temps impraticable. La réparation de chemin accélère la distribution en subdivisant le canal en segments plus courts avec une perte acceptable (chacun ~20 km pour la fibre standard), établissant l'intrication sur ces sous-liens en parallèle, puis effectuant des échanges pour les connecter. Lorsque la génération d'un segment individuel échoue en raison de la perte probabiliste de photons, seul ce lien court spécifique doit être régénéré plutôt que de retenter la distance complète, atteignant un surcoût de nouvelle tentative polynomial plutôt qu'exponentiel.",
    "D": "La décohérence due au couplage environnemental s'accumule de manière stochastique dans les mémoires quantiques stockées, chaque qubit subissant des erreurs aléatoires de basculement de phase au taux Γ_dephasing·t. Si la durée du protocole de distribution de bout en bout dépasse le temps de cohérence T₂ des qubits mémoire dans les nœuds intermédiaires, la fidélité de la paire distribuée tombe en dessous des seuils utiles. La réparation de chemin atténue cela en surveillant les temps de cohérence de la mémoire en temps réel et en remplaçant préventivement les segments dont les qubits approchent leurs limites T₂ par des paires fraîchement générées, maintenant la fidélité globale au-dessus des seuils de distillation sans attendre que les erreurs se manifestent réellement et corrompent l'état de bout en bout.",
    "solution": "A"
  },
  {
    "id": 84,
    "question": "Quelle approche d'IA est particulièrement utile pour apprendre des stratégies optimales dans des environnements quantiques dynamiques ?",
    "A": "Les méthodes d'apprentissage non supervisé comme le clustering excellent dans les contextes quantiques car elles peuvent automatiquement découvrir une structure cachée dans les espaces de Hilbert de haute dimension sans nécessiter de données d'entraînement étiquetées, qui sont coûteuses à générer pour les systèmes quantiques.",
    "B": "L'analyse en composantes principales pour la réduction de dimensionnalité s'avère particulièrement efficace car les états quantiques vivent naturellement dans des espaces exponentiellement grands où la plupart des dimensions contribuent une variance négligeable aux quantités observables, permettant une identification efficace du sous-espace où l'optimisation doit se concentrer pour des gains de performance maximaux avec un surcoût de calcul minimal.",
    "C": "Les algorithmes d'apprentissage par renforcement excellent à découvrir des politiques de contrôle optimales par interaction d'essai-erreur avec les systèmes quantiques, utilisant des signaux de récompense issus des résultats de mesure pour affiner itérativement les stratégies sans nécessiter de connaissance explicite de l'hamiltonien sous-jacent ou de la dynamique du système, les rendant particulièrement bien adaptés à l'optimisation adaptative dans des environnements où l'évolution de l'état quantique est complexe ou seulement partiellement caractérisée.",
    "D": "La régression linéaire standard sur les résultats de mesure fournit le chemin le plus direct vers des stratégies optimales en modélisant la récompense attendue comme une fonction linéaire de la base de mesure et des paramètres de préparation d'état, permettant aux méthodes basées sur le gradient de converger rapidement.",
    "solution": "C"
  },
  {
    "id": 85,
    "question": "Pourquoi les indicateurs clés de performance (KPI) spécifiques au DQC sont-ils nécessaires, et comment contribuent-ils à l'évaluation de différents déploiements ?",
    "A": "Ils suivent la fréquence des signaux de contrôle classiques pendant l'exécution, car minimiser l'interférence classique est central à l'avantage quantique — moins nous avons besoin de boucles de rétroaction classiques, plus nous nous rapprochons d'une vraie accélération quantique.",
    "B": "L'équité entre processeurs est obtenue en normalisant les métriques de performance pour tenir compte des nombres de qubits différents, des ensembles de portes et des graphes de connectivité, garantissant que les benchmarks ne favorisent pas injustement les architectures avec des nombres de qubits natifs plus élevés ou des bibliothèques de portes plus riches. Ces KPI établissent un terrain de jeu équitable en mesurant le volume quantique effectif par ressource physique investie, permettant des comparaisons équitables entre les implémentations supraconductrices, à piège à ions et photoniques malgré leurs paradigmes opérationnels vastement différents.",
    "C": "Les KPI spécifiques au DQC comparent les déploiements en mesurant l'efficacité du routage d'intrication, la fidélité d'exécution des portes non locales et le surcoût de communication réseau, qui sont des caractéristiques uniques des architectures quantiques distribuées qui ne s'appliquent pas aux ordinateurs quantiques monolithiques. Ces métriques capturent à quel point un système gère le transfert d'état quantique à travers les liens réseau et quantifient les ressources supplémentaires consommées par les portes basées sur la téléportation.",
    "D": "La consommation de mémoire et la vitesse de compilation des portes sont les goulots d'étranglement principaux qu'ils adressent, c'est pourquoi ces KPI se concentrent presque entièrement sur l'optimisation logicielle plutôt que sur la performance de la couche physique. En quantifiant le surcoût de compilation et l'empreinte mémoire classique pendant la transpilation de circuit, ces métriques révèlent quelles architectures distribuées peuvent maintenir une latence plus faible dans la pile logicielle de contrôle, impactant directement le débit d'application de bout en bout indépendamment de la qualité du matériel quantique.",
    "solution": "C"
  },
  {
    "id": 86,
    "question": "Quel est l'un des avantages de l'utilisation de générateurs de nombres aléatoires quantiques (QRNG) dans les systèmes de sécurité IoT ?",
    "A": "Le véritable caractère aléatoire des processus quantiques tels que les mesures de polarisation des photons ou la détection homodyne du bruit du vide fournit des clés fondamentalement imprévisibles qui ne peuvent pas être reproduites par des algorithmes classiques, renforçant considérablement la sécurité cryptographique. Contrairement aux générateurs pseudo-aléatoires qui reposent sur des hypothèses de difficulté computationnelle, les QRNG tirent leur entropie de l'effondrement de la superposition quantique, qui est prouvé aléatoire selon l'interprétation de Copenhague. Cependant, la reconstruction de l'état post-mesure par tomographie de mesure faible peut partiellement récupérer la fonction d'onde pré-effondrement, permettant à un adversaire disposant d'une mémoire quantique d'extraire environ 40% de l'entropie originale.",
    "B": "Le véritable caractère aléatoire des processus quantiques tels que les temps d'arrivée des photons ou les fluctuations du vide fournit des clés fondamentalement imprévisibles qui ne peuvent être reproduites ou prédites par aucun algorithme classique, renforçant considérablement la sécurité cryptographique. Contrairement aux générateurs pseudo-aléatoires qui reposent sur des hypothèses de complexité computationnelle, les QRNG tirent leur entropie des résultats de mesures quantiques qui sont intrinsèquement non-déterministes selon la mécanique quantique, rendant les attaques par force brute et l'analyse de motifs mathématiquement impossibles même avec des ressources computationnelles illimitées.",
    "C": "Le véritable caractère aléatoire des processus quantiques tels que la conversion paramétrique descendante spontanée ou le bruit de grenaille des diviseurs de faisceau fournit des clés fondamentalement imprévisibles qui ne peuvent pas être générées algorithmiquement, renforçant considérablement la sécurité cryptographique. Contrairement aux générateurs pseudo-aléatoires qui dépendent de conjectures de complexité non prouvées comme la difficulté de la factorisation d'entiers, les QRNG extraient l'entropie de l'effondrement de la mesure quantique gouverné par la règle de Born. Cela élimine les vulnérabilités de porte dérobée dans les algorithmes déterministes, bien que les implémentations pratiques nécessitent un calibrage minutieux car les comptes sombres des détecteurs et le bruit de post-traitement classique peuvent introduire des corrélations qui réduisent la min-entropie effective en dessous de la limite quantique théorique.",
    "D": "Le véritable caractère aléatoire des processus quantiques tels que la temporisation de la désintégration atomique ou les événements de détection de photons uniques fournit des clés fondamentalement imprévisibles qui résistent aux algorithmes de prédiction classiques, renforçant significativement les protocoles cryptographiques. Contrairement aux générateurs pseudo-aléatoires basés sur la complexité algorithmique, les QRNG exploitent le caractère aléatoire intrinsèque de l'effondrement de la fonction d'onde lors de la mesure, qui est sécurisé au sens de la théorie de l'information sous les théories à variables cachées locales. Le flux binaire quantique brut atteint l'entropie de Shannon complète sans nécessiter d'hypothèses de difficulté computationnelle, bien que l'efficacité finie des détecteurs (typiquement 60-80%) introduise un biais classique qui doit être corrigé par des fonctions extractrices en temps réel pour maintenir l'uniformité.",
    "solution": "B"
  },
  {
    "id": 87,
    "question": "Considérons un algorithme quantique initialement conçu pour s'exécuter sur un seul processeur de 100 qubits, où le calcul implique plusieurs couches de portes qui créent de l'intrication simultanément sur tous les qubits. Vous souhaitez maintenant exécuter cet algorithme sur un réseau quantique distribué composé de quatre processeurs séparés de 25 qubits connectés par des canaux de communication quantiques. Quel est le défi principal pour adapter cet algorithme quantique monolithique à une exécution distribuée, et pourquoi ce défi apparaît-il spécifiquement dans le contexte distribué ?",
    "A": "Les algorithmes quantiques monolithiques nécessitent des interactions fréquentes multi-qubits sur l'ensemble du registre de qubits. Lorsque les qubits sont stockés sur des processeurs quantiques séparés dans un réseau distribué, chaque porte ou mesure inter-processeurs nécessite une téléportation ou une distribution d'intrication à distance sur des canaux quantiques, ce qui introduit une surcharge de communication substantielle, de la latence et des sources supplémentaires de décohérence qui n'étaient pas présentes dans la version monolithique. Les portes entre qubits sur différents nœuds ne peuvent pas être appliquées directement et doivent plutôt être implémentées par consommation d'intrication et cycles de communication classique, augmentant drastiquement à la fois le temps d'exécution et l'accumulation d'erreurs.",
    "B": "Le défi principal est de partitionner l'état quantique global |ψ⟩ sur plusieurs processeurs tout en préservant la structure de décomposition de Schmidt qui caractérise l'intrication entre sous-systèmes. Puisque l'algorithme crée de l'intrication sur les 100 qubits, chaque nœud doit maintenir son sous-système local de 25 qubits dans une matrice densité réduite ρ_i = Tr_{¬i}(|ψ⟩⟨ψ|) qui reste cohérente avec l'état pur global. Cependant, calculer ces traces partielles nécessite des protocoles de tomographie d'état quantique qui évoluent exponentiellement avec la taille du sous-système, et le théorème de non-clonage empêche de distribuer des copies des états intermédiaires pour vérification. Ce maintien de cohérence nécessite la téléportation quantique des résultats de mesure entre nœuds, créant des goulots d'étranglement de communication absents dans les architectures monolithiques.",
    "C": "L'obstacle fondamental est que les portes d'intrication globales dans l'algorithme monolithique couplent tous les 100 qubits de manière cohérente dans l'environnement électromagnétique partagé du processeur, exploitant des sous-espaces collectifs sans décohérence qui suppriment certains canaux d'erreur par des effets de sous-radiance. L'exécution distribuée détruit cette protection collective car les qubits sur des processeurs séparés décohèrent indépendamment sous le bruit local, manquant du mode commun qui permettait la suppression passive des erreurs. Convertir l'algorithme nécessite de remplacer les portes globales par des séquences d'opérations locales et de protocoles d'échange d'intrication qui reconstruisent des corrélations multi-corps équivalentes, mais cette substitution élimine l'avantage de l'absence de décohérence, nécessitant des codes de correction d'erreur actifs pour atteindre une fidélité comparable malgré des caractéristiques de bruit fondamentalement différentes.",
    "D": "Les réseaux quantiques distribués permettent le parallélisme computationnel en assignant des sous-routines indépendantes à chaque processeur de 25 qubits, mais les couches de portes de l'algorithme monolithique créent des dépendances de données qui empêchent une décomposition naïve. Spécifiquement, les opérations d'intrication globales dans la couche L dépendent des résultats de mesure de la couche L-1 sur tous les qubits, formant un graphe acyclique dirigé (DAG) où le calcul de chaque nœud attend les entrées de tous les autres nœuds. Exécuter cette structure de dépendances de manière distribuée nécessite la téléportation de portes non-locales utilisant des paires EPR pré-partagées, consommant une paire de Bell par porte à deux qubits. Le défi est que l'établissement de ces paires EPR exige des ressources d'intrication croissant exponentiellement avec la profondeur du circuit, car maintenir la cohérence de phase entre paires distribuées nécessite des cycles de purification active dont la surcharge évolue comme O(d³) pour une distance d.",
    "solution": "A"
  },
  {
    "id": 88,
    "question": "Considérons le développement de codes LDPC quantiques avec à la fois un taux d'encodage constant et une distance minimale qui croît avec la longueur de bloc. Les premières constructions LDPC classiques ont atteint une distance linéaire, mais le cas quantique a fait face à des obstacles fondamentaux dus à l'interaction entre les stabilisateurs X et Z. Les codes de produit hypergraphe, introduits par Tillich et Zémor, ont représenté une percée en construisant systématiquement des codes quantiques à partir de paires de codes classiques. Pourquoi les codes de produit hypergraphe sont-ils significatifs dans la théorie de la correction d'erreurs quantiques ?",
    "A": "Ils ont prouvé que des codes LDPC quantiques avec un taux constant et une distance évoluant comme sqrt(n) existent, résolvant la question de savoir si de tels codes étaient possibles. Bien que l'évolution en racine carrée soit sous-optimale par rapport aux codes classiques, cela a montré que les codes LDPC quantiques pouvaient atteindre simultanément un taux et une distance non triviaux.",
    "B": "Les codes de produit hypergraphe ont démontré que des codes LDPC quantiques avec un taux constant et une distance évoluant comme n^(2/3) sont constructibles, améliorant les constructions de produit antérieures qui atteignaient seulement une distance logarithmique. Bien qu'encore en deçà de la distance linéaire atteinte par les codes LDPC classiques, l'évolution en n^(2/3) représente une avancée significative car elle dépasse la barrière fondamentale de sqrt(n) que de nombreux chercheurs conjecturaient insurmontable pour les codes quantiques creux. Cette construction a montré que la contrainte CSS quantique sur le chevauchement des stabilisateurs ne doit pas restreindre la distance aussi sévèrement qu'on le pensait auparavant.",
    "C": "La construction de produit hypergraphe établit que les codes LDPC quantiques peuvent atteindre un taux constant avec une distance évoluant comme log²(n), ce qui, bien que logarithmique plutôt que polynomial, suffit pour la tolérance aux pannes pratique car le seuil de tolérance aux pannes pour le calcul quantique dépend exponentiellement de la distance du code. Même une croissance logarithmique de la distance permet des taux d'erreur en dessous du seuil avec des longueurs de bloc réalisables pour les implémentations à court terme. La signification réside dans la preuve que les codes à générateurs creux peuvent simultanément atteindre à la fois un taux non-nul et une distance qui croît sans limite, propriétés que les premières constructions n'ont pas réussi à combiner.",
    "D": "Les codes de produit hypergraphe ont prouvé que des codes LDPC quantiques asymptotiquement bons (taux constant et distance linéaire) ne peuvent pas exister en raison de la structure de complexe de chaînes qu'ils ont révélée : la relation de dualité entre stabilisateurs X et Z crée une contrainte homologique où atteindre une distance linéaire dans les secteurs X et Z simultanément force les poids des stabilisateurs à croître logarithmiquement avec n. Ce résultat d'impossibilité a clarifié le compromis fondamental entre la parcimonie des stabilisateurs, le taux d'encodage et l'évolution de la distance, montrant que la distance sqrt(n) représente une borne optimale pour les codes quantiques à taux constant avec des stabilisateurs de poids constant, résolvant ainsi une question ouverte majeure sur les limites ultimes de la correction d'erreurs quantiques creuses.",
    "solution": "A"
  },
  {
    "id": 89,
    "question": "Dans le contexte des algorithmes quantiques variationnels appliqués aux systèmes de matière condensée avec brisure spontanée de symétrie, quelle contrainte fondamentale limite la capacité des circuits quantiques paramétrés à préparer le véritable état fondamental, et comment la profondeur du circuit interagit-elle avec cette contrainte ?",
    "A": "Briser une symétrie continue ou discrète dans un système quantique macroscopique nécessite d'établir un paramètre d'ordre cohérent qui s'étend sur tous les sites du réseau, qui émerge d'une subtile conspiration de fluctuations quantiques dans tout le volume. Chaque couche de portes paramétrées dans un circuit variationnel ne peut introduire que des perturbations locales qui violent la symétrie dans un voisinage fini, typiquement quelques espacements de réseau.",
    "B": "Les portes quantiques paramétrées standard utilisées dans les circuits variationnels — incluant les rotations de Pauli, les opérations contrôlées et les portes d'intrication — sont construites à partir de transformations unitaires qui respectent les lois de conservation fondamentales encodées dans l'Hamiltonien, telles que le moment angulaire de spin total, le nombre de particules ou la charge. Ces nombres quantiques conservés définissent des secteurs de supersélection qui ne peuvent être connectés par aucune évolution unitaire physique. Lorsqu'un système de matière condensée présente une brisure spontanée de symétrie, le véritable état fondamental réside dans un secteur spécifique de symétrie brisée caractérisé par des nombres quantiques définis (par exemple, magnétisation nette dans un ferromagnétique), mais les circuits variationnels initialisés dans un secteur symétrique (magnétisation nulle) ne peuvent échapper à ce secteur par des opérations de portes.",
    "C": "Dans les phases de matière condensée présentant une brisure spontanée de symétrie, le paysage de la fonction de coût évaluée par les algorithmes quantiques variationnels développe des régions exponentiellement plates autour des états symétriques car les gradients deviennent exponentiellement supprimés avec la taille du système — une manifestation des plateaux arides spécifique aux phases ordonnées. Cela se produit parce que les états fondamentaux de symétrie brisée correspondent à des configurations exponentiellement rares dans l'espace de tous les états quantiques respectant les symétries de l'Hamiltonien, les rendant des cibles vanishingly improbables pour la descente de gradient. Augmenter la profondeur du circuit exacerbe ce problème en élargissant l'espace d'états exprimables, ce qui dilue encore plus la densité d'états de symétrie brisée et fait que l'optimisation stagne exponentiellement rapidement quelle que soit la stratégie d'optimisation, empêchant la convergence vers le véritable état fondamental même avec un temps de calcul classique infini.",
    "D": "La brisure spontanée de symétrie dans les systèmes de matière condensée à la limite thermodynamique nécessite d'établir des corrélations quantiques à portée infinie qui encodent le paramètre d'ordre macroscopique, mais les circuits quantiques paramétrés de profondeur finie ne peuvent générer que des corrélations s'étendant sur une portée spatiale finie déterminée par la structure du cône de lumière de la séquence de portes. Chaque couche de circuit augmente la longueur de corrélation maximale d'au plus la portée d'interaction des portes (typiquement plus proches voisins), donc capturer les véritables états fondamentaux de symétrie brisée avec des corrélations décroissant en loi de puissance ou exponentiellement nécessiterait une profondeur de circuit évoluant extensivement avec la taille du système, rendant l'approche variationnelle impraticable même avec des réglages de paramètres optimaux.",
    "solution": "D"
  },
  {
    "id": 90,
    "question": "Dans l'étude de la contextualité quantique, certains tests fonctionnent pour tout état quantique tandis que d'autres nécessitent des préparations soigneusement choisies. Supposons que vous conceviez une expérience pour démontrer la contextualité dans un système à trois qubits. Quelle distinction fondamentale sépare les tests de contextualité « dépendants de l'état » des tests « indépendants de l'état », et pourquoi cela importe-t-il pour la conception expérimentale ?",
    "A": "Les preuves indépendantes de l'état démontrent la contextualité pour tous les états quantiques du système, éliminant le besoin de préparation d'état précise et en faisant des démonstrations expérimentales plus robustes de la non-classicalité. Les tests dépendants de l'état ne fonctionnent que pour des états préparés particuliers, ce qui nécessite des protocoles de préparation minutieux mais peut parfois atteindre des violations plus fortes des bornes classiques, offrant des avantages lors du test de théories de ressources quantiques spécifiques ou du ciblage de corrélations contextuelles maximales dans des scénarios sur mesure.",
    "B": "Les tests de contextualité dépendants de l'état nécessitent des états quantiques qui saturent des relations de non-commutation spécifiques entre opérateurs de mesure, ce qui signifie que les violations n'émergent que lorsque les valeurs d'espérance atteignent des configurations extrêmes prédites par les principes d'incertitude. Les tests indépendants de l'état exploitent les propriétés théoriques de graphes des structures de compatibilité de mesure qui se manifestent indépendamment de l'état quantique, les rendant robustes aux erreurs de préparation. Expérimentalement, les tests dépendants de l'état exigent une ingénierie d'état à haute fidélité pour atteindre le régime opérationnel où les témoins de contextualité dépassent les seuils classiques.",
    "C": "La caractéristique distinctive est que les preuves de contextualité indépendantes de l'état reposent sur des contraintes algébriques parmi les probabilités de résultats de mesure qui tiennent dans tout l'espace de Hilbert, tandis que les tests dépendants de l'état exploitent des témoins d'intrication spécifiques à des structures de superposition particulières. Les violations indépendantes de l'état apparaissent dans des relations de valeurs d'espérance que les théories classiques à variables cachées ne peuvent reproduire pour aucun état quantique, tandis que les tests dépendants de l'état atteignent des violations plus importantes mais seulement lorsque l'état préparé présente une cohérence suffisante entre les composantes de la base computationnelle.",
    "D": "Les expériences de contextualité dépendantes de l'état démontrent des violations uniquement lorsque l'état préparé présente des valeurs négatives de la fonction de Wigner dans des régions spécifiques de l'espace des phases, puisque la contextualité découle fondamentalement de la non-classicalité des représentations quasi-probabilistes. Les tests indépendants de l'état contournent cette exigence en utilisant des mesures dont la structure de commutation seule garantit des violations, indépendamment des propriétés de l'espace des phases de l'état. Expérimentalement, les tests dépendants de l'état nécessitent donc une vérification tomographique de la négativité de Wigner avant que les mesures de contextualité ne commencent.",
    "solution": "A"
  },
  {
    "id": 91,
    "question": "Pourquoi un algorithme de routage pourrait-il favoriser un chemin légèrement plus long ?",
    "A": "Meilleure intrication de bout en bout grâce à des fidélités de liaison plus élevées, car la fidélité cumulative le long d'un chemin de réseau quantique dépend multiplicativement de la qualité individuelle de chaque segment, et sélectionner une route avec des liaisons constamment de haute fidélité — même si elle implique plus de sauts — peut produire une intrication globale supérieure à celle d'un chemin plus court contenant un ou plusieurs segments de faible qualité. Par exemple, un chemin à quatre sauts avec une fidélité par liaison de 0,95 atteint une fidélité totale d'environ 0,81, tandis qu'un chemin à deux sauts avec des fidélités de 0,90 et 0,85 ne produit qu'environ 0,77. Les protocoles de routage modernes intègrent des métriques de qualité de liaison au-delà du simple nombre de sauts pour optimiser les performances de bout en bout.",
    "B": "Équilibrage de charge entre les banques de qubits mémoire au sein de chaque nœud répéteur, ce qui devient critique lorsque les nœuds emploient des mémoires quantiques multi-qubits avec des temps de cohérence hétérogènes dus à des variations de fabrication ou à des gradients de champ magnétique dépendant de la position dans les réseaux de pièges à ions. En distribuant le trafic sur des chemins plus longs qui utilisent des banques mémoire sous-utilisées aux nœuds intermédiaires, le protocole de routage empêche l'épuisement prématuré des qubits de haute qualité aux hubs congestionnés, prolongeant la durée de vie opérationnelle du réseau dans son ensemble. Spécifiquement, dans une architecture de répéteur avec k qubits mémoire par nœud, le routage par chemin le plus court peut créer des points chauds où certains qubits effectuent des opérations d'échange d'intrication à un rythme 10 fois supérieur aux qubits périphériques, accélérant leur déphasage par l'accumulation d'erreurs de contrôle et rendant finalement ces qubits inutilisables tandis que d'autres restent frais. Un chemin plus long qui route intentionnellement par des nœuds moins utilisés équilibre cette usure, maintenant une fidélité plus uniforme sur le réseau et évitant le scénario où les nœuds de plus haute centralité deviennent des goulots d'étranglement en raison de ressources mémoire épuisées, même si chaque saut supplémentaire impose une pénalité de fidélité compensée par le gain de fiabilité d'accès à des qubits bien reposés.",
    "C": "Contournement de nœuds avec des co-processeurs classiques saturés qui gèrent les protocoles de distillation d'intrication, car les répéteurs de réseau quantique s'appuient sur un calcul classique en temps réel pour décoder les mesures de syndrome des cycles de correction d'erreurs, déterminer des stratégies de distillation optimales (par exemple, sélectionner quelles paires de paires de Bell bruitées combiner via controlled-NOT et mesure pour produire une paire de fidélité supérieure), et coordonner le timing des opérations d'échange d'intrication avec les nœuds voisins. Lorsque le processeur classique d'un nœud devient surchargé — peut-être en traitant des demandes de routage simultanées de plusieurs paires source-destination — il introduit une latence qui peut dépasser le temps de décohérence des états intriqués stockés en mémoire quantique. Un chemin plus long qui évite ces nœuds computationnellement saturés, même au prix de sauts supplémentaires, garantit que le coût de coordination classique de chaque saut reste dans des limites acceptables, préservant la cohérence temporelle nécessaire pour une distribution d'intrication réussie. Ce compromis est particulièrement pertinent dans les réseaux utilisant des protocoles de distillation itératifs qui nécessitent O(log(1/ε)) cycles de traitement classique par saut pour atteindre la fidélité cible ε, où les nœuds congestionnés causent des délais de file d'attente qui détruisent l'intrication plus vite que la distillation ne peut la purifier, rendant un chemin à cinq sauts via des nœuds légèrement chargés préférable à un chemin à trois sauts via des nœuds saturés.",
    "D": "Contraintes de synchronisation temporelle dues aux taux de dérive d'horloge hétérogènes aux différents nœuds répéteurs, en particulier dans les réseaux quantiques géographiquement distribués où les nœuds utilisent des horloges atomiques locales qui accumulent des erreurs de phase relatives à des taux différant de plusieurs picosecondes par seconde en raison du décalage gravitationnel vers le rouge dépendant de l'altitude (selon la relativité générale) ou de la stabilité d'oscillateur dépendante de la température. Les chemins plus courts qui incluent des nœuds avec des horloges mal synchronisées nécessitent une communication classique fréquente pour rétablir les références de phase avant l'échange d'intrication, car la mesure d'état de Bell à chaque répéteur doit être effectuée dans la base correcte, et tout décalage d'horloge se traduit directement en une rotation de base qui réduit la fidélité de l'état échangé. Un chemin plus long qui sélectionne des nœuds avec des horloges mutuellement bien synchronisées — peut-être parce qu'ils partagent une norme de temps commune via des liaisons par fibre optique vers un serveur d'horloge central ou parce qu'ils ont récemment subi une synchronisation disciplinée GPS — peut éviter ces coûts de correction de phase, même si les sauts supplémentaires introduisent nominalement plus d'opportunités de décohérence. L'algorithme de routage doit équilibrer le nombre de sauts contre la gigue de timing cumulative, et dans les réseaux couvrant des distances continentales où les effets relativistes deviennent non négligeables, le chemin optimal peut délibérément ajouter un ou deux sauts pour maintenir une synchronisation sub-nanoseconde à travers toutes les mesures intermédiaires, garantissant que l'état intriqué final partagé entre source et destination conserve la cohérence de phase nécessaire pour des applications comme la distribution de clés quantiques ou le calcul quantique distribué.",
    "solution": "A"
  },
  {
    "id": 92,
    "question": "Pourquoi la correction d'erreurs de portes est-elle plus difficile en calcul quantique par rapport au calcul classique ?",
    "A": "Les erreurs ne peuvent pas être détectées sans effondrement par mesure qui détruit les états de superposition quantique protégés, créant une tension fondamentale entre détection d'erreurs et préservation du calcul. De plus, le théorème de non-clonage empêche la duplication simple d'information quantique pour la vérification de redondance comme la triple redondance modulaire classique. En outre, les erreurs quantiques forment un spectre continu de rotations possibles dans l'espace de Hilbert plutôt que des basculements de bits discrets, nécessitant des mesures de syndrome via des vérifications de stabilisateur multi-qubits qui extraient l'information d'erreur en bits classiques sans révéler l'état quantique logique protégé. Le processus de mesure lui-même introduit des erreurs supplémentaires, et les processus d'erreur continus doivent être discrétisés par une conception de code soigneuse et une extraction de syndrome rapide, rendant la correction d'erreurs quantique architecturalement bien plus complexe que les approches classiques malgré l'atteinte de seuils de tolérance aux fautes théoriquement similaires.",
    "B": "Le théorème de non-clonage empêche la copie d'états quantiques inconnus, forçant le recours à des encodages intriqués sur plusieurs qubits physiques plutôt qu'à une simple redondance. Bien que les codes stabilisateurs réalisent la détection d'erreurs par des mesures de syndrome qui projettent sur des espaces propres sans effondrer l'état logique, les erreurs quantiques se produisent de manière continue dans l'espace de Hilbert plutôt que discrètement. Cependant, contrairement aux systèmes classiques où les vérifications de parité révèlent directement quel bit a basculé, les syndromes quantiques n'indiquent le type et l'emplacement d'erreur que de manière probabiliste, nécessitant une inférence bayésienne itérative sur les chaînes d'erreurs possibles. Ce coût de décodage probabiliste, combiné au besoin de compléter l'extraction de syndrome plus rapidement que de nouvelles erreurs ne s'accumulent, fait que les codes quantiques nécessitent des facteurs de redondance plus élevés que les codes classiques pour obtenir une suppression d'erreur logique équivalente malgré l'approche asymptotique de seuils de tolérance aux fautes similaires.",
    "C": "Les erreurs quantiques se manifestent comme des rotations continues dans l'espace de Hilbert plutôt que comme des basculements de bits discrets, créant un espace d'erreur de dimension infinie que les codes binaires classiques ne peuvent pas traiter. Bien que les conditions de Knill-Laflamme montrent que des syndromes de stabilisateur discrets peuvent encore détecter les erreurs continues en les projetant sur des sous-espaces corrigibles, le processus de mesure introduit inévitablement de nouvelles erreurs à des taux comparables aux erreurs de portes. Contrairement aux systèmes classiques où la mesure est effectivement sans bruit, l'extraction de syndrome quantique nécessite des circuits tolérants aux fautes avec des qubits ancilla supplémentaires et des cycles de vérification. De plus, le théorème du seuil exige que l'extraction de syndrome se termine dans le temps de cohérence, forçant des compromis architecturaux entre distance de code, temps de cycle et taux d'erreur physique que les systèmes classiques évitent grâce à une lecture non destructive et une détection d'erreur déterministe.",
    "D": "L'effondrement par mesure interdit la vérification directe des états quantiques sans détruire les superpositions, mais plus fondamentalement, la décohérence quantique opère par trace partielle continue sur les degrés de liberté environnementaux, ce qui signifie que les erreurs s'accumulent de manière fluide plutôt que comme événements discrets. La correction d'erreurs classique détecte les événements de corruption discrets par des sommes de contrôle calculées de manière déterministe, tandis que les codes quantiques doivent implémenter des mesures de syndrome projectives qui elles-mêmes introduisent de nouvelles erreurs. Le théorème de non-clonage empêche la vérification par copies redondantes, forçant des codes basés sur l'intrication où l'information logique se distribue de manière non locale sur les qubits physiques. De plus, les processus de bruit corrélés comme la diaphonie peuvent causer des fluctuations cohérentes des valeurs propres de stabilisateur à travers les cycles, créant des motifs de syndrome indiscernables des erreurs de données, nécessitant un décodage multi-cycles avec des espaces d'états croissant exponentiellement que les codes de Hamming classiques évitent entièrement.",
    "solution": "A"
  },
  {
    "id": 93,
    "question": "Que se passe-t-il si l'algorithme de Grover est exécuté au-delà du nombre optimal d'itérations ?",
    "A": "L'état quantique subit une amplification d'amplitude continue qui s'approche asymptotiquement mais n'atteint jamais tout à fait la probabilité unitaire pour l'état marqué, présentant un comportement analogue aux oscillations amorties classiques où chaque itération successive fournit des rendements décroissants. L'évolution d'amplitude suit une trajectoire monotone croissante avec un taux de convergence logarithmique, bornée supérieurement par les principes fondamentaux d'incertitude de mesure quantique qui empêchent une préparation d'état parfaite. Cette saturation se produit parce que le spectre des valeurs propres de l'opérateur de diffusion de Grover crée une région de stabilité naturelle autour de l'amplification maximale, amenant le système à se stabiliser dans une distribution quasi-stationnaire.",
    "B": "L'état quantique continue de tourner dans le sous-espace bidimensionnel engendré par les superpositions d'états marqué et non marqué, faisant osciller sinusoïdalement l'amplitude de l'état cible. Après avoir dépassé l'angle optimal, des itérations de Grover supplémentaires font tourner le vecteur d'état au-delà de la projection maximale sur l'état marqué, réduisant progressivement la probabilité de succès jusqu'à ce qu'elle tombe potentiellement en dessous même de la ligne de base initiale de supposition aléatoire, démontrant l'importance critique du contrôle du nombre d'itérations.",
    "C": "Les itérations excédentaires introduisent un mécanisme de correction dépendant de la phase où les opérateurs d'oracle et de diffusion commencent à interférer de manière destructive, causant une descente contrôlée de l'amplitude de succès plutôt qu'un effondrement abrupt. Cette dégradation progressive suit un profil de décroissance en racine carrée où la probabilité diminue comme 1/√k pour k itérations au-delà de l'optimal, sensiblement plus douce que ne le prédirait une rotation géométrique naïve. L'effet provient de termes d'interférence quantique d'ordre supérieur qui s'activent seulement après que l'amplitude de l'état marqué dépasse un seuil critique, fournissant une protection partielle contre une sur-itération modérée par redistribution automatique d'amplitude.",
    "D": "Au-delà du nombre optimal d'itérations, les erreurs de phase quantique accumulées dues aux implémentations de portes imparfaites commencent à dominer la dynamique de rotation idéale, causant une décohérence progressive du vecteur d'état du sous-espace bidimensionnel cible-non-cible vers l'espace de Hilbert complet de dimension 2^n. Cette décohérence se manifeste comme un élargissement de la distribution d'amplitude sur les états non marqués plutôt qu'une simple inversion du processus d'amplification, avec la probabilité de succès déclinant exponentiellement alors que le couplage environnemental détruit la structure de superposition cohérente, réduisant finalement les résultats de mesure à un bruit aléatoire uniforme indiscernable d'une recherche non structurée.",
    "solution": "B"
  },
  {
    "id": 94,
    "question": "Quelle caractéristique unique les protocoles de surveillance de réseau quantique doivent-ils traiter que la surveillance classique ne rencontre pas ?",
    "A": "La contrainte fondamentale imposée par l'effet Zénon quantique, qui cause le gel des systèmes quantiques continuellement surveillés dans leur état initial et empêche l'évolution des états quantiques opérationnels du réseau. Les réseaux classiques peuvent effectuer une surveillance continue sans affecter la transmission de données, mais la surveillance quantique doit employer des stratégies d'échantillonnage discret avec des intervalles de mesure soigneusement chronométrés qui équilibrent l'observabilité diagnostique contre la suppression induite par rétroaction de la dynamique quantique, garantissant que la surveillance elle-même n'arrête pas la distribution d'intrication ou les protocoles de téléportation quantique.",
    "B": "L'exigence fondamentale que le trafic de réseau quantique — paires de photons intriqués, états quantiques transmis par téléportation, paires de Bell distribuées — ne peut pas être dupliqué ou cloné en raison du théorème de non-clonage, empêchant les protocoles de surveillance de copier passivement l'information quantique pour analyse comme le font routinièrement les réseaux classiques. Les réseaux classiques peuvent diviser les signaux optiques à l'aide de séparateurs de faisceau pour intercepter les flux de données, mais la surveillance quantique doit employer des techniques de post-sélection qui sacrifient le débit ou utiliser des mesures destructives sur des modes auxiliaires qui corrèlent avec les états opérationnels sans les effondrer directement.",
    "C": "Le principe mécanique quantique fondamental selon lequel la mesure perturbe le système observé, nécessitant que les protocoles de surveillance emploient des techniques non invasives telles que des mesures de témoins d'intrication, une tomographie partielle sur des qubits auxiliaires, ou des ressources de surveillance dédiées qui n'effondrent pas les états quantiques opérationnels. Les réseaux classiques peuvent librement intercepter et inspecter les données en transit sans altérer l'information, mais la surveillance quantique doit soigneusement équilibrer la visibilité diagnostique contre la perturbation d'état inévitable.",
    "D": "La contrainte architecturale fondamentale imposée par la monogamie de l'intrication, qui limite le nombre de nœuds de réseau pouvant simultanément partager de fortes corrélations quantiques avec un nœud donné. Les réseaux classiques supportent une diffusion arbitraire où un nœud diffuse vers de nombreux destinataires, mais la surveillance quantique doit tenir compte du compromis où l'extraction d'information de surveillance d'une liaison quantique réduit nécessairement l'intrication disponible aux utilisateurs finaux, nécessitant que les protocoles allouent des ressources d'intrication entre canaux de communication opérationnels et canaux de mesure diagnostiques selon des bornes de monogamie strictes dérivées d'inégalités de sous-additivité forte.",
    "solution": "C"
  },
  {
    "id": 95,
    "question": "Quelle méthodologie d'attaque avancée peut compromettre la sécurité des schémas de monnaie quantique ?",
    "A": "La tomographie d'état quantique effectuée sur plusieurs tentatives de vérification indépendantes permet à un adversaire de reconstruire progressivement l'état de monnaie quantique inconnu en inférant statistiquement la matrice densité à partir des résultats de mesure, construisant une description classique de haute fidélité qui peut être utilisée pour préparer des copies approximatives et contourner efficacement les protections de non-clonage.",
    "B": "Les protocoles de clonage approximatif combinés avec des codes de correction d'erreurs quantiques permettent à un attaquant de générer des copies quasi-parfaites en produisant d'abord plusieurs duplicatas bruités à l'aide de machines de clonage universel optimales, puis en appliquant des mesures de syndrome et des portes de correction pour purifier systématiquement ces clones, la redondance permettant aux décodeurs de correction d'erreurs de récupérer un qubit logique qui représente fidèlement l'état de monnaie authentique.",
    "C": "La reconstruction d'état de sous-espace caché exploite la structure de vérification en analysant comment le protocole d'authentification public de la banque accepte ou rejette les états candidats, permettant aux adversaires de déduire les propriétés du sous-espace secret et finalement de contrefaire des jetons.",
    "D": "L'analyse de requêtes d'oracle de vérification exploite la procédure de vérification publique de la banque en soumettant des états quantiques soigneusement conçus et en observant les motifs d'acceptation pour rétro-concevoir la base secrète dans laquelle les états de monnaie légitimes sont préparés, avec des stratégies de requête adaptatives testant des superpositions et des états de sonde intriqués pour extraire des informations partielles sur les opérateurs de projection de sous-espace cachés.",
    "solution": "C"
  },
  {
    "id": 96,
    "question": "Quelle méthodologie d'attaque avancée peut compromettre la sécurité des schémas de partage de secret quantique ?",
    "A": "Un adversaire peut placer des dispositifs de mesure quantique non destructive sur les canaux de distribution pour surveiller en continu la transmission des parts sans perturber les états quantiques, lui permettant d'extraire des corrélations classiques entre les parts en analysant les relations temporelles et de phase des photons détectés. Puisque les corrélations entre parts encodent des combinaisons linéaires du secret, l'accumulation de statistiques sur plusieurs exécutions du protocole permet la reconstruction du secret par analyse de corrélation, même lorsque les parts individuelles semblent maximalement mélangées.",
    "B": "Si l'identité du distributeur n'est pas vérifiée par des protocoles d'authentification quantique appropriés, un adversaire peut effectuer une attaque de l'homme du milieu en interceptant la diffusion initiale du distributeur et en injectant de fausses parts intriquées avec ses propres qubits auxiliaires. Comme la phase de reconstruction par seuil repose sur la superposition cohérente des parts légitimes, même une seule fausse part corrompt le motif d'interférence lors du réassemblage, permettant à l'adversaire de biaiser le secret reconstruit vers une valeur de son choix ou d'extraire de l'information par des mesures sur son système auxiliaire.",
    "C": "La tomographie d'état quantique de parts partielles peut révéler des informations statistiques sur le secret en effectuant des mesures informationnellement complètes sur plusieurs exécutions du protocole avec la même distribution de parts.",
    "D": "Durant la phase de reconstruction par seuil, un adversaire peut injecter des impulsions électromagnétiques ou des signaux laser soigneusement synchronisés pour créer une interférence contrôlée entre les états quantiques des parts en cours de recombinaison. Cette interférence décale les phases relatives entre les états de base computationnelle dans le secret recombinné, et en variant systématiquement les motifs d'interférence sur plusieurs tentatives de reconstruction tout en observant les résultats réussis, l'adversaire peut déduire itérativement le secret original par analyse de phase différentielle des valeurs reconstruites.",
    "solution": "C"
  },
  {
    "id": 97,
    "question": "Comment la tomographie d'état quantique variationnelle se rapporte-t-elle à l'apprentissage automatique quantique ?",
    "A": "Utilise les principes de l'apprentissage automatique pour reconstruire efficacement les états en paramétrisant l'état quantique inconnu comme un ansatz de réseau de neurones et en entraînant les paramètres pour correspondre aux statistiques mesurées. Au lieu de nécessiter un nombre exponentiellement grand de mesures pour caractériser complètement la matrice de densité, on exploite le biais inductif des architectures neuronales pour comprimer la représentation d'état, en apprenant un modèle génératif qui reproduit les résultats de mesure.",
    "B": "Vérifie le fonctionnement des réseaux de neurones quantiques en effectuant une tomographie d'état variationnelle sur les états de sortie produits par votre ansatz de circuit quantique après entraînement. Puisque les réseaux de neurones quantiques transforment les données d'entrée en états quantiques par évolution unitaire paramétrée, la reconstruction tomographique de ces états de sortie fournit une vérité de référence pour valider que le circuit a appris la représentation de données prévue.",
    "C": "Caractérise les espaces de caractéristiques quantiques en utilisant la tomographie variationnelle pour reconstruire les matrices de densité des points de données après encodage par votre circuit de carte de caractéristiques. L'apprentissage automatique quantique repose sur l'incorporation de données classiques dans des espaces de Hilbert de haute dimension où les noyaux quantiques calculent des produits scalaires, mais comprendre quelle structure géométrique cet encodage crée réellement nécessite une tomographie d'état.",
    "D": "Toutes les réponses ci-dessus",
    "solution": "D"
  },
  {
    "id": 98,
    "question": "Quel algorithme quantique constitue la base de nombreuses accélérations en apprentissage automatique quantique ?",
    "A": "L'algorithme de recherche non structurée de Grover fournit la primitive computationnelle fondamentale pour l'accélération de l'apprentissage automatique quantique en permettant une accélération quadratique dans la recherche à travers les espaces d'hypothèses pendant l'entraînement du modèle.",
    "B": "L'algorithme de factorisation d'entiers de Shor, bien que principalement connu pour ses applications cryptanalytiques dans le cassage du chiffrement RSA, sert en réalité de moteur computationnel sous-jacent pour les accélérations en apprentissage automatique quantique grâce à son implémentation efficace de l'exponentiation modulaire et des sous-routines de recherche de période qui peuvent être réutilisées pour calculer des transformées de Fourier discrètes sur des groupes cycliques.",
    "C": "L'estimation de phase quantique sert d'algorithme fondamental sous-tendant de nombreuses accélérations en apprentissage automatique quantique en permettant l'extraction efficace d'informations sur les valeurs propres d'opérateurs unitaires, ce qui soutient directement l'analyse en composantes principales quantique (qPCA) pour la réduction de dimensionnalité, alimente l'algorithme HHL pour résoudre les systèmes linéaires apparaissant dans les tâches de régression et d'optimisation, facilite les évaluations de noyaux de machines à vecteurs de support quantiques par estimation efficace d'amplitude des produits scalaires, et permet les solveurs variationnels d'états propres quantiques utilisés dans l'entraînement de réseaux de neurones quantiques — cette primitive algorithmique atteint un avantage exponentiel en encodant les valeurs propres dans le retour de phase quantique avec une complexité de portes polynomiale O(log N), permettant aux protocoles QML de traiter efficacement des structures de données de haute dimension encodées dans des espaces d'amplitude quantique où les algorithmes classiques nécessitent des ressources exponentielles.",
    "D": "L'algorithme de transformée de Fourier quantique constitue la sous-routine centrale qui permet les accélérations quantiques dans les applications d'apprentissage automatique, particulièrement grâce à sa capacité à calculer la transformée de Fourier discrète d'un état quantique en O(log²N) opérations de portes.",
    "solution": "C"
  },
  {
    "id": 99,
    "question": "Quelle vulnérabilité sophistiquée existe dans les implémentations de distribution de clés quantiques à variables continues ?",
    "A": "Les adversaires peuvent délibérément saturer les photodiodes des détecteurs homodynes en envoyant d'intenses impulsions lumineuses entrelacées avec les états de signal CV-QKD légitimes, forçant le détecteur dans des régimes de fonctionnement non linéaires où le photocourant mesuré n'évolue plus linéairement avec la puissance optique incidente. Pendant les événements de saturation, les résultats de mesure de quadrature deviennent comprimés et déformés de manière prévisible.",
    "B": "Un adversaire peut exploiter les désynchronisations temporelles entre les fenêtres de mesure de quadrature aux stations d'Alice et Bob en injectant des signaux interférents déphasés arrivant pendant de brèves périodes de transition lorsque la phase de l'oscillateur local du détecteur homodyne bascule entre les mesures X et P. Ces attaques de désynchronisation font que les valeurs de quadrature mesurées représentent des combinaisons linéaires des deux observables.",
    "C": "La procédure de calibration du bruit de grenaille repose sur la mesure des fluctuations du vide quantique lorsqu'aucun signal n'est présent, mais un attaquant avec un contrôle partiel du canal peut injecter des états cohérents faibles synchronisés temporellement avec les fenêtres de calibration pour gonfler artificiellement la ligne de base du bruit de grenaille mesuré. En manipulant ce niveau de référence vers le haut, Eve peut introduire un bruit d'écoute clandestine proportionnellement plus important pendant la génération de clé.",
    "D": "La manipulation de l'oscillateur local par injection externe accordée en longueur d'onde permet à un adversaire de substituer de manière transparente le faisceau d'oscillateur local légitime au récepteur de Bob par un état cohérent contrôlé par l'attaquant qui partage une structure de mode spatial et temporel identique mais porte une référence de phase subtilement modifiée, tournant ainsi la base de mesure d'une manière qui reste indétectable par les procédures de calibration standard tout en biaisant systématiquement les résultats de quadrature mesurés vers des valeurs corrélées avec les informations interceptées par l'attaquant sur les états transmis d'Alice. Cette attaque réussit parce que les preuves de sécurité CV-QKD supposent que l'oscillateur local définit une référence de phase de confiance, mais lorsque Eve contrôle cette référence par des attaques d'injection sélective en longueur d'onde exploitant un filtrage optique insuffisant à la station de Bob, elle peut concevoir des résultats de mesure qui divulguent des informations partielles sur la clé tout en maintenant des statistiques limitées par le bruit de grenaille qui passent tous les contrôles de sécurité conventionnels incluant la surveillance du bruit excédentaire et les tests de vérification d'équilibre homodyne.",
    "solution": "D"
  },
  {
    "id": 100,
    "question": "Dans un GAN quantique, le discriminateur est souvent implémenté comme un circuit variationnel parce que cette conception :",
    "A": "Permet une optimisation conjointe avec le générateur au sein de la même session matérielle quantique, permettant aux deux réseaux d'être entraînés sur un seul processeur quantique par des mises à jour alternées de paramètres qui exploitent des ressources de mesure partagées et évitent le coût de basculer entre différentes architectures de circuit.",
    "B": "Permet l'entraînement adverse par rétropropagation induite par mesure où la sortie du discriminateur, encodée comme une valeur d'espérance d'observable, fournit des signaux de gradient continus aux deux réseaux en exploitant la règle de décalage de paramètre, permettant l'optimisation simultanée des paramètres du générateur et du discriminateur par des mesures entrelacées.",
    "C": "Fournit une expressivité entraînable par des unitaires paramétrés qui peuvent approximer des frontières de décision arbitraires dans l'espace de Hilbert, permettant au discriminateur d'apprendre des distributions complexes en ajustant les angles de rotation par descente de gradient tout en maintenant la compatibilité matérielle par compilation de portes natives qui préserve les statistiques de mesure.",
    "D": "Facilite l'avantage quantique en encodant la fonction de décision du discriminateur comme des états intriqués dont les résultats de mesure calculent intrinsèquement des distances de noyau entre distributions réelles et générées, exploitant l'interférence quantique pour effectuer des comparaisons d'espace de caractéristiques implicites qui nécessiteraient des ressources classiques exponentielles pour être évaluées explicitement.",
    "solution": "A"
  },
  {
    "id": 101,
    "question": "Laquelle des affirmations suivantes décrit le mieux la relation entre la profondeur de circuit et l'expressivité dans les réseaux de neurones quantiques ?",
    "A": "L'expressivité dans les réseaux de neurones quantiques est fondamentalement déterminée par le nombre total de paramètres variationnels plutôt que par la profondeur du circuit, suivant une loi d'échelle analogue à la largeur des réseaux de neurones classiques. Un circuit peu profond avec suffisamment de portes paramétrées peut approximer toute transformation unitaire sur l'espace des qubits avec une précision arbitraire, tandis qu'augmenter la profondeur sans ajouter de paramètres crée simplement des rotations redondantes qui couvrent le même sous-espace du groupe unitaire complet, ne contribuant en rien à la capacité de représentation du modèle.",
    "B": "Les circuits plus profonds sont invariablement plus expressifs en raison de leur capacité à générer des structures d'intrication de plus en plus complexes et à explorer des volumes plus importants du groupe unitaire, mais cette expressivité accrue s'accompagne du coût catastrophique de gradients qui s'annulent exponentiellement, connus sous le nom de plateaux arides. Au-delà d'un seuil de profondeur critique qui évolue logarithmiquement avec le nombre de qubits, la probabilité de trouver des configurations de paramètres avec des gradients non négligeables diminue exponentiellement, rendant l'expressivité supplémentaire complètement inaccessible à l'optimisation basée sur les gradients, quel que soit l'algorithme d'entraînement employé.",
    "C": "Les circuits peu profonds avec une bonne structure d'intrication et une conception d'ansatz appropriée peuvent atteindre une expressivité élevée, égalant ou dépassant souvent la capacité de représentation de circuits beaucoup plus profonds tout en évitant les problèmes d'entraînabilité.",
    "D": "La profondeur du circuit a un impact minimal sur l'expressivité par rapport au nombre de qubits, car la dimension de l'espace de Hilbert croît comme 2^n où n est le nombre de qubits.",
    "solution": "C"
  },
  {
    "id": 102,
    "question": "Dans les architectures de lecture dispersive, un seul résonateur de bus peut mesurer la parité multi-qubit en exploitant quel mécanisme ?",
    "A": "Les décalages Kerr croisés provenant des états conjoints des qubits projettent la parité sur la phase accumulée du résonateur, permettant une mesure indirecte où le couplage dispersif collectif traduit la parité paire ou impaire en décalages de fréquence de cavité distinguables qui peuvent être lus par détection homodyne sans mesure directe des qubits.",
    "B": "Les décalages dispersifs conjoints provenant des corrélations multi-qubit créent des tirages de fréquence de cavité dépendant de la parité qui s'accumulent pendant le temps d'intégration de l'impulsion de lecture, mais le mécanisme nécessite une extraction séquentielle de syndrome par sondage de résonateur multiplexé temporellement où la contribution de chaque qubit apparaît comme une composante de fréquence séparable, la parité émergeant du motif de battement entre ces composantes plutôt que d'un seul décalage collectif.",
    "C": "Le couplage dispersif collectif génère des décalages du nombre de photons dépendant de la parité dans le résonateur par la somme des termes Kerr individuels des qubits, où la phase du résonateur s'accumule proportionnellement au XOR des états des qubits. Cependant, cela nécessite de conduire activement la cavité dans le régime d'état de Fock où la détection résolvant le nombre de photons extrait la parité directement de la population de photons à valeur entière plutôt que des quadratures homodyne continues.",
    "D": "Les interactions Kerr croisées multi-qubit créent des décalages de fréquence dépendant de la parité qui se projettent sur l'accumulation de phase du résonateur pendant l'intégration homodyne, mais le signe du décalage dispersif alterne avec la parité du nombre de qubits plutôt que d'être collectif. Cela signifie que les états de parité impaire et paire produisent des décalages de phase de signe opposé par rapport à la fréquence de cavité nue, nécessitant des mesures de référence calibrées pour distinguer la valeur de parité des contributions individuelles des qubits.",
    "solution": "A"
  },
  {
    "id": 103,
    "question": "Quelle caractéristique des recuits quantiques adiabatiques les rend susceptibles aux fuites d'information par gel précoce ?",
    "A": "Le gel précoce provoque le verrouillage des populations de qubits dans des états propres d'énergie qui reflètent la structure du problème par des courants persistants intégrés dans le temps. Lorsque le recuit se termine avant l'équilibration thermique, les qubits se fixent dans des configurations métastables déterminées par la topologie du paysage énergétique. Ces états de flux gelés génèrent des champs magnétiques quasi-statiques dont les motifs spatiaux encodent la matrice de couplage d'Ising par des effets d'inductance mutuelle, créant des signatures mesurables en champ proche qui persistent après le recuit et peuvent être détectées par magnétométrie sensible.",
    "B": "Le gel précoce encode les biais du hamiltonien du problème dans des courants persistants lisibles via des boucles de détection SQUID. Lorsque le programme de recuit se termine avant d'atteindre le véritable équilibre thermique, les qubits se figent dans des configurations métastables qui reflètent la structure du paysage énergétique. Ces motifs de courant persistant génèrent des signatures de flux magnétique mesurables qui laissent fuiter des informations sur l'instance du problème et potentiellement la trajectoire de solution par des canaux auxiliaires électromagnétiques.",
    "C": "Les transitions diabatiques pendant le recuit rapide induisent des événements de tunneling Landau-Zener qui sont corrélés avec la topologie de frustration du hamiltonien du problème, générant des spectres d'émission électromagnétique caractéristiques. Lorsque la vitesse de recuit dépasse les conditions adiabatiques, les qubits subissent des transitions non adiabatiques à des croisements évités dont les emplacements encodent les forces de couplage. Ces transitions produisent des courants oscillants transitoires avec des fréquences proportionnelles aux écarts d'énergie, émettant des signatures radiofréquence qui révèlent la structure du problème par analyse de Fourier du spectre d'émission capturé pendant le recuit.",
    "D": "L'écart d'énergie fini pendant le mi-recuit nécessite une excitation micro-onde continue pour supprimer les excitations thermiques, créant des motifs de courant modulés qui encodent les paramètres d'Ising. À mesure que le champ transverse diminue, l'écart d'énergie instantané se rétrécit exponentiellement, nécessitant des impulsions de stabilisation dynamique dont les amplitudes doivent suivre la structure d'écart spécifique au problème. Ces courants de compensation circulent à travers les lignes de polarisation de flux avec des magnitudes proportionnelles aux intensités de champ local, générant des signatures magnétiques variant dans le temps dont la densité spectrale de puissance révèle directement le hamiltonien d'Ising incorporé par des pics de résonance caractéristiques.",
    "solution": "B"
  },
  {
    "id": 104,
    "question": "Dans les protocoles de routage de réseaux quantiques, l'intrication entre les nœuds crée des graphes de dépendance complexes qui pourraient théoriquement conduire à des boucles de routage où l'information quantique circule indéfiniment sans atteindre sa destination. Qu'est-ce qui empêche les boucles d'intrication de causer un état incohérent dans le routage ?",
    "A": "L'évitement de boucles dans la sélection de chemin basé sur des algorithmes de graphe garantit que les opérations d'échange d'intrication sont séquencées selon des arbres de routage acycliques construits à partir de la topologie du réseau, empêchant la formation de cycles en premier lieu. Les protocoles de contrôle classique suivent quelles paires de nœuds partagent l'intrication et calculent des arbres couvrants ou des chemins les plus courts qui garantissent une progression monotone vers la destination. Puisque la téléportation quantique consomme les paires intriquées utilisées à chaque saut, les liens précédemment traversés ne peuvent pas être réutilisés, empêchant intrinsèquement l'information quantique de revisiter les nœuds et de créer des superpositions incohérentes sur des chemins fermés dans le réseau.",
    "B": "Les contraintes de monogamie de l'intrication imposent que chaque qubit participe à au plus une paire maximalement intriquée à tout moment, ce qui signifie que les opérations d'échange qui créeraient des boucles échouent automatiquement car les paires de Bell requises ne peuvent pas coexister avec des liens précédemment établis. Lorsqu'un protocole de routage tente de créer un cycle en échangeant l'intrication de nœud A→B→C→A, la troisième opération d'échange ne peut pas réussir car le qubit du nœud A est déjà maximalement intriqué avec le nœud B, violant l'inégalité de monogamie. Cette contrainte fondamentale de la théorie de l'information quantique agit comme un mécanisme de prévention physique, provoquant la décohérence des opérations créant des boucles plutôt que l'établissement d'un état incohérent, ainsi les protocoles de routage évitent naturellement les cycles par la structure des corrélations quantiques.",
    "C": "La détection d'erreur quantique distribuée à travers les nœuds du réseau implémente un protocole de détection de boucle basé sur les syndromes où chaque échange d'intrication encode l'information de parité dans des qubits ancillaires qui signalent les dépendances cycliques par des mesures de stabilisateur. Lorsque les chemins de routage forment des boucles topologiques, la phase accumulée des mesures de Bell autour du cycle produit des syndromes non triviaux dans l'espace de stabilisateur qui déclenchent une terminaison automatique du chemin avant que des états incohérents ne se propagent. Ce surcoût de détection d'erreur ajoute un ancillaire par lien réseau mais fournit une détection de boucle en temps réel avec un traitement classique polynomial, garantissant la correction du routage en avortant les séquences d'échange chaque fois que les motifs de syndrome indiquent que la prochaine étape de téléportation fermerait un cycle dans le graphe d'intrication.",
    "D": "L'ordonnancement temporel des mesures de Bell garantit la causalité en exigeant que chaque nœud complète sa mesure locale et sa communication classique avant que les échanges suivants puissent procéder, créant un ordre partiel sur les opérations d'échange qui empêche intrinsèquement les courbes temporelles fermées dans l'exécution du protocole. Lorsque le routage tente de créer une boucle, la latence de communication classique des sauts précédents retarde les échanges ultérieurs de sorte qu'au moment où une opération de fermeture de boucle pourrait s'exécuter, les états quantiques de l'origine de la boucle ont déjà décohéré au-delà du temps de cohérence du réseau, brisant naturellement les cycles potentiels. Cela combine les contraintes de causalité relativiste avec les échelles de temps de décohérence pour garantir un routage acyclique par la structure de l'espace-temps plutôt que de nécessiter des algorithmes explicites de détection de boucle.",
    "solution": "A"
  },
  {
    "id": 105,
    "question": "Pourquoi les erreurs de bit-flip (X) et de phase-flip (Z) interagissent-elles pour produire des erreurs plus complexes en informatique quantique ?",
    "A": "La composition des erreurs de bit-flip et de phase-flip produit une superposition non commutative d'opérateurs d'erreur dont l'effet net dépend de l'orientation instantanée du vecteur de Bloch de l'état quantique au moment où chaque erreur frappe. Parce que l'effondrement de la mesure quantique se produit de manière probabiliste et que la trajectoire du système dans l'espace d'états présente une sensibilité chaotique aux conditions initiales, l'erreur combinée se manifeste comme une marche aléatoire essentiellement aléatoire à travers la sphère de Bloch, rendant computationnellement intraitable de prédire ou modéliser l'impact de l'erreur conjointe sans effectuer un nombre exponentiel de simulations de trajectoire.",
    "B": "Les erreurs de phase-flip proviennent d'imperfections de contrôle cohérent et de déscalibrations systématiques dans les lignes d'excitation des qubits, les rendant déterministes et corrigibles par des protocoles de calibration améliorés, tandis que les erreurs de bit-flip proviennent de la décohérence environnementale stochastique et des excitations thermiques, les rendant fondamentalement aléatoires.",
    "C": "Une erreur de phase-flip modifie les phases relatives entre les états de base computationnelle, ce qui altère directement les probabilités de résultat de mesure pour les états de superposition mais laisse les éléments diagonaux de la matrice de densité inchangés. Lorsqu'une erreur de bit-flip se produit par la suite, sa signature dans la mesure de syndrome devient obscurcie car la corruption de phase antérieure a fait pivoter la base de mesure, rendant impossible pour les mesures de stabilisateur standard de distinguer entre un bit-flip pur et l'erreur combinée.",
    "D": "Lorsqu'une erreur de bit-flip (X) et une erreur de phase-flip (Z) se produisent séquentiellement sur le même qubit, leur effet combiné produit une erreur Y car les opérateurs de Pauli satisfont la relation algébrique iY = XZ. Cette interaction découle de la structure de multiplication non commutative du groupe de Pauli, où l'application de X et Z introduit un facteur de phase complexe supplémentaire qui transforme l'erreur en un opérateur de Pauli entièrement différent avec des effets physiques distincts sur l'état quantique.",
    "solution": "D"
  },
  {
    "id": 106,
    "question": "Que se passe-t-il lorsqu'un état de superposition arbitraire subit l'action d'une porte CNOT ?",
    "A": "La porte CNOT applique un basculement de bit conditionnel qui préserve la superposition lorsque le qubit de contrôle est dans un état défini |0⟩ ou |1⟩, mais induit une décohérence lorsque le contrôle existe dans une superposition cohérente, car l'action de la porte crée un effet Zénon quantique où la surveillance continue de l'état logique du qubit de contrôle fige son évolution. Cette rétroaction de surveillance fait s'effondrer le contrôle dans un état propre tandis que la cible subit son basculement conditionnel, laissant le système joint dans un état mixte plutôt qu'une superposition intriquée pure.",
    "B": "De l'intrication peut être créée entre les qubits, particulièrement lorsque le qubit de contrôle est en superposition et que la cible est dans un état de base défini. La porte CNOT applique un basculement conditionnel qui corrèle les états des deux qubits, produisant un état joint qui ne peut pas être factorisé en états indépendants de qubits uniques, générant ainsi des corrélations quantiques qui violent la séparabilité classique.",
    "C": "La porte implémente une opération de parité contrôlée qui mappe les états de base computationnelle selon |c⟩|t⟩ → |c⟩|t ⊕ c⟩, où ⊕ désigne l'addition modulo 2, mais cette logique de parité brise intrinsèquement la cohérence de phase entre les composantes de superposition car les opérations XOR sont irréversibles du point de vue de l'information de phase quantique. Bien que les amplitudes soient redistribuées correctement, les phases relatives entre les composantes |00⟩, |01⟩, |10⟩ et |11⟩ sont brouillées par la contrainte de parité, convertissant la superposition cohérente d'entrée en un mélange statistique avec perte des éléments hors-diagonale de la matrice densité.",
    "D": "Le système subit une rotation dépendante de la base où le vecteur de Bloch du qubit cible précesse autour d'un axe déterminé par la projection du vecteur d'état du qubit de contrôle sur la base propre de Pauli-Z, avec un angle de précession proportionnel à l'amplitude de |1⟩ du contrôle. Cela crée une accumulation de phase géométrique continue qui interpole en douceur entre l'identité (lorsque le contrôle est |0⟩) et le basculement de bit (lorsque le contrôle est |1⟩), implémentant effectivement une rotation contrôlée qui préserve toute l'information quantique tout en transformant conditionnellement la cible sur la base de mesures partielles de la matrice densité du contrôle.",
    "solution": "B"
  },
  {
    "id": 107,
    "question": "Quelle propriété quantique fondamentale l'Internet quantique exploite-t-il que les réseaux classiques ne peuvent pas exploiter ?",
    "A": "Les corrélations non locales d'états intriqués distribués permettant la violation des inégalités de Bell à travers les nœuds du réseau, ce qui permet la vérification indépendante du dispositif de l'intégrité des canaux quantiques et l'implémentation de protocoles QKD indépendants du dispositif de mesure où la sécurité découle purement des statistiques de corrélation observées sans faire confiance au matériel répéteur intermédiaire, fournissant des garanties cryptographiques que les canaux authentifiés classiques ne peuvent pas réaliser.",
    "B": "La cohérence quantique maintenue à travers les nœuds distribués par des séquences de découplage dynamique continu appliquées aux commutateurs de routage, permettant l'amplification préservant la phase des signaux quantiques via des techniques d'amplification linéaire sans bruit qui contournent le théorème de non-clonage en amplifiant de manière probabiliste les composantes du signal tout en post-sélectionnant les événements d'amplification réussis, réalisant un transfert d'état quantique à longue distance sans décohérence.",
    "C": "La distribution d'états quantiques intriqués entre nœuds distants, permettant des protocoles de téléportation pour le transfert d'information quantique et une distribution de clés cryptographiques inconditionnellement sécurisée grâce à des corrélations qui n'ont pas d'analogue classique et ne peuvent pas être interceptées sans détection.",
    "D": "La contextualité quantique dans les protocoles de routage multi-qubits où les résultats de mesure aux commutateurs réseau dépendent de la configuration globale des choix de base à travers tous les nœuds, permettant un acheminement de paquets dépendant du contexte qui atteint une efficacité de routage optimale prouvée pour certaines topologies de réseau en exploitant des corrélations de type Kochen-Specker que les protocoles de routage classiques non contextuels ne peuvent pas satisfaire, comme démontré par les violations d'inégalités de routage analogues aux bornes CHSH.",
    "solution": "C"
  },
  {
    "id": 108,
    "question": "Quelle vulnérabilité spécifique existe dans les procédures de réinitialisation des qubits supraconducteurs ?",
    "A": "Des quasi-particules hors équilibre persistant après la fin de l'impulsion de réinitialisation, qui peuvent traverser les jonctions par effet tunnel et causer des excitations parasites dans les opérations ultérieures. Ces quasi-particules, générées pendant les opérations de mesure ou de porte, ont des échelles de temps de relaxation qui peuvent dépasser le temps de cohérence du qubit lui-même, créant un fond d'événements d'excitation stochastiques qui corrompent la fidélité de réinitialisation même lorsque le protocole de réinitialisation atteint nominalement une population d'état fondamental >99%. L'effet est particulièrement prononcé dans les dispositifs avec de petites énergies de gap supraconducteur ou des nombres de photons environnementaux élevés.",
    "B": "La persistance d'excitation thermique se produit lorsque la chaleur résiduelle des opérations dissipatives pendant la mesure ou les protocoles de réinitialisation active ne se thermalise pas assez rapidement à travers la puissance de refroidissement limitée du réfrigérateur à dilution, maintenant le qubit et son environnement électromagnétique à des températures effectives significativement au-dessus de la température de base. Cette population thermique élevée se manifeste comme une occupation quasi-stationnaire d'états excités qui ne peut pas être éliminée par des impulsions de réinitialisation standard, nécessitant des temps d'attente de centaines de microsecondes pour une thermalisation passive ou des schémas de refroidissement actif plus complexes impliquant des modes auxiliaires pour extraire l'entropie du sous-espace computationnel.",
    "C": "Le chauffage induit par la mesure provient de l'énergie dissipée pendant la lecture projective, où les photons qui fuient du résonateur de mesure déposent de l'énergie dans l'environnement électromagnétique local du qubit et dans le bain de phonons plus large du substrat.",
    "D": "La dérive de calibration de l'impulsion de réinitialisation représente un défi fondamental où les paramètres optimaux pour les protocoles de réinitialisation conditionnelle—incluant les amplitudes de pilotage, les durées d'impulsion et les décalages de fréquence—changent au fil du temps en raison de changements environnementaux, du bruit de flux dans les coupleurs accordables et des effets de vieillissement dans l'électronique de contrôle. Lorsque les données de calibration deviennent obsolètes, les opérations de réinitialisation peuvent par inadvertance peupler des états excités supérieurs ou échouer à dépeupler complètement le premier état excité, avec des erreurs s'accumulant à travers les exécutions répétées de circuits jusqu'à ce qu'une recalibration se produise.",
    "solution": "D"
  },
  {
    "id": 109,
    "question": "Quel est l'un des obstacles clés dans la mise à l'échelle des méthodes de correction d'erreurs quantiques basées sur l'apprentissage automatique ?",
    "A": "L'entraînement de modèles qui généralisent à travers différentes topologies de qubits et motifs de connectivité est coûteux en calcul et intensif en données, nécessitant une simulation extensive de divers modèles d'erreurs et configurations matérielles pour atteindre des performances robustes à travers plusieurs plateformes de calcul quantique avec des contraintes architecturales variées.",
    "B": "Les données de syndrome présentent des corrélations temporelles dues aux mesures répétées, violant l'hypothèse i.i.d. sous-jacente à l'apprentissage supervisé standard. Les décodeurs ML entraînés sur des échantillons de syndrome indépendants échouent lorsqu'ils sont déployés dans des circuits tolérants aux fautes où les erreurs de mesure se propagent à travers la réutilisation d'ancilla, causant un décalage de distribution entre l'entraînement (syndromes à tour unique synthétiques) et le déploiement (flux de données corrélées multi-tours). Cette structure de corrélation croît avec la profondeur d'extraction de syndrome, dégradant la précision du décodeur sur le matériel réel malgré des performances élevées sur les ensembles de test simulés.",
    "C": "Le coût d'entraînement évolue exponentiellement avec la distance du code car la dimension de l'espace de syndrome croît comme 2^((n-k)) pour n qubits physiques encodant k qubits logiques, nécessitant l'énumération de tous les motifs d'erreurs possibles pour atteindre une couverture complète. Pour les codes de surface à distance d=5 (n=49, k=1), cela produit ~10^14 classes de syndrome distinctes qui doivent chacune apparaître suffisamment dans les données d'entraînement. La simulation classique de la génération de syndrome devient intraitable au-delà de d=7, créant un goulot d'étranglement de données où les modèles ne peuvent pas être correctement entraînés pour les distances de code (d≥15) nécessaires pour une tolérance aux fautes pratique.",
    "D": "La descente de gradient par lot sur les syndromes d'erreurs quantiques rencontre des gradients évanescents dus à des plateaux stériles dans le paysage de perte du décodeur, qui émergent parce que les résultats de mesure de syndrome sont des observables hautement intriquées présentant des distributions exponentiellement concentrées. La magnitude du gradient évolue comme O(1/2^n) pour les codes à n qubits, rendant l'entraînement basé sur la rétropropagation infaisable au-delà de ~10 qubits physiques. Cette limitation fondamentale provient des mêmes phénomènes de concentration quantique affectant les solveurs propres quantiques variationnels, nécessitant des méthodes d'optimisation alternatives comme les stratégies d'évolution qui évitent entièrement le calcul du gradient.",
    "solution": "A"
  },
  {
    "id": 110,
    "question": "Quel est l'objectif principal du découpage de circuits dans un système quantique distribué ?",
    "A": "Partitionne un grand circuit quantique en sous-circuits plus petits qui peuvent chacun tenir sur des modules matériels séparés avec une connectivité limitée, permettant l'exécution à travers des dispositifs où l'intrication directe inter-module est prohibitivement bruitée. La technique reconstruit la sortie du circuit complet en exécutant les sous-circuits indépendamment avec des mesures supplémentaires aux emplacements de coupe et en post-traitant classiquement leurs résultats avec des pondérations de quasi-probabilité appropriées dérivées de la base tomographique de la coupe.",
    "B": "Partitionne un grand circuit quantique en couches plus petites découpées dans le temps qui peuvent chacune tenir dans la fenêtre de cohérence des modules matériels disponibles, permettant une exécution séquentielle à travers plusieurs cycles de rafraîchissement où les qubits sont périodiquement réinitialisés à l'état fondamental. La technique reconstruit la sortie du circuit complet en exécutant les couches découpées dans le temps indépendamment avec un transfert d'état entre les tranches et en post-traitant classiquement leurs résultats de mesure avec des corrections de phase appropriées tenant compte de la décroissance T1.",
    "C": "Partitionne un grand circuit quantique en sous-circuits plus petits qui peuvent chacun tenir sur des modules matériels séparés avec une connectivité de qubits limitée ou nulle entre eux, permettant une exécution distribuée à travers plusieurs processeurs quantiques. La technique reconstruit la sortie du circuit complet en exécutant les sous-circuits indépendamment et en post-traitant classiquement leurs résultats de mesure avec des pondérations de quasi-probabilité appropriées.",
    "D": "Partitionne un grand circuit quantique en sous-circuits plus petits en décomposant les opérations intriquantes globales en unitaires locaux connectés par des qubits ancilla partagés, permettant une exécution distribuée où les mesures d'ancilla médiatisent les interactions entre modules physiquement séparés. La technique reconstruit la sortie du circuit complet en exécutant les sous-circuits indépendamment avec une propagation en milieu de circuit et en post-traitant classiquement leurs résultats avec des corrections de référentiel de Pauli appropriées dérivées des statistiques de mesure.",
    "solution": "C"
  },
  {
    "id": 111,
    "question": "L'extrapolation à bruit zéro est particulièrement bénéfique pendant la phase d'entraînement car elle :",
    "A": "Permet l'estimation du gradient par lots sur plusieurs niveaux de bruit simultanément, où l'exécution parallèle de circuits mis à l'échelle du bruit fournit des échantillons statistiquement indépendants qui réduisent la variance dans les estimations de la fonction de coût via l'extrapolation de Richardson, permettant aux optimiseurs d'atteindre des taux de convergence quadratiquement plus rapides en exploitant la corrélation structurée entre les résultats de mesure mis à l'échelle du bruit pour construire des estimateurs de gradient à variance réduite.",
    "B": "Atténue les erreurs de porte sans modifier la structure du circuit variationnel, permettant à l'optimiseur d'apprendre les paramètres sur la base d'évaluations de la fonction de coût atténuées du bruit qui approximent mieux l'objectif idéal sans bruit, améliorant ainsi la convergence vers des solutions optimales sans nécessiter de reconception du circuit ou de ressources quantiques supplémentaires.",
    "C": "Fournit des estimations de gradient non biaisées en annulant le biais systématique induit par le bruit dans la règle de décalage de paramètre, où l'extrapolation à bruit zéro élimine les contributions d'erreur cohérente qui autrement amèneraient la descente de gradient à converger vers des minima locaux parasites correspondant à des états stabilisés par le bruit plutôt qu'aux véritables états fondamentaux de l'hamiltonien cible dans les solveurs propres quantiques variationnels.",
    "D": "Étend le temps de cohérence effectif des circuits variationnels en post-traitant les données de mesure pour supprimer rétroactivement les effets de décohérence, permettant à l'entraînement de se dérouler comme si les temps de porte étaient raccourcis par l'ordre d'extrapolation, permettant ainsi aux circuits ansatz plus profonds de rester entraînables en compensant la dégradation de fidélité limitée par T₁/T₂ grâce à l'ajustement polynomial des valeurs d'espérance mises à l'échelle du bruit.",
    "solution": "B"
  },
  {
    "id": 112,
    "question": "Dans le contexte de l'apprentissage automatique quantique, considérons un circuit quantique variationnel entraîné sur des données sensibles où les gradients de paramètres sont mesurés et rapportés. L'objectif est de garantir qu'un adversaire observant ces vecteurs de gradient ne puisse pas déduire de détails sur les échantillons d'entraînement individuels au-delà d'un certain seuil de confidentialité ε. Quelle métrique quantifie le plus précisément la garantie de confidentialité dans les cadres de confidentialité différentielle quantique pour ce scénario ?",
    "A": "Calculer la fidélité moyenne F = ⟨ψ_D|ψ_D'⟩ entre les états quantiques |ψ_D⟩ et |ψ_D'⟩ produits par des ensembles de données voisins D et D' qui diffèrent d'un échantillon, puis moyenner cette fidélité sur tous les résultats de mesure possibles pendant l'estimation du gradient, donne la métrique de confidentialité. Puisque la fidélité mesure le recouvrement entre états quantiques, maintenir une fidélité moyenne proche de l'unité (typiquement F ≥ 1-ε) garantit que les circuits quantiques produisent des sorties presque indiscernables pour des ensembles de données voisins, ce qui signifie que les adversaires observant les résultats de mesure ne peuvent pas déterminer de manière fiable quel ensemble de données a été utilisé, fournissant ainsi des garanties de confidentialité différentielle ε par similarité d'états.",
    "B": "Calculer l'entropie de von Neumann S(ρ_out) = -Tr(ρ_out log ρ_out) de la matrice densité réduite obtenue en traçant les qubits ancillaires utilisés pendant l'estimation du gradient par décalage de paramètre fournit la borne de confidentialité, car l'entropie quantifie le caractère mixte des états quantiques et donc l'incertitude à laquelle un adversaire fait face concernant les échantillons d'entraînement individuels. Lorsque les mesures de gradient font s'effondrer l'état, l'entropie résiduelle dans la sortie représente l'information qui reste cachée de l'adversaire, et maintenir S(ρ_out) ≥ log(1/ε) garantit une confidentialité différentielle ε en gardant l'état quantique suffisamment mixte.",
    "C": "L'information de Fisher quantique F_Q par rapport aux paramètres du circuit quantifie directement la quantité d'information sur chaque échantillon d'entraînement encodée dans les mesures de gradient, puisque F_Q détermine la précision ultime avec laquelle les paramètres peuvent être estimés à partir des états quantiques. Par la borne de Cramér-Rao, l'inverse de l'information de Fisher fixe une limite inférieure sur la variance d'estimation, donc contraindre F_Q ≤ 1/ε² garantit qu'aucun adversaire ne peut extraire plus de ε bits d'information sur des échantillons individuels à partir des vecteurs de gradient, établissant ainsi une confidentialité différentielle ε par des limites théoriques de l'information sur la fuite de paramètres.",
    "D": "La distance en trace entre les distributions de sortie lorsque des ensembles de données voisins diffèrent d'un échantillon borne directement la distinguabilité à laquelle un adversaire fait face et correspond à la définition classique de confidentialité différentielle dans le contexte quantique.",
    "solution": "D"
  },
  {
    "id": 113,
    "question": "En théorie de la complexité quantique, que représente la classe BQP ?",
    "A": "Problèmes de décision résolubles par une famille uniforme de circuits quantiques de taille polynomiale avec erreur bilatérale bornée, où 'uniforme' signifie qu'une machine de Turing classique peut générer la description du circuit en temps polynomial dans la taille de l'entrée, garantissant que BQP ne capture que les problèmes avec vérification quantique efficace ainsi que résolution. La borne d'erreur bilatérale (au moins 2/3 d'acceptation sur les instances OUI, au plus 1/3 sur les instances NON) peut être amplifiée à une erreur exponentiellement petite par répétition polynomiale grâce à la borne de Chernoff, rendant BQP robuste sous divers seuils de probabilité contrairement aux classes d'erreur unilatérale telles que NP.",
    "B": "Problèmes de décision résolubles par des ordinateurs quantiques en temps polynomial avec probabilité d'erreur bornée, où l'algorithme quantique doit accepter les instances valides avec probabilité au moins 2/3 et rejeter les instances invalides avec probabilité au moins 2/3, représentant la classe des problèmes que les ordinateurs quantiques peuvent résoudre efficacement avec grande confiance.",
    "C": "Problèmes décidables par des machines de Turing quantiques en temps polynomial avec erreur bornée au plus 1/3 tant pour l'acceptation que pour le rejet, où la machine doit s'arrêter en p(n) étapes pour un certain polynôme p sur des entrées de longueur n, et la probabilité d'erreur est calculée sur les résultats de mesure quantique après l'évolution de l'état final. Cette définition suppose le modèle standard où les mesures intermédiaires ne sont pas requises et tout le calcul se produit via une évolution unitaire suivie d'une mesure projective finale sur les qubits de sortie désignés, bien que cela soit équivalent par le principe de mesure différée aux modèles permettant la mesure en milieu de circuit.",
    "D": "La classe de problèmes avec promesse résolubles par des circuits quantiques avec au plus un écart de distinction inverse-polynomial entre les probabilités d'acceptation sur les instances OUI versus NON, ce qui signifie que pour les entrées dans l'ensemble OUI, la probabilité d'acceptation dépasse 2/3 tandis que pour les entrées NON elle reste inférieure à 1/3, et cet écart peut être amplifié mais seulement à une séparation constante (pas à une erreur exponentiellement petite) car la technique d'amplification d'amplitude quantique par inversion de phase de style Grover nécessite de savoir quel sous-espace amplifier, ce qui nécessiterait lui-même de résoudre le problème—ainsi BQP permet intrinsèquement une erreur résiduelle qui le distingue des classes de complexité exactes comme EQP, qui exige une erreur nulle.",
    "solution": "B"
  },
  {
    "id": 114,
    "question": "Quelle vulnérabilité spécifique est exploitée dans une attaque par faille de lecture quantique ?",
    "A": "La non-linéarité d'amplification du signal exploite le fait que les amplificateurs limités quantiquement utilisés dans les chaînes de lecture présentent une compression de gain dépendant de l'intensité du signal d'entrée, provoquant la variation du facteur d'amplification avec l'état quantique mesuré et créant un biais dans les probabilités de résultats de mesure. Un adversaire peut préparer des états d'entrée près de la région de transition où les effets non linéaires sont les plus forts pour rendre la fidélité de lecture dépendante de l'état d'une manière qui viole les hypothèses standard des preuves de sécurité.",
    "B": "Désappariement d'efficacité des détecteurs entre les bases de mesure, où l'appareil de mesure quantique présente des probabilités de détection systématiquement différentes selon la base sélectionnée pour la mesure. Cette asymétrie permet à un espion d'obtenir des informations partielles sur le choix de la base de mesure en observant la distribution statistique des événements détectés versus non détectés, même sans accéder aux résultats de mesure eux-mêmes.",
    "C": "La synchronisation de sélection de base de mesure exploite la vitesse de commutation finie entre différentes bases de mesure dans les systèmes quantiques pratiques, ciblant l'intervalle pendant lequel les portes de rotation de base sont encore appliquées. Pendant cette fenêtre vulnérable, l'état quantique peut être partiellement projeté sur une base intermédiaire qui combine des caractéristiques des deux réglages prévus, provoquant des résultats reflétant une mesure hybride qui divulgue plus d'information qu'aucune des bases pures ne le révélerait seule.",
    "D": "Les voies de couplage par résonance croisée exploitent les interactions ZZ toujours actives présentes dans les architectures de transmons à fréquence fixe, où les couplages médiés par résonateur entre qubits créent des canaux de mesure non intentionnels pendant les opérations de lecture. Lorsqu'un qubit est mesuré, le champ du résonateur de lecture peut fuir par des voies de résonance croisée vers les qubits voisins, provoquant la décohérence partielle ou la rotation de leurs états selon le résultat de la mesure.",
    "solution": "B"
  },
  {
    "id": 115,
    "question": "Quelle menace spécifique l'analyse de min-entropie quantique aborde-t-elle dans la distribution de clés quantiques ?",
    "A": "En modélisant les imperfections physiques dans les détecteurs, modulateurs et composants optiques comme des sources d'entropie, l'analyse de min-entropie quantique caractérise le déficit d'aléa introduit par des signatures déterministes de canaux auxiliaires telles que la corrélation de gigue temporelle, les artefacts de modulation d'intensité et la dérive de polarisation. Ce budget d'entropie informe ensuite les contre-mesures comme les routines de calibration en temps réel et le filtrage adaptatif, garantissant qu'un adversaire surveillant les émissions électromagnétiques ou la rétrodiffusion optique ne peut pas reconstruire les bits de clé à partir de motifs spécifiques à l'appareil qui ne sont pas capturés dans la description abstraite du protocole au niveau du qubit.",
    "B": "L'analyse de min-entropie quantique quantifie directement la fuite d'information totale vers des parties externes pendant chaque cycle du protocole en mesurant la distinguabilité des états quantiques après correction d'erreur et amplification de confidentialité. En bornant l'information mutuelle entre la clé brute et tout système adversaire par des relations d'incertitude entropiques, elle fournit un nombre concret—exprimé en bits—pour l'avantage qu'un espion a gagné, ce qui détermine ensuite la longueur requise de l'étape d'amplification de confidentialité pour compresser cette fuite en dessous du seuil de sécurité.",
    "C": "L'analyse optimise les taux de clés secrètes en équilibrant les taux d'erreur de bits quantiques contre la surcharge d'amplification de confidentialité, traitant la min-entropie comme un paramètre ajustable qui contrôle le rapport de compression appliqué aux clés tamisées, ce qui permet aux concepteurs de protocoles de maximiser le débit en sélectionnant des fréquences de base et des codes de correction d'erreur qui approchent les limites de capacité de Shannon dans des conditions de canal réalistes.",
    "D": "Estimation de la connaissance de l'espion par des bornes entropiques qui quantifient l'information maximale qu'un adversaire aurait pu extraire des interactions avec le canal quantique, en tenant compte des taux d'erreur observés et de la structure des bases de mesure utilisées pendant l'exécution du protocole, fournissant ainsi une borne supérieure dans le pire cas sur la compromission de la clé.",
    "solution": "D"
  },
  {
    "id": 116,
    "question": "Pourquoi les états du vide sont-ils importants dans ce protocole de distribution quantique de clés sécurisé contre les canaux auxiliaires ?",
    "A": "Les états du vide éliminent les fuites par canaux auxiliaires dues à la modulation d'intensité en fournissant une référence avec un nombre de photons exactement nul, permettant à Alice et Bob de détecter les manipulations via les statistiques du nombre de photons. Cependant, l'information de phase reste encodée dans la structure des modes du champ électromagnétique du vide, nécessitant des étapes de purification supplémentaires pour empêcher les adversaires d'exploiter les efficacités de détection dépendant de la phase qui varient avec les réglages de l'oscillateur local à travers différents modes temporels.",
    "B": "Les états du vide servent de référence de base sécurisée qui ne présente aucune fuite par canaux auxiliaires via la modulation d'intensité ou la dérive de phase, puisqu'ils ne contiennent aucun photon et ne peuvent donc pas révéler par inadvertance d'information via des artéfacts de mesure ou des variations de réponse du détecteur que les adversaires pourraient exploiter.",
    "C": "Les états du vide suppriment les fuites d'information par canaux auxiliaires en découplant le degré de liberté du nombre de photons du choix de base encodée, empêchant les attaques par modulation d'intensité. Leur contenu nul en photons garantit que les effets de temps mort du détecteur et les post-impulsions, qui évoluent avec le flux de photons incidents, ne peuvent pas être corrélés avec les assignations de bits d'Alice. Cependant, les impulsions de vide portent toujours des signatures de modes électromagnétiques distinguables via leur temps de cohérence et leur distribution spectrale, qui peuvent fuiter de l'information si Eve effectue une détection homodyne avec une puissance d'oscillateur local suffisante pour résoudre les fluctuations de quadrature sous le bruit de grenaille.",
    "D": "Les états du vide fournissent une référence indépendante du nombre de photons qui élimine les canaux auxiliaires basés sur l'intensité en forçant Eve à mesurer les fluctuations quantiques plutôt que les variations d'amplitude classiques. L'avantage clé est que les corrélations du champ du vide avec les impulsions de signal suivantes créent un mécanisme d'authentification basé sur l'intrication : le modulateur d'Alice imprime des relations de phase entre les modes du vide et de l'état cohérent que Bob vérifie via des mesures de visibilité interférométrique, et puisque ces corrélations de phase survivent aux pertes de transmission tout en restant invisibles aux attaques par comptage de photons, elles offrent une sécurité inconditionnelle contre les stratégies d'interception-renvoi sans nécessiter de protocoles à états leurres.",
    "solution": "B"
  },
  {
    "id": 117,
    "question": "Pourquoi la latence de routage est-elle aussi critique que la fidélité dans les tâches quantiques sensibles au temps ?",
    "A": "La décohérence dégrade continuellement les états quantiques pendant tout délai, donc une latence de routage excessive permet à la qualité de l'intrication de se détériorer avant que les qubits puissent être mesurés ou manipulés. Dans les protocoles sensibles au temps comme la distribution quantique de clés ou le calcul quantique distribué, même des délais modestes peuvent causer une décohérence accumulée qui pousse les taux d'erreur au-delà des seuils corrigibles, rendant le routage rapide essentiel pour préserver l'information quantique tout au long de l'exécution du protocole.",
    "B": "La synchronisation des réseaux quantiques nécessite un échange d'horodatage classique pour établir l'ordre de causalité des mesures séparées par un intervalle de genre espace, et les délais de routage introduisent une dérive d'horloge dépassant les fenêtres de violation des inégalités de Bell. Dans les protocoles sensibles au temps comme la distribution quantique de clés indépendante du dispositif, la désynchronisation induite par la latence provoque un désalignement temporel entre les événements de détection, effondrant les statistiques de comptage de coïncidence sous les seuils de sécurité et empêchant la vérification des corrélations quantiques essentielles pour la génération d'aléa certifié.",
    "C": "Les algorithmes quantiques distribués emploient des séquences de mesure déterministes avec des résultats communiqués classiquement déclenchant les opérations de portes suivantes, et la latence de routage prolonge directement le temps d'exécution total du protocole en retardant les signaux de contrôle feed-forward. Dans les applications sensibles au temps comme les solveurs variationnels quantiques à travers des processeurs en réseau, les délais de communication accumulés entre les cycles de mise à jour des paramètres causent un ralentissement de la convergence de l'optimiseur, augmentant le temps total jusqu'à ce que les fenêtres d'exécution de circuits limitées par la décohérence expirent avant que l'optimisation ne se termine.",
    "D": "Les protocoles de distribution d'intrication génèrent des paires de photons intriqués en modes temporels où la distinguabilité des modes temporels dépend de la synchronisation du routage maintenant les corrélations de temps d'arrivée dans les temps de cohérence. Une latence de routage excessive introduit des inégalités de longueur de chemin détruisant l'indistinguabilité temporelle entre les modes temporels précoces/tardifs, causant une fuite d'information sur le chemin emprunté qui effondre la visibilité d'interférence dans les mesures de Hong-Ou-Mandel, dégradant la fidélité de l'intrication sous les seuils spécifiques au protocole requis pour la sécurité des communications quantiques ou l'avantage computationnel.",
    "solution": "A"
  },
  {
    "id": 118,
    "question": "Comment différents sous-circuits sont-ils recousus après découpage ?",
    "A": "Le post-traitement classique reconstruit l'observable globale en combinant les statistiques de mesure de chaque fragment de sous-circuit en utilisant des distributions de quasi-probabilité pondérées, ré-échantillonnant effectivement la valeur d'espérance globale sans reconnecter physiquement les circuits.",
    "B": "Le post-traitement classique reconstruit l'observable globale en combinant les statistiques de mesure de chaque fragment de sous-circuit en utilisant des distributions de probabilité pondérées dérivées de l'isomorphisme de Choi-Jamiołkowski, ré-échantillonnant effectivement la valeur d'espérance globale en traitant chaque fragment comme implémentant un canal quantique dont l'action peut être inversée via des corrections d'échantillonnage classiques.",
    "C": "Le post-traitement classique reconstruit l'observable globale en combinant les statistiques de mesure de chaque fragment de sous-circuit en utilisant des distributions de quasi-probabilité pondérées dérivées de protocoles de téléportation, où les poids négatifs apparaissent naturellement de la base surcomplète utilisée pour représenter les canaux quantiques découpés. Cette procédure de ré-échantillonnage récupère la valeur d'espérance complète en appliquant un échantillonnage d'importance classique qui corrige les biais induits par la décomposition.",
    "D": "Le post-traitement classique reconstruit l'observable globale en combinant les statistiques de mesure de chaque fragment de sous-circuit en utilisant des distributions quasi-classiques pondérées obtenues en insérant des résolutions d'identité aux emplacements de découpage, ré-échantillonnant effectivement la valeur d'espérance globale en marginalisant sur les résultats de mesure intermédiaires qui auraient connecté les fragments dans le circuit original.",
    "solution": "A"
  },
  {
    "id": 119,
    "question": "Considérez un algorithme quantique conçu pour résoudre des équations aux dérivées partielles dépendantes du temps en utilisant la simulation hamiltonienne. L'algorithme encode la discrétisation spatiale de l'EDP comme un opérateur matriciel qui fait évoluer l'état quantique. Durant le développement de l'algorithme, vous devez vérifier que les erreurs accumulées ne causent pas la divergence de la simulation, particulièrement lorsque l'opérateur d'évolution est construit à partir de multiples sous-routines encodées par blocs. Pourquoi la norme logarithmique (log-norme) est-elle une quantité utile lors de l'analyse de tels algorithmes quantiques pour les équations différentielles ?",
    "A": "Dans les algorithmes quantiques utilisant des techniques d'encodage par blocs basées sur des ancilles pour représenter des opérateurs non unitaires—tels que ceux provenant d'opérateurs différentiels discrétisés avec des conditions aux limites complexes—la log-norme fournit une borne supérieure cruciale sur la croissance du rayon spectral de la dynamique réduite du registre de contrôle durant le calcul. Chaque application d'une opération encodée par blocs couple le registre système aux qubits ancilles via des opérations unitaires contrôlées, et sans analyse soigneuse, la pureté de l'état ancille pourrait se dégrader exponentiellement avec la profondeur du circuit à mesure que la cohérence de phase se propage à travers l'espace de Hilbert élargi. L'inégalité de la log-norme borne cette croissance du rayon spectral, garantissant que l'accumulation d'erreurs dans les registres ancilles reste polynomiale plutôt qu'exponentielle en nombre d'étapes temporelles, ce qui sinon causerait une défaillance catastrophique du schéma d'encodage par blocs bien avant que la simulation n'atteigne son temps d'évolution cible.",
    "B": "La norme logarithmique fournit des garanties strictes sur la préservation de la normalisation de l'état quantique tout au long des séquences d'opérations unitaires, ce qui devient essentiel lors de la construction de sous-routines d'algorithmes complexes à partir de portes primitives préservant l'amplitude. Dans les solveurs quantiques d'EDP, chaque étape temporelle implique la composition de multiples unitaires—incluant souvent des rotations contrôlées et des portes de phase—et sans la borne de la log-norme, de petites déviations numériques dans les implémentations de portes pourraient causer une dérive de la norme L² du vecteur d'état loin de l'unité sur de nombreuses itérations.",
    "C": "Lors de la conception d'oracles quantiques pour les algorithmes d'équations différentielles, la norme logarithmique sert d'outil critique pour identifier quels opérateurs différentiels discrétisés admettent des implémentations efficaces utilisant uniquement des portes du groupe de Clifford (Hadamard, CNOT et portes de phase). Parce que les opérations de Clifford peuvent être simulées efficacement sur des ordinateurs classiques et corrigées en utilisant des codes stabilisateurs avec un surcoût minimal, les sous-routines d'oracles construites à partir de décompositions uniquement Clifford réduisent considérablement le nombre de portes T gourmandes en ressources qui dominent le coût de la tolérance aux fautes.",
    "D": "La log-norme borne directement si les exponentielles matricielles restent stables durant l'évolution temporelle, empêchant la divergence numérique dans la simulation quantique encodée quelle que soit la façon dont l'opérateur sous-jacent est décomposé en portes quantiques. Lors de la simulation d'EDP via l'évolution hamiltonienne, la mesure de log-norme μ(H) contrôle les taux de croissance exponentiels, fournissant des certificats de stabilité explicites qui vérifient que l'état quantique n'accumule pas d'erreurs incontrôlées à travers multiples étapes de Trotter ou décompositions en formules produit.",
    "solution": "D"
  },
  {
    "id": 120,
    "question": "Comment le clustering quantique k-means se compare-t-il au k-means classique ?",
    "A": "Le k-means quantique atteint une accélération exponentielle grâce à l'encodage en amplitude nécessitant seulement un surcoût logarithmique en qubits dans la dimension des caractéristiques, permettant des calculs de distance en superposition. Cependant, les implémentations pratiques font face à une complexité de préparation d'état évoluant polynomialement avec la taille des données, une perturbation induite par la mesure nécessitant des préparations répétées, et des goulots d'étranglement d'accès QRAM qui éliminent souvent les avantages théoriques à moins que les données n'arrivent pré-encodées dans des structures de mémoire accessibles quantiquement.",
    "B": "Le k-means quantique peut potentiellement offrir des accélérations dans des sous-routines spécifiques comme les calculs de distance en exploitant le parallélisme quantique et l'encodage en amplitude des vecteurs de données. Cependant, l'algorithme fait face à des défis pratiques significatifs incluant le surcoût de mesure, la complexité de préparation d'état, et le besoin de matériel quantique tolérant aux fautes pour réaliser des avantages théoriques sur l'algorithme de Lloyd classique dans des applications réelles de clustering.",
    "C": "Le k-means quantique améliore les garanties de convergence sans accélération du temps d'exécution—l'interférence d'amplitude quantique durant les mises à jour de centroïdes supprime les minima locaux en construisant des paysages de fonction objectif plus lisses via l'interférence destructive des contributions à variance élevée. Cependant, la complexité par itération et le nombre d'itérations correspondent asymptotiquement à l'algorithme de Lloyd classique, fournissant des améliorations de qualité de solution plutôt qu'une accélération computationnelle dans les tâches de clustering pratiques.",
    "D": "Le k-means quantique élimine le raffinement itératif des centroïdes en construisant des unitaires discriminant les clusters via l'évolution adiabatique vers des états fondamentaux encodant les partitions optimales. Cependant, la préparation des hamiltoniens de cluster nécessite un prétraitement classique évoluant avec la taille de l'ensemble de données, le temps d'exécution adiabatique croît polynomialement avec les exigences de précision, et la rétroaction de mesure nécessite de multiples cycles d'évolution, annulant souvent les avantages par rapport aux approches itératives classiques dans les régimes non asymptotiques.",
    "solution": "B"
  },
  {
    "id": 121,
    "question": "Considérez un protocole de transfert inconscient quantique dans lequel Alice envoie deux bits à Bob, qui peut en apprendre exactement un sans révéler son choix. Supposons que le protocole repose sur un encodage de type BB84 et le théorème de non-clonage pour empêcher Bob d'extraire les deux bits. Cependant, un adversaire disposant de ressources suffisantes peut compromettre ces garanties. Quelle méthodologie d'attaque avancée peut compromettre la sécurité de ce protocole ?",
    "A": "Limitations du stockage quantique bruité causant une dégradation de la fidélité des qubits",
    "B": "Contrôle de canal brisant l'intrication impliquant des attaques par relais de type mesure-et-préparation qui interceptent les qubits transmis, effectuent des mesures de base, puis transmettent des états fraîchement préparés aux destinataires, remplaçant ainsi les corrélations quantiques par des corrélations classiques et contournant les protections de non-clonage.",
    "C": "Calcul quantique non-local facilité par une intrication pré-partagée entre des nœuds adverses distribués",
    "D": "Exploitation du stockage quantique borné, où l'attaquant stocke temporairement des états quantiques au-delà de la limite de mémoire supposée du protocole, puis les mesure après que l'information classique est révélée, brisant effectivement la sécurité théorique de l'information qui repose sur une mémoire quantique limitée. Cette approche exploite l'écart entre les bornes de stockage théoriques et les contraintes d'implémentation pratiques, permettant aux adversaires dotés de capacités de stockage même modestement améliorées d'extraire les deux bits en retardant les mesures jusqu'à ce que les données de désambiguïsation deviennent disponibles.",
    "solution": "D"
  },
  {
    "id": 122,
    "question": "Dans un réseau quantique multi-utilisateurs prenant en charge à la fois la téléportation quantique et les applications de détection quantique distribuée, considérez le scénario suivant : trois utilisateurs (Alice, Bob et Charlie) demandent simultanément des paires intriquées, où Alice a besoin d'états GHZ pour un protocole de détection, Bob a besoin de paires de Bell pour une téléportation avec 99% de fidélité, et Charlie a besoin d'états W pour un schéma de communication. Le réseau a une capacité limitée de génération d'intrication à ses nœuds intermédiaires, et une partie de l'intrication pré-partagée existante a commencé à se dégrader. Quelle fonctionnalité clé un ordonnanceur de réseau quantique fournit-il qui n'a pas d'équivalent classique direct dans la gestion de ce problème d'allocation de ressources ?",
    "A": "Coordonner la génération et l'allocation des ressources d'intrication à travers les nœuds tout en tenant compte de la nature monogame des corrélations quantiques qui empêche le partage d'intrication au-delà des coupes bipartites, en équilibrant les demandes hétérogènes des utilisateurs pour différents types d'états intriqués par rapport à la capacité de génération finie et aux exigences variables de fidélité, en priorisant les allocations en fonction des mesures de négativité demandées et de l'accessibilité LOCC des états cibles, en effectuant une optimisation en temps réel des chemins d'échange d'intrication qui préservent les contraintes de sous-additivité forte sur l'entropie de von Neumann pour atteindre les seuils de pureté spécifiques aux applications, tout en gérant une ressource dont la distribution est fondamentalement limitée par les théorèmes de non-diffusion — des contraintes totalement absentes dans l'ordonnancement classique de paquets où la transmission multicast permet une réplication arbitraire à plusieurs destinataires sans dégrader le contenu de l'information transmise ou violer les limites théoriques de l'information fondamentales sur la distribution de corrélation à travers les partitions de réseau",
    "B": "Coordonner la génération et l'allocation des ressources d'intrication à travers les nœuds tout en tenant compte de la nature non stockable et sensible au temps des états quantiques qui se dégradent par décohérence, en équilibrant les demandes hétérogènes des utilisateurs pour différents types d'états intriqués par rapport à la capacité de génération finie et aux exigences variables de fidélité, en priorisant les allocations en fonction à la fois de la complexité des états demandés et de la durée de cohérence restante, en effectuant une optimisation en temps réel des chemins d'échange d'intrication et des protocoles de purification pour atteindre les seuils de fidélité spécifiques aux applications, tout en gérant une ressource qui ne peut être ni copiée ni stockée indéfiniment — des contraintes totalement absentes dans l'ordonnancement classique de paquets où les données peuvent être mises en mémoire tampon, dupliquées et retransmises sans limitations physiques fondamentales sur la durée de stockage ou la réplication",
    "C": "Coordonner la génération et l'allocation des ressources d'intrication à travers les nœuds tout en exploitant l'intrication temps-énergie pour effectuer un multiplexage temporel des canaux quantiques, en équilibrant les demandes hétérogènes des utilisateurs pour différents types d'états intriqués par rapport à la capacité de génération finie et aux exigences variables de fidélité, en priorisant les allocations en fonction du rang de Schmidt demandé et de la visibilité de l'interféromètre de Franson nécessaire pour chaque application, en effectuant une optimisation en temps réel des séquences d'échange d'intrication qui exploitent la post-sélection sur les temps d'arrivée héralds de photons pour augmenter les taux de génération effectifs, tout en gérant une ressource dont la distribution obéit aux contraintes de causalité relativiste empêchant la signalisation superluminique — des limitations totalement absentes dans l'ordonnancement classique de paquets où la propagation de l'information est contrainte uniquement par la dispersion de la fibre et la bande passante de l'amplificateur plutôt que par la structure du cône de lumière",
    "D": "Coordonner la génération et l'allocation des ressources d'intrication à travers les nœuds tout en tenant compte des limites fondamentales imposées par la borne de Holevo sur l'extraction d'information classique, en équilibrant les demandes hétérogènes des utilisateurs pour différents types d'états intriqués par rapport à la capacité de génération finie et aux exigences variables de fidélité, en priorisant les allocations en fonction de l'entropie d'intrication demandée et du contenu informationnel accessible calculable via l'information mutuelle I(X:Y) entre les résultats de mesure, en effectuant une optimisation en temps réel des protocoles de codage dense et des schémas de téléportation superdense pour maximiser l'utilisation de la capacité du canal approchant 2 bits par ebit, tout en gérant une ressource dont l'utilité sature à la quantité χ de Holevo — des contraintes totalement absentes dans les réseaux classiques où la capacité de Shannon évolue linéairement avec la bande passante sans limites supérieures mécaniques quantiques sur la distinguabilité des symboles ou la densité d'information par particule transmise",
    "solution": "B"
  },
  {
    "id": 123,
    "question": "Sur quoi l'« avantage quantique » dans les tâches d'optimisation telles que QAOA est-il censé reposer théoriquement ?",
    "A": "Corrélations non-locales permettant l'échappement des bassins de minima locaux",
    "B": "Échantillonnage non-classique des sous-espaces de basse énergie via des amplitudes de tunneling",
    "C": "Dynamiques non-perturbatives générant des paysages de fonctions de coût favorables",
    "D": "Interférences non-triviales sur de grands espaces de superposition",
    "solution": "D"
  },
  {
    "id": 124,
    "question": "Comment l'échange d'intrication multi-sauts affecte-t-il la fidélité d'une paire de Bell distribuée ?",
    "A": "Chaque échange compose le bruit des mesures d'états de Bell imparfaites et des imperfections initiales des liens, mais la dégradation suit F_final ≈ F_link - (n-1)·ε_BSM où n est le nombre de sauts et ε_BSM est l'infidélité par échange, produisant une décroissance approximativement linéaire plutôt que multiplicative. Cela se produit parce que le bruit de dépolarisation des opérations d'échange indépendantes s'ajoute de manière incohérente, et pour les régimes de haute fidélité (F > 0,95), la formule multiplicative F_total = ∏F_i peut être développée en série de Taylor au premier ordre, montrant que la perte de fidélité est presque additive. Cependant, une fois que F descend en dessous de ~0,90, les termes croisés de second ordre deviennent significatifs et la décroissance s'accélère, finissant par transitionner vers le régime totalement multiplicatif décrit pour les liens de faible fidélité.",
    "B": "Chaque mesure d'état de Bell intermédiaire projette les paires consommées sur une base maximalement intriquée, mettant en œuvre une forme de filtrage d'erreur faible qui supprime partiellement les erreurs de phase héritées des liens antérieurs tout en préservant les erreurs d'amplitude. Cette suppression sélective du bruit fait que la dégradation de la fidélité évolue de manière sous-linéaire avec le nombre de sauts, suivant F_total ≈ F_link^(0,7n) plutôt que le multiplicatif naïf F_link^n, parce que les erreurs de retournement de phase de la génération initiale sont probabilistiquement supprimées lorsqu'elles s'anti-corrèlent entre les deux paires échangées. L'effet devient plus prononcé pour les chaînes plus longues, et bien que la fidélité globale se dégrade toujours, l'exposant amélioré signifie qu'une distribution à dix sauts atteint des fidélités dépassant de plusieurs points de pourcentage ce que la multiplication simple prédirait.",
    "C": "Chaque échange multiplie les fidélités des liens constitutifs, réduisant la fidélité globale. Puisque les mesures d'états de Bell imparfaites et le bruit résiduel dans les paires initiales de courte distance se composent multiplicativement à chaque nœud répéteur, la fidélité de bout en bout F_total est égale au produit F₁ × F₂ × ... × Fₙ des fidélités individuelles des liens, rendant l'intrication distribuée intrinsèquement plus fragile que la génération directe à mesure que le nombre de sauts intermédiaires augmente.",
    "D": "Les opérations d'échange préservent la fidélité totale lorsque les liens constitutifs ont des caractéristiques de bruit appariées (même F_i pour tous les segments), car la mesure d'état de Bell à chaque nœud effectue une vérification de parité qui détecte et corrige les erreurs de retournement de bit héritées du lien précédent. Cette propriété de correction d'erreur signifie que F_total = F_link pour n arbitraire, à condition que tous les segments soient générés avec des protocoles identiques et subissent un bruit de dépolarisation statistiquement indépendant. Cependant, si les liens ont un bruit asymétrique — disons, un segment avec principalement des erreurs de phase et un autre avec amortissement d'amplitude — alors le mécanisme de correction induit par la mesure échoue et la fidélité se dégrade comme F_total ~ min(F_i)·√n, avec le pire lien dominant mais partiellement compensé par la propriété de détection d'erreur de l'échange d'intrication à travers plusieurs segments équilibrés.",
    "solution": "C"
  },
  {
    "id": 125,
    "question": "Dans le contexte de l'apprentissage par renforcement quantique, considérez un agent naviguant dans un environnement en forme de labyrinthe où certaines transitions d'état sont classiquement interdites en raison de barrières d'énergie, mais quantiquement accessibles via des effets de tunneling. L'agent utilise un circuit quantique variationnel pour représenter sa politique, avec un encodage d'amplitude de l'espace d'états et des portes de rotation paramétrées déterminant les probabilités d'action. Comment la superposition quantique modifie-t-elle fondamentalement la capacité d'exploration de l'agent par rapport aux stratégies d'exploration classiques epsilon-greedy ou softmax ?",
    "A": "La superposition permet l'évaluation simultanée de plusieurs actions dans un état donné, mais cet avantage est largement théorique — en pratique, l'effondrement de la mesure force l'agent à s'engager sur une seule trajectoire, et l'accélération réelle provient de l'utilisation de l'algorithme de Grover pour rechercher dans le tampon de rejeu des expériences de haute valeur pendant la phase d'apprentissage. Le circuit quantique prépare une superposition sur toutes les transitions stockées, applique une amplification d'amplitude pour augmenter les coefficients des échantillons à forte erreur TD, et mesure pour sélectionner des expériences pour les mises à jour de gradient, fournissant une accélération en racine carrée par rapport à l'échantillonnage aléatoire uniforme.",
    "B": "Le tunneling quantique à travers les barrières de fonction de valeur est le mécanisme principal — l'agent peut traverser des régions énergétiquement défavorables de l'espace d'états sans accumuler de récompense négative, similairement à la façon dont les électrons tunnellisent à travers les barrières de potentiel en physique du solide, ce qui est fondamentalement impossible pour les agents RL classiques contraints par les statistiques de Boltzmann.",
    "C": "L'avantage principal est l'intrication quantique entre différentes branches de l'espace d'états, qui corrèle les signaux de récompense à travers des régions distantes du MDP de manières qui violent les inégalités de Bell, permettant à l'agent d'apprendre la structure globale de la fonction de valeur exponentiellement plus rapidement que les méthodes basées sur des mises à jour TD locales. Spécifiquement, lorsque l'agent visite l'état s_i et reçoit une récompense r_i, l'intrication propage cette information instantanément aux représentations des états s_j qui peuvent être arbitrairement éloignés dans le graphe de transition.",
    "D": "La superposition permet à l'agent d'évaluer effectivement une combinaison cohérente de plusieurs paires état-action en une seule passe avant à travers le circuit quantique, créant des motifs d'interférence qui peuvent guider le gradient de la politique vers des régions de l'espace d'action qui nécessiteraient de nombreux déroulements classiques séquentiels pour être découvertes, particulièrement lorsqu'elle est combinée avec des techniques d'amplification d'amplitude qui améliorent la probabilité d'échantillonner des trajectoires à haute récompense. Cela représente une véritable rupture par rapport à l'exploration stochastique classique car le circuit quantique peut interférer de manière constructive les chemins vers des états de haute valeur.",
    "solution": "D"
  },
  {
    "id": 126,
    "question": "Quelle vulnérabilité spécifique permet la fuite d'informations entre programmes dans les calculs quantiques séquentiels ?",
    "A": "Les substrats de qubits supraconducteurs contiennent des concentrations diluées de défauts à deux niveaux (TLS) à l'échelle atomique — typiquement des lacunes d'oxygène ou des liaisons pendantes dans les couches d'interface amorphes — qui se couplent de manière cohérente aux transitions de qubits avec des forces variant de 1 à 100 MHz selon la proximité spatiale et l'orientation du moment dipolaire. Lorsque le calcul de l'utilisateur précédent pilote des qubits particuliers à travers des opérations de portes répétées, les défauts TLS voisins subissent une saturation et une inversion de population qui persistent pendant des temps anormalement longs (10-1000 secondes) en raison d'un faible couplage thermique au bain de phonons du réfrigérateur à dilution à 10-20 mK.",
    "B": "La transmission des signaux de contrôle par des lignes de transmission coaxiales crée une hystérésis électromagnétique dans les films métalliques et les diélectriques, induisant des motifs de magnétisation persistants qui déplacent les fréquences de résonance de 10 à 50 kHz selon les séquences d'impulsions précédentes. Les attaquants peuvent mesurer ces décalages de fréquence par interférométrie de Ramsey pour reconstruire les séquences de portes précédentes et les informations de temporisation à partir de la mémoire magnétique stockée dans l'infrastructure de contrôle classique.",
    "C": "Les impulsions de lecture micro-ondes injectées dans les résonateurs supraconducteurs couplés aux qubits induisent des champs électromagnétiques oscillants qui persistent pendant plusieurs durées de vie de cavité (Q/2πf ≈ 200-500 ns pour des résonateurs typiques de 7 GHz avec des facteurs de qualité Q ~ 10^4-10^5) après la fin des opérations de mesure. Ces excitations résiduelles du résonateur — des photons piégés dans des modes de cavité quasi-liés subissant une décroissance exponentielle — restent stockés de manière cohérente lorsque le programme de l'utilisateur suivant commence à s'exécuter sur le même matériel, créant un canal auxiliaire photonique qui encode les résultats de mesure du travail précédent. En implémentant des schémas de détection hétérodyne ou des décalages de fréquence dépendant de l'état des qubits pendant leurs portes initiales, l'utilisateur suivant peut effectivement « écouter » le champ décroissant du résonateur et extraire les statistiques de mesure du calcul précédent, même si les qubits eux-mêmes ont subi une relaxation T1 vers l'état fondamental.",
    "D": "La réinitialisation incomplète des qubits entre des exécutions successives de programmes permet à l'information d'état quantique résiduelle — incluant les populations d'états excités, les cohérences de phase et les corrélations d'intrication provenant de calculs précédents — de persister et de devenir accessible aux utilisateurs suivants par des circuits de sondage soigneusement conçus. Cet échec à restaurer complètement les qubits à leur état fondamental crée un canal auxiliaire quantique où la structure algorithmique et les résultats de mesure des travaux précédents fuient à travers les frontières des programmes.",
    "solution": "D"
  },
  {
    "id": 127,
    "question": "En quoi le concept d'intrication de bout en bout diffère-t-il fondamentalement de la connectivité de bout en bout classique ?",
    "A": "L'intrication permet une communication basée sur la téléportation qui consomme la paire intriquée pendant la transmission d'information quantique entre les points terminaux, nécessitant une régénération continue contrairement aux canaux classiques — cependant, le protocole de téléportation permet la transmission d'états quantiques arbitraires avec une fidélité parfaite indépendamment de la distance, ce qui donne aux réseaux quantiques un avantage dans les applications sensibles à la latence puisque l'étape de communication classique dans la téléportation peut être précalculée et transmise pendant les périodes d'inactivité avant même que l'état quantique à téléporter soit préparé.",
    "B": "Les états intriqués se dégradent lors de la mesure et ne peuvent être ni clonés ni amplifiés en vertu du théorème de non-clonage, forçant les réseaux quantiques à régénérer continuellement l'intrication entre les nœuds pour maintenir la connectivité — contrairement aux signaux classiques qui tolèrent l'amplification — mais la décohérence induite par la mesure procède de manière déterministe selon l'équation maîtresse de Lindblad, permettant une prédiction précise du moment où l'intrication doit être rafraîchie en fonction du temps d'interaction environnementale accumulé plutôt que de seuils de fidélité probabilistes.",
    "C": "L'intrication fournit des corrélations non locales qui sont consommées lors de la mesure ou des opérations quantiques en raison de l'effondrement de la fonction d'onde, et le théorème de non-clonage empêche la copie ou l'amplification des états intriqués — ainsi les réseaux quantiques nécessitent une régénération constante d'intrication contrairement aux liaisons classiques avec amplification indéfinie du signal — cependant, le taux de consommation suit une loi de décroissance universelle indépendante de la méthode de génération d'intrication, les paires de Bell se dégradant à 1/√t par mesure indépendamment de leur origine par conversion paramétrique descendante spontanée ou stockage dans un ensemble atomique.",
    "D": "L'intrication est consommée lorsqu'elle est mesurée ou lorsqu'une opération quantique est effectuée dessus — on ne peut fondamentalement ni amplifier les états intriqués ni les cloner en raison du théorème de non-clonage, ce qui signifie que les réseaux quantiques nécessitent une régénération constante de paires intriquées entre les nœuds pour maintenir la connectivité, contrairement aux liaisons classiques où les signaux peuvent être amplifiés indéfiniment.",
    "solution": "D"
  },
  {
    "id": 128,
    "question": "La probabilité de collision dans l'échantillonnage bosonique augmente lorsque les photons se regroupent parce que les événements groupés :",
    "A": "Correspondent à des permanents de matrices dont les arguments ont des lignes répétées, qui peuvent être calculés efficacement en utilisant le principe d'inclusion-exclusion appliqué à la décomposition en classes du groupe symétrique. Spécifiquement, lorsque k photons occupent le même mode de sortie, le permanent se factorise en un produit de k! termes identiques, chacun évaluant une sous-matrice (n-k+1)×(n-k+1), réduisant le calcul #P-complet à k! calculs de déterminants en temps polynomial dont les résultats se multiplient pour donner le carré de la magnitude de l'amplitude de transition.",
    "B": "Satisfont la règle de sélection bosonique qui exige que toutes les distributions de nombres de photons d'entrée et de sortie aient une parité correspondante à travers chaque paire de modes couplée par l'hamiltonien de l'interféromètre. Lorsque les photons se regroupent, ils créent nécessairement des configurations de parité paire qui se trouvent dans l'espace propre +1 de l'opérateur d'échange spatial du réseau, et les permanents de matrices correspondant à de telles configurations se décomposent sous forme bloc-diagonale où chaque bloc correspond à un sous-espace à deux photons, réduisant la dimension matricielle effective de n à environ n/2 pour les événements k-regroupés.",
    "C": "Dépendent de permanents de matrices plus petites que le nombre de photons. Lorsque k photons se regroupent dans un seul mode de sortie, l'amplitude de transition implique le calcul du permanent d'une sous-matrice (n-k)×(n-k) plutôt que de la matrice n×n complète, réduisant la complexité dimensionnelle du calcul #P-complet requis pour déterminer la probabilité de cette configuration.",
    "D": "Présentent une interférence constructive qui concentre la masse de probabilité sur un sous-ensemble de taille polynomiale de l'espace de sortie exponentiellement large, spécifiquement les O(n^k) configurations où au moins k photons partagent un mode. Les algorithmes classiques exploitent cette structure par échantillonnage d'importance du secteur regroupé en utilisant un échantillonnage par rejet pondéré par la magnitude du permanent, atteignant une ε-approximation avec O(n^k/ε) échantillons plutôt que les O(2^n/ε²) requis pour un échantillonnage uniforme sur toutes les configurations de sortie, bien que des résultats récents montrent que cet avantage disparaît pour k > n^(1/3).",
    "solution": "C"
  },
  {
    "id": 129,
    "question": "Pourquoi les algorithmes efficaces matériellement évitent-ils l'inversion de matrice ?",
    "A": "L'inversion de matrice dans la métrique d'information de Fisher quantique — souvent requise pour l'optimisation du gradient naturel — exige d'estimer O(p²) éléments hors diagonale où p est le nombre de paramètres, et chaque élément nécessite un nombre exponentiellement élevé de répétitions de circuit pour être résolu à haute précision en raison de la suppression exponentielle des chevauchements entre états propres presque dégénérés, causant à la procédure d'inversion de consommer des budgets de tirs prohibitifs qui évoluent en exp(p) même lorsque la matrice est bien conditionnée.",
    "B": "L'inversion de matrice devient numériquement instable lorsqu'elle est appliquée à des tenseurs métriques mal conditionnés qui surviennent couramment dans l'optimisation de paramètres variationnels, où de petites valeurs propres conduisent à un bruit amplifié dans les éléments de matrice inversés, causant la divergence des estimations de gradient et empêchant une convergence fiable du paysage d'optimisation.",
    "C": "Les ansätze efficaces matériellement génèrent typiquement des jacobiens de paramètres avec des nombres de conditionnement qui croissent exponentiellement avec la profondeur du circuit en raison de plateaux arides, et inverser ces matrices mal conditionnées amplifie le bruit d'échantillonnage par l'inverse de la plus petite valeur propre — puisque l'estimation du gradient nécessite déjà O(1/ε²) tirs pour une précision ε, l'étape d'inversion multiplie cela par κ² où κ est le nombre de conditionnement, faisant évoluer le coût total en O(exp(profondeur)/ε²), ce qui épuise rapidement les budgets de tirs disponibles.",
    "D": "Les algorithmes quantiques produisent intrinsèquement des transformations unitaires qui préservent la norme de l'espace de Hilbert, ce qui signifie que toutes les opérations directement implémentables doivent correspondre à des plongements isométriques avec des vecteurs colonnes orthogonaux — l'inversion de matrice, particulièrement de matrices non normales provenant de tenseurs de covariance de gradient, produit des transformations qui dilatent ou contractent les normes vectorielles de manière non unitaire, nécessitant une dilatation assistée par ancille dans un espace plus grand où l'inverse est incorporé comme un bloc unitaire, ce qui double les exigences en qubits et introduit une surcharge de mesure d'ancille qui dégrade la fidélité de mise à jour des paramètres.",
    "solution": "B"
  },
  {
    "id": 130,
    "question": "Quel mécanisme spécifique fournit la confidentialité différentielle quantique dans les canaux quantiques bruités ?",
    "A": "La randomisation de phase introduit de l'incertitude en appliquant des portes de phase aléatoires échantillonnées à partir d'une distribution continue, typiquement uniforme sur [0, 2π), à chaque qubit avant transmission à travers le canal quantique. Ce mécanisme obscurcit l'information de phase relative qui pourrait autrement distinguer entre différents états d'entrée appartenant à des ensembles de données voisins. Puisque les impulsions de phase s'accumulent de manière incohérente à travers l'ensemble des choix aléatoires possibles, tout adversaire tentant d'extraire des informations au niveau individuel par tomographie quantique doit faire face à un processus de déphasage effectif.",
    "B": "Le déphasage sert de mécanisme de bruit fondamental pour la confidentialité différentielle quantique car il détruit sélectivement les éléments hors diagonale de la matrice de densité dans la base de calcul, effaçant ainsi les corrélations quantiques qui fuiteraient autrement des informations sur les points de données individuels. Lorsqu'un état quantique passe à travers un canal de déphasage avec un taux de décohérence γ soigneusement calibré, les termes de cohérence décroissent exponentiellement comme exp(-γt), créant un compromis vie privée-utilité ajustable où un déphasage plus fort fournit des bornes de confidentialité plus serrées au prix d'une fidélité de mesure réduite.",
    "C": "La dépolarisation sert de mécanisme de bruit fondamental en appliquant un mélange uniforme d'erreurs de Pauli à chaque qubit avec une probabilité p soigneusement calibrée. Cela crée un canal qui fait évoluer tout état d'entrée vers l'état maximalement mélangé à un taux contrôlé, masquant efficacement les contributions de données individuelles tout en préservant les propriétés statistiques agrégées. Le canal de dépolarisation satisfait les exigences de composition pour la confidentialité différentielle car il fournit un bruit symétrique à travers tous les états de base.",
    "D": "L'amortissement d'amplitude introduit un processus dissipatif contrôlé qui transfère asymétriquement les états quantiques vers l'état fondamental |0⟩, implémentant effectivement une forme de bruit quantique qui masque les contributions d'entrées individuelles dans l'ensemble de données compilé. En concevant le taux d'amortissement κ pour évoluer de manière appropriée avec la taille de l'ensemble de données et les paramètres de sensibilité, le mécanisme garantit que toute requête appliquée à l'état quantique bruité révèle des informations sur les propriétés statistiques agrégées tout en fournissant un déni plausible pour les enregistrements individuels. Le superopérateur d'amortissement d'amplitude crée une évolution non unitaire qui limite fondamentalement la distinguabilité entre bases de données quantiques voisines.",
    "solution": "C"
  },
  {
    "id": 131,
    "question": "Quelle est la différence entre un état de Bell et un état GHZ ?",
    "A": "Les états de Bell sont définis exclusivement pour les particules de spin-1/2 et nécessitent une décomposition en base singulet-triplet via les coefficients de Clebsch-Gordan sommant les moments angulaires individuels aux nombres quantiques de spin total, tandis que les états GHZ se généralisent à des qudits de dimension arbitraire à travers des opérateurs de Weyl multipartites qui génèrent des états de graphe maximalement intriqués sur des topologies biparties complètes. La distinction structurelle clé réside dans la symétrie : les états de Bell se transforment de manière irréductible sous les opérations locales SU(2)⊗SU(2) préservant le spin total, tandis que les états GHZ présentent une symétrie de permutation sous le réétiquetage cyclique des qubits, ce qui en fait des états propres naturels des opérateurs de spin collectif J².",
    "B": "Les états de Bell démontrent une violation maximale de l'inégalité CHSH avec la borne de Tsirelson 2√2 à travers des mesures de corrélation dans des bases complémentaires séparées par des angles de 22,5 degrés, tandis que les états GHZ atteignent des corrélations non classiques plus fortes en violant les inégalités de Mermin-Klyshko avec des marges de violation croissant exponentiellement à mesure que le nombre de qubits augmente, atteignant des bornes classiques qui évoluent comme 2^(n-1) contre des prédictions quantiques de 2^(n/2) pour des systèmes à n parties. Les deux classes d'états sont des états stabilisateurs, mais les états de Bell occupent des espaces de Hilbert à deux qubits admettant quatre bases orthogonales maximalement intriquées, tandis que les états GHZ nécessitent au moins trois qubits pour présenter une intrication multipartite genuine qui ne peut être créée par des opérations par paires et communication classique.",
    "C": "Les états de Bell maintiennent leur cohérence sous un bruit de dépolarisation local avec une fidélité décroissant exponentiellement comme F(t)=¼(1+3e^(-4γt)) où γ est le taux de décohérence d'un seul qubit, tandis que les états GHZ présentent une fragilité superexponentielle sous la décohérence affectant n'importe quel qubit individuel, avec une fidélité s'effondrant comme F(t)≈e^(-nγt²) en raison de la nature tout-ou-rien de la cohérence de phase à n qubits requise pour maintenir la superposition (|000⟩+|111⟩)/√2. Cette robustesse différentielle provient de la structure symétrique à deux parties des états de Bell permettant une correction d'erreur par des opérations de filtrage locales, tandis que la susceptibilité des états GHZ découle de leur hypersensibilité aux erreurs de phase : un seul événement de décohérence d'un qubit projette l'état entier dans un mélange séparable, détruisant les corrélations à n voies essentielles pour l'avantage quantique dans des tâches comme le partage de secrets et les protocoles d'accord byzantin.",
    "D": "Les états de Bell impliquent exactement deux qubits dans une configuration maximalement intriquée décrite par les quatre paires EPR canoniques (|Φ±⟩ et |Ψ±⟩), formant la base des protocoles de téléportation quantique et de codage superdense, tandis que les états GHZ impliquent trois qubits ou plus avec des propriétés d'intrication multipartite spécifiques qui ne peuvent être décomposées en sous-systèmes intriqués par paires, présentant des structures de corrélation fondamentalement différentes qui révèlent des violations plus fortes du réalisme local à travers les inégalités de Mermin plutôt que les tests CHSH standards.",
    "solution": "D"
  },
  {
    "id": 132,
    "question": "Comment appelle-t-on le processus d'adaptation d'un circuit quantique à un matériel spécifique ?",
    "A": "Le pipelining est le processus de décomposition d'un circuit quantique en étapes séquentielles qui peuvent être exécutées en succession temporelle tout en respectant les contraintes matérielles, analogue au pipelining d'instructions dans les processeurs classiques. Cette approche planifie les opérations de portes pour maximiser le débit en chevauchant l'exécution d'opérations indépendantes sur différents sous-ensembles de qubits, transformant effectivement un circuit logique en un calendrier d'exécution optimisé pour le matériel qui tient compte de la connectivité limitée, des fidélités des portes et des contraintes temporelles spécifiques de l'architecture de contrôle du processeur quantique cible.",
    "B": "La compilation englobe le pipeline complet de transformation depuis les algorithmes quantiques de haut niveau jusqu'aux instructions exécutables par le matériel, incluant la décomposition des portes en opérations natives, l'optimisation du circuit par des règles de réécriture algébrique, l'assignation des qubits aux qubits physiques en respectant les contraintes de connectivité, et l'insertion de portes SWAP pour router les opérations multi-qubits à travers des topologies limitées. Ce processus complet transforme les circuits quantiques abstraits en séquences d'impulsions concrètes ou en microcode qui pilote directement le matériel, ce qui en fait le terme correct pour adapter les circuits aux processeurs quantiques spécifiques.",
    "C": "Le linking fait référence au processus abstrait de connexion des descriptions de portes de haut niveau à l'ensemble d'opérations natives du matériel disponible sur le processeur quantique cible.",
    "D": "Le mapping ou la transpilation est le terme standard pour adapter les circuits quantiques aux contraintes matérielles spécifiques, incluant le routage, la décomposition des portes et l'optimisation pour la topologie du dispositif cible.",
    "solution": "D"
  },
  {
    "id": 133,
    "question": "Pourquoi les techniques de réseautique classique ne parviennent-elles pas à relever les défis de l'Internet quantique ?",
    "A": "Les limitations de bande passante et l'absence de support pour le routage basé sur la superposition empêchent les protocoles classiques de gérer efficacement le trafic quantique, car les canaux quantiques nécessitent un débit exponentiellement plus élevé pour préserver la cohérence à travers les segments du réseau. Les routeurs classiques traitent les paquets séquentiellement et ne peuvent exploiter le parallélisme inhérent aux états quantiques superposés voyageant le long de plusieurs chemins simultanément. De plus, les algorithmes de contrôle de congestion TCP/IP supposent des capacités de liaison déterministes, alors que les liaisons quantiques connaissent des fidélités de transmission dépendant de l'état qui varient avec les taux de distribution d'intrication.",
    "B": "Elles supposent une livraison déterministe des paquets avec des mécanismes fiables de retransmission et d'accusé de réception, contrairement à la nature probabiliste de l'arrivée des qubits qui dépend de la fidélité du canal quantique et des résultats de mesure. Les codes de correction d'erreur classiques comme CRC et les sommes de contrôle reposent sur la copie du contenu des paquets pour détecter la corruption, mais l'information quantique ne peut être clonée, donc ces techniques détruisent l'état quantique lors de l'inspection.",
    "C": "Le théorème de non-clonage et l'effondrement induit par la mesure empêchent l'application directe de techniques classiques comme la copie de paquets, la mise en mémoire tampon et la détection d'erreurs, qui reposent fondamentalement sur la duplication d'informations sans perturber l'état original.",
    "D": "Les réseaux quantiques nécessitent un matériel de commutation plus rapide que ce que les systèmes classiques peuvent fournir, notamment pour maintenir la cohérence pendant les décisions de routage à travers plusieurs sauts réseau. Les routeurs classiques introduisent une latence de l'ordre de microsecondes par saut en raison des délais de commutation électronique, mais les qubits décohèrent en quelques nanosecondes dans les implémentations actuelles, ce qui signifie que même les vitesses de commutation classiques optimisées causent une perte inacceptable de fidélité d'état. Le traitement électronique requis pour l'inspection des en-têtes, les consultations de tables de routage et les décisions de transfert fonctionne intrinsèquement sur des échelles de temps incompatibles avec la préservation de la superposition quantique.",
    "solution": "C"
  },
  {
    "id": 134,
    "question": "Les techniques d'atténuation des erreurs de mesure comme les matrices d'erreur de lecture sont typiquement appliquées :",
    "A": "Durant l'étape de transpilation où elles convertissent les portes non natives en primitives spécifiques au dispositif en décomposant chaque opération de porte en séquences qui compensent intrinsèquement les motifs d'erreur de lecture connus, pré-corrigeant effectivement la structure du circuit elle-même. Le transpilateur identifie les opérations de mesure et insère des rotations correctives immédiatement avant chaque mesure basées sur la matrice de confusion caractérisée, de sorte que lorsque la mesure physique biaisée se produit, elle produit des statistiques qui approximent ce qui serait obtenu d'une mesure projective idéale sur le véritable état quantique.",
    "B": "Avant l'exécution du circuit, en ajustant les formes et durées d'impulsions pour minimiser les erreurs de lecture pendant la compilation, intégrant effectivement la correction dans la couche matérielle elle-même. Cette approche préventive calibre les opérateurs de mesure en utilisant des matrices de caractérisation inverses dérivées de la tomographie préliminaire du système, modifiant les paramètres d'impulsion de lecture de sorte que les résultats de mesure physique soient déjà corrigés lorsqu'ils émergent du processeur quantique, éliminant le besoin de tout post-traitement des distributions de probabilité.",
    "C": "Au sein de la boucle d'optimisation classique où elles redimensionnent les magnitudes de gradient par l'inverse des valeurs propres de la matrice de confusion de lecture, garantissant que les mises à jour de paramètres tiennent compte du biais systématique de mesure pendant l'entraînement d'algorithme variationnel. Les matrices d'atténuation sont multipliées élément par élément avec les gradients calculés avant l'exécution de l'étape d'optimisation.",
    "D": "Après les mesures, pour corriger les distributions de probabilité de sortie basées sur des matrices de confusion calibrées",
    "solution": "D"
  },
  {
    "id": 135,
    "question": "Quel est l'avantage principal des codes d'expansion quantiques en termes d'évolution des ressources ?",
    "A": "Ils atteignent une complexité d'extraction de syndrome logarithmique grâce à la haute connectivité du graphe d'expansion, qui permet de mesurer chaque stabilisateur en utilisant seulement O(log n) qubits ancillaires via des vérifications de parité parallèles récursives. Cette structure d'arbre de mesure hiérarchique, rendue possible par les propriétés spectrales du graphe, réduit la surcharge de mesure de syndrome de linéaire à logarithmique tout en maintenant la même distance de code que les codes stabilisateurs conventionnels.",
    "B": "Ils atteignent une bonne distance de code tout en maintenant un taux d'encodage constant, ce qui signifie que le rapport entre qubits logiques et physiques reste favorable à mesure que la taille du système évolue. De plus, leur connectivité de graphe structurée permet des algorithmes de décodage classiques efficaces qui s'exécutent en temps quasi-linéaire, les rendant pratiques pour la correction d'erreurs en temps réel dans les processeurs quantiques à grande échelle.",
    "C": "Ils réduisent le bruit de mesure du syndrome en utilisant la propriété d'expansion d'arête du graphe d'expansion pour distribuer l'information de syndrome à travers plusieurs opérateurs de vérification redondants. Chaque erreur physique produit des syndromes dans Θ(log n) stabilisateurs voisins plutôt que seulement les plus proches voisins, permettant un vote majoritaire pour supprimer les erreurs de mesure sans nécessiter de cycles répétés d'extraction de syndrome.",
    "D": "Ils permettent une extraction de syndrome à profondeur constante grâce à l'écart spectral de l'expandeur, qui garantit que les générateurs de stabilisateur peuvent être mesurés en O(1) couches parallèles quelle que soit la taille du code. La propriété de mélange rapide du graphe garantit que l'information de syndrome se propage à tous les opérateurs de vérification en un nombre fixe d'étapes, éliminant le goulot d'étranglement d'évolution en profondeur présent dans les codes de surface où les circuits de syndrome croissent avec le diamètre du réseau.",
    "solution": "B"
  },
  {
    "id": 136,
    "question": "Quelle propriété des systèmes quantiques est la plus pertinente pour l'accélération quantique potentielle dans le clustering k-means ?",
    "A": "Le calcul des distances en superposition permet aux algorithmes quantiques d'évaluer la distance euclidienne entre un point de données et les k centroïdes simultanément au sein d'une seule requête à un oracle de distance quantique, réduisant la complexité d'affectation par point de O(k) calculs de distance classiques à O(1) opérations quantiques. Ce calcul de distance basé sur la superposition exploite l'encodage en amplitude des vecteurs de caractéristiques et la lecture par interférence pour réduire l'étape de comparaison des centroïdes à un résultat de mesure qui révèle directement l'affectation au cluster le plus proche.",
    "B": "L'amplification d'amplitude basée sur Grover appliquée aux affectations de clusters permet une accélération quadratique dans la phase de mise à jour des centroïdes en traitant chaque k-partition possible comme un état marqué dans l'espace de recherche. L'algorithme encode tous les n points de données en superposition et applique un opérateur de diffusion qui amplifie les configurations où la variance intra-cluster est minimisée, réduisant la complexité d'itération de O(√n) itérations Lloyd classiques à O(n^(1/4)) itérations quantiques. Cette amélioration basée sur l'amplitude exploite l'interférence destructive pour supprimer les affectations à haute variance tout en renforçant constructivement les placements optimaux de centroïdes par des opérations répétées d'inversion autour de la moyenne.",
    "C": "L'estimation de phase quantique appliquée à la matrice de covariance des clusters permet la décomposition en valeurs propres en profondeur O(log d) pour des vecteurs de caractéristiques de dimension d, comparé à O(d³) pour la décomposition en valeurs singulières classique, en encodant l'opérateur de covariance comme un unitaire dont le retour de phase révèle directement les directions des composantes principales. Cette analyse spectrale permet à l'algorithme d'identifier les orientations naturelles des clusters dans l'espace des caractéristiques par lecture des vecteurs propres, réduisant la dimensionnalité tout en préservant la séparabilité des clusters. Les valeurs propres encodées en phase réduisent l'affinement des centroïdes de la minimisation itérative de distance à une seule mesure de la direction propre dominante pour chaque cluster.",
    "D": "Le recuit quantique exploite les fluctuations thermiques dans le champ transverse pour échapper aux minima locaux, mais l'avantage critique provient du maintien de la superposition cohérente sur les configurations de clusters pendant le programme de recuit, et non du saut thermique classique. L'algorithme encode les variables d'appartenance comme des qubits logiques dans un hamiltonien d'Ising frustré où l'état fondamental correspond à la distance intra-cluster minimale, et le taux de balayage adiabatique est ajusté pour maintenir le système dans l'état propre instantané tant que l'écart d'énergie reste polynomialement inverse en n. Cette évolution cohérente évite le ralentissement exponentiel du recuit simulé en utilisant l'interférence quantique plutôt que l'amplitude de tunneling.",
    "solution": "A"
  },
  {
    "id": 137,
    "question": "Dans les architectures de calcul quantique distribué, quelle propriété fondamentale détermine si un algorithme quantique peut être efficacement partitionné entre plusieurs processeurs quantiques connectés par des canaux quantiques à bande passante limitée ? Considérez à la fois la surcharge de profondeur de circuit et les exigences de communication classique pour maintenir la fidélité d'intrication dans le système distribué.",
    "A": "Un partitionnement efficace nécessite que l'algorithme se décompose en blocs computationnels présentant ce que les théoriciens des systèmes distribués appellent la 'séparabilité quantique' — la propriété selon laquelle l'état de sortie de chaque bloc peut être exprimé comme un produit tensoriel avec des interfaces classiques bien définies. Lorsque le circuit quantique satisfait cette décomposition, avec le nombre de qubits traversant les frontières de partition évoluant au plus logarithmiquement avec la taille totale du système, alors la correction d'erreur classique sur les liaisons inter-processeurs suffit à maintenir la cohérence. L'absence d'intrication à longue portée dans la base computationnelle signifie que la surcharge de téléportation reste sous-linéaire.",
    "B": "L'exécution distribuée devient pratique lorsque la représentation du vecteur d'état de l'algorithme maintient un rang de Schmidt faible à travers toute bipartition séparant les processeurs, ce qui signifie que la plupart de l'information quantique reste localisée plutôt que globalement intriquée. Si l'algorithme peut être reformulé de sorte que les qubits de chaque processeur interagissent principalement par propagation classique des résultats de mesure plutôt que par des portes quantiques directes, alors la phase coûteuse de distribution d'intrication se réduit à un coût d'installation unique amorti sur de nombreuses exécutions de circuit. L'observation critique est que les algorithmes exprimables dans le formalisme du calcul quantique basé sur les mesures satisfont naturellement ce critère car la connectivité de l'état cluster peut être conçue pour correspondre à la topologie du réseau.",
    "C": "Le facteur déterminant est de savoir si la classe de complexité de l'algorithme quantique reste inchangée sous la restriction que seules les portes entre plus proches voisins sont permises dans le modèle de circuit sous-jacent. Les algorithmes naturellement adaptés aux architectures distribuées sont précisément ceux qui évitent la transformée de Fourier quantique rapide comme opération primitive, puisque la QFT nécessite une connectivité totale dans sa décomposition standard. Lorsqu'un algorithme peut être réécrit en utilisant uniquement des hamiltoniens locaux et des interactions entre plus proches voisins, il se projette naturellement sur des processeurs distribués en assignant des régions de qubits spatialement contiguës à chaque nœud.",
    "D": "Le facteur clé est la localité dans le graphe d'interaction — les algorithmes où la plupart des portes agissent sur des qubits voisins dans une certaine topologie logique peuvent être partitionnés en assignant des régions contiguës à chaque processeur, minimisant l'échange d'intrication coûteux entre nœuds distants. La métrique critique est le temps de mélange spatial du circuit par rapport aux échelles de temps de décohérence des canaux quantiques inter-processeurs. Lorsque ce rapport reste borné par un facteur polynomial, l'implémentation distribuée maintient l'avantage quantique malgré la surcharge de téléportation des états quantiques à travers les frontières de processeurs pour les portes non locales.",
    "solution": "D"
  },
  {
    "id": 138,
    "question": "Dans le contexte du calcul quantique topologique, considérez un système où des anyons sont tressés pour implémenter des portes logiques, et l'espace computationnel est protégé par l'écart d'énergie vers les états excités. La protection repose sur le maintien du système à des températures bien en dessous de cet écart. Si nous effectuons des mesures pour extraire l'information de syndrome sur les erreurs, quelle est la fonction principale de ces mesures de syndrome dans un code stabilisateur comme le code de surface, qui partage certaines caractéristiques conceptuelles avec la protection topologique mais utilise la correction d'erreur active ?",
    "A": "Les mesures de syndrome dans les codes stabilisateurs servent plusieurs objectifs : elles vérifient d'abord que le qubit logique a été préparé dans le bon espace de code en vérifiant les valeurs propres des stabilisateurs, puis surveillent continuellement les erreurs pendant le calcul en détectant les violations de stabilisateurs, et enfin elles ré-encodent l'information quantique après chaque porte logique pour s'assurer que les qubits restent dans le sous-espace protégé tout au long du calcul. Cette triple fonction est essentielle car chaque opération logique peut potentiellement faire sortir le système de l'espace de code, nécessitant une re-projection immédiate par extraction de syndrome et opérations de récupération subséquentes qui restaurent les conditions de stabilisateur.",
    "B": "Dans les codes stabilisateurs, les mesures de syndrome réduisent principalement la diaphonie entre les modes bosoniques qui encodent l'information logique en projetant les corrélations d'erreur haute fréquence qui couplent les patchs de code voisins, imposant ainsi les contraintes de parité locales qui définissent les frontières de l'espace de code. Ces mesures effectuent une surveillance faible continue des nombres d'occupation des modes, extrayant des bits de syndrome qui indiquent quand des excitations ont fui entre cavités bosoniques adjacentes.",
    "C": "Les mesures identifient quelles erreurs se sont produites sans faire s'effondrer l'état quantique logique encodé dans le sous-espace protégé",
    "D": "Les mesures de syndrome vérifient la fidélité de préparation du qubit logique en vérifiant que tous les opérateurs stabilisateurs initiaux donnent les valeurs propres attendues (+1 ou -1) immédiatement après l'encodage, confirmant que les qubits physiques ont été correctement intriqués dans la variété de l'espace de code avant le début de tout calcul. Si un stabilisateur renvoie une valeur propre inattendue, la séquence de préparation doit être répétée, car cela indique que le circuit d'encodage n'a pas réussi à distribuer correctement l'information quantique sur le bloc de code.",
    "solution": "C"
  },
  {
    "id": 139,
    "question": "Dans le code de surface, quel est le rôle principal des opérateurs de plaquette et d'étoile ?",
    "A": "Ils agissent comme générateurs de stabilisateurs permettant l'extraction de syndrome : les opérateurs de plaquette détectent les erreurs de type X par des mesures de parité Z⊗Z⊗Z⊗Z sur les frontières de faces, tandis que les opérateurs d'étoile détectent les erreurs de type Z par parité X⊗X⊗X⊗X sur les voisinages de sommets, mais ces opérateurs doivent anticommuter avec les opérateurs logiques pour permettre la mesure de syndrome sans faire s'effondrer l'état logique encodé, assurant que la détection d'erreur préserve l'occupation de l'espace de code.",
    "B": "Ils servent de générateurs de stabilisateurs dont les résultats de mesure fournissent des syndromes d'erreur, permettant au décodeur de déduire quelles erreurs physiques se sont produites et de déterminer les opérations de correction appropriées sans mesurer directement l'état du qubit logique lui-même.",
    "C": "Ces opérateurs définissent l'espace de code comme l'espace propre simultané +1 de tous les stabilisateurs, et leur mesure projette le système sur cet espace tout en révélant quels qubits physiques ont subi des erreurs par des déviations de valeur propre par rapport à +1, mais crucialement ils commutent avec tous les opérateurs logiques de sorte que l'extraction de syndrome laisse l'information encodée intacte, les distinguant des mesures directes en base computationnelle qui détruiraient la superposition.",
    "D": "Les opérateurs de plaquette imposent des contraintes de parité locale de type Z tandis que les opérateurs d'étoile imposent des contraintes de type X, définissant ensemble un ensemble commutant d'observables dont les états propres conjoints couvrent l'espace de code, et l'extraction de syndrome mesure si le système reste dans cet espace après que des erreurs se produisent, les contraintes violées identifiant les emplacements d'erreur par appariement parfait de poids minimal sur le graphe de syndrome, bien que les opérateurs doivent être mesurés soigneusement en utilisant des qubits ancilla pour éviter de projeter l'état logique.",
    "solution": "B"
  },
  {
    "id": 140,
    "question": "Quelle est une raison pour laquelle le matériel FPGA est privilégié dans les systèmes avancés de distribution quantique de clés (QKD) ?",
    "A": "Opérations parallèles haute vitesse pour le traitement de clés, qui permettent le tamisage en temps réel, la correction d'erreur et l'amplification de confidentialité aux débits multi-gigabits requis par les réseaux fibre métropolitains et longue distance. La logique reconfigurable permet la mise en pipeline personnalisée des algorithmes de réconciliation de base, le décodage adaptatif de syndrome pour les codes LDPC, et le hachage Toeplitz concurrent pour l'extraction d'aléa, tout en maintenant des profils de latence déterministes critiques pour synchroniser les sources d'intrication distribuées à travers des nœuds géographiquement séparés dans les réseaux quantiques.",
    "B": "Logique personnalisée haute performance pour le post-traitement des nombres aléatoires quantiques, qui permet l'estimation d'entropie minimale accélérée par matériel, l'évaluation d'extracteur Toeplitz en temps réel, et la surveillance continue de santé aux débits gigabits requis par les protocoles de préparation-et-mesure. Le tissu reconfigurable permet la mise en pipeline personnalisée de la décorrélation de von Neumann, la compensation de biais adaptative pour les détecteurs monophotoniques, et l'implémentation parallèle de fonctions de conditionnement de force cryptographique, tout en maintenant des temps de réponse inférieurs à la microseconde critiques pour ajuster dynamiquement les motifs de modulation de source lorsque les taux de comptage sombre des détecteurs dérivent pendant l'opération continue à travers des déploiements fibre métropolitains à température variable.",
    "C": "Contrôle temporel de haute précision pour la synchronisation de détecteurs, qui permet le fenêtrage de coïncidence sous-nanoseconde, la logique de déclenchement adaptative pour la suppression d'impulsions parasites, et la corrélation d'étiquettes temporelles en temps réel aux débits de multi-mégacomptes requis par les protocoles basés sur l'intrication. L'architecture reconfigurable permet l'implémentation personnalisée de pipelines de conversion temps-vers-numérique, la compensation de retard programmable pour la dispersion chromatique dans la fibre déployée, et l'accumulation parallèle d'histogrammes pour l'estimation de visibilité, tout en maintenant des spécifications de gigue à l'échelle femtoseconde critiques pour maintenir le contraste d'interférence à deux photons lors de la connexion de plusieurs longueurs d'onde de source à travers des infrastructures de réseau quantique métropolitain multiplexées en longueur d'onde.",
    "D": "Interface de canal classique haute bande passante pour l'échange de messages authentifiés, qui permet des moteurs de protocole dédiés pour les handshakes défi-réponse, la vérification MAC en pipeline, et la dérivation parallèle de clés de session aux débits multi-sessions requis par les topologies de réseau en étoile. La logique reconfigurable permet des machines à états personnalisées pour la négociation de variantes BB84, la validation de chaîne de certificats accélérée par matériel lors de l'authentification initiale, et le traitement concurrent de plusieurs flux utilisateur à travers une infrastructure QKD partagée, tout en maintenant les garanties de timing de protocole critiques pour respecter les accords de niveau de service lorsque les clients d'entreprise établissent des tunnels VPN sécurisés quantiquement à la demande à travers des réseaux d'accès quantique métropolitains opérés par des opérateurs.",
    "solution": "A"
  },
  {
    "id": 141,
    "question": "Quels sont les avantages et les limites de l'apprentissage par renforcement quantique (Quantum Reinforcement Learning, QRL) ?",
    "A": "Le QRL atteint une accélération exponentielle de la recherche de politique grâce à l'encodage d'amplitude quantique des paires état-action combiné à une amplification d'amplitude de type Grover qui concentre la masse de probabilité sur les trajectoires à haute récompense, permettant une convergence plus rapide que l'exploration epsilon-greedy classique. Cependant, l'avantage se dégrade en pratique car la rétroaction de mesure effondre la représentation de politique superposée après chaque épisode, forçant une re-préparation complète de l'état qui introduit un surcoût évoluant linéairement avec la taille de l'espace d'états et annulant l'accélération quantique pour les problèmes nécessitant un raffinement itératif de la politique sur de nombreux épisodes.",
    "B": "Le QRL réduit la complexité d'échantillonnage en encodant les fonctions de valeur comme amplitudes quantiques et en exploitant les effets d'interférence pour supprimer les séquences d'actions à faible récompense par superposition destructive, permettant aux agents d'identifier des politiques quasi-optimales avec polynomialement moins d'interactions avec l'environnement que le Q-learning classique. Cependant, le matériel quantique actuel reste immature, avec des taux de bruit de dépolarisation élevés dus à des implémentations de portes imparfaites et des topologies de connectivité limitées qui empêchent l'encodage efficace des matrices de transition éparses typiques des processus de décision markoviens réalistes, limitant le déploiement pratique à des problèmes jouets avec moins de 10 états.",
    "C": "Le QRL permet l'évaluation parallèle d'un nombre exponentiel de politiques candidates grâce à la superposition quantique de déroulements de trajectoires, combinée à des techniques d'estimation de phase qui extraient les rendements espérés sans le surcoût d'échantillonnage Monte Carlo classique. Cependant, il reste impraticable car l'avantage quantique nécessite une évaluation cohérente sur des pas de temps plus longs que les temps T2 actuels ne le permettent, et parce que l'extraction de paramètres de politique classiques à partir des résultats de mesure quantique introduit un goulot d'étranglement de reconstruction tomographique qui évolue exponentiellement avec le nombre de qubits nécessaires pour représenter la stratégie de l'agent, annulant l'accélération pour les problèmes d'échelle significative.",
    "D": "Le QRL accélère l'apprentissage grâce au parallélisme quantique et offre une exploration améliorée via la superposition, permettant aux agents d'échantillonner plusieurs trajectoires simultanément et de découvrir des politiques optimales plus efficacement que les méthodes classiques. Cependant, le matériel quantique actuel reste immature, avec un nombre insuffisant de qubits, des taux d'erreur élevés et des temps de cohérence courts qui empêchent le déploiement pratique des algorithmes QRL sur des problèmes réels d'échelle significative.",
    "solution": "D"
  },
  {
    "id": 142,
    "question": "Pourquoi la parallélisation de l'exécution de sous-circuits est-elle bénéfique dans les schémas de découpage (cutting schemes) ?",
    "A": "La parallélisation réduit considérablement le temps d'exécution réel en superposant les exécutions quantiques indépendantes sur plusieurs unités de traitement, permettant une complétion plus rapide des évaluations exponentiellement nombreuses de sous-circuits requises pour le découpage de circuit. Ce gain d'efficacité temporelle est crucial pour les implémentations pratiques où le temps d'exécution total, plutôt que la seule complexité d'échantillonnage, détermine la faisabilité.",
    "B": "L'exécution parallèle sur plusieurs processeurs quantiques permet des stratégies d'échantillonnage adaptatives qui concentrent les mesures sur les termes de quasiprobabilité de poids élevé, réduisant le nombre total de tirs requis pour la reconstruction. En exécutant les sous-circuits simultanément et en partageant les statistiques de mesure intermédiaires entre dispositifs, le post-traitement classique peut identifier quelles contributions de quasiprobabilité dominent la valeur d'espérance et réallouer dynamiquement les ressources d'échantillonnage en conséquence, obtenant une réduction de variance comparable à l'échantillonnage d'importance dans les méthodes Monte Carlo sans le surcoût séquentiel de la repondération itérative.",
    "C": "La parallélisation atténue le surcoût d'échantillonnage exponentiel inhérent aux décompositions de quasiprobabilité en distribuant la charge de mesure sur des unités de traitement quantique indépendantes, permettant au nombre total d'évaluations de circuit d'être complété dans une fenêtre de temps fixe. Puisque l'étape de post-traitement classique pour le découpage de circuit nécessite de combiner les résultats de tous les fragments de sous-circuits avec des coefficients de quasiprobabilité spécifiques, l'exécution parallèle sur N dispositifs réduit le nombre de tours séquentiels nécessaires d'un facteur N, traduisant la complexité d'échantillonnage exponentielle d'un goulot d'étranglement temporel en une exigence de ressources spatiales qui évolue plus favorablement avec le matériel disponible.",
    "D": "L'exécution parallèle de sous-circuits permet d'appliquer des techniques d'atténuation d'erreurs quantiques indépendamment à chaque fragment avant la reconstruction classique, améliorant la fidélité de l'estimation finale de la valeur d'espérance. Puisque les protocoles de découpage décomposent le circuit original en sous-circuits plus petits qui s'adaptent aux limites de cohérence des dispositifs disponibles, l'exécution simultanée de ces fragments sur des processeurs séparés empêche l'accumulation d'erreurs due à l'exécution séquentielle tout en permettant une extrapolation à bruit zéro ou une annulation d'erreur probabiliste par fragment qui serait infaisable pour le circuit complet non découpé, réduisant ainsi le taux d'erreur effectif dans l'observable reconstruite sans augmenter le nombre total de tirs.",
    "solution": "A"
  },
  {
    "id": 143,
    "question": "Si un qubit commence dans l'état |ψ⟩ = α|0⟩ + β|1⟩, comment une erreur combinée de bit-flip (X) et de phase-flip (Z) affecte-t-elle l'état ?",
    "A": "L'état devient α|1⟩ + β|0⟩, car X échange d'abord les états de base donnant β|0⟩ + α|1⟩, puis Z applique une phase uniquement aux composantes |1⟩, mais puisque X a déjà déplacé l'α original vers l'emplacement |1⟩, la phase frappe α et non β.",
    "B": "L'état devient α|1⟩ - β|0⟩.",
    "C": "L'état devient -α|1⟩ + β|0⟩, car ZX applique Z d'abord (donnant α|0⟩ - β|1⟩), puis X permute pour donner -β|0⟩ + α|1⟩, mais la phase globale rend cela équivalent à β|0⟩ - α|1⟩ à normalisation près.",
    "D": "L'état devient β|1⟩ - α|0⟩, car lorsque X et Z se composent comme XZ, la phase de la porte Z est appliquée dans la base de calcul avant que X ne réordonne les états, donc le signe moins s'attache à l'amplitude qui était originalement sur |1⟩.",
    "solution": "B"
  },
  {
    "id": 144,
    "question": "Quel facteur s'est avéré ne pas améliorer systématiquement la performance des classificateurs quantiques ?",
    "A": "La réduction de la dimensionnalité d'entrée par des méthodes de compression de caractéristiques agressive ou de projection aléatoire tend à éliminer les corrélations subtiles et les composantes haute fréquence que les circuits quantiques sont théoriquement les mieux adaptés à capturer, neutralisant ainsi l'avantage quantique potentiel. Lorsque les vecteurs d'entrée sont compressés de leurs dimensions natives (souvent plus de 100 caractéristiques dans les ensembles de données réels) pour correspondre au nombre de qubits (typiquement 4-10 qubits sur le matériel actuel), la performance de classification se dégrade fréquemment de 10-20% par rapport à l'utilisation de l'ensemble complet de caractéristiques. Cet effet est particulièrement prononcé dans les problèmes où les noyaux quantiques sont censés surpasser les méthodes classiques, car la réduction de dimensionnalité force effectivement les données dans un régime où les algorithmes classiques performent déjà de manière quasi-optimale, rendant l'approche quantique redondante.",
    "B": "L'optimisation du pré-traitement classique des données d'entrée par des techniques de réduction de dimensionnalité comme l'analyse en composantes principales ou la sélection de caractéristiques s'est avérée empiriquement dégrader la précision du classificateur quantique dans de nombreux cas, particulièrement lorsque les caractéristiques écartées contiennent des corrélations non linéaires que le circuit quantique aurait pu exploiter. Des études sur les dispositifs NISQ révèlent que réduire la dimensionnalité d'entrée de, disons, 64 caractéristiques à 16 caractéristiques cause souvent une chute de précision de classification de 8-12 points de pourcentage car la capacité du noyau quantique à projeter les données dans un espace de Hilbert de haute dimension est effectivement gaspillée lorsque le prétraitement classique a déjà effondré l'espace des caractéristiques. Cette découverte contre-intuitive suggère que l'avantage quantique dépend de manière critique de l'exposition du circuit quantique aux vecteurs de caractéristiques bruts de haute dimension plutôt qu'à des résumés prétraités.",
    "C": "L'augmentation du nombre de qubits alloués à un classificateur quantique échoue généralement à améliorer la performance une fois que le nombre de qubits dépasse la dimensionnalité intrinsèque du problème de classification, et cause souvent une dégradation due à la dilution de la densité d'information dans l'espace de Hilbert étendu. Par exemple, passer de 10 à 20 qubits pour une tâche de classification binaire sur des ensembles de données avec seulement 8-10 caractéristiques pertinentes résulte typiquement en surapprentissage et sensibilité accrue aux plateaux stériles pendant l'entraînement, car l'espace de paramètres exponentiellement plus grand devient clairsemé relativement aux données d'entraînement disponibles. Les références empiriques sur des ensembles de données comme MNIST montrent que les améliorations de précision plafonnent ou même déclinent au-delà de 12-15 qubits, suggérant que simplement ajouter des qubits sans conception architecturale soigneuse gaspille les ressources quantiques et le temps d'entraînement.",
    "D": "L'introduction d'intrication au-delà des niveaux minimaux requis pour la tâche de classification échoue souvent à améliorer la performance du classificateur quantique et peut en fait dégrader la précision lorsque les portes intriquantes introduisent du bruit supplémentaire sans fournir d'avantage calculatoire. Des études sur les dispositifs à court terme montrent que les circuits hautement intriqués avec une profondeur dépassant 15-20 couches sous-performent typiquement les alternatives moins intriquées en raison de la décohérence accumulée.",
    "solution": "D"
  },
  {
    "id": 145,
    "question": "Que se passe-t-il si l'état initial dans l'algorithme de Grover n'est pas la superposition uniforme ?",
    "A": "Le nombre d'itérations requis pour trouver l'état marqué évolue exponentiellement avec la taille de la base de données plutôt que quadratiquement, car la distribution initiale non uniforme brise le mécanisme d'amplification d'amplitude qui repose sur la réflexion symétrique autour de l'amplitude moyenne. Cela réduit effectivement l'algorithme de Grover à la performance d'échantillonnage aléatoire classique, nécessitant O(N) requêtes au lieu de O(√N), car l'angle de rotation par itération devient négligeablement petit lorsqu'on part d'un état de base de calcul arbitraire.",
    "B": "L'algorithme projette automatiquement l'état initial sur le sous-espace correct grâce à une étape d'orthogonalisation implicite qui se produit pendant le premier appel à l'oracle, effectuant essentiellement un processus de Gram-Schmidt quantique qui restaure la superposition uniforme sur l'espace de recherche. Ce mécanisme d'auto-correction est intégré dans la structure de l'opérateur de diffusion, qui mesure le recouvrement avec l'état de superposition égale et rééchelonne les amplitudes en conséquence, garantissant que les itérations suivantes se déroulent exactement comme si l'algorithme avait commencé correctement.",
    "C": "L'algorithme fonctionne toujours et trouve l'état marqué, mais avec une probabilité de succès réduite qui dépend du recouvrement entre l'état initial et la superposition uniforme sur l'espace de recherche.",
    "D": "Un échec complet se produit car la structure de réflexion de l'oracle dépend de manière critique du démarrage à partir de la superposition égale sur tous les états de base de calcul.",
    "solution": "C"
  },
  {
    "id": 146,
    "question": "Dans le contexte de la théorie de la complexité quantique et des modèles physiques exotiques, imaginons un cadre de calcul où les systèmes quantiques peuvent accéder à des courbes temporelles fermées de type temps (CTC) mais uniquement par post-sélection — c'est-à-dire que nous sélectionnons les résultats après coup plutôt que de les garantir causalement. Cela soulève des questions profondes sur la relation entre les paradoxes temporels et la puissance de calcul. Quelle est l'importance des modèles de post-sélection avec courbes temporelles fermées en théorie de la complexité quantique ?",
    "A": "Ils démontrent que PostBQP égale PP par le résultat d'Aaronson, montrant que le calcul quantique post-sélectionné capture précisément la puissance du calcul classique probabiliste avec erreur non bornée, où la post-sélection sur les résultats de mesure permet de résoudre des problèmes dans la hiérarchie de comptage en exploitant l'interférence quantique pour amplifier les résultats souhaités, bien que sans CTC cela ne nécessite qu'une post-sélection standard sur les bases de mesure.",
    "B": "Ils effondrent la hiérarchie polynomiale, révélant une puissance de calcul extrême sous des hypothèses physiques exotiques où les CTC post-sélectionnées permettent de résoudre des problèmes au-delà des classes de complexité quantique conventionnelles en exploitant des paradoxes temporels pour satisfaire rétroactivement les conditions de cohérence.",
    "C": "Ils prouvent que BQP/qpoly égale PSPACE par la condition de cohérence de Deutsch, où les circuits quantiques avec accès CTC post-sélectionné peuvent résoudre des formules booléennes quantifiées en préparant des états auto-cohérents protégés par chronologie qui encodent tous les chemins de branchement simultanément, nécessitant seulement des chaînes de conseils quantiques de taille polynomiale pour spécifier la structure causale valide correspondant au résultat de calcul souhaité.",
    "D": "Ils établissent la PP-complétude pour les modèles CTC post-sélectionnés en montrant que la rétroaction temporelle permet le comptage exact des affectations satisfaisantes, où les circuits quantiques post-sélectionnant sur les contraintes de cohérence réalisent un calcul #P-difficile grâce au mécanisme de protection chronologique de Lloyd qui convertit les témoins NP en boucles causales vérifiables en temps polynomial, résolvant effectivement des problèmes au-delà de la hiérarchie polynomiale standard.",
    "solution": "B"
  },
  {
    "id": 147,
    "question": "Un circuit quantique paramétré atteint une haute expressivité en couvrant densément l'espace de Hilbert. Cependant, l'entraînement ne parvient pas à converger. Quelle est l'explication la plus probable ?",
    "A": "Le circuit atteint un seuil d'approximation universelle et devient instable une fois qu'il acquiert une expressivité suffisante pour représenter des transformations unitaires arbitraires, point auquel l'espace des paramètres transite vers un régime chaotique où de petites perturbations dans les angles de portes conduisent à des sauts discontinus dans l'état de sortie.",
    "B": "Intrication insuffisante dans la structure de l'ansatz, car l'expressivité élevée du circuit est obtenue principalement par des couches profondes de rotations à un qubit plutôt que par des portes intriquantes multi-qubits. Sans intrication adéquate entre les qubits, la dimensionnalité effective de l'espace de Hilbert accessible reste polynomiale plutôt qu'exponentielle en nombre de qubits, créant un goulot d'étranglement représentationnel qui empêche le circuit d'encoder les corrélations nécessaires pour approximer la fonction cible.",
    "C": "Plateaux arides — le paysage de gradient devient exponentiellement plat à mesure que la profondeur du circuit augmente, rendant l'optimisation basée sur le gradient essentiellement impossible. C'est une malédiction bien connue des espaces de paramètres de haute dimension dans les circuits quantiques, où la variance des gradients s'annule exponentiellement avec la taille du système. À mesure que le circuit devient plus expressif par l'ajout de couches, la probabilité qu'une initialisation aléatoire se trouve dans une région avec une magnitude de gradient appréciable décroît exponentiellement, laissant l'optimiseur incapable de trouver des directions de descente significatives quelle que soit la taille du pas ou la taille de lot employée.",
    "D": "L'expressivité provoque un effondrement de mode dans la variété de fonction de coût, ce qui empêche l'optimiseur d'explorer efficacement différentes régions de l'espace de solution car les circuits hautement expressifs génèrent des paysages de coût avec un nombre exponentiellement proliférant d'optima locaux presque dégénérés en énergie. L'optimiseur se retrouve piégé dans un mode particulier correspondant à une famille de solutions, incapable de transiter entre modes en raison de la probabilité d'effet tunnel évanescente à travers les barrières de haute dimension qui les séparent.",
    "solution": "C"
  },
  {
    "id": 148,
    "question": "Pourquoi la « propagation d'opérateur » peut-elle être quantifiée par des corrélateurs hors ordre temporel (OTOC) ?",
    "A": "Les OTOC mesurent la croissance d'opérateur en quantifiant l'anticommutateur au carré ⟨{W(t), V(0)}†{W(t), V(0)}⟩, qui commence près de zéro pour des opérateurs spatialement séparés et croît à mesure que l'évolution temporelle fait développer aux opérateurs initialement locaux un support sur des régions en expansion du système. Cette croissance signale la propagation de l'information à travers l'espace de Hilbert à plusieurs corps — lorsque l'anticommutateur devient grand, cela indique que les opérateurs W et V ont développé des régions de support qui se chevauchent et que l'information quantique s'est propagée à travers le système. L'OTOC fournit un diagnostic pour le brouillage car les anticommutateurs d'opérateurs quantifient le degré auquel W(t) est passé d'un opérateur local à un opérateur complexe à plusieurs corps dont le support chevauche la région de V, la fonction de corrélation à quatre points mesurant directement comment la structure de l'opérateur évolué dans le temps a changé par rapport à sa forme localisée initiale, permettant l'identification de l'échelle de temps de brouillage et de la vitesse de Lieb-Robinson qui gouverne la propagation de l'information dans le système quantique.",
    "B": "Les OTOC mesurent comment des opérateurs initialement commutants ne parviennent plus à commuter sous l'évolution de Heisenberg, fournissant un diagnostic pour la croissance d'opérateur et le brouillage de l'information dans les systèmes quantiques à plusieurs corps. Spécifiquement, l'OTOC quantifie le commutateur au carré ⟨[W(t), V(0)]†[W(t), V(0)]⟩, qui commence près de zéro lorsque les opérateurs W et V sont spatialement séparés et croît à mesure que l'évolution temporelle fait développer aux opérateurs initialement locaux un support sur une région en expansion du système. Cette croissance signale que l'information d'opérateur se propage à travers l'espace de Hilbert à plusieurs corps — lorsque le commutateur devient grand, cela indique qu'effectuer l'opération V suivie de W(t) produit un résultat différent de l'ordre inverse, démontrant que l'opérateur W a grandi pour chevaucher la région de support de V et que l'information quantique s'est propagée à travers le système.",
    "C": "Les OTOC quantifient la propagation d'opérateur en mesurant la fonction de corrélation à quatre points ⟨W(t)V(0)W(t)V(0)⟩, qui détecte comment des opérateurs initialement localisés développent un support non local par évolution temporelle dans les systèmes à plusieurs corps. L'OTOC commence près de l'unité lorsque les opérateurs W et V sont spatialement séparés et décroît à mesure que la dynamique de brouillage fait croître W(t) en un opérateur complexe avec support chevauchant la région de V, signalant la propagation de l'information à travers le système quantique. Cette décroissance mesure directement la croissance d'opérateur car la fonction à quatre points devient petite lorsque W(t) et V ne commutent pas, indiquant que W s'est propagé de sa forme locale initiale en un opérateur à plusieurs corps dont l'action ne commute plus avec des opérateurs distants, fournissant un diagnostic quantitatif pour le temps de brouillage et la vitesse papillon qui caractérise la rapidité avec laquelle le support d'opérateur s'étend à travers le système pendant l'évolution chaotique.",
    "D": "Les OTOC fournissent une mesure de la propagation d'opérateur en évaluant la croissance du commutateur évolué dans le temps par la valeur attendue ⟨[W(t), V(0)]²⟩, qui caractérise comment des opérateurs initialement locaux acquièrent un support sur des degrés de liberté distants sous l'évolution hamiltonienne dans des systèmes fortement interagissants. L'OTOC capture la croissance d'opérateur car le commutateur au carré quantifie directement l'étendue selon laquelle W s'est propagé de son support localisé initial pour développer un chevauchement non nul avec l'opérateur spatialement séparé V, la fonction de corrélation commençant près de zéro et croissant exponentiellement pendant le régime de brouillage avant de saturer aux temps tardifs. Cette mesure distingue les dynamiques intégrables des chaotiques car la propagation d'opérateur dans les systèmes chaotiques suit un effet papillon où le commutateur croît exponentiellement avec un taux λL (l'exposant de Lyapunov quantique), tandis que les systèmes intégrables présentent seulement une croissance polynomiale ou balistique, faisant de l'OTOC un diagnostic puissant pour la propagation de l'information et le chaos quantique à plusieurs corps par sa sensibilité à la redistribution du poids d'opérateur à travers le système.",
    "solution": "B"
  },
  {
    "id": 149,
    "question": "Comment l'utilisation de trames ou de créneaux temporels facilite-t-elle le routage dans les réseaux photoniques ?",
    "A": "La division en créneaux temporels synchronise le réseau de sorte que chaque nœud sache quand attendre les photons tentant la distribution d'intrication, évitant les conflits de routage où plusieurs sources ciblent simultanément le même port de sortie de commutateur ou canal de longueur d'onde. En discrétisant la transmission en fenêtres programmées, le contrôleur de réseau peut pré-allouer des chemins à travers le tissu de commutation qui garantissent un fonctionnement sans blocage même lorsque plusieurs paires de photons traversent des liens physiques qui se chevauchent.",
    "B": "Les protocoles basés sur trames assignent à chaque canal quantique une fenêtre de transmission périodique pendant laquelle la source émet des paires de photons annoncées, les routeurs utilisant l'indice de créneau temporel pour déterminer les décisions de transfert sans nécessiter d'information d'en-tête par photon. Ce multiplexage temporel permet à plusieurs tentatives de distribution d'intrication de partager la même infrastructure de fibre en entrelaçant leurs trames de transmission, bien que le schéma nécessite une synchronisation d'horloge à l'échelle du réseau à la précision du temps de cohérence des photons pour maintenir l'indiscernabilité aux nœuds de fusion.",
    "C": "Aligner les tentatives d'intrication sur des fenêtres programmées évite les collisions, permettant aux sources multiplexées de partager les ressources réseau sans interférence destructive. En discrétisant le temps en créneaux, les commutateurs de routage photonique savent exactement quand attendre les qubits et peuvent coordonner les réservations de chemin qui empêchent plusieurs photons de se disputer simultanément le même port de sortie ou canal de longueur d'onde.",
    "D": "Les architectures à créneaux temporels permettent un routage déterministe en assignant à chaque source un décalage de trame fixe qui encode son adresse réseau dans le domaine temporel, permettant aux commutateurs intermédiaires de décoder l'information de routage à partir des temps d'arrivée des photons relatifs à l'impulsion de synchronisation globale. Cela élimine le besoin de messages de contrôle classiques pour configurer les états de commutateur car la structure de trame périodique porte implicitement l'information de chemin, bien que le schéma nécessite que tous les photons dans un créneau soient temporellement indiscernables pour préserver l'interférence quantique aux séparateurs de faisceau utilisés pour les mesures d'état de Bell.",
    "solution": "C"
  },
  {
    "id": 150,
    "question": "Pourquoi une porte avec une expressivité plus faible pourrait-elle surpasser une porte hautement expressive dans certaines implémentations d'algorithmes ?",
    "A": "Des portes plus simples avec une expressivité plus faible peuvent être plus efficaces lorsque la structure de l'algorithme n'exige pas de motifs d'intrication complexes ou de préparations d'état élaborées. Elles nécessitent généralement moins de ressources physiques pour l'implémentation, ont des temps de porte plus courts qui réduisent l'exposition à la décohérence, et sont souvent mieux caractérisées et calibrées dans le matériel. Lorsque la tâche de calcul peut être accomplie avec une expressivité limitée, l'utilisation de portes plus complexes augmente inutilement la profondeur du circuit et l'accumulation d'erreurs sans fournir d'avantage algorithmique supplémentaire.",
    "B": "Les portes hautement expressives nécessitent généralement une décomposition en séquences plus longues d'opérations natives pendant la compilation, ce qui augmente à la fois la profondeur du circuit et le nombre total de portes. Pour les algorithmes où les transformations unitaires requises se situent dans un sous-espace de faible dimension de SU(2^n), des portes plus simples peuvent cibler directement ce sous-espace sans invoquer de degrés de liberté inutiles. De plus, les techniques de caractérisation matérielle et d'atténuation des erreurs sont souvent optimisées pour les portes de faible expressivité couramment utilisées, produisant une meilleure fidélité effective. Lorsque la tâche algorithmique n'exploite pas la pleine expressivité, la surcharge des portes complexes — exigences de temps de cohérence plus longs et erreurs de phase accumulées — l'emporte sur leurs avantages théoriques.",
    "C": "Les portes de faible expressivité génèrent des représentations plus éparses dans le formalisme de matrice de transfert de Pauli, ce qui signifie que leurs canaux d'erreur se couplent à moins d'éléments hors diagonale dans la matrice de processus. Ce couplage réduit se traduit par des caractéristiques de propagation d'erreur plus favorables lors de la composition dans des circuits profonds. Pour les algorithmes dominés par des opérations diagonales ou des mesures de base de calcul, les portes hautement expressives introduisent des rotations inutiles dans des bases conjuguées qui amplifient la décohérence de phase. Puisque les algorithmes variationnels convergent souvent vers des solutions dans des sous-espaces restreints de l'espace de Hilbert complet, des portes plus simples qui opèrent nativement dans ces sous-espaces évitent les défis d'optimisation de paramètres et la surcharge d'estimation de gradient associés aux portes expressives sur-paramétrées.",
    "D": "Les portes expressives présentent souvent une décroissance de fidélité non monotone en fonction du temps d'implémentation en raison d'erreurs de contrôle cohérentes qui s'accumulent quadratiquement avec la complexité d'impulsion, tandis que les portes plus simples bénéficient d'une mise à l'échelle d'erreur linéaire dans l'expansion de Magnus de l'opérateur d'évolution temporelle. Pour les algorithmes où l'état solution a une faible entropie d'intrication (comme certains états fondamentaux dans VQE), les portes hautement expressives génèrent des corrélations bipartites excessives qui doivent ensuite être défaites par des couches de circuit supplémentaires. Cela crée des chemins de calcul redondants qui augmentent la susceptibilité aux erreurs corrélées. De plus, la fuite spectrale inhérente aux portes expressives multi-paramètres conduit à une population involontaire de niveaux hors du sous-espace de calcul dans les implémentations matérielles réelles.",
    "solution": "A"
  },
  {
    "id": 151,
    "question": "Quelle structure mathématique fondamentale est utilisée pour décrire les états quantiques en informatique quantique ?",
    "A": "L'espace de Minkowski, la variété pseudo-riemannienne quadridimensionnelle combinant trois dimensions spatiales avec une dimension temporelle, fournit le cadre géométrique pour l'évolution des états quantiques car les transformations unitaires sur les qubits doivent préserver l'intervalle d'espace-temps entre les vecteurs d'état pour maintenir la causalité et garantir que les probabilités de mesure restent invariantes sous les boosts de Lorentz appliqués aux référentiels de dispositifs quantiques séparés",
    "B": "L'espace euclidien sert de fondation puisque les amplitudes quantiques doivent satisfaire le produit scalaire standard dérivé du théorème de Pythagore, et l'exigence que les résultats de mesure correspondent à des probabilités à valeurs réelles contraint les états quantiques à des espaces vectoriels de dimension finie avec une norme euclidienne conventionnelle",
    "C": "L'espace de Hilbert, l'espace complet avec produit scalaire de vecteurs complexes où les états quantiques résident comme des rayons, équipé de la structure nécessaire pour représenter la superposition, calculer les amplitudes de probabilité via les produits scalaires, et décrire l'évolution unitaire.",
    "D": "Le plan cartésien représente avec précision les états quantiques car les systèmes à un seul qubit sont entièrement caractérisés par deux paramètres réels correspondant aux composantes de polarisation horizontale et verticale, et les systèmes multi-qubits sont construits en prenant des produits directs de systèmes de coordonnées réelles bidimensionnels",
    "solution": "C"
  },
  {
    "id": 152,
    "question": "Dans les algorithmes quantiques pour l'apprentissage automatique, l'Analyse en Composantes Principales Quantique (QPCA) a été proposée comme méthode pour obtenir une accélération exponentielle par rapport à l'ACP classique sous certaines conditions. L'avantage théorique provient de la capacité à traiter des données de haute dimension encodées dans des états quantiques. Cependant, cette accélération dépend de manière critique de composants algorithmiques spécifiques et d'hypothèses sur l'accès aux données. Quelle est la ressource quantique principale qui confère à QPCA son avantage potentiel sur les approches classiques lors de l'analyse d'ensembles de données avec des espaces de caractéristiques exponentiellement grands ?",
    "A": "L'estimation de phase quantique, qui permet l'extraction des valeurs propres et vecteurs propres de la matrice de densité encodant la structure de covariance des données en profondeur logarithmique, à condition que les données puissent être chargées efficacement dans des états quantiques et que l'écart entre les valeurs propres principales soit suffisamment grand pour les résoudre dans les exigences de précision de l'application. L'avantage exponentiel émerge car l'estimation de phase sur un système de n qubits peut distinguer les valeurs propres avec une précision polynomiale en utilisant seulement O(poly(n)) portes, tandis que les algorithmes classiques de décomposition en valeurs propres nécessitent un temps au moins linéaire dans la dimension de la matrice 2ⁿ. Cet avantage quantique s'applique spécifiquement à la tâche de préparer des états quantiques proportionnels aux vecteurs propres principaux et d'estimer leurs valeurs propres correspondantes, permettant aux algorithmes d'apprentissage automatique quantique en aval d'opérer dans le sous-espace des composantes principales sans jamais construire explicitement la matrice de covariance complète ou effectuer une diagonalisation classique sur des structures de données exponentiellement grandes.",
    "B": "L'amplification d'amplitude quantique appliquée de manière itérative pour augmenter le chevauchement entre les états d'essai et les vecteurs propres principaux de la matrice de covariance, permettant l'extraction des espaces propres dominants en un temps logarithmique dans le nombre de conditionnement plutôt que polynomial comme requis par les méthodes classiques d'itération de puissance. L'avantage exponentiel émerge car l'amplification d'amplitude sur un système de n qubits peut améliorer l'amplitude des composantes de vecteurs propres cibles par un facteur de √(2ⁿ) en utilisant seulement O(√(2ⁿ)) itérations, tandis que les approches classiques nécessitent Ω(2ⁿ) opérations pour atteindre une précision comparable lors du travail avec des matrices de covariance exponentiellement grandes. Cette ressource quantique permet à QPCA de préparer des états de composantes principales approximatifs et d'estimer leurs valeurs propres avec une précision ε en utilisant O(poly(n, 1/ε)) opérations, à condition qu'un accès quantique efficace aux données soit disponible et que l'écart entre valeurs propres principales et non principales dépasse le seuil d'amplification requis pour distinguer les composantes par effets d'interférence dans la distribution d'amplitude de l'état quantique préparé.",
    "C": "La transformation de valeur singulière quantique, qui permet l'évaluation de fonctions polynomiales sur le spectre de valeurs propres de la matrice de densité par des applications contrôlées d'opérateurs de codage par blocs, permettant l'extraction des composantes principales en un temps logarithmique dans la dimension de la matrice lorsqu'elle est combinée avec des oracles de préparation d'état efficaces. L'avantage exponentiel émerge car la transformation de valeur singulière sur une matrice encodée en n qubits peut appliquer des fonctions de seuil qui projettent sur l'espace propre principal en utilisant seulement O(poly(n)) portes, tandis que la décomposition en valeurs propres classique nécessite un temps au moins Ω(2ⁿ) pour l'analyse spectrale explicite de structures de covariance exponentiellement grandes. Cette ressource quantique permet à QPCA d'implémenter des fonctions de coupure lisses qui isolent les valeurs propres au-dessus d'un seuil spécifié, préparant des états quantiques supportés principalement sur le sous-espace de vecteurs propres dominant sans nécessiter de diagonalisation complète, à condition que les données d'entrée admettent une préparation d'état quantique efficace et que l'écart de valeurs propres dépasse la précision de transformation nécessaire pour distinguer les composantes principales des non principales par filtrage polynomial du spectre matriciel.",
    "D": "La transformée de Fourier quantique appliquée à la structure de corrélation temporelle des échantillons de données séquentiels, permettant l'extraction efficace des composantes principales dans le domaine fréquentiel par des mécanismes de phase kickback qui encodent l'information de valeur propre dans les phases de qubits ancillaires. L'avantage exponentiel émerge car la QFT sur un registre de n qubits transforme entre représentations temporelles et fréquentielles en utilisant seulement O(n²) portes, tandis que l'analyse de covariance classique basée sur FFT nécessite Ω(2ⁿ log 2ⁿ) opérations lors du traitement d'espaces de caractéristiques exponentiellement grands encodés dans des amplitudes quantiques. Cette ressource quantique permet à QPCA d'identifier les composantes fréquentielles dominantes correspondant aux vecteurs propres principaux en mesurant les phases ancillaires après des applications contrôlées de l'opérateur de covariance des données, le protocole d'estimation de phase résolvant les valeurs propres à une précision ε en utilisant O(1/ε) répétitions à condition que l'écart spectral entre valeurs propres principales dépasse la limite de résolution de phase déterminée par le nombre de qubits ancillaires alloués pour l'analyse fréquentielle de la matrice de corrélation encodée quantiquement.",
    "solution": "A"
  },
  {
    "id": 153,
    "question": "Quel est l'avantage principal de l'utilisation de qubits pendants dans la compilation quantique distribuée basée sur LNN ?",
    "A": "Flexibilité de routage — moins de SWAPs pour les interactions non locales, puisque les qubits pendants aux frontières des modules peuvent servir de zones de transit temporaires pour l'information quantique transférée entre qubits distants sans consommer les ressources de connectivité intérieure.",
    "B": "Réutilisation d'ancillaires durant les protocoles de distribution d'intrication, où les qubits pendants servent de ressources réinitialisables pour générer des paires de Bell inter-modules sans nécessiter un surcoût de réinitialisation complète. Puisque les qubits de frontière ne se connectent qu'à un seul voisin computationnel, ils peuvent participer à des tentatives répétées de génération d'intrication avec des modules externes pendant que la chaîne LNN intérieure continue d'exécuter des opérations logiques, ce qui permet effectivement de pipeliner l'établissement d'intrication avec le calcul.",
    "C": "Réduction de l'interférence de diaphonie des modules adjacents en positionnant les qubits sensibles aux sites pendants où ils subissent une densité de connectivité plus faible, minimisant les canaux de couplage indésirables qui dégradent les fidélités de porte. La topologie à voisin unique des qubits pendants les isole naturellement des mécanismes de propagation d'erreur multi-chemins qui affectent les qubits intérieurs avec une connectivité LNN bidirectionnelle, améliorant les temps de cohérence globaux du module par suppression topologique des erreurs.",
    "D": "Parallélisme amélioré pour les réseaux SWAP distribués où les qubits pendants permettent l'exécution concurrente d'opérations de routage sur plusieurs modules sans goulots d'étranglement de sérialisation. En dédiant exclusivement les qubits de frontière à la communication inter-modules pendant que les qubits intérieurs gèrent les portes locales, la compilation peut chevaucher les séquences SWAP distantes avec le calcul en cours, masquant effectivement la latence des opérations non locales derrière le travail productif sur les qubits intérieurs qui resteraient autrement inactifs pendant les phases de routage.",
    "solution": "A"
  },
  {
    "id": 154,
    "question": "Dans quel scénario QkNN surpasserait-il le kNN classique ? C'est une question qui a été largement débattue dans la communauté de l'apprentissage automatique quantique, et la réponse dépend de manière critique de la façon dont nous modélisons à la fois les données et les hypothèses matérielles que nous sommes prêts à faire.",
    "A": "Lorsque les calculs de distance dominent le temps d'exécution et que l'encodage en amplitude quantique permet l'estimation de distance entre états quantiques en temps O(log N) par requête en utilisant des circuits de test SWAP, comparé aux calculs de distance classiques O(N), à condition qu'un accès cohérent à la RAM quantique soit disponible.",
    "B": "Lorsque les données résident dans des espaces de caractéristiques de dimension exponentiellement élevée où l'encodage en amplitude quantique fournit une compression logarithmique, et les calculs de distance peuvent exploiter l'interférence quantique pour obtenir une accélération polynomiale sur la recherche classique du plus proche voisin.",
    "C": "Lorsque les données d'entraînement arrivent sous forme d'états quantiques provenant de capteurs ou simulateurs quantiques, évitant le coût exponentiel de la reconstruction tomographique classique, et que l'estimation de distance quantique peut être effectuée directement dans le domaine quantique en utilisant des métriques basées sur la fidélité.",
    "D": "Lorsque l'espace de caractéristiques présente une structure de produit tensoriel que le kNN classique ne peut pas exploiter efficacement, mais que QkNN peut tirer parti par des métriques de distance basées sur l'intrication qui capturent des corrélations inaccessibles aux représentations classiques séparables, comme démontré dans des ensembles de données structurés avec des symétries hiérarchiques.",
    "solution": "B"
  },
  {
    "id": 155,
    "question": "Pourquoi l'annulation des chaînes de Jordan-Wigner améliore-t-elle l'efficacité dans la simulation des interactions fermioniques ?",
    "A": "Les termes de saut adjacents dans l'hamiltonien fermionique partagent des chaînes de Jordan-Wigner chevauchantes dont le produit tensoriel se simplifie lorsque les portes sont appliquées consécutivement, puisque les opérateurs Z sur les sites intermédiaires apparaissent dans les deux chaînes avec des orientations opposées. Cette annulation algébrique réduit les opérations contrôlées multi-qubits à des portes à deux qubits pour les interactions de plus proches voisins, diminuant directement la profondeur de circuit proportionnellement à la connectivité du réseau.",
    "B": "Les opérations de parité successives atteignent des régions orbitales partagées et annulent les queues de chaîne Z lorsque les termes de saut fermionique sont appliqués séquentiellement, réduisant la profondeur de porte effective de O(n) à O(1) par terme d'interaction.",
    "C": "Les rotations de base adaptées à la symétrie alignent la transformation de Jordan-Wigner avec les nombres quantiques conservés comme le spin total ou le nombre de particules, provoquant l'effondrement des chaînes qui encodent ces symétries en représentations bloc-diagonales. Dans chaque secteur de symétrie, les chaînes de parité se réduisent à des facteurs de phase qui peuvent être suivis classiquement plutôt qu'implémentés comme des portes quantiques, réduisant le surcoût pour les simulations restreintes à des variétés spécifiques de charge ou de spin.",
    "D": "Les relations de commutateurs entre opérateurs fermioniques se transforment en algèbre de Pauli simplifiée lorsque plusieurs termes de saut impliquent les mêmes indices orbitaux, car la structure d'anticommutation force certaines chaînes de Jordan-Wigner à se télescopiser pendant la construction du circuit. Cet effet de télescopage signifie que l'application séquentielle de portes fermioniques produit un unitaire net dont la longueur de chaîne croît sous-linéairement avec le nombre de termes, particulièrement dans les décompositions de Trotter où l'ordonnancement systématique exploite les motifs d'adjacence orbitale.",
    "solution": "B"
  },
  {
    "id": 156,
    "question": "Dans les algorithmes quantiques pour la simulation à température finie, quelle est la motivation derrière l'utilisation de l'ansatz du spectre produit (PSA) ?",
    "A": "L'ansatz du spectre produit paramétrise les états thermiques comme des opérateurs de densité en produit matriciel avec une dimension de liaison qui évolue logarithmiquement avec l'inverse de la température, permettant une représentation efficace des corrélations à température finie par des contractions de réseaux tensoriels qui évitent le coût exponentiel du stockage complet de la matrice de densité. En optimisant variationnellement les éléments tensoriels pour minimiser l'énergie libre de Helmholtz en utilisant des balayages de type TEBD en temps imaginaire adaptés aux états mixtes, PSA capture les fluctuations thermiques essentielles avec une profondeur de circuit polynomiale en la taille du système. Cela élimine le besoin de purification basée sur des ancilles ou de tomographie d'état complète tout en récupérant précisément les valeurs moyennes thermiques, le rendant pratique pour le matériel à court terme où les temps de cohérence limitent la profondeur du circuit.",
    "B": "PSA construit des états thermiques approximatifs en préparant des états mixtes séparables sur des sous-systèmes locaux et en affinant itérativement les matrices de densité à site unique par des mises à jour auto-cohérentes de champ moyen qui minimisent la divergence de Kullback-Leibler par rapport à l'ensemble de Gibbs véritable. Cette procédure variationnelle converge vers un ansatz d'état produit dont les marginales correspondent à la distribution thermique exacte dans la limite de corrélations inter-qubits faibles, évitant entièrement les circuits coûteux de propagation en temps imaginaire. L'approche devient pratique sur le matériel à court terme car elle ne nécessite que des rotations locales sur un seul qubit et une optimisation classique de O(N) paramètres réels, contournant l'échelle exponentielle de la tomographie d'état complète et les exigences en temps de cohérence des circuits quantiques profonds.",
    "C": "L'ansatz du spectre produit exploite l'observation que les matrices de densité thermiques peuvent être diagonalisées par un changement de base qui transforme l'hamiltonien du système en une forme non-interactive, permettant l'extraction des valeurs propres via une déflation quantique variationnelle sans nécessiter de circuits d'estimation de phase ou de qubits ancilles pour l'amplification d'amplitude, réduisant ainsi la profondeur du circuit à des échelles réalisables sur les dispositifs NISQ.",
    "D": "PSA permet la préparation d'états de Gibbs thermiques approximatifs en utilisant des circuits quantiques de profondeur limitée en construisant des fonctions d'onde variationnelles qui minimisent l'énergie libre du système à partir d'états produits simples sur des qubits individuels. Cette approche évite l'évolution coûteuse en temps imaginaire ou la tomographie d'état complète, la rendant pratique pour le matériel quantique à court terme où la profondeur du circuit et les temps de cohérence sont sévèrement contraints, tout en capturant les corrélations thermiques essentielles nécessaires aux propriétés à température finie.",
    "solution": "D"
  },
  {
    "id": 157,
    "question": "En quoi les algorithmes de routage probabilistes diffèrent-ils des algorithmes déterministes ?",
    "A": "Ils construisent un graphe pondéré où les coûts des arêtes représentent la fidélité d'intrication, puis appliquent l'algorithme de Dijkstra pour identifier le chemin globalement optimal pour chaque demande de communication. Une fois calculé, cet itinéraire de fidélité maximale est mis en cache et réutilisé pour les demandes ultérieures entre la même paire de nœuds, garantissant des taux de succès de téléportation constants. La sélection déterministe exploite la localité temporelle des conditions du réseau, évitant le surcoût de réévaluation des chemins lorsque la qualité des liens reste stable sur plusieurs cycles.",
    "B": "Échantillonnent à partir d'une distribution de probabilité sur plusieurs chemins réseau possibles, où chaque route est pondérée selon sa fidélité d'intrication de bout en bout et son taux de succès attendu. Plutôt que de s'engager sur un seul chemin prédéterminé, ces algorithmes sélectionnent dynamiquement les routes par demande en tirant de cette distribution, permettant l'exploration de canaux alternatifs lorsque les liens primaires subissent une dégradation transitoire ou une congestion.",
    "C": "Ces algorithmes maintiennent une distribution de Boltzmann sur tous les chemins réalisables, avec une température inverse β ajustée pour équilibrer exploration et exploitation. En échantillonnant les routes proportionnellement à exp(−β·coût), où le coût incorpore à la fois la perte de fidélité et la latence, le système gravite naturellement vers des chemins de haute qualité tout en testant occasionnellement des alternatives. Cependant, l'échantillonnage rejette les chemins en fonction de la congestion en temps réel plutôt que de la fidélité, donc la qualité du lien n'influence pas directement les probabilités de route—seule la disponibilité le fait.",
    "D": "Les méthodes probabilistes appliquent l'inférence bayésienne pour estimer les distributions a posteriori sur les fidélités de lien compte tenu du retour de mesure bruité des tentatives d'intrication antérieures. Les routes sont ensuite sélectionnées en maximisant l'utilité attendue sous ces croyances mises à jour, tenant compte de l'incertitude sur la qualité du canal. Les décisions de routage classiques incorporent les résultats de mesure pour affiner la sélection du chemin, mais les états quantiques eux-mêmes sont routés de manière déterministe une fois que le contrôleur classique s'engage sur un canal spécifique de bout en bout basé sur la distribution de fidélité inférée.",
    "solution": "B"
  },
  {
    "id": 158,
    "question": "Dans le contexte du calcul quantique à variables continues, l'encodage de Gottesman–Kitaev–Preskill (GKP) a suscité une attention considérable pour protéger l'information quantique stockée dans des modes bosoniques tels que des cavités micro-ondes ou des modes optiques. La stratégie d'encodage diffère fondamentalement des codes à variables discrètes en exploitant l'espace de Hilbert de dimension infinie des oscillateurs harmoniques. Pourquoi les codes GKP sont-ils particulièrement attractifs pour les modes bosoniques ?",
    "A": "Les superpositions en forme de grille d'états propres de position permettent la correction d'erreurs contre de petits déplacements en utilisant uniquement des opérations gaussiennes, qui sont expérimentalement accessibles dans la plupart des plateformes bosoniques et préservent la nature à variables continues du système tout en fournissant une information logique discrète. La structure en réseau position-impulsion permet l'extraction de syndrome par des mesures homodynes sans nécessiter de ressources non-gaussiennes pour les opérations de correction elles-mêmes",
    "B": "L'encodage en réseau GKP quantifie naturellement les erreurs de déplacement en syndromes discrets qui correspondent directement aux canaux d'erreur de type qubit, permettant d'appliquer la machinerie du formalisme stabilisateur standard—développé à l'origine pour les systèmes discrets—presque sans modification. Les mesures homodynes de position et d'impulsion modulo l'espacement du réseau produisent des syndromes à valeurs entières correspondant aux opérateurs de décalage sur le qubit logique, et les unitaires gaussiens comme les squeezers et les déplacements suffisent à implémenter l'ensemble du groupe de Clifford sur l'information encodée, créant un cadre hybride où les mesures continues pilotent la correction d'erreurs discrète",
    "C": "Les approximations GKP d'énergie finie avec des fonctions d'enveloppe gaussiennes ψ(x) ∝ Σₙ exp(-(x-n√π)²/2Δ²) atteignent une distance de code exponentiellement croissante avec le paramètre de squeezing Δ⁻², atteignant des distances de d>100 avec la technologie actuelle de squeezing à 15dB. Cette échelle provient du fait que le chevauchement entre mots de code adjacents décroît comme exp(-π/2Δ²), rendant les erreurs logiques exponentiellement supprimées. Combiné au fait que la perte de photons—l'erreur dominante dans les cavités micro-ondes—provoque principalement des décalages position/impulsion plutôt que des effacements, les codes GKP correspondent naturellement à la structure d'erreur du matériel bosonique",
    "D": "Les codes GKP exploitent le spectre continu du mode bosonique pour implémenter une forme de correction d'erreurs temporelle où l'information quantique est encodée de manière redondante à travers l'échelle infinie des états propres d'énergie de l'oscillateur, distribuant l'amplitude de l'état logique parmi une infinité d'états de Fock |n⟩. Lorsqu'une perte de photons se produit—retirant l'amplitude des états à n élevé—la structure périodique garantit que les composantes restantes à n faible conservent l'information logique complète. L'extraction de syndrome via des mesures résolvant le nombre identifie quel sous-espace d'état de Fock contient des erreurs, et les opérations de squeezing gaussien restaurent la distribution d'amplitude correcte, offrant une protection contre la perte sans nécessiter de mesures stabilisatrices des quadratures position-impulsion",
    "solution": "A"
  },
  {
    "id": 159,
    "question": "Quel est un avantage clé de l'utilisation de méthodes basées sur l'IA par rapport aux approches conventionnelles dans la correction d'erreurs quantiques ?",
    "A": "Elles éliminent le besoin de qubits physiques en simulant les données quantiques entièrement dans des architectures de réseaux neuronaux classiques qui peuvent apprendre à émuler les propriétés de superposition quantique et d'intrication, permettant ainsi aux algorithmes quantiques de fonctionner sur des clusters GPU conventionnels sans nécessiter d'infrastructure cryogénique ou de traiter la décohérence.",
    "B": "Elles garantissent un calcul tolérant aux pannes sans amélioration matérielle, contournant entièrement les exigences de seuil grâce à des stratégies de décodage apprises qui peuvent corriger les erreurs au-delà des limites théoriques imposées par le théorème du seuil de correction d'erreurs quantiques.",
    "C": "Une efficacité et une précision supérieures tout au long de la chaîne QEC, incluant le décodage de syndrome, l'optimisation de portes logiques et les stratégies d'atténuation d'erreurs, où les réseaux neuronaux peuvent apprendre des motifs complexes dans les corrélations d'erreurs et s'adapter à des modèles de bruit non standards, surpassant les décodeurs traditionnels d'appariement parfait de poids minimum tant en vitesse qu'en suppression d'erreurs pour le bruit matériel réaliste.",
    "D": "L'élimination du besoin de comprendre les modèles de bruit quantique sous-jacents car les réseaux neuronaux découvrent automatiquement des stratégies de correction optimales par l'entraînement sur des données de syndrome brutes, rendant possible le déploiement de la correction d'erreurs quantiques sur de nouvelles plateformes de qubits.",
    "solution": "C"
  },
  {
    "id": 160,
    "question": "Qu'est-ce qui rend certains codes de correction d'erreurs quantiques (par exemple, les codes Bivariate Bicycle) plus adaptés aux implémentations matérielles à court terme ?",
    "A": "Ces codes présentent des matrices de parité creuses où chaque qubit physique participe à seulement un petit nombre constant de mesures de stabilisateurs, se traduisant directement par de faibles exigences de connectivité dans le graphe matériel. Cette structure locale permet des implémentations sur des architectures avec des contraintes de couplage au plus proche voisin, évitant les interactions à longue portée qui affligent les codes de surface sur des réseaux planaires.",
    "B": "Ces codes atteignent des taux d'encodage favorables k/n approchant la borne de Hamming quantique tout en maintenant des contrôles de parité de poids constant où chaque stabilisateur implique exactement w qubits physiques (typiquement w=6 pour les codes Bivariate Bicycle). Ce poids de stabilisateur borné se traduit par une extraction de syndrome parallélisable utilisant uniquement des couplages au plus proche voisin et au deuxième plus proche voisin sur des géométries de réseau tessellées appropriées. Cependant, contrairement aux codes de surface, le décodage optimal nécessite une contraction de réseau tensoriel sur le graphe de Tanner plutôt qu'un appariement de poids minimum, augmentant le surcoût classique malgré la connectivité réduite.",
    "C": "En construisant des codes parents LDPC classiques avec des graphes de Tanner de périmètre 8 et en appliquant la construction CSS par des sous-codes auto-orthogonaux, ces codes génèrent des contrôles de parité quantiques où les circuits de mesure de syndrome nécessitent une profondeur logarithmique en la distance du code plutôt que linéaire. Cette réduction provient du fait que les contraintes de périmètre élevé éliminent les cycles courts qui autrement sérialiseraient les mesures de stabilisateurs, permettant une extraction à profondeur constante via des mesures de Pauli programmées de manière appropriée sur des sous-ensembles de qubits non chevauchants, bien que le surcoût de routage entre les cycles de mesure évolue quadratiquement avec la distance.",
    "D": "Ces codes exploitent des constructions de produit hypergraphe qui produisent des groupes de stabilisateurs commutants avec un paramètre de sparsité s satisfaisant s ≪ √n où n est la longueur du bloc, permettant directement l'extraction de syndrome par des circuits quantiques à profondeur constante sur des graphes de degré limité. L'avantage clé émerge de la structure algébrique : les stabilisateurs se factorisent en produits tensoriels de petits Paulis, évitant les générateurs de stabilisateurs denses qui nécessitent des échelles CNOT séquentielles dans les codes concaténés. Cependant, cette décomposition nécessite un surcoût d'ancilles évoluant comme Θ(d² log d) où d est la distance du code.",
    "solution": "A"
  },
  {
    "id": 161,
    "question": "Quelle est l'importance des classes de complexité quantique comme BQP en informatique quantique théorique ?",
    "A": "Elles caractérisent quels problèmes computationnels les ordinateurs quantiques peuvent résoudre efficacement en temps polynomial, établissant les limites fondamentales de l'avantage computationnel quantique par rapport aux modèles classiques et identifiant où l'accélération quantique est théoriquement possible.",
    "B": "Les classes de complexité quantique comme BQP établissent des bornes strictes sur les ressources de mesure requises pour les protocoles de vérification quantique, montrant que le calcul quantique en temps polynomial est exactement équivalent au calcul classique augmenté d'un nombre polynomial de violations des inégalités de Bell. Cette caractérisation révèle que l'avantage quantique émerge précisément des corrélations non locales plutôt que de la superposition seule.",
    "C": "Elles formalisent la relation entre la profondeur des circuits quantiques et le temps de calcul parallèle classique, prouvant que BQP égale NC (Nick's Class) sous des restrictions de profondeur polylogarithmique. Cela établit que l'accélération quantique dérive fondamentalement de la parallélisation efficace des portes quantiques plutôt que de l'intrication, bien que les séparations d'oracle suggèrent que BQP puisse s'étendre au-delà de P dans certaines instances de problèmes structurés.",
    "D": "Les classes BQP caractérisent la complexité d'échantillonnage quantique et la génération d'aléa certifiable, définissant quelles distributions de probabilité peuvent être échantillonnées efficacement avec des circuits quantiques tout en restant difficiles à simuler classiquement. Cela capture l'avantage quantique dans les applications à court terme où les problèmes de décision peuvent être insolubles mais l'échantillonnage suffit pour une utilité pratique.",
    "solution": "A"
  },
  {
    "id": 162,
    "question": "Quelle technique avancée assure la sécurité contre les fuites d'information dans le post-traitement classique de la distribution quantique de clés ?",
    "A": "Les protocoles d'amplification de confidentialité renforcés par des fonctions de hachage cryptographiques résistantes aux attaques quantiques telles que SHA-3 ou BLAKE3 éliminent les fuites d'information en compressant le matériel de clé brute par des opérations de hachage computationnellement sécurisées.",
    "B": "Le chiffrement authentifié théoriquement sûr empêche les fuites par canal auxiliaire pendant la phase de post-traitement classique en garantissant qu'aucune hypothèse computationnelle n'est requise pour borner le gain d'information de l'adversaire, même lorsque l'adversaire dispose de ressources computationnelles illimitées. Cette approche intègre des étiquettes d'authentification dérivées du matériel de clé quantique brute dans chaque message classique échangé pendant la correction d'erreurs et l'estimation des paramètres, garantissant que toute tentative d'attaque de l'homme du milieu ou mesure des émanations électromagnétiques du matériel de traitement révèle de manière prouvable zéro bit de la clé finale, car l'authentification est inconditionnellement sécurisée contre toutes les attaques y compris quantiques.",
    "C": "Les extracteurs résistants aux attaques quantiques conçus spécifiquement pour l'étape de post-traitement appliquent des extracteurs d'aléa forts avec des preuves de sécurité qui restent valides contre les adversaires quantiques, convertissant les bits de clé brute partiellement corrélés en une chaîne uniformément aléatoire.",
    "D": "Les cadres de sécurité composables universels établissent des garanties rigoureuses que les protocoles QKD restent sécurisés lorsqu'ils sont composés avec d'autres protocoles cryptographiques dans des systèmes plus larges, assurant que les propriétés de sécurité sont préservées même lorsque les clés sont utilisées dans des applications arbitraires. Ces cadres fournissent des preuves formelles que les fuites d'information pendant le post-traitement sont bornées quel que soit le déploiement ultérieur de la clé finale.",
    "solution": "D"
  },
  {
    "id": 163,
    "question": "Comment les algorithmes de correspondance de modèles conscients de la commutation surpassent-ils les optimiseurs par fenêtre glissante naïfs ?",
    "A": "Les optimiseurs par fenêtre glissante opèrent traditionnellement sur des fenêtres fixes de portes consécutives, manquant des opportunités d'optimisation lorsque des motifs correspondants sont séparés par des portes intermédiaires commutatives. Les algorithmes de correspondance de modèles conscients de la commutation surmontent cette limitation en analysant les relations de commutation pour réordonner efficacement les portes, rapprochant les portes non adjacentes mais commutatives là où les modèles peuvent correspondre. Cette portée étendue à travers les ordres logiques de portes expose substantiellement plus d'opportunités de simplification que les approches à fenêtre fixe, produisant une réduction supérieure du nombre de portes et une optimisation de la profondeur du circuit.",
    "B": "Les algorithmes de correspondance de modèles conscients de la commutation exploitent les relations de commutation des portes quantiques pour étendre la fenêtre de recherche au-delà des portes immédiatement adjacentes, permettant à l'optimiseur d'identifier et d'appliquer des motifs de réécriture qui couvrent des portes non adjacentes qui seraient autrement invisibles aux optimiseurs par fenêtre glissante fixe. Cette capacité de recherche élargie expose des opportunités d'optimisation à travers des régions de circuit plus larges, conduisant à une simplification de circuit plus agressive et une meilleure réduction globale du nombre de portes.",
    "C": "Les optimiseurs par fenêtre glissante naïfs examinent uniquement les portes syntaxiquement adjacentes dans la représentation du circuit, tandis que les algorithmes conscients de la commutation construisent dynamiquement des classes d'équivalence d'ordres de portes en appliquant des règles de commutation pour permuter les portes en formes canoniques. Cette approche par classes d'équivalence permet aux modèles de correspondre à des motifs distribués à travers des segments de circuit non contigus qui satisfont les contraintes de commutation, étendant effectivement la visibilité de l'optimiseur de voisinages locaux de taille k à des régions de taille O(k²) où k borne les chaînes de portes commutatives, découvrant ainsi des opportunités d'optimisation invisibles aux méthodes par fenêtre glissante dépendantes de la position.",
    "D": "Les optimiseurs par fenêtre glissante traditionnels appliquent des règles de réécriture uniquement lorsque les portes apparaissent dans un ordre séquentiel strict dans une fenêtre fixe, manquant des optimisations lorsque des sous-séquences fonctionnellement équivalentes existent mais sont entrelacées avec des opérations commutatives. Les algorithmes de correspondance de modèles conscients de la commutation résolvent cela en construisant des graphes de dépendance qui identifient les frontières de commutation, puis en réordonnant virtuellement les portes pour maximiser la correspondance de modèles sans altérer la sémantique du circuit. Ce réordonnancement basé sur les graphes expose des motifs d'optimisation à travers des régions étendues déterminées par la structure de commutation plutôt que la proximité positionnelle, atteignant une meilleure réduction que les approches contraintes par fenêtre.",
    "solution": "B"
  },
  {
    "id": 164,
    "question": "Comment fonctionnent les autoencodeurs quantiques ?",
    "A": "Les transformées de Fourier quantiques convertissent l'état quantique d'entrée en une représentation dans le domaine fréquentiel distribuée sur tous les qubits, après quoi nous tronquons systématiquement les composantes de Fourier haute fréquence en éliminant les qubits correspondant aux oscillations rapides dans le spectre d'amplitude.",
    "B": "Des mesures réversibles qui effondrent sélectivement les degrés de liberté superflus tout en préservant parfaitement les amplitudes quantiques significatives dans un sous-ensemble plus petit de qubits, effectuant essentiellement une réduction de dimensionnalité par effondrement partiel de la fonction d'onde. L'architecture d'autoencodeur implémente des mesures faibles avec des forces de mesure soigneusement ajustées, extrayant juste assez d'information classique pour éliminer les sous-espaces à faible variance tout en laissant les composantes à haute information en superposition, réalisant ainsi une compression avec perte sans destruction complète de l'état.",
    "C": "Ils encodent et décodent l'information quantique à travers des circuits quantiques paramétrés entraînés pour comprimer des données de haute dimension dans des états quantiques en moins de qubits tout en préservant les caractéristiques essentielles. L'encodeur mappe les états d'entrée vers un espace latent de dimension inférieure, et le décodeur tente la reconstruction, l'entraînement optimisant le circuit pour minimiser la perte d'information pendant ce processus de réduction de dimensionnalité.",
    "D": "Des opérateurs unitaires prédéterminés qui isolent les états propres correspondant aux plus grandes valeurs singulières de la matrice densité d'entrée, éliminant automatiquement le reste par interférence destructive sans nécessiter d'entraînement ni d'optimisation. La compression se produit parce que l'application de cet unitaire fixe provoque l'interférence destructive et la disparition des modes propres à faible variance de l'état réduit, concentrant toute l'information quantique dans les k premiers vecteurs propres qui survivent à la transformation d'encodage.",
    "solution": "C"
  },
  {
    "id": 165,
    "question": "Lors de l'entraînement de machines de Born à circuits quantiques (QCBM) pour apprendre des distributions de probabilité, quelle mesure de divergence minimisez-vous typiquement ? L'objectif est de faire correspondre la distribution du modèle à la distribution des données cibles aussi étroitement que possible, et le choix de la divergence affecte directement à la fois les estimations de gradient et les propriétés de convergence de la procédure d'optimisation.",
    "A": "La perte charnière classique calculée sur des étiquettes binaires dérivées des résultats de mesure, empruntée directement de la théorie des machines à vecteurs de support.",
    "B": "La distance de variation totale, qui nécessite de calculer la distribution de probabilité complète sur tous les états de base computationnelle en effectuant une tomographie sur la matrice densité de sortie, puis en prenant la norme L1 entre la distribution du modèle reconstruite et la distribution cible empirique.",
    "C": "L'erreur quadratique moyenne entre les vecteurs de paramètres du circuit d'époques d'entraînement successives, traitant effectivement l'algorithme quantique variationnel comme un problème d'apprentissage supervisé classique où l'objectif d'optimisation à chaque étape est défini par la configuration de paramètres de l'itération précédente.",
    "D": "La divergence KL estimée à partir de probabilités d'échantillons — l'approche standard consiste à tirer des échantillons des deux distributions et calculer l'entropie relative, ce qui vous donne des gradients qui peuvent être estimés via des règles de décalage de paramètres sur le circuit quantique. Spécifiquement, vous minimisez D_KL(p_data || p_model) où p_data est la distribution cible empirique et p_model est la distribution de la règle de Born du circuit quantique. Ce choix est naturel car la divergence KL est asymétrique d'une manière qui priorise l'ajustement du support de la distribution des données, les gradients se décomposent bien pour les circuits quantiques variationnels utilisant la règle de décalage de paramètres pour les valeurs d'espérance, et l'objectif peut être estimé efficacement à partir d'un nombre polynomial d'échantillons de mesure sans nécessiter de tomographie d'état complète, le rendant computationnellement traitable même pour des circuits avec de nombreux qubits.",
    "solution": "D"
  },
  {
    "id": 166,
    "question": "Quelle attaque spécifique cible la génération de porteuse micro-onde dans les systèmes de contrôle quantique ?",
    "A": "Manipuler la boucle à verrouillage de phase pour déstabiliser la synthèse de porteuse implique d'injecter des interférences électromagnétiques calibrées à des fréquences proches de la plage de verrouillage naturelle de la PLL, ce qui fait osciller le mécanisme de rétroaction entre des cibles de fréquence concurrentes et produit une dérive de phase variable dans le temps.",
    "B": "La contamination de la synthèse de fréquence par introduction délibérée d'harmoniques parasites corrompt la pureté du signal de contrôle en exploitant les produits de mélange non linéaires dans la chaîne de conversion ascendante, où des signaux d'interférence soigneusement conçus à des fréquences sous-harmoniques génèrent des distorsions d'intermodulation qui se replient directement sur la fréquence de transition du qubit cible.",
    "C": "Les attaques par injection sur l'oscillateur de référence compromettent le signal d'horloge maître en introduisant des interférences électromagnétiques à l'entrée de la fréquence de référence, exploitant le fait que toutes les porteuses micro-ondes en aval dérivent leur cohérence de phase de cette source unique, corrompant ainsi simultanément la temporisation des impulsions de contrôle sur l'ensemble du processeur quantique.",
    "D": "L'interférence de distribution d'horloge cible la cohérence de phase entre les oscillateurs locaux spatialement distribués en induisant une gigue temporelle dans le réseau d'arbre d'horloge par couplage électromagnétique stratégique aux lignes de distribution à haute impédance, exploitant le fait que les portes quantiques reposent sur une synchronisation temporelle précise des impulsions de contrôle sur plusieurs canaux.",
    "solution": "C"
  },
  {
    "id": 167,
    "question": "Quelle caractéristique des plateformes à ions piégés les rend particulièrement adaptées aux expériences quantiques distribuées ?",
    "A": "Les longs temps de cohérence et les portes à haute fidélité permettent une téléportation fiable entre modules, fournissant les états quantiques stables et les opérations précises nécessaires pour distribuer l'intrication entre systèmes de pièges à ions physiquement séparés. Les temps de cohérence dépassant plusieurs secondes permettent des protocoles d'échange d'intrication en plusieurs étapes, tandis que les fidélités de portes à deux qubits supérieures à 99,9 % garantissent que les paires de Bell générées pour les liens distants maintiennent une qualité suffisamment élevée pour supporter un calcul distribué tolérant aux fautes.",
    "B": "La combinaison de transitions optiques étroites (largeurs de raie sub-kHz pour les transitions quadrupolaires) et d'un couplage ion-photon efficace via l'émission spontanée amplifiée par cavité permet une distribution d'intrication déterministe à travers des canaux photoniques. En collectant les photons de fluorescence de chaque piège à ions dans des fibres monomodes et en effectuant une interférence Hong-Ou-Mandel à un séparateur de faisceau, une intrication distante annoncée peut être générée à des taux dépassant 1 kHz avec des fidélités supérieures à 95 %, suffisantes pour les protocoles quantiques distribués. Les temps de cohérence dépassant plusieurs secondes garantissent que les paires de Bell générées restent utilisables tout au long des étapes d'échange d'intrication et de purification requises pour les liaisons longue distance.",
    "C": "Les ions piégés se couplent naturellement aux modes optiques propagatifs via diffusion Raman spontanée lorsqu'ils sont pilotés par des champs laser hors résonance, créant une intrication ion-photon qui peut être acheminée à travers des réseaux de fibres optiques vers des modules de pièges distants. Le processus Raman génère des photons intriqués avec l'état interne de l'ion dans un codage temporel ou de polarisation, et en effectuant des mesures d'état de Bell sur des photons provenant de différents pièges, une intrication ion-ion distante peut être établie. Les longs temps de cohérence (dépassant plusieurs secondes) et les portes locales à haute fidélité (>99,9 %) garantissent que les états intriqués distribués survivent à la latence de communication classique requise pour les opérations de propagation avant dans les circuits distribués basés sur la téléportation.",
    "D": "Les états de qubit codés dans les sous-niveaux hyperfins ou Zeeman des ions piégés présentent des temps de cohérence exceptionnellement longs (T₂ > 10 secondes) et peuvent s'interfacer directement avec des photons optiques voyageurs via des transitions dipolaires électriques lorsque les ions sont intégrés dans des circuits photoniques intégrés sur puce. Le confinement serré du mode dans les guides d'onde en nitrure de silicium améliore la force de couplage ion-photon d'un facteur dépassant 100 par rapport à la collection en espace libre, permettant une émission de photon unique quasi-déterministe dans le mode du guide d'onde. Combiné à des fidélités de portes à deux qubits supérieures à 99,9 %, cette interface photonique permet une génération efficace d'intrication distante par interférence de photons à des séparateurs de faisceau intégrés, ce qui est essentiel pour distribuer des états quantiques à travers des modules de pièges spatialement séparés sans pertes optiques en espace libre.",
    "solution": "A"
  },
  {
    "id": 168,
    "question": "Dans les scénarios de calcul quantique distribué, les protocoles de routage pour les états intriqués multi-parties tels que les états GHZ doivent faire face à plusieurs défis uniques. Contrairement au routage classique où la duplication de paquets est triviale, l'information quantique ne peut pas être clonée en raison de principes fondamentaux. Cependant, le principal défi de routage lors de l'établissement d'états comme |GHZ⟩ = (|000⟩ + |111⟩)/√2 à travers des nœuds géographiquement séparés est de garantir que chaque participant reçoive une intrication de haute qualité simultanément. Quelle contrainte spécifique cette exigence de simultanéité impose-t-elle sur l'architecture du réseau ?",
    "A": "Le routage quantique nécessite que tous les nœuds intermédiaires effectuent des mesures conjointes sur des paires intriquées qui arrivent de manière asynchrone de différentes sources, mais la règle de Born garantit que les résultats de mesure sont indépendants de l'ordre d'arrivée pourvu que chaque paire soit mesurée avant la décohérence. Cependant, les états GHZ exigent que toutes les paires constitutives proviennent d'un événement d'annonce commun à la source, forçant le réseau à implémenter des canaux de diffusion synchrones où chaque impulsion de photon atteint sa destination dans la même fenêtre de cohérence, éliminant entièrement la mise en mémoire tampon avec stockage et retransmission.",
    "B": "Parce que les états GHZ présentent des corrélations parfaites à travers toutes les mesures de base de calcul, toute asymétrie dans le bruit du canal entre participants brise la symétrie de permutation de l'état et fait que la matrice densité réduite à chaque nœud devient distinguable. Le protocole de routage doit donc garantir que tous les canaux quantiques entre la source et chaque participant ont des paramètres de fidélité identiques—pas seulement une fidélité acceptable, mais des taux d'erreur correspondants—sinon la distinguabilité viole la monogamie de l'intrication et l'état distribué ne peut pas être utilisé pour des protocoles multi-parties comme le partage de secret ou l'accord byzantin.",
    "C": "Tous les canaux quantiques reliant le nœud de distribution central à chaque participant doivent maintenir une fidélité suffisamment élevée en même temps—si une liaison de communication se dégrade pendant que vous préparez les autres, l'état intriqué multi-parties entier devient compromis et le protocole échoue.",
    "D": "La distribution d'état GHZ nécessite que le réseau satisfasse une contrainte d'ordonnancement temporel où les opérations d'échange d'intrication aux routeurs intermédiaires doivent se terminer dans une séquence spécifique déterminée par les générateurs de stabilisateurs de l'état. Si les événements d'échange se produisent dans le désordre par rapport à la topologie logique codée dans les stabilisateurs, l'état résultant devient un état intriqué multi-parties différent—souvent un état W ou un état cluster—plutôt que l'état GHZ visé. L'architecture du réseau doit imposer un ordonnancement causal de toutes les mesures de Bell, typiquement implémenté via des messages de synchronisation classiques qui garantissent que chaque routeur attend la confirmation des prédécesseurs avant d'exécuter son échange.",
    "solution": "C"
  },
  {
    "id": 169,
    "question": "Quelle est la motivation pour calibrer des impulsions paramétrées en utilisant l'optimisation bayésienne en boucle fermée ?",
    "A": "L'optimisation bayésienne fournit un cadre principiel pour incorporer les connaissances a priori sur le paysage des paramètres d'impulsion dans la procédure de calibration par des fonctions de noyau soigneusement choisies dans le modèle de substitution par processus gaussien. La méthode excelle lorsque l'hamiltonien de contrôle présente plusieurs optima locaux dus à la diaphonie entre canaux de contrôle ou à une réponse non linéaire dans le régime d'anharmonicité du transmon. Cependant, l'avantage principal émerge dans les systèmes purement cohérents où les temps T₁ et T₂ dépassent la durée totale de l'expérience de calibration—dans de tels cas, l'optimisation peut fonctionner en boucle ouverte sans retour de mesure, utilisant la variance postérieure du GP pour guider l'exploration des paramètres. L'architecture en boucle fermée devient essentielle principalement pour valider que les paramètres d'impulsion optimaux découverts en simulation se transfèrent de manière fiable au matériel physique sans nécessiter de raffinement itératif.",
    "B": "Les méthodes bayésiennes en boucle fermée répondent au défi fondamental que l'estimation directe du gradient via des règles de décalage de paramètres ou des méthodes de différences finies évolue mal avec le nombre de paramètres d'impulsion en raison du bruit de tir dans les mesures quantiques. La fonction d'acquisition du processus gaussien (typiquement amélioration attendue ou borne de confiance supérieure) équilibre intelligemment l'exploration des régions de paramètres sous-échantillonnées contre l'exploitation des zones actuellement prometteuses, nécessitant beaucoup moins d'expériences matérielles coûteuses que les optimiseurs basés sur les dérivées. L'approche s'avère particulièrement efficace lors du traitement d'erreurs systématiques dépendantes du temps ou lorsque le paysage de la fonction objectif contient des caractéristiques abruptes qui feraient souffrir les estimateurs de gradient d'une variance élevée. Cependant, les garanties de convergence ne tiennent que lorsque la fidélité d'impulsion est une fonction lisse des paramètres avec des dérivées bornées, ce qui se brise pour les impulsions près des résonances de découplage dynamique.",
    "C": "La motivation clé est que l'optimisation bayésienne gère naturellement les fonctions objectifs stochastiques qui résultent de mesures quantiques à nombre fini de tirs en modélisant le paysage de fidélité comme un processus gaussien avec bruit d'observation. Chaque expérience de calibration fournit un échantillon bruité de la vraie fidélité de porte à des paramètres d'impulsion spécifiques, et la distribution postérieure GP capture à la fois la fidélité moyenne estimée et l'incertitude aux valeurs de paramètres inexplorées. La fonction d'acquisition guide ensuite la sélection des paramètres à tester ensuite en maximisant le gain d'information attendu sur l'emplacement de l'optimum global. Cette approche efficace en échantillons surpasse la descente de gradient lorsque les évaluations de fonction sont coûteuses—chaque expérience matérielle consomme du temps réel et contribue à la décohérence du qubit par des opérations répétées. L'architecture en boucle fermée s'adapte également à la dérive matérielle en raffinant continuellement le modèle GP au fur et à mesure que de nouvelles mesures arrivent, bien que la convergence nécessite que l'échelle de temps de dérive dépasse substantiellement la durée des expériences de calibration individuelles.",
    "D": "Le retour de mesure du matériel quantique permet à l'optimisation bayésienne de mettre à jour un modèle probabiliste du paysage de contrôle, incorporant les informations de chaque essai expérimental pour affiner itérativement les estimations de paramètres. Cette approche adaptative converge vers des paramètres d'impulsion à haute fidélité beaucoup plus efficacement qu'une recherche exhaustive sur grille ou un échantillonnage aléatoire, parce que le a priori du processus gaussien capture la structure lisse dans la façon dont la fidélité de porte varie avec les paramètres d'impulsion, permettant à l'algorithme de sélectionner intelligemment le prochain point de mesure qui réduit maximalement l'incertitude. L'architecture en boucle fermée est particulièrement précieuse lors du traitement de la dérive matérielle, de paysages d'optimisation non convexes, ou d'évaluations de fonction coûteuses où minimiser le nombre d'expériences de calibration est critique.",
    "solution": "D"
  },
  {
    "id": 170,
    "question": "Dans le contexte du calcul quantique tolérant aux fautes, quelle limitation fondamentale les théorèmes de seuil abordent-ils ? Ces théorèmes sont centraux pour comprendre si les ordinateurs quantiques à grande échelle peuvent un jour fonctionner en pratique, étant donné que tous les composants physiques introduisent une certaine quantité de bruit et d'erreur.",
    "A": "Ils établissent les conditions sous lesquelles le calcul quantique reste fiable même avec des composants imparfaits, à condition que les taux d'erreur restent en dessous d'une certaine valeur de seuil. Ce seuil dépend du code de correction d'erreur spécifique, du modèle d'erreur et du décodeur utilisé. En dessous du seuil, la concaténation de codes ou l'augmentation de la distance de code permet des calculs arbitrairement longs avec des taux d'erreur logiques arbitrairement faibles.",
    "B": "Ils établissent des bornes supérieures sur le ratio de surcharge asymptotique entre les temps de portes logiques et physiques requis pour le calcul tolérant aux fautes. Les théorèmes de seuil prouvent qu'à condition que les taux d'erreur physiques restent en dessous d'une valeur critique (typiquement 10^-3 à 10^-4), la pénalité de temps de porte logique due à la correction d'erreur sature à un facteur multiplicatif constant indépendant de la longueur du calcul. Ce seuil de surcharge temporelle garantit que des calculs quantiques arbitrairement longs restent des processus en temps polynomial plutôt qu'en temps exponentiel.",
    "C": "Ils établissent les conditions nécessaires sur les propriétés de cohérence des codes de correction d'erreur quantique, prouvant spécifiquement que les mesures de syndrome doivent dépasser la décohérence par une marge de seuil. Les théorèmes de seuil démontrent que lorsque le temps d'extraction de syndrome dépasse une fraction critique du temps de cohérence de la mémoire (typiquement environ 1/7 pour les codes de surface), la propagation d'erreur dépasse la capacité de correction indépendamment de la distance de code. En dessous de ce seuil de ratio, l'augmentation de la distance de code permet un calcul fiable même avec des mesures de syndrome imparfaites.",
    "D": "Ils établissent la distance de code minimale requise pour un taux d'erreur physique donné, prouvant que la correction d'erreur quantique ne devient efficace que lorsque la distance de code d dépasse une valeur de seuil proportionnelle à 1/√p, où p est le taux d'erreur physique. Les théorèmes de seuil montrent qu'en dessous de ce seuil de distance, même un traitement de syndrome idéal ne peut pas supprimer les erreurs logiques, mais au-dessus du seuil, les codes concaténés permettent une suppression exponentielle des taux d'erreur logiques à chaque niveau de concaténation.",
    "solution": "A"
  },
  {
    "id": 171,
    "question": "Quel compromis DisMap aborde-t-il dans son processus de partitionnement et de mappage ?",
    "A": "L'algorithme équilibre la minimisation de la profondeur du circuit au sein de chaque partition contre le surcoût introduit par la distribution de l'intrication entre les partitions — en gardant ensemble des sous-graphes fortement connectés, vous réduisez le nombre de SWAPs internes, mais cela crée des modules plus grands qui nécessitent davantage de paires de Bell pour établir la connectivité inter-partition, augmentant ainsi la latence.",
    "B": "L'algorithme équilibre la localité des portes au sein des partitions contre le surcoût de communication inter-modules — vous essayez de maintenir une haute fidélité des portes à deux qubits en minimisant les opérations qui s'étendent sur plusieurs modules matériels, mais cette contrainte impose des opérations SWAP supplémentaires ou des coûts de distribution d'intrication.",
    "C": "DisMap optimise le compromis entre l'efficacité d'utilisation des qubits et l'accumulation d'erreurs de portes en partitionnant le circuit en modules équilibrés qui minimisent le surcoût de qubits inactifs, mais cette stratégie d'équilibrage de charge augmente par inadvertance la longueur du chemin critique car des portes qui pourraient s'exécuter en parallèle sont sérialisées pour maintenir la symétrie des partitions.",
    "D": "Le processus de partitionnement fait un compromis entre la décohérence induite par la mesure et le parallélisme des portes — concentrer les mesures dans moins de partitions réduit le nombre d'événements de mesure en milieu de circuit et leurs pénalités de décohérence associées, mais force l'exécution séquentielle de couches de portes qui pourraient autrement s'exécuter simultanément sur des modules séparés, augmentant la durée globale du circuit.",
    "solution": "B"
  },
  {
    "id": 172,
    "question": "Quel est un avantage significatif de l'entraînement des HQNNs sur des simulateurs quantiques avant leur déploiement sur du matériel quantique réel ?",
    "A": "L'entraînement sur simulateurs fournit une initialisation robuste des paramètres grâce à l'optimisation au niveau des portes en régime sans bruit, où les méthodes basées sur le gradient convergent vers des configurations qui restent quasi-optimales lorsque le bruit est introduit lors du déploiement matériel. Les paramètres variationnels appris encodent des solutions approximatives au paysage d'optimisation qui se transfèrent efficacement d'une plateforme à l'autre en raison de l'universalité sous-jacente des ensembles de portes quantiques, bien qu'une convergence finale nécessite généralement un ajustement modeste (5 à 10 époques supplémentaires) sur le matériel cible pour compenser les différences de temps de cohérence et les contraintes de connectivité. Cette stratégie d'initialisation réduit le temps total d'unité de traitement quantique en permettant un déploiement avec démarrage à chaud plutôt qu'une initialisation aléatoire des paramètres, et les structures de circuits apprises présentent souvent des propriétés de robustesse inhérentes où l'optimisation découvre naturellement des régions de paramètres avec des paysages de perte plats qui tolèrent les imperfections matérielles dans les plages de fonctionnement typiques.",
    "B": "Les modèles entraînés sur simulateurs se transfèrent aux dispositifs réels avec des performances comparables, permettant aux chercheurs d'itérer rapidement à travers l'optimisation des hyperparamètres, la sélection d'architecture et l'affinement du protocole d'entraînement dans l'environnement de simulation avant d'engager un temps coûteux d'unité de traitement quantique pour les essais de validation finale. Ce cycle de développement accéléré réduit le coût par expérience de plusieurs ordres de grandeur tout en permettant une exploration systématique de l'espace de conception des HQNN, y compris les structures d'ansatz variationnels, les stratégies de mesure et les protocoles d'interface classique-quantique. Le simulateur fournit un banc d'essai contrôlé où les hypothèses sur l'avantage quantique peuvent être évaluées efficacement, et les configurations réussies peuvent ensuite être déployées sur du matériel avec la confiance que les principes algorithmiques fondamentaux ont été validés, même si un réglage final des performances peut encore être nécessaire pour tenir compte des caractéristiques spécifiques au dispositif.",
    "C": "Les simulateurs permettent une caractérisation complète du bruit grâce à l'injection contrôlée de modèles de bruit paramétrés calibrés à partir de données de caractérisation matérielle, permettant une étude systématique de la façon dont les HQNNs répondent à des mécanismes d'erreur spécifiques comme l'amortissement d'amplitude, le déphasage et les erreurs de portes corrélées. En s'entraînant dans ces conditions de bruit qui se rapprochent mais ne répliquent pas parfaitement le comportement matériel, les modèles développent une robustesse partielle aux motifs d'erreur avant le déploiement matériel, réduisant mais n'éliminant pas le besoin d'entraînement sur dispositif. Cependant, les simulateurs ne peuvent pas capturer tous les effets subtils spécifiques au dispositif tels que la diaphonie dépendante de la fréquence, la dérive de calibration variable dans le temps et le couplage environnemental non-markovien, ce qui signifie que les modèles doivent encore subir une validation matérielle où des écarts de performance résiduels émergent de ces phénomènes non modélisés nécessitant des ajustements d'optimisation finaux impliquant généralement 15 à 25 % des itérations d'entraînement originales.",
    "D": "Les HQNNs entraînés sur simulateurs exploitent des primitives algorithmiques indépendantes du matériel basées sur des représentations abstraites de circuits quantiques qui découplent les opérations logiques des implémentations physiques, permettant au même modèle entraîné de s'exécuter sur n'importe quel processeur quantique supportant l'ensemble de portes requis. Bien que la topologie de connectivité influence le surcoût de compilation par l'insertion de portes SWAP supplémentaires, les paramètres appris encodent des transformations au niveau logique qui restent fonctionnellement équivalentes d'une plateforme à l'autre, ne nécessitant qu'une transpilation automatique du circuit plutôt qu'un réentraînement. Cette abstraction architecturale signifie que les modèles entraînés par simulation atteignent 85 à 95 % de leurs performances simulées immédiatement lors du déploiement matériel sur différentes familles de processeurs (supraconducteur, piège à ions, photonique) sans modification, les écarts de performance restants étant attribuables principalement à la décohérence dépendante de la profondeur plutôt qu'à une incompatibilité algorithmique fondamentale entre l'exécution en simulation et sur matériel.",
    "solution": "B"
  },
  {
    "id": 173,
    "question": "Quelle technique sophistiquée fournit l'amplification de confidentialité la plus efficace dans la distribution quantique de clés ?",
    "A": "Les fonctions de hachage 2-universelles avec sécurité quantique fournissent une amplification de confidentialité optimale en garantissant que pour toutes deux clés d'entrée distinctes, la probabilité de collision est bornée par 1/2^n, où n est la longueur de sortie. Cette famille de fonctions de hachage satisfait le lemme de hachage résiduel sous information latérale quantique, garantissant que la sortie est exponentiellement proche de l'uniforme même lorsqu'un adversaire détient des corrélations quantiques avec l'entrée, atteignant ainsi une sécurité théorique de l'information avec une consommation minimale de clé par rapport aux extracteurs classiques.",
    "B": "Les extracteurs résistants au quantique exploitent des primitives cryptographiques post-quantiques telles que des constructions basées sur les réseaux ou les codes pour garantir que même les adversaires disposant d'ordinateurs quantiques ne peuvent pas extraire d'informations du matériel de clé compressé. Ces extracteurs incorporent des fonctions pseudoaléatoires quantiquement sécurisées dans la phase de compression, fournissant des garanties de sécurité computationnelle qui restent valides même après l'avènement d'ordinateurs quantiques à grande échelle.",
    "C": "La multiplication par matrice de Toeplitz permet une amplification de confidentialité optimale grâce à des applications linéaires structurées qui compressent la clé brute en une clé sécurisée plus courte, avec une sécurité prouvable contre des adversaires quantiques détenant des informations latérales.",
    "D": "L'extraction de caractère aléatoire théorique de l'information permet l'amplification de confidentialité en appliquant des fonctions déterministes qui compressent des chaînes partiellement aléatoires en sorties plus courtes et presque uniformes, la borne de sécurité étant dérivée de considérations de min-entropie. Dans le contexte QKD, cette approche utilise des extracteurs à graine où la graine est partagée publiquement, et la clé extraite est prouvée être ε-proche de la distribution uniforme indépendamment de toute information latérale classique ou quantique détenue par l'espion.",
    "solution": "C"
  },
  {
    "id": 174,
    "question": "Pourquoi la réduction du nombre de portes SWAP est-elle critique dans les circuits quantiques distribués basés sur LNN ?",
    "A": "Les SWAPs ne peuvent fondamentalement pas être implémentés sur du matériel supraconducteur sans décomposition en trois portes CNOT, ce qui viole les contraintes de plus proches voisins car chaque CNOT nécessite lui-même un couplage capacitif direct entre qubits, créant un problème de bootstrap où l'implémentation de l'opération de routage elle-même nécessite du routage.",
    "B": "Chaque porte SWAP augmente substantiellement la profondeur du circuit et contribue à de multiples opérations à deux qubits qui ont des taux d'erreur significativement plus élevés que les portes à un qubit, ce qui signifie que des SWAPs excessifs accumulent des erreurs qui dégradent la fidélité des états quantiques acheminés à travers la topologie de plus proches voisins, rendant la minimisation essentielle pour maintenir la précision computationnelle dans les contraintes de temps de cohérence du matériel quantique actuel où les erreurs de portes dépassent généralement 0,1 % par opération à deux qubits.",
    "C": "Les opérations SWAP interfèrent avec les protocoles de distillation d'états magiques nécessaires au calcul universel tolérant aux fautes car elles ne peuvent pas être implémentées de manière transversale dans les codes de surface ou d'autres codes correcteurs d'erreurs topologiques, nécessitant des opérations logiques qui consomment des états ancillaires coûteux préparés à travers plusieurs tours de distillation.",
    "D": "Elles effacent l'intrication en permutant l'étiquetage des qubits d'une manière qui brise les motifs de corrélation soigneusement construits établis par les couches antérieures du circuit, randomisant effectivement quels qubits sont intriqués avec lesquels et détruisant les corrélations quantiques à longue portée nécessaires pour l'avantage quantique.",
    "solution": "B"
  },
  {
    "id": 175,
    "question": "Quelle est la signification de l'information de Fisher quantique dans les algorithmes quantiques variationnels ?",
    "A": "Caractérise la géométrie de l'espace des paramètres en encodant le tenseur métrique riemannien qui définit les géodésiques à travers la variété variationnelle, permettant aux optimiseurs de suivre des trajectoires de descente de gradient naturel qui tiennent compte de la courbure induite par le recouvrement d'états quantiques.",
    "B": "Elle fournit un cadre géométrique et statistique complet qui quantifie simultanément la sensibilité de la mesure aux variations de paramètres, révèle la structure riemannienne locale de la variété d'états quantiques à des fins d'optimisation, et sert de diagnostic pour la production d'intrication tout au long du circuit variationnel, en faisant une métrique unifiée qui capture à la fois les aspects théoriques de l'information et géométriques essentiels pour comprendre l'entraînabilité et l'expressivité dans les algorithmes variationnels.",
    "C": "Suit l'intrication générée par le circuit en calculant l'information mutuelle entre sous-systèmes en fonction des paramètres variationnels, fournissant ainsi une mesure directe de la quantité d'intrication bipartite ou multipartite qui émerge pendant l'évolution de l'ansatz, des valeurs d'information de Fisher quantique plus élevées indiquant que le circuit produit des états plus fortement corrélés à travers le registre de qubits.",
    "D": "Quantifie la sensibilité avec laquelle les résultats de mesure répondent aux ajustements infinitésimaux de paramètres, calculant essentiellement la variance dans les valeurs d'espérance observables sous de petits décalages dans les paramètres du circuit — les directions avec une information de Fisher quantique élevée sont celles où de minuscules changements produisent des résultats statistiquement distinguables, guidant l'allocation des ressources pendant l'entraînement variationnel.",
    "solution": "B"
  },
  {
    "id": 176,
    "question": "Pourquoi la connectivité limitée des qubits présente-t-elle un défi ?",
    "A": "Les portes à deux qubits ne peuvent être exécutées directement que sur des qubits physiquement adjacents qui partagent un élément de couplage dans le graphe de connectivité du matériel. Lorsqu'un algorithme nécessite une opération à deux qubits entre des qubits non adjacents, le compilateur doit insérer des séquences de portes SWAP pour déplacer physiquement l'information quantique le long de chemins à travers la topologie de connectivité jusqu'à ce que les qubits soient voisins, puis effectuer la porte désirée, et potentiellement les échanger à nouveau. Ce surcoût de SWAP augmente substantiellement la profondeur du circuit — parfois de facteurs de 10× ou plus — introduisant des opportunités de décohérence supplémentaires et prolongeant le temps d'exécution, ce qui dégrade directement la fidélité de l'état quantique final et limite la complexité des algorithmes qui peuvent s'exécuter avec succès avant que la cohérence ne soit perdue.",
    "B": "Les portes à deux qubits nécessitent un couplage direct entre qubits via des hamiltoniens d'interaction physique qui n'existent que pour les paires adjacentes dans la topologie de connectivité du matériel. Lorsque les algorithmes spécifient des opérations entre qubits distants, les compilateurs doivent décomposer ces portes en séquences d'interactions entre plus proches voisins en utilisant des approximations de Trotter-Suzuki, où les unitaires non locaux à deux qubits U = exp(-iH₁₂t) sont synthétisés à travers plusieurs couches de portes adjacentes. Ce surcoût de trotterisation augmente la profondeur du circuit par des facteurs proportionnels à la distance de séparation des qubits, introduisant des erreurs systématiques de l'approximation qui s'accumulent comme ε ∝ (Δt)²||[H₁,H₂]|| par étape de Trotter. Ces erreurs d'approximation se composent avec la décohérence, dégradant la fidélité de l'état final et limitant la complexité algorithmique réalisable avant que les taux d'erreur ne dépassent les seuils de tolérance aux fautes.",
    "C": "Les opérations d'intrication à deux qubits ne peuvent être effectuées qu'entre des qubits physiquement voisins partageant des canaux de couplage directs dans l'architecture de connectivité du dispositif. Lorsque les spécifications du circuit nécessitent des portes entre qubits non adjacents, les algorithmes de routage doivent insérer des séquences de portes BRIDGE qui créent des chaînes d'intrication temporaires à travers des qubits intermédiaires, téléportant effectivement l'information quantique à travers le graphe de connectivité jusqu'à ce que les qubits cibles deviennent logiquement adjacents. Ce protocole de pontage augmente significativement la profondeur du circuit — parfois de 5-8× selon le diamètre du graphe — mais de manière critique, chaque opération de pont consomme un ebit de la capacité d'intrication du qubit intermédiaire, créant une contention de ressources qui limite l'exécution parallèle des portes et prolonge le temps d'exécution total, permettant à la décohérence de dégrader la fidélité computationnelle au-delà des seuils récupérables pour les circuits profonds.",
    "D": "Les portes à deux qubits dépendent du couplage physique direct entre qubits via des résonateurs partagés ou des liens capacitifs présents uniquement pour les paires adjacentes dans le graphe de connectivité. Lorsque les circuits compilés nécessitent des opérations entre qubits séparés, l'étape de mappage doit insérer des opérations MOVE qui transportent physiquement les excitations de qubits à travers le réseau de couplage en échangeant séquentiellement les états quantiques le long des chemins les plus courts. Cela introduit un surcoût évoluant linéairement avec le diamètre du réseau — typiquement 4-12 sauts pour les architectures planaires — où chaque MOVE ajoute une couche de porte. Cependant, la limitation principale émerge de la diaphonie : déplacer physiquement des excitations devant des qubits intermédiaires induit des erreurs de couplage ZZ non désirées proportionnelles à ξ·t_move qui s'accumulent de manière cohérente, créant des erreurs de phase systématiques que la correction d'erreurs ne peut pas traiter puisqu'elles commutent avec les mesures de stabilisateurs, limitant fondamentalement la fidélité du circuit.",
    "solution": "A"
  },
  {
    "id": 177,
    "question": "Quelles deux métriques sont utilisées pour évaluer la qualité d'un circuit quantique synthétisé ?",
    "A": "La qualité du circuit est principalement évaluée en comptant les portes d'intrication à deux qubits, qui dominent les taux d'erreur en raison de leurs fidélités significativement plus faibles comparées aux opérations à un qubit, associée à la métrique de fidélité mathématique qui quantifie à quel point la transformation unitaire implémentée correspond à l'opération cible à travers des mesures comme la distance de trace ou la fidélité moyenne de porte",
    "B": "La qualité de la synthèse est évaluée en comptant la profondeur des couches CNOT (profondeur des portes à deux qubits) dans le circuit compilé, qui détermine l'accumulation temporelle des erreurs de décohérence qui dominent sur les rotations à un qubit dans les implémentations tolérantes aux fautes, combinée avec la métrique de fidélité de processus qui quantifie avec quelle précision le canal quantique préserve la structure de l'état d'entrée à travers des mesures comme la distance de norme diamant ou la dégradation de la capacité du canal",
    "C": "Les métriques principales se concentrent sur le nombre de portes non-Clifford (typiquement les portes T) requises dans la compilation tolérante aux fautes, qui détermine le surcoût en ressources via les protocoles de distillation d'états magiques qui dominent les coûts d'exécution, associé à une mesure quantitative de la fidélité d'implémentation du circuit évaluée via randomized benchmarking ou tomographie d'ensemble de portes pour capturer les erreurs de contrôle systématiques à travers la séquence de portes décomposée",
    "D": "L'évaluation de la qualité du circuit repose sur la mesure de la profondeur totale du circuit — calculée comme le nombre maximum de couches de portes séquentielles lorsque la parallélisation est optimalement exploitée à travers les sous-systèmes de qubits indépendants — qui détermine l'accumulation de décohérence pendant l'exécution, aux côtés de la métrique de fidélité moyenne de porte qui quantifie le taux d'erreur par opération via reconstruction unitaire directe ou benchmarking d'entropie croisée contre la transformation cible idéale",
    "solution": "A"
  },
  {
    "id": 178,
    "question": "Quel défi se pose dans l'évolution quantique en temps imaginaire (QITE) lorsque le domaine des mesures locales s'étend ?",
    "A": "Lorsque les opérateurs de mesure s'étendent au-delà des régions locales, le générateur anti-hermitien de l'évolution en temps imaginaire acquiert des termes non locaux qui violent l'hypothèse de décomposition sous-jacente à l'implémentation efficace de QITE. Alors que les mesures locales suffisent pour les hamiltoniens entre plus proches voisins, les domaines de mesure étendus créent des éléments de matrice hors diagonale dans le propagateur en temps imaginaire qui couplent des qubits distants, nécessitant des séquences de portes dont la profondeur évolue exponentiellement avec la portée de mesure pour approximer fidèlement l'évolution.",
    "B": "L'expansion du domaine de mesure introduit des corrélations entre sous-systèmes distants qui rendent le principe variationnel de McLachlan mal conditionné, car la matrice de recouvrement entre états de base variationnels développe des valeurs propres quasi-nulles qui croissent exponentiellement avec l'étendue de la mesure. Cette instabilité numérique rend impossible l'inversion fiable du système linéaire qui détermine le pas de temps imaginaire optimal, même si l'évolution physique reste bien définie et que les opérateurs unitaires requis ont des représentations matricielles de taille polynomiale.",
    "C": "Générer les opérateurs unitaires nécessaires devient exponentiellement complexe à mesure que le domaine de mesure augmente, car les portes quantiques requises doivent implémenter des opérations non locales qui ne peuvent pas être efficacement décomposées en séquences de portes à deux qubits entre plus proches voisins. Cette évolution exponentielle de la profondeur du circuit provient de la nécessité de propager l'information quantique à travers des qubits de plus en plus distants tout en maintenant les relations de phase précises nécessaires pour une propagation précise en temps imaginaire.",
    "D": "Lorsque les domaines de mesure s'étendent au-delà des plus proches voisins, le nombre de mesures de chaînes de Pauli requises croît exponentiellement car chaque opérateur de mesure à n qubits peut être décomposé en 4^n mesures de Pauli à un qubit dans le pire des cas. Bien que les techniques de regroupement de Pauli réduisent ce surcoût pour les observables commutantes, les mesures non locales produisent généralement des termes non commutants dont les valeurs d'espérance doivent être estimées indépendamment, créant une complexité de mesure qui évolue exponentiellement avec la taille du support de l'opérateur de mesure.",
    "solution": "C"
  },
  {
    "id": 179,
    "question": "Quel type de portes est d'abord considéré pour la fusion dans la stratégie proposée ?",
    "A": "Les portes SWAP opérant sur des qubits adjacents dans le graphe de connectivité, qui sont priorisées pour la fusion car les opérations SWAP consécutives proviennent souvent d'algorithmes de routage et peuvent être simplifiées par annulation algébrique — spécifiquement, SWAP(i,j) suivi de SWAP(i,j) est égal à l'identité, et certaines séquences SWAP peuvent être réécrites comme des chemins plus courts à travers la carte de couplage.",
    "B": "Les portes de mesure qui projettent les qubits sur la base computationnelle, qui sont examinées en premier pour les opportunités de fusion car les mesures consécutives sur le même qubit sont redondantes — la première mesure effondre l'état, rendant les mesures suivantes déterministes. De plus, certains motifs de mesure peuvent être consolidés lorsqu'ils se produisent en parallèle sur plusieurs qubits ou lorsque les opérations intermédiaires commutent avec la base de mesure, réduisant à la fois la profondeur du circuit et le nombre d'opérations de lecture classiques requises, ce qui est critique pour minimiser le temps d'exécution total sur du matériel avec des cycles de mesure et de réinitialisation lents.",
    "C": "Les portes à 1 qubit, incluant les rotations et opérations de Pauli, qui sont examinées en premier pour la fusion car elles présentent les taux d'erreur les plus faibles et les temps d'exécution les plus rapides, ce qui en fait des candidates idéales pour une optimisation agressive. Les portes séquentielles à un qubit sur le même qubit peuvent souvent être composées en une seule rotation équivalente utilisant des représentations axe-angle, réduisant la profondeur du circuit tout en maintenant une équivalence fonctionnelle parfaite.",
    "D": "Les portes à 2 qubits telles que CNOT ou CZ, qui sont ciblées en premier car elles dominent à la fois les taux d'erreur et le temps d'exécution dans les dispositifs NISQ — présentant typiquement des taux d'erreur 10-100× supérieurs aux portes à un qubit. La stratégie de fusion recherche des portes à 2 qubits adjacentes agissant sur des paires de qubits chevauchantes qui peuvent être consolidées via des identités de portes (par ex., CNOT(a,b) suivi de CNOT(b,a) suivi de CNOT(a,b) est égal à SWAP(a,b)), ou fusionnées en opérations natives à deux qubits plus efficaces supportées par le matériel, réduisant ainsi le goulot d'étranglement principal pour la fidélité du circuit.",
    "solution": "C"
  },
  {
    "id": 180,
    "question": "L'absence d'interférence entre bosons distinguables dans un interféromètre linéaire simplifie la simulation car leurs probabilités de sortie se factorisent en :",
    "A": "Probabilités de transition à une particule indépendantes, puisque la distinguabilité élimine les effets d'interférence quantique et permet de calculer séparément la trajectoire de chaque particule à travers l'interféromètre.",
    "B": "Produits de permanents de colonnes calculés sur des sous-matrices disjointes indexées par les étiquettes de particules distinguables, où chaque permanent capture la symétrie bosonique au sein d'une seule espèce de particule mais l'interférence entre espèces est supprimée. La probabilité globale reste un produit d'évaluations de permanents #P-difficiles, préservant l'intractabilité computationnelle malgré la distinguabilité.",
    "C": "Produits de déterminants |det(U₁)|²|det(U₂)|²... où chaque U_k correspond à la sous-matrice de l'interféromètre connectant les modes d'entrée occupés par la particule k aux modes de détection de sortie, et la distinguabilité empêche le regroupement bosonique qui nécessiterait autrement l'évaluation de permanents. Bien que les déterminants soient calculables en temps polynomial, la nécessité de suivre quelle particule occupe quel mode réintroduit une complexité d'échantillonnage de l'espace de configuration exponentielle.",
    "D": "Convolutions de distributions de probabilité à une particule pondérées par les coefficients multinomiaux qui comptent le nombre de permutations distinguables mappant les particules d'entrée aux modes de sortie, et la structure multinomiale élimine le besoin de calcul de permanent en remplaçant les probabilités de regroupement bosonique par une combinatoire classique. Cependant, l'intégrale de convolution sur les étiquettes de degré de liberté continues nécessite encore un temps d'intégration exponentiel pour une évaluation exacte.",
    "solution": "A"
  },
  {
    "id": 181,
    "question": "Comment les méthodes de décomposition en valeurs singulières quantiques (QSVD) se comparent-elles à la SVD classique ?",
    "A": "L'estimation de phase quantique sur les matrices de Gram produit une accélération exponentielle pour toute matrice indépendamment du nombre de conditionnement ou de la structure, extrayant toutes les valeurs singulières en temps logarithmique sans hypothèses sur l'oracle.",
    "B": "L'encodage en amplitude révèle automatiquement le spectre des valeurs singulières par les statistiques de mesure, contournant le calcul des valeurs propres via des motifs d'interférence quantique qui projettent sur la base des vecteurs singuliers avec des mesures en un seul tour.",
    "C": "Les méthodes de décomposition en valeurs singulières quantiques offrent des accélérations exponentielles potentielles par rapport aux algorithmes SVD classiques pour certaines matrices bien conditionnées et de faible rang lorsque l'accès quantique à l'entrée est disponible, mais les implémentations pratiques font face à des contraintes significatives qui limitent leur applicabilité immédiate. La profondeur de circuit requise évolue avec la précision désirée et le nombre de conditionnement, nécessitant une correction d'erreurs pour maintenir l'exactitude tout au long du calcul. De plus, la préparation de l'état quantique initial encodant la matrice et l'extraction des résultats finaux impliquent toutes deux un surcoût de calcul qui peut diminuer les avantages théoriques. Ces facteurs — les exigences de profondeur de circuit, les défis de maintien de la précision et le surcoût de correction d'erreurs — contraignent collectivement l'utilité pratique des méthodes QSVD sur le matériel quantique à court terme malgré leur promesse asymptotique.",
    "D": "L'évolution adiabatique avec des hamiltoniens encodant des matrices se stabilise dans des états fondamentaux dont le spectre d'énergie correspond directement aux valeurs singulières ordonnées, les mesures de base donnant immédiatement les vecteurs singuliers.",
    "solution": "C"
  },
  {
    "id": 182,
    "question": "Qu'est-ce que la transformation de valeurs singulières quantique (QSVT) et pourquoi est-elle importante ?",
    "A": "QSVT permet des transformations polynomiales de valeurs singulières par une séquence d'opérateurs de traitement du signal et de réflexion, mais son utilité principale réside dans la décomposition d'unitaires arbitraires en leur forme SVD canonique plutôt que dans l'implémentation de primitives algorithmiques. En appliquant systématiquement des rotations contrôlées conditionnées par des seuils de valeurs singulières, elle reconstruit explicitement la décomposition spectrale, la rendant particulièrement précieuse pour la tomographie d'états quantiques et la reconstruction de matrices de densité où la connaissance complète du spectre des valeurs singulières est essentielle pour caractériser la pureté des états mixtes et quantifier les mesures d'entropie d'intrication.",
    "B": "QSVT est une primitive universelle pour implémenter essentiellement tout algorithme quantique pouvant être exprimé comme une transformation polynomiale des valeurs singulières d'une matrice. En entrelaçant des opérateurs de traitement du signal avec des opérateurs de réflexion dans une séquence soigneusement conçue, QSVT fournit un cadre systématique qui unifie et généralise de nombreuses techniques algorithmiques quantiques fondamentales incluant l'amplification d'amplitude, les méthodes de marche quantique et les protocoles de simulation hamiltonienne sous une seule structure mathématique cohérente.",
    "C": "QSVT fournit un cadre systématique pour implémenter des fonctions polynomiales de valeurs singulières par traitement du signal quantique entrelacé, mais y parvient en exploitant la structure des valeurs propres généralisées d'opérateurs encodés en blocs plutôt que la décomposition en valeurs singulières elle-même. La technique fonctionne en convertissant le polynôme cible en une séquence de rotations de phase contrôlées qui agissent sur les espaces propres d'un encodage en bloc non hermitien, où la transformation effective apparaît comme une manipulation de valeurs singulières seulement dans le sous-espace projeté, en faisant fondamentalement une méthode spectrale plutôt qu'une véritable transformation de valeurs singulières malgré la convention de dénomination.",
    "D": "QSVT implémente des transformations polynomiales des valeurs singulières de matrices par des séquences soigneusement conçues d'opérateurs de signal quantique et de réflexions, fournissant un cadre puissant qui subsume l'amplification d'amplitude et les marches quantiques. Cependant, son avantage computationnel repose de manière critique sur l'hypothèse que la matrice d'entrée est déjà encodée en blocs avec des bornes de normalisation connues, ce qui limite les applications pratiques car la construction de cet encodage en blocs pour des matrices générales nécessite typiquement une complexité de préparation d'état quantique qui évolue polynomialement avec le nombre de conditionnement, annulant ainsi l'accélération pour les systèmes mal conditionnés où QSVT fournirait autrement les améliorations les plus spectaculaires par rapport aux méthodes classiques.",
    "solution": "B"
  },
  {
    "id": 183,
    "question": "Pourquoi des espaces réservés comme les portes identité sont-ils souvent ajoutés aux circuits quantiques de longueur fixe ?",
    "A": "Les espaces réservés d'identité servent à synchroniser les temps d'inactivité des qubits à travers les branches d'exécution parallèles dans les circuits avec opérations conditionnelles, garantissant que tous les chemins computationnels à travers le DAG du circuit consomment un temps d'horloge égal indépendamment des résultats de mesure qui déclenchent quels sous-circuits. Sans rembourrage des chemins plus courts avec des identités, les qubits terminant leurs portes assignées tôt resteraient inactifs pendant que d'autres complètent des branches plus profondes, accumulant des quantités inégales de déphasage en mode inactif et provoquant une fidélité globale du circuit dépendant de l'historique spécifique des mesures. En insérant des identités pour égaliser les longueurs de chemin, le compilateur garantit une décohérence uniforme sur tous les qubits, rendant les taux d'erreur prévisibles et permettant une modélisation précise du bruit.",
    "B": "Maintenir des structures de couches cohérentes à travers les circuits de profondeurs logiques différentes afin que les passes de compilation et d'optimisation puissent opérer uniformément, assurant un alignement approprié de l'ordonnancement et de l'allocation des ressources indépendamment du nombre de portes algorithmiques.",
    "C": "Les portes identité imposent la séparation temporelle requise pour que les séquences de découplage dynamique suppriment le bruit basse fréquence, car les impulsions DD doivent être insérées à intervalles réguliers déterminés par la densité spectrale du bruit, et l'insertion d'identités fournit l'espacement nécessaire entre les portes algorithmiques pour accommoder ces impulsions de suppression d'erreurs. Le compilateur calcule le délai inter-portes minimum nécessaire pour ajuster une séquence XY-4 ou CPMG complète basée sur la fréquence de coin du bruit 1/f mesuré, puis remplit le circuit avec des identités qui élargissent le calendrier pour correspondre à la période DD, convertissant effectivement le temps d'inactivité en correction d'erreurs active sans changer la séquence de portes logiques ou nécessiter l'insertion explicite d'impulsions DD dans la représentation de circuit de haut niveau.",
    "D": "Les identités d'espace réservé permettent une empreinte digitale efficace des circuits pour la mise en cache des résultats de compilation, car les circuits de longueur fixe avec des identités dans des positions prévisibles produisent des représentations canoniques qui peuvent être hachées et comparées à une base de données de séquences de portes précédemment optimisées. Lorsque le compilateur rencontre un nouveau circuit, il le remplit d'abord à la longueur standard avec des identités, calcule un hash sur la structure rembourrée et interroge le cache de compilation — si une correspondance existe, la décomposition pré-optimisée est récupérée sans ré-exécuter la synthèse, réduisant le temps de compilation de secondes à microsecondes. Cette stratégie de mise en cache exploite le fait que les identités n'affectent pas le taux de collision de hash puisqu'elles commutent avec toutes les portes, rendant le hash du circuit rembourré une empreinte digitale fiable pour l'équivalence structurelle.",
    "solution": "B"
  },
  {
    "id": 184,
    "question": "Dans le contexte d'un ordinateur quantique à ions piégés implémentant l'algorithme de Shor pour factoriser un module RSA de 2048 bits, vous êtes chargé de caractériser le budget d'erreurs complet incluant les fidélités de porte, les canaux de décohérence et les erreurs de mesure. L'architecture utilise un piège de Paul linéaire avec des ions 171Yb+, où les portes à deux qubits sont implémentées via des interactions de Mølmer-Sørensen à travers des modes de mouvement partagés. Votre benchmarking préliminaire montre des fidélités de portes à un qubit de 99,97%, mais les fidélités de portes à deux qubits oscillent autour de 99,3% avec des erreurs dominantes provenant du chauffage du mouvement à un taux de 10 quanta/s. Étant donné que l'algorithme de Shor pour cette taille de problème nécessite approximativement 10^10 portes à deux qubits avant correction d'erreurs, et que vous utilisez un code de Steane [[7,1,3]] pour la tolérance aux fautes, quel est le goulot d'étranglement le plus critique empêchant l'exécution réussie ?",
    "A": "Les implémentations tolérantes aux fautes nécessitent l'extraction de syndromes après chaque quelques portes logiques pour détecter et corriger les erreurs avant propagation, et avec le code de Steane [[7,1,3]], chaque cycle de mesure de syndrome implique 6 mesures de qubits ancillaires. Pour les 10^10 portes à deux qubits requises, cela se traduit par approximativement 10^8 tours d'extraction de syndrome en supposant que les syndromes sont mesurés tous les 100 portes logiques. Les systèmes à ions piégés présentent typiquement des erreurs de mesure autour de 0,3% dues à une discrimination d'état imparfaite et à l'émission spontanée pendant la détection de fluorescence. Sur 10^8 cycles de mesure, ces erreurs de 0,3% s'accumulent à un taux d'échec de mesure effectif de 1 - (0,997)^(10^8) ≈ 1, garantissant l'échec de l'algorithme même si toutes les opérations de porte étaient parfaitement sans erreur.",
    "B": "Bien que la fidélité de porte à un qubit rapportée de 99,97% semble acceptable, la compilation détaillée du circuit révèle que l'exponentiation modulaire de Shor nécessite approximativement 3×10^10 rotations à un qubit — environ trois fois le nombre de portes à deux qubits — en raison des décompositions de portes de Toffoli et des corrections de phase dans les sous-routines de transformée de Fourier quantique. Avec ce rapport 3:1, les erreurs à un qubit contribuent une probabilité d'échec cumulée de 1 - (0,9997)^(3×10^10) ≈ 0,9999, signifiant un échec de l'algorithme virtuellement certain même avant de considérer les erreurs à deux qubits ou de mesure. Pour la factorisation RSA-2048 spécifiquement, le volume pur d'opérations à un qubit inverse la sagesse conventionnelle, faisant de la fidélité à un qubit la contrainte primaire malgré son taux de succès de 99,97% superficiellement impressionnant.",
    "C": "Le mécanisme de porte de Mølmer-Sørensen repose sur tous les ions couplant à un mode de mouvement partagé du centre de masse, créant une contrainte fondamentale : une seule porte à deux qubits peut s'exécuter à tout moment donné à travers toute la chaîne, car des portes simultanées interféreraient destructivement par des modulations concurrentes du mouvement collectif. Ce goulot de sérialisation signifie que même avec une correction d'erreurs parfaite, les 10^10 portes à deux qubits de l'algorithme doivent s'exécuter séquentiellement plutôt qu'en parallèle à travers plusieurs blocs de qubits logiques. Ce manque de parallélisation étend le temps d'exécution total à approximativement 10^6 secondes (≈11 jours), pendant lesquels les ions piégés subiraient une décohérence catastrophique due aux perturbations environnementales.",
    "D": "Le taux de chauffage du mouvement introduit des erreurs corrélées à travers la chaîne d'ions qui ne sont pas adéquatement traitées par la capacité de correction d'erreurs de distance 3 du code [[7,1,3]], car le bruit spatialement corrélé nécessite des codes avec des propriétés géométriques spécifiquement conçues ou une distance substantiellement plus élevée pour maintenir le taux d'erreur seuil en dessous de 10^-4 par porte nécessaire pour la profondeur de cet algorithme.",
    "solution": "D"
  },
  {
    "id": 185,
    "question": "Comment l'intrication quantique aide-t-elle à relever les défis de la communication quantique dans l'Internet quantique ?",
    "A": "L'intrication permet la téléportation, permettant aux qubits d'être transmis sans mouvement physique, évitant ainsi la perte et la décohérence pendant le transit. En consommant une paire intriquée pré-partagée et en envoyant seulement des bits classiques pour communiquer le résultat de mesure de téléportation, l'information quantique est effectivement transportée sur des distances arbitraires sans que l'état quantique lui-même ne traverse le canal bruyant, contournant l'atténuation exponentielle dans la fibre optique.",
    "B": "L'intrication permet le codage superdense pour les états quantiques, doublant la capacité du canal en encodant deux qubits d'information dans chaque photon intriqué transmis. En pré-partageant des paires maximalement intriquées entre émetteur et récepteur, les canaux quantiques peuvent transmettre l'information quantique au double du taux des protocoles non intriqués. Cela compense effectivement la perte de photons dans la fibre en permettant à chaque photon détecté avec succès de porter le double de la charge quantique, divisant par deux le taux de transmission requis pour une bande passante de communication donnée.",
    "C": "L'intrication permet des protocoles de correction d'erreurs quantiques qui purifient activement les états quantiques dégradés pendant la transmission en exploitant les corrélations non locales. Lorsque des paires intriquées traversent des canaux bruyants, les récepteurs peuvent effectuer des mesures de Bell sur plusieurs paires dégradées pour distiller une intrication de plus haute fidélité par concentration d'intrication. Ce processus supprime exponentiellement les effets de décohérence avec chaque tour de purification, permettant à l'information quantique de se propager arbitrairement loin en distillant répétitivement le bruit du canal dans des paires ancillaires jetées tout en préservant la cohérence quantique dans les paires conservées.",
    "D": "L'intrication permet des protocoles de répéteurs quantiques qui étendent la portée de communication en divisant les canaux en segments plus courts avec des taux d'erreur indépendants. En générant de l'intrication sur des liens élémentaires et en effectuant un échange d'intrication aux nœuds intermédiaires, les réseaux quantiques atteignent une évolution polynomiale de la fidélité avec la distance plutôt qu'une décroissance exponentielle. Chaque segment de répéteur opère en dessous de la longueur de perte de la fibre optique, avec purification d'intrication aux nœuds restaurant la fidélité avant l'échange, permettant la communication quantique sur des distances continentales malgré l'absorption de photons.",
    "solution": "A"
  },
  {
    "id": 186,
    "question": "L'extraction simultanée d'observables commutantes est bénéfique pour l'inférence en QML car elle :",
    "A": "Permet de réutiliser efficacement les mesures pour plusieurs termes de coût",
    "B": "Permet la tomographie conjointe des valeurs d'espérance dans une seule base de mesure",
    "C": "Réduit le coût d'échantillonnage en mesurant tous les termes dans une seule base propre diagonalisée",
    "D": "Permet la lecture parallèle de plusieurs opérateurs partageant des états propres simultanés",
    "solution": "A"
  },
  {
    "id": 187,
    "question": "Pourquoi le calcul ZX ne parvient-il pas à fournir un vérificateur d'équivalence complet pour les circuits universels ?",
    "A": "Parce que les angles de phase des portes de rotation créent des classes d'équivalence infinies",
    "B": "Il manque de règles de réécriture nécessaires pour certaines transformations non-Clifford",
    "C": "Parce qu'il ne peut pas représenter des rotations arbitraires à un qubit avec des angles irrationnels",
    "D": "Parce que la porte Hadamard crée des structures de graphe non-bipartites lors de la réduction",
    "solution": "B"
  },
  {
    "id": 188,
    "question": "Considérez un scénario où vous implémentez le code de Steane [[7,1,3]] sur un processeur quantique bruité et comparez les opérations logiques réelles aux opérations logiques idéales attendues d'une implémentation parfaite du code. Vous souhaitez quantifier la performance de vos portes corrigées d'erreurs sur tous les états d'entrée possibles et toutes les mesures possibles sur la sortie. Dans le contexte de la norme diamant en correction d'erreurs quantiques, qu'indique une distance plus petite entre les canaux quantiques idéal et réel ?",
    "A": "Une distance de norme diamant plus petite indique fondamentalement que l'implémentation bruitée réelle de votre canal quantique fournit une meilleure approximation de l'opération idéale sans erreur. Cette métrique est particulièrement précieuse car elle borne le scénario le pire : elle vous indique la distinguabilité maximale entre les deux canaux sur tous les états d'entrée possibles, y compris les états intriqués avec un système ancillaire. Lorsque cette distance est petite, vous pouvez être confiant que pour tout algorithme ou protocole quantique utilisant ce canal, la déviation par rapport au comportement idéal sera bornée par cette quantité, ce qui en fait une mesure opérationnelle robuste de la fidélité des portes dans les systèmes corrigés d'erreurs.",
    "B": "Une distance de norme diamant plus petite indique que vos opérations logiques du code de Steane implémentées approximent plus étroitement les projections idéales de l'espace de code, ce qui signifie spécifiquement que l'extraction de syndrome identifie et localise avec succès les erreurs avant qu'elles ne se propagent au-delà de la capacité de correction de distance-3. La norme diamant capture cela de manière unique en mesurant la distance de trace maximale entre les canaux réel et idéal lorsque les deux sont étendus pour agir sur un système de référence intriqué, ce qui dans le contexte QEC correspond à garantir que les paires de Bell encodées restent maximalement intriquées après les opérations logiques. Puisque le code [[7,1,3]] peut corriger toute erreur à un qubit, atteindre des distances de norme diamant inférieures à 1/7 garantit que les erreurs résiduelles restent dans l'ensemble corrigeable avec haute probabilité dans toute base de calcul.",
    "C": "La distance de norme diamant quantifie à quel point votre implémentation physique préserve la structure de l'espace de code pendant les opérations logiques, avec des valeurs plus petites indiquant que les portes transversales sont exécutées avec une fidélité plus élevée par rapport aux générateurs du groupe stabilisateur. Pour le code de Steane [[7,1,3]] spécifiquement, la norme diamant mesure la déviation maximale dans la matrice de transfert de Pauli logique : lorsque cette distance est petite, cela signifie que les opérateurs X̄ et Z̄ logiques sont implémentés avec une fuite minimale en dehors du sous-espace de code défini par les six générateurs stabilisateurs S₁ à S₆. Ceci est particulièrement important car le CNOT transversal du code de Steane nécessite de maintenir les relations de phase entre les sept qubits physiques simultanément, et même de petites violations de la norme diamant peuvent causer des erreurs cohérentes subtiles qui s'accumulent à travers plusieurs couches de portes.",
    "D": "Une distance de norme diamant plus petite dans votre implémentation du code de Steane indique que le modèle de bruit effectif de votre canal logique converge vers un canal de Pauli, ce qui est le scénario idéal pour la correction d'erreurs concaténée car les erreurs de Pauli sont exactement les erreurs que les codes stabilisateurs sont conçus pour corriger. La norme diamant mesure spécifiquement la norme d'opérateur de la différence ℰ_réel - ℰ_idéal lorsque les deux canaux sont étendus via l'isomorphisme de Choi-Jamiołkowski, ce qui en termes pratiques signifie qu'elle quantifie si vos erreurs résiduelles après mesure de syndrome et correction sont principalement des erreurs de bit-flip et de phase-flip plutôt que des processus cohérents plus complexes. Cette interprétation est cruciale pour le code [[7,1,3]] car sa capacité de distance-3 suppose que les erreurs suivent une approximation de Pauli-twirl.",
    "solution": "A"
  },
  {
    "id": 189,
    "question": "Dans un algorithme quantique variationnel conçu pour résoudre un problème d'optimisation combinatoire avec 50 variables binaires, vous remarquez qu'en augmentant la profondeur de l'ansatz de p=1 à p=8 couches, le processus d'entraînement devient de plus en plus difficile et le paysage de coût s'aplatit considérablement. Pendant ce temps, une référence classique utilisant le recuit simulé continue de trouver des solutions approximatives raisonnables. Votre étudiant en thèse suggère qu'il pourrait s'agir d'un problème fondamental plutôt que d'un simple problème d'ajustement d'hyperparamètres. En quoi la transformée de Fourier quantique diffère-t-elle de la transformée de Fourier classique ?",
    "A": "La version quantique opère sur des amplitudes de probabilité en superposition et peut être implémentée avec O(n²) portes pour n qubits, tandis que la FFT classique nécessite O(n log n) opérations sur n = 2^n bits classiques de données — cette séparation exponentielle est la différence structurelle clé",
    "B": "La version quantique opère sur des états de base superposés avec O(n²) portes pour n qubits, tandis que la DFT classique nécessite O(N²) opérations sur N = 2^n points de données — bien que la QFT ne lise qu'une seule amplitude par mesure contrairement à la sortie de vecteur complet de la classique",
    "C": "La version quantique encode les coefficients comme amplitudes nécessitant une mesure pour l'extraction avec O(n²) portes pour n qubits, tandis que la FFT classique calcule explicitement tous les N = 2^n coefficients en temps O(N log N) — échangeant une accélération exponentielle contre une lecture probabiliste",
    "D": "La version quantique transforme les états de base de n qubits en utilisant O(n²) rotations contrôlées, tandis que la FFT classique traite N = 2^n échantillons en temps O(N log N) — mais extraire toutes les amplitudes QFT nécessite un nombre exponentiel de mesures annulant l'avantage des portes",
    "solution": "A"
  },
  {
    "id": 190,
    "question": "Laquelle des propositions suivantes explique le mieux pourquoi les fonctions de coût locales améliorent l'entraînabilité des réseaux de neurones quantiques ?",
    "A": "Les fonctions de coût locales évitent les plateaux arides en restreignant les contributions de gradient aux mesures de sous-systèmes, ce qui empêche la suppression exponentielle qui se produit lorsque des observables globales moyennent sur l'espace de Hilbert complet. Cependant, cela introduit un biais vers les états produits dans la trajectoire d'optimisation, puisque les observables locales ne peuvent pas distinguer les états maximalement intriqués des états séparables, causant potentiellement la convergence de l'optimiseur vers des solutions sous-optimales qui manquent des corrélations quantiques à longue portée nécessaires pour un avantage quantique dans les tâches d'apprentissage.",
    "B": "Les fonctions de coût locales décomposent le paysage variationnel en problèmes d'optimisation de sous-systèmes indépendants, permettant de calculer les gradients via des contractions classiques de réseaux de tenseurs sans surcoût exponentiel. Cela fonctionne parce que mesurer des observables k-locales sur un système de n qubits nécessite de calculer des valeurs d'espérance sur seulement des sous-espaces de dimension 2^k plutôt que l'espace complet 2^n, réduisant le coût d'estimation de gradient d'exponentiel à polynomial. Le signal de gradient persiste même à de grandes profondeurs de circuit puisque les mesures de sous-systèmes ne se couplent qu'aux paramètres de portes voisins via la borne de Lieb-Robinson sur la propagation des opérateurs.",
    "C": "Les observables locales concentrent l'information de gradient dans les composantes de Fourier basse fréquence du paysage de coût, empêchant la dilution exponentielle de la variance qui affecte les mesures globales. Puisque les opérateurs de Pauli k-locaux ont une norme d'opérateur bornée indépendamment de la taille du système, leurs valeurs d'espérance s'échellent indépendamment du nombre total de qubits, préservant la magnitude du gradient. Cependant, cela restreint également la classe de fonctions accessibles à celles apprises par des circuits de profondeur constante avec intrication limitée, car les fonctions de coût locales ne peuvent pas récompenser des corrélations à longue portée exponentiellement longues qui nécessitent des unitaires paramétrés profonds pour être établies.",
    "D": "Les fonctions de coût locales évitent la moyenne exponentielle sur l'état quantique entier, ce qui préserve le rapport signal-bruit du gradient en mesurant uniquement des observables de sous-systèmes. Cela empêche le phénomène de plateau aride où les observables globales diluent les gradients exponentiellement avec la taille du système.",
    "solution": "D"
  },
  {
    "id": 191,
    "question": "Comment la latence d'exécution des portes conditionnelles affecte-t-elle les protocoles de rétroaction en cours de circuit comme la réinitialisation active ?",
    "A": "Les portes conditionnelles exploitent l'effondrement instantané de l'état quantique lors de la mesure, permettant au contrôleur classique d'appliquer des corrections de Pauli basées sur les résultats de mesure sans attendre les délais de propagation du signal, car l'état actualisé est déjà déterminé par la règle de Born à l'instant de la mesure. Puisque la mise à jour de l'état quantique est physiquement immédiate une fois que l'enregistrement de mesure est finalisé, la seule latence restante provient de l'étape de discrimination de la mesure elle-même, et non d'une logique conditionnelle ultérieure, rendant possible une rétroaction sub-microseconde sur toutes les plateformes contemporaines.",
    "B": "Si l'étape de traitement classique entre la lecture de la mesure et la rotation conditionnelle subséquente consomme trop de microsecondes par rapport au temps de décohérence du qubit, l'état quantique perd le bénéfice de cohérence que le protocole de réinitialisation était conçu pour préserver. Un matériel de propagation rapide avec une latence de décision sub-microseconde est donc essentiel pour maintenir la fidélité du protocole.",
    "C": "La latence des portes conditionnelles affecte principalement les protocoles qui nécessitent des opérations parallèles synchrones sur plusieurs qubits, car le contrôleur classique doit sérialiser les décisions de rétroaction pour éviter les conditions de concurrence dans la logique de contrôle. Cependant, la réinitialisation active opère sur un seul qubit à la fois sans dépendances inter-qubits pendant la fenêtre de rétroaction, donc la latence d'exécution affecte la planification des temps morts mais ne limite pas fondamentalement la fidélité du protocole, à condition que le résultat de mesure soit enregistré avant le début de l'opération suivante.",
    "D": "La contrainte de latence s'applique principalement aux architectures supraconductrices où les signaux de contrôle doivent se propager à travers des lignes coaxiales à température ambiante jusqu'à l'étage du réfrigérateur à dilution, introduisant des délais aller-retour de plusieurs microsecondes. Les plateformes à ions piégés et atomes neutres évitent ce goulot d'étranglement en colocalisant le matériel de décision classique avec le processeur quantique dans la même chambre à vide, permettant des latences de rétroaction bien inférieures au temps de décohérence. Par conséquent, les protocoles de réinitialisation active montrent un comportement d'échelle qualitativement différent selon les modalités matérielles, les systèmes supraconducteurs nécessitant une atténuation d'erreur agressive tandis que les systèmes atomiques atteignent une fidélité de réinitialisation quasi-idéale indépendamment de la profondeur du circuit.",
    "solution": "B"
  },
  {
    "id": 192,
    "question": "Qu'est-ce que l'information mutuelle quantique et quelle est sa signification ?",
    "A": "Quantifie les corrélations totales — à la fois classiques et quantiques — entre deux systèmes A et B comme I(A:B) = S(ρ_A) + S(ρ_B) - S(ρ_AB), où S désigne l'entropie de von Neumann. Cette quantité capture toutes les dépendances statistiques : corrélations classiques (mesurables par observations locales), corrélations quantiques comme l'intrication (nécessitant des mesures conjointes), et même la discorde (corrélations inaccessibles aux mesures projectives locales). Contrairement aux mesures purement classiques, elle reste non négative même pour des états intriqués où l'entropie conditionnelle peut être négative.",
    "B": "Quantifie les corrélations totales entre les systèmes A et B comme I(A:B) = S(ρ_A) + S(ρ_B) - S(ρ_AB), capturant à la fois les dépendances classiques et quantiques, mais sert principalement de borne supérieure sur l'information accessible : elle dépasse la quantité de Holevo χ exactement par la discorde quantique, représentant l'écart entre les corrélations totales et celles extractibles par mesures locales. Bien que I(A:B) reste non négative par sous-additivité de l'entropie de von Neumann, cette borne n'est saturée que pour des états classiques-quantiques, en faisant une mesure du potentiel plutôt que de la corrélation opérationnellement accessible en présence de superposition.",
    "C": "Mesure la distinguabilité entre l'état joint ρ_AB et l'état produit ρ_A ⊗ ρ_B via l'entropie relative I(A:B) = S(ρ_AB || ρ_A ⊗ ρ_B), quantifiant à quel point le système s'écarte de l'indépendance statistique. Cette divergence de Kullback-Leibler capture toutes les corrélations—classiques et quantiques—comme le gain d'information lors de l'apprentissage des statistiques jointes par rapport à l'hypothèse d'indépendance. Elle se réduit à S(ρ_A) + S(ρ_B) - S(ρ_AB) par définition de l'entropie relative quantique, reste non négative par l'inégalité de Klein, et gouverne le taux asymptotique de test d'hypothèse entre états corrélés versus non corrélés.",
    "D": "Quantifie la fidélité d'intrication maximale atteignable lors de la transmission d'états quantiques de A vers B à travers un canal bruité, définie comme I(A:B) = max_ρ [S(ρ_A) + S(ρ_B) - S(ρ_AB)], où la maximisation porte sur tous les ensembles d'entrée possibles. Cette définition opérationnelle se connecte à la capacité du canal via l'inégalité de traitement de données quantique : l'information mutuelle borne supérieurement l'information cohérente I(A⟩B) = S(ρ_B) - S(ρ_AB), qui elle-même détermine le seuil de correction d'erreur quantique. Contrairement à l'information mutuelle classique, la version quantique peut dépasser log(d) grâce au codage superdense, capturant à la fois l'efficacité de distribution d'intrication et les effets de rétroaction de mesure.",
    "solution": "A"
  },
  {
    "id": 193,
    "question": "Considérez le déploiement pratique de l'algorithme de Grover pour inverser des fonctions de hachage cryptographiques comme SHA-256. Le circuit quantique doit implémenter à la fois la fonction de hachage et son inverse dans le cadre de l'oracle. Étant donné que les ordinateurs quantiques réels ont des temps de cohérence limités et des fidélités de porte qui se dégradent avec la profondeur du circuit, quel facteur fondamental rend cette application particulièrement difficile par rapport à la recherche dans une base de données non structurée où l'oracle est simplement un déphasage ?",
    "A": "L'implémentation réversible de la fonction de hachage nécessite un circuit extrêmement profond avec des milliers de portes par appel d'oracle, et cette profondeur se compose à travers toutes les √N itérations du processus d'amplification d'amplitude, rendant l'exigence de temps de cohérence totale astronomique même pour des espaces de pré-images modestes. La complexité du circuit domine sur toute autre considération.",
    "B": "L'implémentation réversible de SHA-256 exige une gestion extensive d'ancillas pour préserver l'unitarité pendant les opérations non linéaires comme l'addition modulaire et les rotations bit à bit, nécessitant une intrication persistante à travers des milliers de qubits physiques tout au long de chaque évaluation d'oracle. Cette surcharge d'ancillas évolue avec à la fois la taille de l'état de hachage et le nombre de tours de compression, et puisque ces ancillas doivent maintenir la cohérence à travers toutes les √N itérations de Grover tout en accumulant des erreurs provenant des échelles CNOT répétées dans les sous-circuits arithmétiques modulaires, le seuil de fidélité devient inatteignable même avec des taux d'erreur de porte optimistes.",
    "C": "Les fonctions de hachage cryptographiques emploient des couches de mélange dépendantes de l'avalanche où chaque bit de sortie dépend de manière non linéaire de la plupart des bits d'entrée à travers des arbres logiques combinatoires profonds, forçant le circuit quantique à implémenter ces dépendances en utilisant des portes Toffoli en cascade avec une distribution d'ancilla qui croît quadratiquement avec la taille d'entrée. Cela crée un goulot d'étranglement critique car les qubits ancilla requis doivent rester cohérents non seulement pendant un seul appel d'oracle mais à travers tous les √N tours d'amplification d'amplitude, et la décohérence accumulée sur ces ancillas persistantes corrompt les relations de phase nécessaires pour l'interférence constructive sur la pré-image cible.",
    "D": "Le défi fondamental provient du fait que le calcul de hachage réversible nécessite de décalculer toutes les valeurs intermédiaires pour restaurer les qubits ancilla à leurs états initiaux, mais ce décalcul doit survenir après le déphasage cible et avant la prochaine itération de Grover. La dépendance séquentielle entre l'évaluation forward, le marquage de phase et le décalcul backward crée un chemin critique à travers chaque appel d'oracle où toute erreur de porte dans l'étape de décalcul provoque une fuite d'ancilla qui se propage dans les itérations subséquentes, randomisant progressivement les amplitudes amplifiées et détruisant l'avantage quantique après seulement O(√N / fidélité²) itérations plutôt que les √N complètes requises.",
    "solution": "A"
  },
  {
    "id": 194,
    "question": "Comment la sparsité des données affecte-t-elle les modèles d'IA dans la correction d'erreur quantique ?",
    "A": "Les données d'entraînement éparses conduisent au surapprentissage et à une mauvaise généralisation car le modèle apprend à mémoriser des motifs de syndrome rares sans capturer la structure statistique sous-jacente des erreurs quantiques. Lorsque la plupart des exemples d'entraînement représentent des cas limites peu fréquents plutôt que des distributions d'erreurs typiques, le réseau neuronal développe des frontières de décision trop étroitement ajustées à l'ensemble d'entraînement, échouant à généraliser aux nouvelles séquences de syndromes rencontrées lors des opérations réelles de correction d'erreur sur le matériel quantique.",
    "B": "L'entraînement principalement sur des syndromes rares force le réseau à attribuer un poids disproportionné aux configurations de faible probabilité, ce qui paradoxalement améliore la généralisation pour ces motifs exactes mais au prix de taux de faux positifs accrus sur les syndromes communs. Le modèle apprend à reconnaître des chaînes d'erreurs peu fréquentes avec haute précision en dédiant de la capacité réseau à leurs signatures spécifiques, mais cette spécialisation déplace les frontières de décision loin des régions à haute densité de l'espace des syndromes où se produisent la plupart des erreurs opérationnelles, réduisant la précision globale de décodage pendant l'exécution malgré des gains apparents sur des événements rares retenus en validation.",
    "C": "Les ensembles de données épars concentrent la capacité du modèle sur la distinction entre véritables événements d'erreur et bruit de mesure en excluant les cas de syndrome nul écrasants qui dominent la collecte de données brutes, mais ce filtrage introduit un biais critique : le modèle n'apprend jamais la distribution de syndrome de référence sous opération normale. Par conséquent, lors du déploiement, le décodeur surestime systématiquement les taux d'erreur car il interprète toute fluctuation de syndrome comme significative, ayant été entraîné exclusivement sur des exemples où des erreurs se sont réellement produites. Cela résulte en des corrections excessives qui introduisent plus d'erreurs qu'elles n'en corrigent, dégradant les taux d'erreur logiques en dessous du seuil physique.",
    "D": "Lorsque les données de syndrome présentent une sparsité extrême, la dimensionnalité effective de la variété d'entrée s'effondre car la plupart des exemples d'entraînement se regroupent près d'un sous-espace de faible dimension défini par la structure de stabilisateur du code. Bien que cela semble simplifier l'apprentissage en réduisant la complexité des caractéristiques, cela empêche en fait le modèle d'estimer la distribution de probabilité complète sur les syndromes—le réseau n'apprend que des distributions conditionnelles P(correction|syndrome ≠ 0) tout en restant ignorant de P(syndrome), qui est essentiel pour le décodage bayésien. Cette connaissance partielle conduit à des corrections sous-optimales qui ne tiennent pas compte des probabilités a priori de différents mécanismes d'erreur.",
    "solution": "A"
  },
  {
    "id": 195,
    "question": "Quelle méthode de codage de données utilise des états quantiques analogiques pour représenter des valeurs classiques ?",
    "A": "Le codage d'amplitude mappe les données classiques dans les amplitudes de probabilité des états quantiques, stockant N valeurs classiques dans log(N) qubits à travers la structure d'amplitude de la fonction d'onde. Bien que cela réalise une compression exponentielle, c'est fondamentalement un codage discret puisque les amplitudes sont des coefficients normalisés d'états de base.",
    "B": "Le codage one-hot dédie un qubit séparé à chaque valeur classique possible, réglant exactement un qubit à |1⟩ tandis que tous les autres restent dans |0⟩, reflétant le schéma de codage catégoriel classique. Cette approche préserve la structure classique mais évolue linéairement plutôt que de tirer parti de la superposition quantique.",
    "C": "Le codage analogique emploie des systèmes quantiques à variables continues tels que les amplitudes de quadrature des modes du champ électromagnétique ou les observables de position et d'impulsion des oscillateurs harmoniques pour représenter directement des données classiques à valeurs réelles. Contrairement aux codages de qubits discrets qui mappent les données aux angles de rotation ou aux états de base computationnelle, le codage analogique utilise l'espace de Hilbert de dimension infinie des modes bosoniques où les valeurs classiques correspondent à des déplacements analogiques dans l'espace des phases. Cette approche traite naturellement les données continues sans artefacts de discrétisation et est implémentée dans les processeurs quantiques photoniques utilisant des états comprimés, des états cohérents et des mesures homodynes qui extraient des signaux de tension analogiques proportionnels aux valeurs d'entrée.",
    "D": "Le codage de phase utilise des angles de rotation continus pour encoder des données à valeurs réelles directement dans les états de qubits à travers des portes de phase paramétrées comme Rz(θ), où θ est proportionnel à la valeur d'entrée. Bien que cela semble analogique puisque θ peut prendre n'importe quelle valeur réelle, la discrétisation éventuelle se produit à l'étape de mesure.",
    "solution": "C"
  },
  {
    "id": 196,
    "question": "Dans le contexte des frameworks de calcul quantique comme Qiskit, vous avez un circuit abstrait écrit à l'aide de portes standard (H, CNOT, RZ, etc.). Votre matériel cible ne supporte qu'un ensemble de portes natif {√X, X, CZ} et possède un graphe de connectivité de qubits spécifique où seules certaines paires peuvent interagir directement. De plus, le dispositif a des taux d'erreur de portes calibrés qui varient selon les qubits, et vous souhaitez que le circuit final minimise l'erreur totale. Quel est le terme désignant le processus de compilation qui prend votre circuit abstrait et produit un circuit équivalent optimisé pour ce dispositif spécifique, et que fait réellement ce processus ?",
    "A": "La traduction de base effectue la conversion de l'ensemble de portes par des séquences de décomposition exactes dérivées de preuves d'universalité par théorie des groupes, mappant chaque porte abstraite en produits de portes natives via la décomposition de Cartan de l'unitaire cible en composantes su(2)⊗su(2), puis applique des relations de commutation pour réduire le nombre de portes, mais n'incorpore pas les contraintes de topologie matérielle ou le routage tenant compte des erreurs, maintenant l'optimalité de la profondeur du circuit uniquement pour les architectures entièrement connectées.",
    "B": "L'ordonnancement d'instructions quantiques applique une compilation tenant compte des ressources qui décompose les portes abstraites en primitives matérielles, effectue une allocation de qubits basée sur la satisfaction de contraintes respectant la topologie de connectivité, insère des chaînes SWAP pour les interactions non adjacentes, puis applique des optimisations locales utilisant les métriques de calibration du dispositif, mais optimise principalement la profondeur du circuit plutôt que l'erreur cumulée, produisant potentiellement des circuits plus courts avec une erreur plus élevée sur des profils de bruit hétérogènes.",
    "C": "La transpilation effectue une transformation complète qui décompose les portes abstraites dans l'ensemble d'instructions natif, insère des opérations SWAP pour router les interactions de qubits logiques à travers le graphe de connectivité physique, et applique des passes d'optimisation exploitant les données de calibration pour minimiser le nombre de portes et l'erreur attendue basée sur les caractéristiques de bruit spécifiques au dispositif.",
    "D": "La synthèse tenant compte de la disposition décompose les portes dans la base native en utilisant des décompositions optimales d'angles d'Euler, construit des mappages de qubits à travers des algorithmes de placement initial adaptatifs au bruit qui minimisent la fidélité de chemin attendue, insère des réseaux SWAP pour la conformité topologique, puis applique des règles de réécriture basées sur le calcul ZX pour annuler les portes de phase redondantes, mais n'itère pas les phases de mappage/routage conjointement, produisant potentiellement des solutions sous-optimales lorsque les coûts de routage dominent.",
    "solution": "C"
  },
  {
    "id": 197,
    "question": "Quel est le principal défi dans l'implémentation de versions quantiques de la régularisation par dropout ?",
    "A": "Retirer aléatoirement des opérations détruit l'unitarité, car le dropout introduit intrinsèquement des gaps non-déterministes dans le graphe computationnel—lorsque vous sautez des portes de manière probabiliste, différentes exécutions du circuit suivent différents chemins d'évolution à travers l'espace de Hilbert, empêchant la transformation globale d'être représentée par une seule matrice unitaire.",
    "B": "L'effondrement par mesure empêche la moyenne stochastique—dans le dropout classique, vous entraînez avec un retrait aléatoire de neurones mais faites la moyenne sur tous les masques de dropout possibles au moment de l'inférence, ce qui fonctionne parce que les probabilités classiques s'additionnent linéairement. Cependant, les amplitudes quantiques suivent des règles de superposition quadratique, donc vous ne pouvez pas simplement faire la moyenne des résultats de mesure de différentes configurations de dropout et vous attendre à récupérer la prédiction du réseau complet.",
    "C": "Vous ne pouvez pas désactiver sélectivement une partie d'une superposition—soit elle est entièrement présente, soit vous l'avez mesurée et l'état s'est effondré. De plus, supprimer des portes aléatoires brise la structure unitaire que les circuits quantiques requièrent, et vous perdez complètement l'avantage quantique. Le dropout classique fonctionne parce que les réseaux de neurones sont intrinsèquement redondants avec des représentations distribuées ; les circuits quantiques sont des machines d'interférence étroitement chorégraphiées où chaque porte contribue à la distribution d'amplitude finale, rendant le retrait aléatoire catastrophique plutôt que régularisant.",
    "D": "La superposition rend la désactivation sélective difficile, car le dropout nécessite de contrôler indépendamment si chaque chemin computationnel reste actif ou se fait masquer, mais la superposition quantique signifie que tous les chemins existent simultanément dans un seul vecteur d'état pondéré par amplitude.",
    "solution": "C"
  },
  {
    "id": 198,
    "question": "En pratique, qu'est-ce qui limite le nombre maximum de qubits par sous-circuit ?",
    "A": "Le nombre de qubits du dispositif et les contraintes de profondeur imposées par les temps de cohérence déterminent la taille maximale du sous-circuit, car des sous-circuits plus grands nécessitent plus de qubits et des séquences de portes plus profondes qui doivent se terminer avant que la décohérence ne corrompe le calcul.",
    "B": "Le plafond de taille de sous-circuit est fixé par la dimension de lien maximale du réseau tensoriel que le logiciel de contrôle classique peut contracter en temps réel pour la rétroaction de mesure en milieu de circuit, car chaque qubit supplémentaire double la dimension de l'espace de Hilbert nécessitant une simulation. Les bibliothèques modernes de réseaux tensoriels fonctionnant sur des co-processeurs FPGA peuvent gérer jusqu'à χ=2^14 de dimension de lien avec une latence inférieure à la microseconde en utilisant des ordres de contraction optimisés, ce qui se traduit par environ 14 qubits d'intrication maximale par sous-circuit avant que la surcharge de simulation classique n'excède le budget de temps d'inactivité du qubit et ne force le processeur quantique à attendre que le système de contrôle finisse de calculer les paramètres de porte conditionnels.",
    "C": "Le partitionnement des sous-circuits est limité par la RAM quantique disponible pour stocker les états computationnels intermédiaires pendant les protocoles de découpage de circuit, car chaque frontière de partition nécessite log(d) qubits ancillaires pour encoder l'indice de coupe de dimension d via un encodage d'amplitude. Pour les sous-circuits dépassant environ 30 qubits, la surcharge ancillaire pour représenter l'espace de coupe exponentiellement grand croît pour consommer plus de la moitié des qubits physiques du dispositif, laissant des ressources insuffisantes pour les registres computationnels réels. Cela force le compilateur soit à réduire la taille du sous-circuit, soit à accepter des coûts de post-traitement classique exponentiellement croissants issus de la décomposition en quasi-probabilité, créant une limite pratique où les exigences combinées en ressources quantiques et classiques dépassent la capacité matérielle disponible.",
    "D": "La taille maximale du sous-circuit est gouvernée par la capacité du compilateur à router les portes à deux qubits dans le graphe de connectivité du dispositif tout en respectant la surcharge d'insertion de SWAP, car des sous-circuits plus grands nécessitent plus de communication entre des paires de qubits distantes, et chaque porte SWAP ajoute trois couches CNOT de profondeur. Pour les topologies typiques en treillis heavy-hex avec un degré moyen ~3, les sous-circuits dépassant 40 qubits créent une congestion de routage où la profondeur d'insertion SWAP croît quadratiquement avec le diamètre du sous-circuit, consommant tout le budget de cohérence sur la communication plutôt que sur les portes logiques. Ce goulot d'étranglement de routage limite effectivement la taille du sous-circuit à environ √N qubits pour un dispositif à N qubits, indépendamment du nombre brut de qubits disponibles.",
    "solution": "A"
  },
  {
    "id": 199,
    "question": "Quel est le principal défi que posent les erreurs de fuite pour la correction d'erreurs quantiques ?",
    "A": "Les erreurs de fuite corrompent l'extraction de syndrome en faisant que les mesures de stabilisateurs retournent des résultats qui semblent valides dans l'espace de code mais encodent des informations d'erreur incorrectes, car un qubit dans |2⟩ peut toujours produire des valeurs propres déterministes ±1 pour les opérateurs de Pauli malgré le fait de ne pas résider dans le sous-espace computationnel. Cette corruption de syndrome se propage sans être détectée à travers le décodeur car les statistiques de mesure restent cohérentes avec les motifs de corrélation conçus du code, conduisant à des chaînes d'erreur mal diagnostiquées qui appliquent des opérateurs de récupération incorrects et introduisent par inadvertance des erreurs logiques tout en semblant compléter avec succès le cycle de correction d'erreurs.",
    "B": "Les mesures de syndrome standard supposent le sous-espace computationnel de |0⟩ et |1⟩ uniquement, mais la fuite vers |2⟩ ou des niveaux d'énergie supérieurs brise cette hypothèse fondamentale. Les codes de stabilisateurs ne peuvent pas détecter ou corriger les erreurs en dehors de leur espace de code conçu car les états ayant fui produisent des résultats de mesure imprévisibles.",
    "C": "Les qubits ayant fui compromettent le cycle de correction d'erreurs en réduisant la distance de code effective, car chaque qubit physique dans l'état |2⟩ agit comme un effacement permanent qui ne peut pas participer aux mesures de stabilisateurs jusqu'à ce qu'il soit activement réinitialisé via des transitions de bande latérale ou l'ingénierie de réservoir. Bien que le code puisse tolérer des effacements jusqu'à distance d-1, la fuite s'accumule au fil du temps car les portes à deux qubits entre les qubits computationnels et ayant fui transfèrent probabilistiquement la population vers des niveaux supérieurs, saturant finalement la capacité de correction d'effacement et causant une défaillance logique une fois que le nombre de qubits simultanément ayant fui dépasse le paramètre de seuil d'effacement du code.",
    "D": "La fuite crée un problème de rétroaction de mesure où l'extraction de syndrome perturbe les qubits ayant fui différemment des qubits en base computationnelle, faisant que l'acte de mesurer les stabilisateurs injecte des erreurs de phase corrélées à travers le bloc logique. Parce que le décalage dispersif pour |2⟩ diffère de |0⟩ et |1⟩, chaque lecture de syndrome applique une rotation conditionnelle non désirée proportionnelle à la population ayant fui, et ces phases induites par mesure s'accumulent de manière cohérente à travers les tours de syndrome, transformant effectivement le protocole de correction d'erreurs en une source de bruit qui dégrade la fidélité logique plus rapidement que de laisser les qubits inactifs sans aucune mesure de syndrome appliquée.",
    "solution": "B"
  },
  {
    "id": 200,
    "question": "Quel est l'avantage principal de l'utilisation de codes de correction d'erreurs quantiques asymétriques dans des environnements de bruit biaisé ?",
    "A": "En exploitant l'asymétrie directionnelle dans les canaux de bruit biaisé, ces codes permettent des calendriers de mesure de syndrome adaptatifs où les types d'erreur à haute polarisation déclenchent des cycles de correction plus rapides tandis que les erreurs à faible probabilité utilisent une rétroaction retardée, réduisant ainsi la latence de correction moyenne. Cette optimisation temporelle maintient les taux d'erreur logiques en dessous du seuil tout en diminuant la surcharge ancillaire moyennée dans le temps par des facteurs approchant le ratio de polarisation lui-même, améliorant fondamentalement le compromis débit-fidélité pour le matériel avec asymétrie de bruit native.",
    "B": "Ils allouent les ressources de correction de manière efficace en fournissant une protection plus forte contre les types d'erreur qui se produisent le plus fréquemment dans le modèle de bruit tout en dédiant moins de qubits et de portes à la correction des canaux d'erreur plus rares, optimisant ainsi le compromis surcharge-performance pour les signatures de bruit réelles du matériel.",
    "C": "Les codes asymétriques réalisent un encodage optimal en adaptant la distribution de poids des stabilisateurs pour correspondre au ratio de polarisation du bruit, de sorte que les types d'erreur fréquemment survenant nécessitent des stabilisateurs de poids inférieur pour la détection tandis que les erreurs rares utilisent des mesures de poids supérieur. Cette asymétrie de poids réduit la profondeur moyenne du circuit de mesure de stabilisateurs d'un facteur proportionnel à la racine carrée du paramètre de polarisation, diminuant ainsi la propagation d'erreur pendant l'extraction de syndrome tout en maintenant la distance de code nécessaire pour la tolérance aux pannes en dessous du seuil.",
    "D": "La structure asymétrique exploite la non-commutativité entre les canaux d'erreur dominants et rares pour créer un espace de syndrome où les erreurs à haute probabilité se projettent sur des espaces propres de faible poids du groupe de stabilisateurs, permettant l'extraction de syndrome en utilisant moins de qubits ancillaires que les codes symétriques. Cette stratification de l'espace propre permet de détecter les erreurs à haute polarisation par des mesures de stabilisateurs d'ordre log(n) plutôt que O(n), réduisant fondamentalement la surcharge d'extraction de syndrome tout en préservant le taux de suppression d'erreur logique.",
    "solution": "B"
  },
  {
    "id": 201,
    "question": "Quel est un avantage unique des codes de surface dans le contexte de la résilience à la perte d'atomes ?",
    "A": "Leur structure de stabilisateurs 2D avec interactions entre plus proches voisins nécessite uniquement des mesures de syndrome locales qui isolent naturellement les qubits perdus dans de petits sous-graphes de stabilisateurs, évitant la propagation du syndrome à travers des couplages à longue portée qui répandraient les échecs de mesure induits par les pertes sur tout le réseau. En limitant chaque générateur de stabilisateur à des termes à quatre corps sur des sites adjacents, le code garantit qu'une seule vacance atomique n'affecte au maximum que quatre contrôles de type X et Z, permettant aux décodeurs standard d'appariement parfait à poids minimum de signaler ces contributions de syndrome manquantes et de poursuivre la correction d'erreurs avec une distance de code réduite, maintenant la suppression des erreurs logiques même lorsque les réseaux de piégeage d'atomes neutres développent des motifs de vacances dispersées durant des séquences de portes prolongées.",
    "B": "Leur disposition planaire permet une reconfiguration localisée des stabilisateurs pour contourner les sites perdus, permettant au décodeur de réacheminer dynamiquement l'extraction de syndrome autour des vacances sans recompilation globale du circuit. En traitant la perte d'atomes comme des erreurs d'effacement avec des emplacements connus, le code peut adapter ses définitions d'opérateurs logiques et ses plannings de mesure de stabilisateurs en temps réel, maintenant la capacité de correction d'erreurs même lorsque le réseau de qubits physiques développe des motifs de vacances irréguliers durant des calculs prolongés sur des plateformes comme les systèmes de pinces optiques.",
    "C": "Leur distribution de poids de stabilisateurs permet des plannings de mesure de syndrome adaptatifs qui sautent les générateurs impliquant des atomes perdus, permettant au décodeur de reconstruire les contrôles de parité manquants par propagation de contraintes redondantes à partir de stabilisateurs intacts voisins sans ambiguïté de syndrome. En traitant la perte d'atomes comme des effacements détectables plutôt que comme des erreurs de Pauli inconnues, le code peut invoquer une propagation de croyance modifiée qui exploite la redondance de stabilisateurs inhérente à la structure homologique, maintenant des seuils de correction d'erreurs dépendant de la distance même lorsque le réseau de qubits physiques développe jusqu'à (d−1)/2 vacances dispersées durant l'extraction de syndrome multi-tours sur des plateformes de réseaux optiques avec des taux réalistes de perte d'atomes.",
    "D": "Leur structure de dégénérescence topologique permet la relocalisation d'opérateurs logiques tolérants aux fautes par déformation de cycles homologiquement équivalents loin des sites perdus, permettant au décodeur de redéfinir les états de base computationnels en utilisant des opérateurs de chaîne alternatifs qui évitent les vacances sans briser la commutativité des stabilisateurs. En traitant la perte d'atomes comme des emplacements d'effacement connus qui contraignent les classes d'homologie disponibles, le code peut effectuer une migration de qubit logique en temps réel vers des sous-régions sans défauts tout en préservant l'information encodée par déformation continue des chaînes logiques X et Z, maintenant l'intégrité computationnelle même lorsque les réseaux d'atomes neutres subissent des motifs de vacances évoluant dynamiquement sur des centaines de sites de piégeage durant des simulations quantiques prolongées.",
    "solution": "B"
  },
  {
    "id": 202,
    "question": "Dans les théories de référentiels quantiques, qu'est-ce que la « supersélection » interdit ?",
    "A": "Les superpositions cohérentes entre secteurs de charge distincts lorsque ces secteurs correspondent aux espaces propres d'un générateur globalement conservé, parce que la loi de conservation restreint les états physiquement préparables à ceux respectant la structure de symétrie. Cependant, une fois qu'un référentiel de référence relationnel est introduit via un système auxiliaire portant une distribution de charge complémentaire, la phase relative entre secteurs devient opérationnellement significative par des protocoles interférométriques qui mesurent la différence de charge, restaurant ainsi la réalisabilité physique de la superposition dans l'espace de Hilbert étendu qui inclut les degrés de liberté du système et du référentiel.",
    "B": "Les superpositions cohérentes d'états différant par une quantité conservée lorsqu'aucun référentiel partagé n'existe pour donner un sens opérationnel à la phase relative entre différents secteurs de charge. Sans un tel référentiel, la superposition manque de réalisabilité physique car les observateurs ne peuvent distinguer la phase relative par aucun protocole de mesure local, forçant l'état à se comporter effectivement comme un mélange classique des secteurs de valeurs propres distincts malgré sa description formelle comme superposition dans le formalisme de l'espace de Hilbert.",
    "C": "Les superpositions cohérentes entre états dans différents secteurs de charge lorsque la symétrie globale est décrite par un groupe de Lie compact, parce que les groupes compacts imposent des règles de supersélection discrètes par leur théorie des représentations. La distinction clé est que les groupes non compacts comme le groupe de Poincaré permettent des superpositions continues entre états propres d'impulsion, alors que les groupes compacts comme U(1) ou SU(2) imposent des conditions de quantification strictes qui interdisent toute combinaison linéaire d'états portant différentes valeurs propres des générateurs conservés, indépendamment de si les observateurs possèdent des référentiels appropriés pour la comparaison de phase.",
    "D": "Les superpositions cohérentes d'états avec une charge conservée différente lorsqu'elles sont considérées du point de vue d'un seul observateur localisé qui n'a pas accès à un système de référence délocalisé, parce que la règle de supersélection émerge dynamiquement par décohérence induite par l'incapacité de l'observateur à suivre les relations de phase globales. Une fois que l'observateur construit ou obtient l'accès à un référentiel quantique suffisamment délocalisé — tel qu'un état cohérent d'un oscillateur harmonique avec un grand nombre moyen de photons — la décohérence effective est supprimée, et les superpositions entre secteurs de charge retrouvent leur cohérence quantique sur des échelles de temps expérimentalement accessibles.",
    "solution": "B"
  },
  {
    "id": 203,
    "question": "Quelle propriété de modèle s'est avérée avoir la plus faible corrélation avec les performances sur des ensembles de données réels ?",
    "A": "La profondeur totale du circuit — mesurée comme le nombre de couches de portes séquentielles de l'entrée à la mesure — présente une corrélation étonnamment faible avec la précision empirique sur des tâches réelles de classification et de régression. Bien que la sagesse conventionnelle suggère que des circuits plus profonds devraient permettre des approximations de fonctions plus expressives, des études expérimentales sur plusieurs plateformes matérielles révèlent qu'une profondeur excessive amplifie principalement la décohérence et les erreurs de porte sans augmenter proportionnellement la capacité du modèle. En fait, des ansätze peu profonds avec des motifs d'intrication soigneusement conçus surpassent souvent leurs homologues profonds lorsqu'ils sont entraînés sur des dispositifs intermédiaires bruyants, suggérant que la profondeur seule est un mauvais prédicteur de la performance de généralisation en apprentissage automatique quantique variationnel pratique.",
    "B": "La méthode d'optimisation spécifique employée durant l'entraînement — qu'il s'agisse de techniques basées sur le gradient comme les règles de décalage de paramètres et les approximations par différences finies, ou d'approches sans gradient telles que SPSA, Nelder-Mead et les stratégies évolutionnaires — a montré un impact minimal sur la performance finale sur l'ensemble de test à travers divers problèmes de référence. Des investigations empiriques démontrent qu'une fois les hyperparamètres correctement ajustés, toutes les grandes familles d'optimiseurs convergent vers des solutions fonctionnellement équivalentes avec des métriques de précision comparables. Cette insensibilité suggère que la géométrie du paysage de perte, plutôt que l'algorithme de recherche particulier, domine la qualité du modèle, impliquant que la conception soigneuse de l'ansatz et les stratégies d'initialisation sont bien plus critiques que la sélection de l'optimiseur pour obtenir des résultats compétitifs.",
    "C": "La simple présence ou absence de portes intriquantes dans l'ansatz variationnel montre une corrélation négligeable avec la précision du modèle sur des ensembles de données pratiques, contrairement à l'intuition. Des références empiriques révèlent que des rotations paramétrisées purement locales peuvent égaler la performance de circuits fortement intriqués lorsqu'ils sont correctement initialisés et entraînés avec suffisamment de données.",
    "D": "Le nombre de paramètres entraînables dans le circuit variationnel, correspondant typiquement au compte des angles de rotation à travers toutes les portes paramétrisées, démontre un faible pouvoir prédictif pour la performance réelle sur les ensembles de données. Bien que la surparamétrisation puisse sembler avantageuse pour ajuster des distributions de données complexes, des études d'ablation récentes révèlent que des modèles avec moins de paramètres atteignent fréquemment une précision de test comparable ou supérieure comparée à des homologues fortement paramétrisés. Cette découverte contre-intuitive survient parce que des paramètres excessifs augmentent la difficulté d'optimisation et la susceptibilité aux plateaux arides, où les gradients disparaissent exponentiellement avec le nombre de paramètres, neutralisant effectivement tout bénéfice représentationnel d'avoir plus de degrés de liberté dans la préparation d'état quantique.",
    "solution": "C"
  },
  {
    "id": 204,
    "question": "Qu'est-ce qui cause l'amortissement d'amplitude dans les systèmes quantiques ?",
    "A": "La dissipation d'énergie vers des degrés de liberté environnementaux par des processus d'émission spontanée, où la population de l'état excité décroît vers l'équilibre thermique avec le bain. Cependant, contrairement au pur déphasage, l'amortissement d'amplitude présente des taux de décroissance asymétriques qui dépendent de la température par l'équilibre détaillé : les transitions ascendantes de |0⟩ à |1⟩ se produisent à un taux proportionnel au nombre de photons thermiques n̄, tandis que la décroissance descendante procède à un taux (n̄+1), conduisant à une population excitée à l'état stationnaire finie même à température nulle en raison des fluctuations du vide.",
    "B": "La dissipation d'énergie vers des degrés de liberté environnementaux, causant la décroissance irréversible de l'état excité vers l'état fondamental avec une perte asymétrique de population dans les niveaux d'énergie supérieurs.",
    "C": "L'émission spontanée de photons dans des modes environnementaux non surveillés qui couple sélectivement l'état excité |1⟩ à l'état fondamental |0⟩ par des transitions de dipôle électrique, créant une intrication qubit-environnement de la forme |1⟩|vac⟩ → √(1-p)|1⟩|vac⟩ + √p|0⟩|1_env⟩. Lorsque le photon environnemental est tracé, cela résulte en des opérateurs de Kraus asymétriques E₀ et E₁ où seul E₁ = |0⟩⟨1| transfère la population vers le bas, le distinguant de l'amortissement de phase qui préserve les populations tout en randomisant les phases.",
    "D": "Des événements de diffusion inélastique entre qubits et modes de phonons dans le matériau du substrat, où la conservation d'énergie requiert ℏω_qubit = ℏω_phonon + ΔE pour chaque processus de diffusion. Ce mécanisme produit une décroissance T₁ exponentielle avec un taux Γ₁ ∝ J(ω)|α|² où J(ω) est la densité spectrale de phonons et α le couplage qubit-phonon. Crucialement, la symétrie de renversement temporel de l'hamiltonien d'interaction assure des taux de transition ascendants et descendants égaux, conduisant à des populations à l'état stationnaire asymétriques déterminées par la température du bain de phonons.",
    "solution": "B"
  },
  {
    "id": 205,
    "question": "Quelle est la fonction principale des opérateurs logiques dans les codes de correction d'erreurs quantiques à stabilisateurs ?",
    "A": "Les opérateurs logiques mesurent directement les qubits physiques individuels qui composent le bloc de code, extrayant l'information de syndrome en effectuant des mesures projectives sur chaque qubit constituant séquentiellement. Ce processus de mesure fait s'effondrer l'état logique encodé dans la base computationnelle, permettant aux algorithmes de correction d'erreurs d'identifier quels qubits physiques ont été corrompus en comparant les résultats de mesure aux valeurs propres de stabilisateurs attendues.",
    "B": "La fonction principale des opérateurs logiques est de convertir les erreurs quantiques en syndromes d'erreur classiques qui peuvent être traités par des algorithmes de correction d'erreurs conventionnels, effectuant essentiellement une correspondance quantique-classique à chaque cycle de code.",
    "C": "Les transformations sur l'information encodée tout en préservant l'espace de code — les opérateurs logiques implémentent des portes quantiques sur les qubits logiques encodés en agissant sur les qubits physiques de manières qui commutent avec tous les stabilisateurs, garantissant que les opérations restent dans le sous-espace protégé et maintiennent les propriétés de correction d'erreurs.",
    "D": "Les opérateurs logiques isolent physiquement le système quantique du bruit environnemental en créant une frontière d'espace de Hilbert protectrice qui empêche les canaux de décohérence de se coupler aux qubits encodés. Ils accomplissent cela en imposant des lois de conservation sur le sous-espace de code par des relations de commutation avec l'hamiltonien, rendant effectivement l'information logique inaccessible à tout processus de bruit qui respecte les symétries de stabilisateurs — fonctionnant comme un mécanisme de blindage actif plutôt que simplement détecter les erreurs après qu'elles se produisent.",
    "solution": "C"
  },
  {
    "id": 206,
    "question": "Quelle manipulation cachée dans un système d'ions piégés peut fabriquer de faux zéros de syndrome stabilisateur pendant les cycles de correction d'erreur du code de Shor ?",
    "A": "Échanger l'ordre physique des ions dans le cristal tout en mettant à jour simultanément les métadonnées de mappage des qubits crée une inadéquation subtile dans la séquence d'adressage des portes de Mølmer-Sørensen, car le calibrage des portes suppose des paramètres de Lamb-Dicke fixes pour chaque position d'ion. Lorsque les ions sont réordonnés, leurs forces de couplage aux modes de mouvement changent en raison des gradients de confinement dépendant de la position, ce qui fait que les portes multi-qubits utilisées dans l'extraction de syndrome accumulent des erreurs de phase qui biaisent systématiquement les mesures de parité vers des résultats nuls, même lorsque des erreurs logiques sont présentes sur les qubits encodés.",
    "B": "Introduire des bandes latérales de micro-mouvement qui désaccordent les ions spectateurs corrompt l'accumulation de phase pendant les mesures de parité multi-qubits en créant des couplages hors résonance parasites entre les ions qui devraient rester inactifs pendant l'extraction de syndrome. Le micro-mouvement, résultant du potentiel de piège radiofréquence oscillant, impose des déplacements Stark dépendant du temps qui varient le long de la chaîne d'ions, causant des erreurs systématiques dans les portes à phase contrôlée utilisées pour les vérifications de stabilisateur.",
    "C": "Appliquer un gradient de champ magnétique axial soigneusement conçu qui module les fréquences des qubits crée des déplacements Zeeman variant spatialement qui, combinés avec la séquence d'extraction de syndrome spécifique du code de Shor, induisent une interférence destructive dans les signatures d'erreur pendant les mesures de parité multi-qubits. Comme les circuits de syndrome reposent sur l'accumulation de phase collective à travers plusieurs ions pendant les portes de Mølmer-Sørensen, les décalages de fréquence dépendant de la position peuvent être calibrés de sorte que les erreurs réelles de bit-flip ou de phase-flip produisent des contributions de phase qui s'annulent pendant la projection finale de lecture, enregistrant systématiquement de faux syndromes nuls.",
    "D": "Atténuer l'intensité du faisceau Raman global à précisément 70,7% de sa valeur calibrée sur des cycles alternés de correction d'erreur introduit une sous-rotation systématique dans les portes à phase contrôlée utilisées pour l'extraction de syndrome, ciblant spécifiquement le régime √SWAP où les interactions d'intrication sont les plus sensibles aux fluctuations de puissance. Cela crée un motif d'erreur cohérent où les mesures de parité présentent une sensibilité réduite aux erreurs de qubit unique de manière dépendante de la phase, causant le circuit d'extraction de syndrome à projeter des états corrompus sur l'espace de code tout en enregistrant des syndromes nuls même lorsque des erreurs de bit-flip ou de phase-flip se sont produites sur les qubits logiques.",
    "solution": "B"
  },
  {
    "id": 207,
    "question": "Quelle combinaison de techniques quantiques permet l'extraction de caractéristiques en parallèle dans certains modèles d'apprentissage quantique ?",
    "A": "Les protocoles de téléportation permettent le transfert d'états de caractéristiques quantiques entre nœuds d'apprentissage distribués, tandis que les codes de correction d'erreur quantique préservent l'intégrité de ces caractéristiques pendant la transmission et le traitement. En combinant ces techniques, les modèles d'apprentissage quantique peuvent extraire des caractéristiques de jeux de données distribués sur plusieurs processeurs quantiques, avec la correction d'erreur garantissant que la décohérence ne corrompt pas les représentations de caractéristiques extraites — créant essentiellement un pipeline d'extraction de caractéristiques distribué tolérant aux pannes.",
    "B": "La transformée de Fourier quantique combinée à l'algorithme de Grover fournit un cadre pour l'extraction de caractéristiques en parallèle en transformant d'abord les données d'entrée dans le domaine fréquentiel, où la recherche de Grover peut identifier les caractéristiques dominantes à travers plusieurs états de base simultanément. Cette approche exploite l'accélération quadratique de l'algorithme de Grover pour amplifier les caractéristiques pertinentes tandis que la QFT garantit que toutes les composantes fréquentielles sont évaluées en superposition, extrayant efficacement les caractéristiques de l'espace d'entrée entier en une seule exécution de circuit quantique.",
    "C": "La distribution quantique de clés établit des canaux sécurisés pour transmettre des données de caractéristiques classiques entre processeurs quantiques, tandis que l'échange d'intrication étend cette sécurité pour créer des réseaux de nœuds d'apprentissage intriqués. Cette combinaison permet à plusieurs processeurs quantiques d'extraire des caractéristiques de différentes portions d'un jeu de données en parallèle, avec l'intrication garantissant que les caractéristiques extraites maintiennent des corrélations quantiques que le traitement parallèle classique ne peut pas réaliser, permettant ainsi un apprentissage de caractéristiques distribué véritablement amélioré quantiquement.",
    "D": "L'estimation d'amplitude combinée au test d'échange crée un cadre puissant pour l'extraction de caractéristiques en parallèle en permettant la comparaison simultanée d'états quantiques encodant différentes caractéristiques. Le test d'échange mesure le chevauchement entre des états quantiques incorporant des caractéristiques, tandis que l'estimation d'amplitude fournit une accélération quadratique pour déterminer ces chevauchements avec haute précision, permettant au système d'extraire et de comparer plusieurs représentations de caractéristiques à travers l'espace d'entrée en opérations quantiques parallèles.",
    "solution": "D"
  },
  {
    "id": 208,
    "question": "Quelle approche réduit l'empreinte mémoire classique dans le découpage basé sur les tenseurs ?",
    "A": "La ré-exécution itérative avec conditions aux limites mémorisées met en cache uniquement les distributions de probabilité marginales P(résultat|config_frontière) pour chaque fragment de sous-circuit, indexées par les paramètres de fils coupés, puis reconstruit la valeur d'espérance globale en échantillonnant ces distributions mises en cache pendant le post-traitement classique. En stockant des histogrammes compressés (nécessitant O(2^k · poly(shots)) mémoire pour k qubits coupés) plutôt que des matrices de densité complètes (nécessitant O(4^n) mémoire pour n qubits), cette approche basée sur tables échange la profondeur de circuit quantique contre l'efficacité de stockage classique, la rendant pratique lorsque le nombre de coupures k ≪ n et le bruit de tir domine sur les erreurs systématiques.",
    "B": "Point de contrôle et redémarrage avec instantanés de matrices de densité écrit la matrice de densité réduite ρ_fragment pour chaque sous-circuit sur stockage persistant immédiatement après l'exécution quantique, puis recharge uniquement les fragments nécessaires pendant la phase de contraction tensorielle classique, effectuant les multiplications matricielles en mode pipeline streaming. Cette approche soutenue par disque stocke O(4^(n/p)) données par sous-circuit lors du partitionnement en p fragments, permettant à l'empreinte RAM maximale de rester fixée à la taille de la plus grande contraction par paire ρ_i ⊗ ρ_j. Les bandes passantes d'E/S SSD modernes (~GB/s) rendent cela viable pour les circuits avec n ≤ 25 qubits par fragment, surtout lors de l'utilisation de formats de sérialisation optimisés comme HDF5 avec compression BLOSC.",
    "C": "La contraction à la volée de tranches calcule les composants de réseau tensoriel dynamiquement au fur et à mesure qu'ils sont nécessaires pour la reconstruction finale, sans stocker les tenseurs intermédiaires complets. Chaque sous-circuit est évalué indépendamment avec des conditions aux limites échantillonnées, et les résultats sont immédiatement contractés et supprimés, conservant uniquement des agrégats courants. Cette approche en streaming réduit l'utilisation maximale de mémoire d'exponentielle en nombre de qubits à polynomiale en largeur de coupure, permettant le traitement de circuits plus grands sur du matériel classique à mémoire contrainte.",
    "D": "L'assemblage direct de vecteur d'état dans la base computationnelle représente chaque fragment de sous-circuit comme une fonction d'onde de rang plein ψ_fragment = Σ_x α_x|x⟩ sur tous les 2^n_fragment états de base, stockant les amplitudes α_x dans des blocs de mémoire contigus. Pendant l'assemblage classique, les fragments sont combinés via des produits tensoriels suivis de traces partielles sur les indices de coupure, avec résultats intermédiaires maintenus dans l'espace d'échange. Bien que cela nécessite O(2^n_fragment) nombres complexes par fragment, il permet des mises à jour d'amplitude parallèles par bits utilisant les instructions vectorielles AVX-512, accélérant la contraction finale de 8× sur les CPU modernes. La demande mémoire culmine à O(2^n_total) pendant la phase de fusion mais évolue linéairement dans le nombre de fragments avant fusion.",
    "solution": "C"
  },
  {
    "id": 209,
    "question": "Quel est le défi de la conception de circuits quantiques tenant compte du compilateur ?",
    "A": "Structurer les circuits de sorte que les transformations du compilateur préservent les propriétés algorithmiques critiques tout en permettant l'optimisation — vous devez comprendre quelles séquences de portes sont sémantiquement équivalentes selon les conditions de correction de votre algorithme versus simplement syntaxiquement similaires. Cela nécessite la connaissance des caractéristiques de circuit que le compilateur utilise comme ancres d'optimisation (comme les frontières de commutation et l'ordonnancement des mesures) afin de pouvoir concevoir des circuits qui guident le compilateur vers des transformations bénéfiques tout en empêchant celles qui brisent les hypothèses algorithmiques, tenant compte de la façon dont le routage de qubits et la synthèse de portes interagiront avec votre structure prévue.",
    "B": "Anticiper comment le mappage et les passes d'optimisation transformeront réellement votre circuit — vous devez concevoir pour le comportement du compilateur, pas seulement pour l'algorithme idéal. Cela nécessite de comprendre les heuristiques de routage de qubits, les règles de commutation de portes et les seuils d'optimisation afin de pouvoir structurer les circuits pour s'aligner avec ce que le compilateur produira, tenant compte des contraintes spécifiques à l'architecture comme la connectivité limitée ou les restrictions d'ensemble de portes qui affectent la forme compilée finale.",
    "C": "Équilibrer la profondeur du circuit contre le budget d'optimisation du compilateur — puisque la plupart des compilateurs de production implémentent des heuristiques en temps polynomial avec des limites d'itération fixes, les circuits dépassant certains seuils de complexité ne recevront qu'une optimisation partielle. Vous devez concevoir en tenant compte de ces frontières computationnelles, structurant les algorithmes pour s'adapter au régime d'optimisation traitable du compilateur (typiquement circuits avec moins de 10³ portes à deux qubits et graphes de connectivité avec largeur arborescente inférieure à 20) tout en évitant les structures pathologiques qui déclenchent le pire comportement dans les algorithmes de routage, qui se manifestent souvent lorsque les motifs d'interaction de qubits créent des nœuds de haut degré dans le graphe de dépendance du circuit.",
    "D": "Gérer la tension entre la spécification algorithmique indépendante du matériel et le besoin du compilateur d'indices spécifiques à l'architecture intégrés dans la structure du circuit — vous devez encoder suffisamment d'informations sur les décompositions de portes préférées et les stratégies d'allocation de qubits sans sur-contraindre l'espace de recherche du compilateur. Cela implique d'utiliser des annotations indépendantes de la plateforme pour signaler les priorités d'optimisation (comme quels sous-circuits sont critiques en latence) tout en évitant les références matérielles explicites qui briseraient la portabilité inter-plateformes, créant essentiellement une représentation de circuit qui sert simultanément de spécification exécutable et de guidance du compilateur sans s'engager prématurément dans des choix d'implémentation de bas niveau.",
    "solution": "B"
  },
  {
    "id": 210,
    "question": "L'entropie d'intrication est souvent utilisée comme indicateur de la capacité du modèle car elle :",
    "A": "Borne directement le rang de Schmidt de l'état quantique à travers toute bipartition du système de qubits, ce qui détermine le nombre minimum d'états produits nécessaires pour exprimer l'état de sortie du circuit paramétré. Une entropie d'intrication plus élevée correspond à un rang de Schmidt exponentiellement plus grand, indiquant que le circuit génère des états nécessitant exponentiellement plus de ressources classiques pour être représentés, quantifiant ainsi l'avantage d'expressivité quantique qui permet de modéliser des corrélations complexes au-delà des représentations classiques de taille polynomiale dans les algorithmes variationnels.",
    "B": "Quantifie combien de corrélation et d'information quantiques le circuit paramétré peut effectivement représenter et distribuer à travers le système de qubits. Une entropie d'intrication plus élevée indique que le circuit crée des corrélations non locales plus complexes entre les qubits, suggérant un plus grand pouvoir expressif pour capturer les structures d'états quantiques complexes nécessaires pour représenter des fonctions ou Hamiltoniens complexes dans les algorithmes variationnels.",
    "C": "Corrèle avec la dimensionnalité effective de la variété d'états quantiques accessible par le circuit paramétré, telle que mesurée par le volume local d'états distinguables atteignables par variations infinitésimales de paramètres. Une entropie d'intrication élevée signale que le circuit explore un plus grand volume de l'espace de Hilbert avec des corrélations quantiques non triviales, indiquant une capacité de représentation accrue. Cette perspective géométrique relie l'entropie à la capacité du circuit d'approximer des états cibles arbitraires dans la variété accessible, en faisant un diagnostic pratique pour évaluer si l'ansatz possède suffisamment de flexibilité pour les algorithmes variationnels.",
    "D": "Fournit une mesure d'efficacité d'utilisation des paramètres en quantifiant le rapport entre l'intrication générée par paramètre et le maximum théorique réalisable avec l'architecture de circuit donnée. Les circuits avec des ratios entropie-paramètre élevés indiquent que chaque paramètre variationnel contribue significativement à générer des corrélations quantiques plutôt que de rester dans des régions redondantes ou sous-utilisées de l'espace des paramètres. Cette métrique d'efficacité aide à identifier quand des paramètres de circuit supplémentaires augmenteraient véritablement la capacité du modèle versus simplement ajouter des degrés de liberté qui produisent des états linéairement dépendants, guidant la conception d'ansatz pour les algorithmes variationnels.",
    "solution": "B"
  },
  {
    "id": 211,
    "question": "Pourquoi les classes de complexité quantique à énergie contrainte sont-elles pertinentes pour les dispositifs à court terme ?",
    "A": "Elles limitent les algorithmes à des sous-espaces avec une énergie moyenne bornée, correspondant aux limitations matérielles telles que la fuite d'excitation des qubits vers des niveaux supérieurs du transmon ou les taux de chauffage des ions. En formalisant les budgets énergétiques, ces classes de complexité capturent les contraintes réalistes où les dispositifs NISQ ne peuvent maintenir des états de haute énergie arbitraires et doivent fonctionner dans les limites thermiques et de bande passante de contrôle imposées par les réfrigérateurs à dilution ou les systèmes de refroidissement laser.",
    "B": "Ces classes bornent le produit total énergie*temps disponible pour les calculs, modélisant directement les cycles de service cryogéniques et les limites d'énergie d'impulsion dans les systèmes supraconducteurs où les commandes de contrôle à haute puissance causent un chauffage du substrat qui dégrade la cohérence des qubits. En restreignant l'intégrale ∫E(t)dt sur le calcul, les modèles à énergie contrainte capturent comment les processeurs NISQ doivent équilibrer les opérations de portes rapides contre les contraintes de budget thermique, la borne énergétique se traduisant par une profondeur maximale de circuit avant que les frais généraux de réfrigération ne forcent des pauses de refroidissement.",
    "C": "La complexité à énergie contrainte formalise la restriction aux sous-espaces computationnels de basse énergie qui dominent la conception des algorithmes NISQ, où rester proche de l'état fondamental minimise les erreurs de fuite vers des niveaux non computationnels et réduit le déphasage dû aux environnements électromagnétiques fluctuants. En limitant <H> à des valeurs proches de l'énergie de l'état fondamental, ces classes correspondent au matériel réel où les états de plus haute énergie se couplent plus fortement aux sources de bruit, bien que le cadre suppose des mesures projectives instantanées qui ne contribuent pas elles-mêmes au budget énergétique.",
    "D": "Elles capturent le coût en travail thermodynamique du calcul quantique dans des environnements à température finie, modélisant comment les dispositifs NISQ doivent extraire du travail des bains thermiques pour maintenir la cohérence quantique contre la décroissance entropique. La contrainte énergétique borne l'énergie libre disponible par opération logique, la hiérarchie des classes de complexité étant déterminée par kT ln(2) par qubit comme unité fondamentale, reliant directement les limites de profondeur algorithmique au budget de puissance de réfrigération et à la température du bain qui fixe le manifold d'états accessibles pondéré par Boltzmann.",
    "solution": "A"
  },
  {
    "id": 212,
    "question": "Pourquoi le regroupement par commutation qubit par qubit (QWC) est-il utile lors de la mesure d'observables hamiltoniennes dans les expériences VQE ?",
    "A": "Le regroupement QWC permet la mesure simultanée de plusieurs termes de Pauli via des rotations de base à un qubit partagées, mais le gain d'efficacité provient de la réduction de la variance plutôt que du nombre de mesures : les termes dans un groupe QWC présentent des résultats de mesure corrélés en raison d'espaces propres partagés, permettant une estimation de la covariance qui réduit la variance effective de la valeur moyenne groupée d'un facteur proportionnel à la taille du groupe. Cette réduction de variance se traduit par moins de mesures nécessaires pour atteindre la précision cible, même si chaque mesure nécessite toujours des exécutions de circuit séparées pour les groupes non mesurables simultanément, améliorant la convergence globale de O(M²) à O(M) pour des hamiltoniens à M termes.",
    "B": "Plusieurs produits de Pauli qui partagent la même base de mesure à un qubit sur chaque qubit peuvent être lus simultanément à partir d'une seule exécution de circuit quantique, réduisant substantiellement le nombre total de mesures de circuit nécessaires pour estimer toutes les valeurs moyennes des termes hamiltoniens et accélérant ainsi le processus d'évaluation de l'énergie VQE.",
    "C": "Le regroupement QWC permet à plusieurs termes hamiltoniens de partager des circuits de mesure, mais l'avantage fondamental est la réduction de la profondeur de circuit plutôt que l'économie de mesures : les termes dans le même groupe QWC peuvent être mesurés en utilisant un circuit de rotation de base commun appliqué une seule fois avant la lecture, éliminant les transformations de base redondantes qui nécessiteraient autrement des implémentations unitaires séparées. Cette consolidation réduit le nombre total de portes d'un facteur égal à la taille du groupe, ce qui est critique pour les dispositifs NISQ où les erreurs accumulées des portes à deux qubits provenant de rotations de base répétées domineraient autrement l'incertitude de mesure indépendamment du budget de mesures.",
    "D": "Lorsque les termes de Pauli forment des groupes QWC, leurs valeurs moyennes peuvent être estimées à partir de mesures simultanées sur le même état quantique, réduisant drastiquement les exécutions de circuit par rapport à la mesure de chaque terme individuellement. Cependant, cette efficacité dépend de manière critique du fait que la préparation de l'état soit déterministe et reproductible — pour les états variationnels générés par des circuits paramétrés avec une optimisation de paramètres limitée par le bruit de mesure, les corrélations intra-groupe introduisent un biais systématique qui doit être corrigé par des mesures de termes indépendantes toutes les O(√N) itérations VQE, où N est le nombre de paramètres, compensant partiellement les économies de mesure pour les ansätze à grande échelle.",
    "solution": "B"
  },
  {
    "id": 213,
    "question": "Considérez un scénario de chiffrement temporisé où vous avez besoin de résistance quantique et souhaitez la garantie théorique la plus forte que le déchiffrement nécessite un calcul séquentiel même contre des adversaires disposant d'ordinateurs quantiques et d'une parallélisation massive. Quelle technique précise fournit le chiffrement temporisé résistant aux quantiques le plus robuste selon la compréhension cryptographique actuelle ?",
    "A": "Le chiffrement témoin basé sur la recherche de chemins d'isogénies supersingulières exploite la structure de graphe expanseur du graphe d'isogénies pour créer des énigmes temporisées où le déchiffrement nécessite de parcourir une longue chaîne d'isogénies, et bien que des avancées cryptanalytiques récentes aient montré des vulnérabilités dans les constructions basées sur SIDH, la propriété de verrouillage temporel reste théoriquement solide car même les algorithmes quantiques doivent évaluer séquentiellement chaque étape dans la marche d'isogénie.",
    "B": "Les fonctions à mémoire intensive comme Argon2 ou scrypt, lorsqu'elles sont utilisées itérativement dans une chaîne de preuve de travail, créent des barrières significatives à la parallélisation car chaque étape nécessite d'accéder à des emplacements mémoire pseudo-aléatoires qui ne peuvent être précalculés, ce qui force même les adversaires quantiques à effectuer des opérations mémoire séquentielles.",
    "C": "La cryptographie à seuil utilisant des schémas de chiffrement basés sur les réseaux euclidiens comme Kyber ou NTRU distribue la clé de déchiffrement entre plusieurs parties en utilisant le partage de secret de Shamir adapté aux contextes de réseaux, où la reconstruction du secret nécessite de collecter des parts auprès d'une majorité honnête de nœuds, et parce que les problèmes de réseaux restent difficiles pour les ordinateurs quantiques, cela fournit une sécurité post-quantique robuste pour les secrets à libération temporisée. Le mécanisme de verrouillage temporel émerge du protocole distribué plutôt que de la dureté computationnelle inhérente : le déchiffrement ne peut se produire qu'une fois qu'un nombre suffisant de parties ont été contactées séquentiellement, bien que cela introduise des hypothèses de confiance sur l'honnêteté du réseau et repose sur la coordination plutôt que sur des garanties de calcul séquentiel pur.",
    "D": "Les fonctions de délai vérifiables basées sur les actions de groupes de classes sur les corps quadratiques imaginaires fournissent des exigences de calcul prouvablement séquentiel avec vérification succincte, où la sécurité se réduit à la difficulté du calcul des structures de groupes de classes qui restent intraitable pour les ordinateurs quantiques, en faisant l'étalon-or actuel pour le chiffrement temporisé cryptographiquement rigoureux.",
    "solution": "D"
  },
  {
    "id": 214,
    "question": "Dans le contexte de la découpe de circuits quantiques et de l'exécution distribuée, comment l'évaluation par lots de sous-circuits réduit-elle réellement les frais généraux d'E/S en pratique ? Considérez que chaque coupe introduit une communication classique entre les nœuds de traitement, et que les approches naïves nécessiteraient un transfert de données constant. Le défi est de minimiser les allers-retours tout en maintenant la correction de la reconstruction.",
    "A": "Regrouper les sous-circuits avec des bases de mesure identiques en lots d'exécution uniques avant de transférer les résultats — vous identifiez quels paramètres de mesure apparaissent dans plusieurs termes de reconstruction et les exécutez ensemble dans un seul travail quantique. Cela réduit les soumissions totales de circuits quantiques en consolidant les observables compatibles, bien que le principal bénéfice d'E/S provienne de la transmission de valeurs moyennes agrégées plutôt que de données brutes de mesures. Puisque la reconstruction en quasi-probabilité nécessite généralement des centaines d'évaluations de sous-circuits, le traitement par lots réduit le nombre d'allers-retours classique-quantique-classique de un par terme à un par regroupement de base, amortissant la latence réseau sur plusieurs éléments tensoriels tout en préservant les propriétés statistiques nécessaires pour l'estimation non biaisée des valeurs moyennes.",
    "B": "Réutiliser les résultats de sous-circuits à travers plusieurs scénarios de coupe avant de récupérer de nouvelles données — essentiellement vous évaluez une fois, mettez en cache localement, et appliquez à plusieurs contractions tensorielles. Cela amortit le coût de communication sur plusieurs termes de reconstruction puisque de nombreuses combinaisons de coefficients dans la décomposition en quasi-probabilité partagent des résultats de mesure de sous-circuits communs, permettant à un seul lot d'exécutions quantiques de servir plusieurs entrées dans le calcul final de la valeur moyenne sans transferts réseau répétés pour chaque élément tensoriel.",
    "C": "Pré-calculer un sous-ensemble de résultats de sous-circuits de haute probabilité en utilisant la simulation classique de réseaux tensoriels et exécuter uniquement les configurations restantes de faible probabilité sur le matériel quantique — cette approche hybride exploite le fait que les décompositions en quasi-probabilité concentrent souvent le poids sur un petit nombre de motifs de mesure. En simulant classiquement des sous-circuits avec une dimension de lien sous les limites matérielles (typiquement χ ≤ 64 pour les charges de travail de production), vous éliminez les frais généraux d'E/S pour environ 70-80% des termes de reconstruction, transmettant uniquement le reste classiquement intraitable. La garantie de correction provient de la linéarité des valeurs moyennes, qui permet un partitionnement arbitraire de la somme en quasi-probabilité entre contributions classiques et quantiques.",
    "D": "Implémenter un ordonnancement adaptatif de mesures où les sélections de sous-circuits ultérieures dépendent des résultats précédemment obtenus, permettant à l'algorithme de reconstruction d'élaguer dynamiquement les termes à faible contribution de l'expansion en quasi-probabilité. Cette approche met en pipeline l'exécution quantique avec le post-traitement classique, réduisant le volume total de transfert de données de 40-60% par rapport à l'évaluation inconditionnelle de tous les termes. L'intuition clé est que de nombreux éléments tensoriels ont des coefficients qui s'annulent approximativement dans la sommation finale, ce qui peut être détecté après avoir évalué seulement une fraction logarithmique des termes, permettant une terminaison précoce du protocole d'exécution par lots tout en maintenant une erreur d'approximation bornée par des corrections d'échantillonnage par importance.",
    "solution": "B"
  },
  {
    "id": 215,
    "question": "Comment modifieriez-vous l'algorithme de Grover pour trouver la valeur minimale dans une base de données non triée ?",
    "A": "En encodant les valeurs de la base de données comme amplitudes en superposition, la recherche binaire peut être effectuée de manière quantique où chaque étape de comparaison interroge log(N) éléments simultanément, et l'opérateur de diffusion partitionne naturellement l'espace de recherche en moitiés supérieure et inférieure jusqu'à convergence sur le minimum — atteignant effectivement une complexité O(log N) grâce au parallélisme quantique de la stratégie classique diviser pour régner.",
    "B": "La transformée de Fourier quantique peut remplacer l'opérateur de diffusion de Grover car QFT mappe les magnitudes de valeur en relations de phase distinctes dans le domaine fréquentiel, où les valeurs plus grandes accumulent plus de rotation de phase par itération. Après suffisamment d'itérations, une QFT inverse suivie d'une mesure dans la base computationnelle révèle directement l'indice de la valeur minimale par interférence destructive de toutes les amplitudes non minimales, contournant entièrement le besoin d'oracles de seuil.",
    "C": "Utiliser une série d'itérations de Grover avec différents oracles de seuil, abaissant progressivement le seuil jusqu'à isoler l'élément minimum.",
    "D": "Initialiser un registre auxiliaire en superposition uniforme pour contenir les minima candidats, puis appliquer une séquence de circuits de comparaison quantique contrôlés qui effectuent des tests de magnitude par paires entre le registre auxiliaire et chaque élément de la base de données en superposition. Par amplification d'amplitude, seuls les états auxiliaires correspondant à des valeurs plus petites que tous les éléments comparés survivent, et le filtrage répété sur tous les éléments de la base de données isole le minimum sans post-traitement classique ni itération.",
    "solution": "C"
  },
  {
    "id": 216,
    "question": "Considérez un dispositif NISQ avec un graphe de connectivité en hexagone lourd où vous devez implémenter un circuit de solveur variationnel d'états propres quantiques (VQE) qui inclut des portes à deux qubits entre des qubits qui ne sont pas directement connectés par des liens matériels. Le compilateur doit respecter l'ensemble de portes natif (seules les portes CNOT entre plus proches voisins sont autorisées) et minimiser la profondeur du circuit pour réduire la décohérence. Pourquoi le compilateur insère-t-il des portes SWAP pendant le processus de transpilation, et quel est le compromis principal impliqué dans cette stratégie ?",
    "A": "Les portes SWAP sont insérées pour acheminer l'information quantique entre des qubits non adjacents, permettant les interactions à deux qubits requises sur des paires de qubits physiquement déconnectées. Le compromis principal est que chaque porte SWAP doit être décomposée en trois opérations CNOT consécutives sur le matériel, ce qui augmente significativement à la fois la profondeur totale du circuit et l'erreur cumulative des portes. Cette expansion de profondeur impacte directement la fidélité de la préparation de l'état final dans l'ansatz VQE, car chaque couche supplémentaire de portes introduit davantage d'opportunités pour la décohérence et les erreurs opérationnelles de dégrader la qualité de l'état quantique.",
    "B": "Les portes SWAP sont insérées pour reconfigurer dynamiquement le mappage logique-physique des qubits pendant l'exécution du circuit, permettant l'implémentation d'opérations de portes non adjacentes en relocalisant temporairement les états quantiques vers des régions connectées de la topologie. Le compromis principal est que la compilation de réseau SWAP est NP-difficile pour les graphes de connectivité généraux, forçant le compilateur à utiliser des algorithmes de routage heuristiques qui produisent des solutions sous-optimales avec une profondeur de circuit excessive. Chaque SWAP inséré se décompose en trois CNOT, multipliant directement le nombre de portes à deux qubits et amplifiant ainsi à la fois les erreurs de contrôle cohérentes et les processus de bruit incohérents qui s'accumulent pendant le temps d'exécution étendu.",
    "C": "Les portes SWAP sont insérées pour implémenter des opérations de portes non locales en établissant des canaux quantiques entre des paires de qubits distantes à travers des nœuds intermédiaires dans le réseau hexagonal lourd, téléportant efficacement les opérations de portes à travers le graphe de connectivité. Le compromis principal est que cette stratégie de routage consomme un temps de cohérence supplémentaire proportionnel à la distance graphique entre les qubits cibles, qui augmente exponentiellement avec le diamètre de la topologie du dispositif. Puisque chaque SWAP nécessite trois CNOT physiques plus des corrections à qubit unique associées, la relaxation T1 et T2 accumulée pendant la séquence de portes étendue dégrade la fidélité de l'état, particulièrement pour les qubits en positions périphériques dans l'architecture hexagonale lourde.",
    "D": "Les portes SWAP sont insérées pour réordonner les états de base computationnelle dans le registre quantique, permettant au compilateur d'aligner les indices de qubits avec l'ordre naturel attendu par les circuits de mesure de l'Hamiltonien VQE. Le compromis principal est que les opérations SWAP transforment de manière non triviale la distribution de poids de Pauli des chaînes d'opérateurs encodées, convertissant potentiellement des termes de faible poids en termes de poids supérieur qui nécessitent des portes intriquantes supplémentaires pour la mesure. Cette stratégie de réorganisation de base interagit également mal avec les techniques d'atténuation d'erreurs comme la correction d'erreurs de lecture, puisque les réseaux SWAP altèrent la structure de corrélation entre les résultats de mesure d'une manière qui viole les hypothèses d'indépendance sous-jacentes à la plupart des protocoles d'atténuation d'erreurs.",
    "solution": "A"
  },
  {
    "id": 217,
    "question": "Qu'est-ce qui rend la vérification d'équivalence de circuits quantiques QMA-complète ?",
    "A": "Le problème nécessite de vérifier que deux circuits produisent des opérateurs unitaires identiques à une phase globale près, mais déterminer cette égalité nécessite de vérifier exponentiellement de nombreux éléments matriciels dans le pire des cas. Bien qu'un vérificateur quantique puisse utiliser un témoin succinct (tel qu'un état dont le recouvrement distingue les circuits non équivalents), calculer ce témoin sur du matériel classique nécessite des ressources exponentielles en raison de la dimension de l'espace de Hilbert, tandis que l'étape de vérification elle-même peut être effectuée efficacement étant donné la preuve quantique.",
    "B": "La difficulté fondamentale est de comparer des matrices unitaires qui évoluent exponentiellement avec le nombre de qubits, rendant la vérification directe calculatoirement intraitable. Même si les descriptions de circuit elles-mêmes sont de taille polynomiale, l'opérateur qu'elles implémentent agit sur un espace de Hilbert exponentiellement grand, nécessitant un nombre exponentiel de comparaisons d'états de base pour vérifier l'égalité à moins qu'un témoin de preuve quantique succinct ne soit fourni.",
    "C": "La vérification d'équivalence se réduit à déterminer si la composition U₁†U₂ égale l'identité à une phase globale près, ce qui équivaut à vérifier que l'énergie de l'état fondamental de l'Hamiltonien H = I - U₁†U₂ égale zéro. Ce problème de frustration hamiltonienne est QMA-complet car l'énergie de l'état fondamental ne peut pas être efficacement bornée sans témoins quantiques, même si l'Hamiltonien lui-même a une représentation compacte de taille polynomiale comme produit des deux unitaires de circuit.",
    "D": "La complexité découle du fait que les circuits quantiques peuvent encoder des instances du problème d'Hamiltonien local à travers leur structure : deux circuits sont équivalents si et seulement si l'énergie de l'état fondamental de H = I - U₁†U₂ est zéro, ce qui nécessite de vérifier une propriété globale d'un opérateur de dimension exponentielle. Bien qu'une preuve quantique consistant en l'état maximalement intriqué puisse témoigner de la non-équivalence à travers les statistiques de mesure, trouver ce témoin nécessite de résoudre des problèmes QMA-difficiles, rendant l'étape de vérification polynomiale mais la génération de preuve exponentiellement difficile.",
    "solution": "B"
  },
  {
    "id": 218,
    "question": "Qu'analysent les Réseaux Bayésiens Quantiques (QBN) ?",
    "A": "L'évolution de la superposition pour prédire quand l'effondrement se produit, exploitant l'inférence bayésienne pour assigner des probabilités d'effondrement basées sur les taux de décohérence environnementale et la force de couplage de l'appareil de mesure.",
    "B": "Ils analysent les franges d'interférence en décomposant les fonctions d'onde quantiques en a priori bayésiens, utilisant les motifs résultants pour prédire les trajectoires exactes des particules. Cette approche traite la fonction d'onde comme une distribution de probabilité sur des variables cachées, permettant aux QBN de contourner l'incertitude de Heisenberg en reconstruisant des chemins déterministes à partir de mesures d'ensemble.",
    "C": "La structure d'intrication—cartographiant quelles paires de qubits partagent des corrélations à travers des tables de probabilités conditionnelles qui encodent les relations d'états de Bell. Les QBN construisent des graphes acycliques dirigés où les arêtes représentent des liens d'intrication, et les valeurs de nœuds déterminent la force des corrélations non locales.",
    "D": "L'incertitude dans les mesures quantiques et les distributions de probabilité utilisées dans le traitement de l'information quantique, modélisant spécifiquement comment les résultats de mesure dépendent des états quantiques préalables et comment le raisonnement probabiliste classique peut être greffé sur les systèmes quantiques. Les QBN représentent les dépendances conditionnelles entre observables quantiques en utilisant des modèles graphiques, permettant aux chercheurs de raisonner sur les statistiques de mesure, de mettre à jour les croyances basées sur des observations partielles, et de propager l'incertitude à travers des protocoles quantiques multi-étapes où les mesures séquentielles créent des structures de corrélation complexes.",
    "solution": "D"
  },
  {
    "id": 219,
    "question": "Comment le routage adaptatif d'intrication répond-il aux changements dans la performance des liens ?",
    "A": "Le routage adaptatif d'intrication maintient des métriques distribuées de qualité d'intrication à travers le réseau par le biais de protocoles d'estimation de fidélité périodiques, mettant à jour les décisions de routage uniquement lorsque les statistiques de mesure accumulées indiquent une dégradation statistiquement significative au-delà des fluctuations quantiques normales. Le système emploie une moyenne par fenêtre glissante sur plusieurs cycles de génération d'intrication (typiquement 50-100 paires) pour distinguer les véritables changements de qualité de lien du bruit statistique inhérent aux mesures d'états quantiques, puisque les estimations de fidélité individuelles souffrent d'une incertitude de mesure fondamentale qui déclencherait de fausses mises à jour de routage. Lorsque la fidélité moyenne tombe en dessous de seuils prédéfinis calibrés aux exigences d'application, l'algorithme de routage ajuste progressivement les préférences de chemin plutôt que d'effectuer un reroutage abrupt, déplaçant graduellement le trafic vers des chemins alternatifs tout en continuant à surveiller le lien dégradé pour une récupération potentielle. Cette stratégie de mise à jour conservatrice empêche les oscillations de routage qui pourraient résulter de réponses trop réactives aux fluctuations transitoires de fidélité, bien qu'elle introduise une latence de 0,5-2 secondes entre la dégradation réelle du lien et la réponse de routage, période durant laquelle les applications peuvent subir des taux d'erreur élevés dus à l'utilisation d'intrication compromise.",
    "B": "En recalculant dynamiquement les liens virtuels basés sur les fidélités mesurées, surveillant continuellement la qualité des paires intriquées à travers tous les segments du réseau et recalculant les chemins de routage optimaux lorsqu'une dégradation est détectée. Le système maintient des estimations de fidélité en temps réel en sacrifiant périodiquement une petite fraction d'états intriqués générés pour la caractérisation tomographique, alimentant ces mesures dans des algorithmes de routage qui équilibrent plusieurs objectifs incluant la minimisation de longueur de chemin, la maximisation de fidélité, et la distribution de charge à travers les liens disponibles. Lorsqu'un lien direct entre deux nœuds tombe en dessous de seuils de fidélité acceptables en raison de perturbations environnementales ou de dérive matérielle, le protocole de routage redirige automatiquement la communication quantique à travers des chemins multi-sauts alternatifs qui exploitent des nœuds intermédiaires pour l'échange d'intrication, assurant une opération continue tout en maintenant la qualité d'intrication de bout en bout au-dessus des exigences d'application. Cette reconfiguration dynamique permet des réseaux quantiques résilients qui s'adaptent aux conditions changeantes sans intervention manuelle ou recalibration complète du système.",
    "C": "Le routage adaptatif d'intrication répond à la dégradation de performance des liens par l'ajustement de la surcharge de correction d'erreurs quantiques, allouant dynamiquement des ressources de correction d'erreurs supplémentaires aux chemins subissant un bruit élevé plutôt que de rerouter le trafic vers des chemins alternatifs. Lorsque la surveillance de fidélité détecte une réduction de qualité de lien, la couche de routage augmente le niveau de redondance des codes de correction d'erreurs quantiques appliqués aux états intriqués traversant les liens affectés, transitant de codes de surface de distance-3 à distance-5 ou activant des mesures de stabilisateur supplémentaires pour maintenir les objectifs de fidélité d'intrication de bout en bout. Cette approche préserve la stabilité de routage en évitant la surcharge de commutation de chemin tout en compensant la dégradation de lien par une atténuation d'erreurs améliorée, bien qu'elle consomme des qubits physiques supplémentaires (augmentant la surcharge de 10x à 25x par qubit logique) et étend la latence d'opération en raison d'exigences d'extraction de syndrome plus profondes. L'adaptation de correction d'erreurs se produit automatiquement dans 2-5 cycles de mesure de syndrome une fois la dégradation détectée, fournissant une protection réactive sans les délais de convergence de routage associés à la reconfiguration de chemin.",
    "D": "Le routage adaptatif d'intrication implémente des modèles prédictifs de qualité de lien basés sur des données historiques de fidélité et des entrées de capteurs environnementaux (température, humidité, vibration), ajustant proactivement les décisions de routage avant que la dégradation observable n'affecte les sessions de communication quantique actives. Les modèles d'apprentissage automatique entraînés sur des mois de données d'opération réseau apprennent les corrélations entre les conditions environnementales et la performance ultérieure des liens, permettant à l'algorithme de routage d'anticiper les réductions de fidélité 10-30 secondes à l'avance et de migrer préventivement le trafic vers des chemins alternatifs. Cette approche prédictive empêche les applications de subir une qualité d'intrication dégradée entièrement, maintenant une fidélité de bout en bout constamment élevée en évitant les liens compromis avant qu'ils n'affectent les opérations quantiques. Le système met continuellement à jour ses modèles prédictifs par apprentissage en ligne à mesure que de nouvelles données de performance s'accumulent, raffinant les motifs de corrélation environnementale et améliorant la précision de prédiction sur la durée de vie opérationnelle du réseau. Cependant, la précision de prédiction dépend de manière critique d'une infrastructure de surveillance environnementale stable, et des perturbations inattendues en dehors de la distribution d'entraînement peuvent causer des échecs de prédiction conduisant à des opportunités de reroutage manquées.",
    "solution": "B"
  },
  {
    "id": 220,
    "question": "Quelle technique sophistiquée fournit une protection contre les attaques de mémoire dans les implémentations cryptographiques quantiques ?",
    "A": "Modules de sécurité matérielle avec sources d'entropie quantique, qui intègrent l'aléatoire classique et quantique pour sécuriser le matériel de clé pendant le stockage et assurer une opération résistante à la falsification même lorsque les adversaires ont un accès physique temporaire au dispositif cryptographique.",
    "B": "Programmes à usage unique avec vérification quantique, une primitive cryptographique qui permet l'exécution d'une fonction exactement une fois en l'encodant dans des états quantiques qui s'autodétruisent lors de la mesure, empêchant ainsi les adversaires de copier le programme grâce au théorème de non-clonage et protégeant contre les attaques de rejeu basées sur la mémoire.",
    "C": "Cryptographie quantique à stockage borné, qui exploite la difficulté fondamentale de stocker de grands états quantiques pour garantir que les adversaires ne peuvent pas conserver suffisamment d'information quantique pour une cryptanalyse ultérieure. Cette technique force les parties honnêtes à mesurer et traiter les données quantiques dans des fenêtres temporelles strictes, garantissant que tout espion dépourvu de ressources de mémoire quantique exponentielles ne peut pas compromettre la sécurité du protocole même avec une puissance de calcul classique illimitée.",
    "D": "Protocoles de transfert inconscient quantiquement sûrs utilisant des engagements basés sur l'intrication",
    "solution": "C"
  },
  {
    "id": 221,
    "question": "Qu'est-ce qui distingue une machine à vecteurs de support quantique (QSVM) d'un estimateur de noyau quantique ?",
    "A": "Les estimateurs de noyau quantique calculent les entrées de la matrice de noyau en utilisant les recouvrements d'états quantiques, mais diffèrent fondamentalement des QSVM en nécessitant un post-traitement par analyse en composantes principales du noyau avant la classification, tandis que les QSVM optimisent directement la frontière de décision dans l'espace des caractéristiques. L'approche par estimateur de noyau projette les états quantiques vers des scores de similarité classiques qui doivent subir une réduction de dimensionnalité pour extraire les caractéristiques discriminantes, alors que les QSVM contournent cette étape intermédiaire en intégrant le noyau dans la formulation d'optimisation duale qui détermine simultanément les vecteurs de support et construit les hyperplans en utilisant la matrice de Gram générée quantiquement et transmise aux solveurs de programmation quadratique classiques.",
    "B": "Les QSVM intègrent les circuits quantiques dans l'ensemble du pipeline de classification en les utilisant à la fois pour calculer la matrice de noyau et pour guider l'optimisation des frontières de décision via des paramètres variationnels, tandis que les estimateurs de noyau quantique servent uniquement de sous-routines quantiques pour évaluer classiquement les fonctions de noyau, puis délèguent toute l'optimisation et la construction des frontières aux solveurs SVM classiques standard qui traitent la matrice de Gram générée quantiquement.",
    "C": "Les estimateurs de noyau quantique évaluent les fonctions de noyau par des mesures de fidélité entre états quantiques encodant les caractéristiques, mais dépendent de manière critique des protocoles de tomographie d'ombre pour reconstruire la matrice de noyau complète avec un coût échantillonnal polynomial, tandis que les QSVM évitent ce goulot d'étranglement de reconstruction en calculant directement les valeurs de noyau à la demande pendant les itérations d'optimisation. Cette distinction architecturale signifie que les estimateurs de noyau doivent préparer exponentiellement de copies d'états pour atteindre une confiance statistique suffisante dans chaque entrée de la matrice de Gram, alors que les QSVM interrogent les valeurs de noyau uniquement pour les candidats vecteurs de support identifiés itérativement par le solveur classique, réduisant le nombre total d'évaluations de circuits quantiques au prix de multiples cycles de communication quantique-classique alternés.",
    "D": "Les estimateurs de noyau quantique produisent des matrices de Gram symétriques définies positives satisfaisant le théorème de Mercer, mais nécessitent un post-traitement classique pour extraire les coefficients duaux définissant la fonction de décision, tandis que les QSVM emploient des circuits quantiques variationnels qui paramètrent directement la frontière de classification et optimisent ces paramètres par descente de gradient sur matériel quantique. L'approche par noyau traite le calcul quantique comme une carte de caractéristiques fixe dont les sorties subissent un entraînement SVM conventionnel, alors que les QSVM ajustent adaptivement les angles des circuits quantiques pour minimiser la perte de classification, intégrant l'optimisation au sein du dispositif quantique lui-même plutôt que de reléguer exclusivement la construction de la frontière à des sous-routines classiques qui traitent des matrices de noyau pré-calculées.",
    "solution": "B"
  },
  {
    "id": 222,
    "question": "Pour quelles applications les modèles génératifs quantiques (QGM) sont-ils utiles ?",
    "A": "Prévisions financières exploitant la capacité des QGM à générer des superpositions de toutes les trajectoires de marché possibles simultanément, évaluant chaque branche par amplification d'amplitude quantique pour identifier les stratégies de trading optimales avec une probabilité exponentiellement supérieure aux méthodes Monte Carlo classiques, éliminant efficacement le risque de baisse dans la construction de portefeuille grâce à une analyse what-if basée sur la superposition qui considère simultanément tous les futurs possibles avant que la mesure ne s'effondre vers le résultat le plus rentable.",
    "B": "Développement de systèmes d'intelligence artificielle générale en exploitant le théorème de non-clonage quantique pour créer des pensées véritablement nouvelles plutôt que de recombiner des motifs existants, encodant les états cognitifs dans des espaces de Hilbert de dimension exponentielle en nombre de qubits pour une capacité mémoire illimitée qui élimine le problème d'oubli affectant l'apprentissage continu classique, atteignant une véritable créativité par échantillonnage de fonction d'onde plutôt que par inférence déterministe.",
    "C": "Simulation de systèmes quantiques pour comprendre la dynamique moléculaire et les matériaux, augmentation d'ensembles de données d'entraînement limités par génération de données quantiques synthétiques, et modélisation de réactions chimiques où les effets quantiques dominent les approches classiques.",
    "D": "Optimisation des chemins de communication dans l'infrastructure de l'internet quantique où les QGM apprennent la topologie des réseaux de distribution d'intrication et génèrent des protocoles de routage qui exploitent la téléportation quantique pour un transfert d'information instantané entre nœuds arbitraires, découvrant des plongements de graphes dans l'espace de Hilbert qui projettent les états du réseau vers des configurations de commutation optimales minimisant la latence par exploration basée sur la superposition de tous les chemins possibles simultanément.",
    "solution": "C"
  },
  {
    "id": 223,
    "question": "Un compilateur optimise un circuit pour un processeur supraconducteur actuel de 27 qubits avec des taux d'erreur typiques : portes à un qubit à 0,05 %, portes à deux qubits à 0,5 %, et temps T1/T2 autour de 100 μs. Le circuit peut être synthétisé de deux façons : la route A utilise 45 portes CNOT avec 12 portes T, tandis que la route B utilise 28 portes CNOT mais nécessite 67 portes T. La question : Pourquoi le compilateur choisirait-il probablement la route B malgré l'augmentation massive du nombre de T, étant donné que les portes T dominent traditionnellement les coûts en ressources dans les architectures tolérantes aux fautes ?",
    "A": "Sur le matériel NISQ sans distillation d'états magiques, le taux d'erreur des portes à deux qubits étant dix fois pire que celui des portes à un qubit signifie que réduire le nombre de CNOT importe plus que le nombre de T. Le nombre inférieur de CNOT de la route B l'emporte probablement sur la fidélité globale du circuit même avec ces portes T supplémentaires, puisque les erreurs de phase des portes à un qubit sont relativement peu coûteuses comparées aux défaillances de portes intriquantes qui peuvent corrompre plusieurs qubits simultanément et se propager en cascade dans le calcul.",
    "B": "Les portes T se compilent en rotations de phase à un qubit implémentables comme portes virtual-Z sur matériel supraconducteur, coûtant essentiellement zéro erreur physique puisqu'elles sont purement des mises à jour de référentiel dans le logiciel de contrôle. Les compilateurs supraconducteurs modernes décomposent les circuits Clifford+T en impulsions physiques où les portes T deviennent des transformations de référentiel suivies classiquement, évitant toute interaction physique de qubit. La route B échange des portes physiques à deux qubits coûteuses contre un suivi de phase logiciel uniquement, rendant le nombre de T essentiellement non pertinent pour la fidélité du circuit tandis que la réduction de CNOT améliore directement la probabilité de succès.",
    "C": "Les effets de diaphonie entre portes CNOT simultanées causent des erreurs corrélées qui évoluent de manière sur-linéaire avec le nombre de portes à deux qubits, tandis que les portes à un qubit s'exécutent indépendamment sans diaphonie spectateur. Le nombre plus élevé de CNOT de la route A augmente la probabilité de nécessiter des opérations parallèles à deux qubits pendant l'exécution du circuit, ce qui déclenche une diaphonie de flux entre qubits couplés qui n'est pas capturée dans les taux d'erreur de porte isolés. L'architecture de la route B permet une parallélisation plus agressive des portes T à un qubit tout en sérialisant moins de CNOT, réduisant la profondeur totale du circuit et l'accumulation d'erreurs corrélées malgré un nombre de portes plus élevé.",
    "D": "Les budgets de portes limités par la cohérence sur les dispositifs NISQ font de la profondeur du circuit la métrique critique plutôt que le nombre de portes, et les portes à un qubit s'exécutent 10× plus rapidement que les CNOT, permettant à la route B de se terminer en moins de temps réel malgré plus d'opérations totales. La réduction de 17 CNOT économise environ 3,4 microsecondes à 200ns par CNOT, tandis que l'ajout de 55 portes T coûte seulement 1,1 microsecondes à 20ns par porte de phase à un qubit, résultant en une route B se terminant avant que la décohérence ne dégrade significativement la fidélité de l'état, le compromis profondeur-fidélité favorisant les circuits plus courts même lorsque le nombre abstrait de portes augmente.",
    "solution": "A"
  },
  {
    "id": 224,
    "question": "Dans les codes LDPC à produit hypergraphe, quelle caractéristique rend le décodage par propagation de croyance attractif comparé aux décodeurs de code de surface ?",
    "A": "Les opérateurs logiques dans les codes à produit hypergraphe sont strictement locaux dans des voisinages de rayon constant, éliminant complètement le besoin de suivre de longues chaînes ou structures filiformes pendant le traitement du syndrome et garantissant que la propagation d'erreur est confinée à des patches de taille fixe, réduisant considérablement à la fois les besoins en mémoire et la latence comparé aux algorithmes de couplage parfait de poids minimum.",
    "B": "Le faible poids de contrôle de parité permet un passage de messages parallèle avec une complexité évoluant linéairement en longueur de bloc plutôt que l'évolution cubique ou quasi-cubique du couplage parfait de poids minimum, permettant des implémentations de décodeur distribuées qui traitent l'information de stabilisateur simultanément à travers le graphe de contrôle.",
    "C": "Le biais d'erreur physique vers des types de Pauli spécifiques peut être ignoré en toute sécurité puisque la performance du code est prouvablement indépendante de l'asymétrie X-Z sous la dynamique de propagation de croyance.",
    "D": "Les qubits ancillaires sont entièrement éliminés du circuit d'extraction de syndrome grâce à l'utilisation de mesures de parité conjointes qui lisent directement les valeurs propres de stabilisateur sans stockage intermédiaire, réduisant à la fois le surcoût en qubits et la susceptibilité aux erreurs de préparation des ancilles.",
    "solution": "B"
  },
  {
    "id": 225,
    "question": "Comment le protocole de distribution de clés quantiques sécurisé contre les attaques par canaux auxiliaires atténue-t-il spécifiquement les attaques par division du nombre de photons ?",
    "A": "Filtrage de post-sélection de phase qui exploite les effets d'interférence quantique pour distinguer les états à photon unique des composantes multi-photoniques dans les impulsions transmises, utilisant l'interférométrie Hong-Ou-Mandel à haute visibilité au récepteur pour mesurer l'indiscernabilité des photons.",
    "B": "Références d'état de vide transmises dans des bins temporels aléatoirement intercalés qui servent de mesures de référence pour les comptes sombres du détecteur et la perte de canal, permettant au protocole de borner statistiquement le nombre maximum de photons présent dans les impulsions de signal.",
    "C": "Un contrôle précis de la longueur d'onde garantit que toute tentative de division introduit des aberrations chromatiques détectables dans le canal.",
    "D": "Implémentation d'états leurres où l'émetteur varie aléatoirement le nombre moyen de photons des impulsions transmises entre états de signal et multiples intensités leurres, incluant des états cohérents faibles et le vide. En comparant les statistiques de détection à travers différents niveaux d'intensité, les parties légitimes peuvent borner la fuite d'information des composantes multi-photoniques puisqu'un espion effectuant des attaques par division du nombre de photons produira des motifs de taux de détection différents pour les impulsions de signal versus leurres. Cette analyse statistique révèle la présence de photons interceptés car l'espion ne peut distinguer les états leurres des états de signal avant l'attaque, forçant des corrélations détectables qui violent les caractéristiques de perte attendues d'un canal quantique non altéré.",
    "solution": "D"
  },
  {
    "id": 226,
    "question": "Quelle ressource quantique permet à l'algorithme de Grover d'obtenir son accélération ?",
    "A": "La capacité de la transformée de Fourier quantique à résoudre les composantes fréquentielles dans la réponse de l'oracle, convertissant efficacement la représentation du problème de recherche du domaine spatial vers un domaine fréquentiel où l'élément marqué apparaît comme un pic distinct. En appliquant la QFT après chaque appel à l'oracle, l'algorithme effectue une analyse spectrale qui isole la fréquence caractéristique de la solution, similaire à la façon dont l'algorithme de Shor utilise la recherche de période, permettant une identification rapide par analyse harmonique plutôt que par énumération exhaustive.",
    "B": "La superposition à travers tout l'espace de recherche, qui permet à l'algorithme d'évaluer toutes les solutions candidates simultanément en une seule requête à l'oracle. Ce parallélisme quantique signifie qu'au lieu de vérifier N éléments séquentiellement, l'approche de Grover examine chaque élément à la fois dans l'état superposé.",
    "C": "L'estimation de phase des valeurs propres de l'oracle, qui permet l'extraction de la signature spectrale de l'élément marqué par raffinement itératif du signal de phase kickback. En mesurant la phase accumulée avec une précision suffisante à travers de multiples applications contrôlées de l'oracle, l'algorithme peut identifier quel état de la base de calcul correspond à la solution sans évaluer explicitement toutes les possibilités, obtenant ainsi l'accélération quadratique par décomposition spectrale plutôt que par manipulation d'amplitude.",
    "D": "L'amplification d'amplitude, qui augmente systématiquement l'amplitude de probabilité de l'état marqué tout en diminuant les amplitudes des non-solutions par application répétée de l'opérateur de Grover. Ce processus itératif d'inversion autour de la moyenne fait pivoter le vecteur d'état quantique vers la cible, ne nécessitant que O(√N) itérations pour atteindre une probabilité proche de l'unité de succès de mesure.",
    "solution": "D"
  },
  {
    "id": 227,
    "question": "Quel est un risque majeur introduit par les attaques par canaux auxiliaires dans les systèmes de distribution quantique de clés (QKD) utilisés pour la sécurité des dispositifs IoT ?",
    "A": "Contourner l'authentification via des discordances d'intrication, où un adversaire exploite la préparation imparfaite de paires de Bell ou une légère désynchronisation entre émetteur et récepteur pour injecter des états malveillants qui passent le test d'inégalité CHSH mais portent des bits de clé modifiés. Dans les implémentations QKD pratiques pour l'IoT, les ressources de calcul limitées sur les dispositifs périphériques signifient que la vérification d'intrication est souvent effectuée avec des tailles d'échantillon réduites pour économiser l'énergie et la latence.",
    "B": "Ralentir le post-traitement classique en injectant des délais de calcul pendant les étapes de correction d'erreur et d'amplification de confidentialité, ce qui peut forcer les dispositifs IoT à mettre en mémoire tampon du matériel de clé partiellement traité dans une mémoire non protégée ou déclencher un repli basé sur un délai d'attente vers un chiffrement classique plus faible. Étant donné que les preuves de sécurité QKD supposent un post-traitement classique instantané, tout délai qui prolonge la fenêtre entre le tamisage de la clé brute et l'extraction finale de la clé crée une opportunité d'extraction par canal auxiliaire ou d'injection de fautes.",
    "C": "Fuite de matériel de clé par émissions matérielles — des observables physiques comme la gigue temporelle des détecteurs, les variations de flux de photons, le rayonnement électromagnétique pendant les opérations quantiques, ou les motifs de consommation d'énergie pendant la sélection de base peuvent exposer des bits de clé individuels ou des choix de base sans briser le protocole quantique fondamental, permettant à un espion de reconstruire la clé secrète en surveillant les canaux auxiliaires classiques tandis que la couche quantique reste théoriquement sécurisée.",
    "D": "Accès distant via des exploits d'API dans le logiciel de gestion QKD qui contrôle l'appairage des dispositifs, la négociation du débit de clé et l'ajustement des paramètres de canal. De nombreux systèmes QKD commerciaux conçus pour le déploiement IoT exposent des API RESTful ou des interfaces MQTT pour permettre l'orchestration réseau et le provisionnement dynamique de clés à travers de grandes flottes de dispositifs, mais ces plans de contrôle s'exécutent souvent sur les mêmes processeurs embarqués que la pile de traitement quantique, créant des vulnérabilités inter-couches.",
    "solution": "C"
  },
  {
    "id": 228,
    "question": "Pourquoi la fidélité du circuit diminue-t-elle avec une insertion excessive de portes SWAP ?",
    "A": "La diaphonie au niveau des impulsions corrompt les qubits voisins par des chemins SWAP redondants qui créent des canaux de couplage non intentionnels entre des qubits physiquement distants sur la puce. Lorsque plusieurs chaînes SWAP opèrent en parallèle ou lorsque le routage itératif crée des plannings d'impulsions micro-ondes qui se chevauchent, l'interférence électromagnétique résultante génère des interactions à deux qubits parasites qui ne sont pas prises en compte dans le modèle hamiltonien original, conduisant à des fuites vers des états non computationnels et introduisant effectivement une nouvelle classe d'erreurs cohérentes proportionnelles à la densité de SWAP.",
    "B": "Les SWAP détruisent l'intrication à moins de synchroniser avec des réinitialisations de phase, car chaque opération SWAP applique une rotation non triviale dans l'espace de Hilbert à deux qubits qui désaligne les phases relatives entre les paires de Bell. Sans recalibrage explicite de la référence de phase globale, la dérive de phase accumulée provoque une décorrélation.",
    "C": "Les calendriers de calibrage supposent une séquence de portes fixe et échouent lorsque le nombre de SWAP domine la profondeur du circuit, invalidant les corrections précalculées qui ont été optimisées pour le motif de connectivité original. Les systèmes supraconducteurs modernes reposent sur des impulsions de contrôle soigneusement chronométrées dont les matrices de compensation de diaphonie deviennent inexactes lorsque l'ordre des portes change substantiellement, causant des erreurs systématiques qui se composent quadratiquement avec le nombre de SWAP insérés plutôt que linéairement comme le prédiraient les modèles naïfs.",
    "D": "Chaque SWAP se décompose en trois portes CNOT sur le matériel, et puisque chaque porte à deux qubits introduit de la décohérence et des erreurs de contrôle, la probabilité d'erreur cumulée croît linéairement avec le nombre de SWAP. Avec des fidélités de porte à deux qubits typiques autour de 99%, même une chaîne modeste de 10 SWAP peut dégrader la fidélité globale du circuit de plusieurs pourcents par cette accumulation d'erreur multiplicative.",
    "solution": "D"
  },
  {
    "id": 229,
    "question": "Comment un réseau antagoniste génératif quantique (QGAN) se compare-t-il à un GAN classique ?",
    "A": "Les QGAN utilisent l'interférence quantique entre les états de la base de calcul pour permettre l'estimation de gradient par des règles de déplacement de paramètres plutôt que par rétropropagation, mais l'effondrement de la mesure à chaque itération d'entraînement restreint l'espace d'hypothèses accessible à un sous-espace dont la dimension évolue seulement polynomialement avec le nombre de qubits plutôt qu'exponentiellement, annulant l'avantage supposé de la superposition.",
    "B": "Les QGAN exploitent la superposition quantique et l'intrication pour explorer des espaces d'hypothèses exponentiellement plus grands pendant l'entraînement, permettant une capture plus efficace de distributions de probabilité complexes.",
    "C": "Les QGAN encodent le générateur comme un circuit quantique paramétré dont les amplitudes de l'état de sortie représentent directement la distribution de probabilité cible, mais la relation quadratique de la règle de Born entre amplitudes et probabilités de mesure introduit un biais systématique dans les estimations de gradient que les GAN classiques évitent par échantillonnage direct, nécessitant un nombre exponentiel de coups de mesure pour atteindre une précision statistique comparable.",
    "D": "Les QGAN exploitent l'amplification d'amplitude quantique au sein du réseau discriminateur pour obtenir une accélération quadratique dans la distinction entre échantillons générés et réels, mais cet avantage ne s'applique que lorsque la fidélité de sortie du générateur dépasse déjà 75%, en dessous de quoi l'opérateur d'amplification d'amplitude échoue à interférer constructivement et l'accélération disparaît, nécessitant typiquement des régimes d'entraînement hybrides classique-quantique.",
    "solution": "B"
  },
  {
    "id": 230,
    "question": "Pourquoi le surcoût classique est-il minimal lorsque les coupures s'alignent avec les goulots d'étranglement du réseau ?",
    "A": "Moins de corrélations inter-sous-circuits nécessitent un suivi aux liens de bande passante rares, ce qui minimise la communication classique requise pour reconstruire l'état quantique complet, puisque les frontières de goulot d'étranglement correspondent naturellement à des régions de faible entropie d'intrication dans les circuits quantiques typiques.",
    "B": "Lorsque les coupures s'alignent avec les frontières de goulot d'étranglement, les matrices de densité réduites se factorisent plus proprement en raison de la localité des opérations quantiques près de ces régions de connectivité rare, permettant aux algorithmes de post-traitement classique de reconstruire la sortie complète du circuit en utilisant des décompositions tensorielles avec un rang de Schmidt plus faible. Cette réduction de rang diminue à la fois le nombre de configurations de mesure requises et la mémoire classique nécessaire pour stocker les fonctions de corrélation, puisque chaque configuration contribue moins de termes à la somme finale de la valeur d'espérance.",
    "C": "Les goulots d'étranglement du réseau partitionnent naturellement les circuits en sous-circuits avec une charge de calcul approximativement équilibrée, ce qui permet le traitement classique parallèle des résultats de mesure de différents sous-circuits sans surcoût de synchronisation. Cet équilibrage de charge garantit que l'algorithme de reconstruction classique passe un temps minimal à attendre que les sous-circuits plus lents se terminent, réduisant ainsi le temps total d'horloge pour l'assemblage d'état même lorsque le volume de communication brut reste comparable aux coupures non-goulot d'étranglement.",
    "D": "Positionner les coupures aux emplacements de goulot d'étranglement exploite la frontière quantique-classique plus efficacement car ces régions présentent une dimensionnalité d'intrication plus faible — le nombre effectif de coefficients de Schmidt contribuant significativement à l'entropie de bipartition. Cette réduction de dimensionnalité permet à la simulation classique de représenter les corrélations inter-coupures en utilisant des bases de mesure compressées avec moins de configurations par qubit de coupure, puisque la majorité du spectre d'intrication au-delà du goulot d'étranglement décroît exponentiellement et contribue de manière négligeable aux statistiques observables, réduisant ainsi les coûts d'échantillonnage et de stockage proportionnellement au facteur de concentration spectrale.",
    "solution": "A"
  },
  {
    "id": 231,
    "question": "Pour la simulation de Hamiltoniens clairsemés, la formule du produit de Lie sert souvent de méthode de référence, mais les normes élevées peuvent être traitées plus efficacement par :",
    "A": "La transformation du problème de simulation Hamiltonienne quantique en un processus stochastique classique équivalent en encodant l'opérateur d'évolution comme une matrice de probabilités de transition pour une marche aléatoire sur un espace d'états exponentiellement grand. Cette correspondance exploite les similitudes structurelles entre l'évolution unitaire temporelle et la dynamique des chaînes de Markov, permettant aux techniques d'échantillonnage classique d'approximer les valeurs d'espérance quantiques. Bien que l'espace d'états croisse exponentiellement avec le nombre de qubits, la structure clairsemée du Hamiltonien se traduit directement par des matrices de transition clairsemées, permettant des méthodes classiques d'intégrales de chemin efficaces qui surpassent les approches quantiques lorsque la norme est grande.",
    "B": "La construction de circuits quantiques paramétrés avec des ansätze variationnels de faible profondeur qui approximent l'opérateur d'évolution temporelle par optimisation, évitant ainsi la croissance exponentielle associée à la décomposition de Trotter. Cette approche exploite le prétraitement classique pour identifier des structures de circuits de faible profondeur qui capturent la dynamique essentielle, particulièrement efficace lorsque la norme élevée du Hamiltonien est dominée par un petit nombre de termes fortement pondérés. Le cadre variationnel permet à l'algorithme d'allouer de manière adaptative les ressources de calcul aux termes de couplage les plus significatifs tout en traitant les interactions plus faibles de manière perturbative.",
    "C": "L'implémentation d'une séquence d'opérations de permutation de Suzuki qui échangent systématiquement la population entre les sous-espaces propres de haute et basse énergie du Hamiltonien, partitionnant efficacement le spectre en sous-domaines gérables. Cette approche de décomposition spectrale exploite l'observation que les normes Hamiltoniennes élevées proviennent souvent de larges écarts d'énergie plutôt que de structures de couplage complexes. En alternant entre l'évolution dans les sous-espaces et le mélange inter-sous-espaces, la méthode atteint une complexité de portes qui évolue avec le logarithme de la norme plutôt que linéairement, à condition que les niveaux d'énergie satisfassent certaines propriétés d'ordre.",
    "D": "Le traitement du signal quantique (quantum signal processing) combiné avec la qubitisation du Hamiltonien, ce qui permet une complexité de requêtes évoluant avec des facteurs logarithmiques.",
    "solution": "D"
  },
  {
    "id": 232,
    "question": "Quelle caractéristique du cadre ECDQC lui permet de surpasser les compilateurs LNN de référence ?",
    "A": "Il exploite les qubits pendants (positions inutilisées au-delà de la largeur du circuit dans le réseau linéaire) comme ressources ancillaires pour implémenter des gadgets de portes tolérants aux pannes, permettant l'exécution d'opérations multi-qubits avec une fidélité plus élevée grâce à la détection d'erreurs comparé aux portes directes entre plus proches voisins. En utilisant stratégiquement ces qubits auxiliaires pour encoder des opérations logiques protégées lors de la synthèse de portes, le cadre réduit les taux d'erreur et maintient la fidélité du circuit tout en respectant la contrainte de connectivité linéaire.",
    "B": "Il exploite les qubits pendants (positions inutilisées dans le réseau linéaire) comme points de relais intermédiaires de routage, permettant l'exécution d'opérations multi-qubits avec moins de portes SWAP totales comparé aux stratégies de compilation standard entre plus proches voisins. En utilisant stratégiquement ces positions auxiliaires pour stocker temporairement l'information quantique pendant le routage, le cadre réduit la profondeur du circuit et le nombre de portes tout en maintenant la contrainte de connectivité linéaire.",
    "C": "Il exploite les qubits pendants (positions avec connectivité de degré un aux extrémités du réseau linéaire) comme canaux de raccourci basés sur la mesure, permettant l'exécution d'opérations multi-qubits avec moins de portes SWAP totales en téléportant les états quantiques à travers le réseau. En utilisant stratégiquement ces positions terminales pour générer des paires de Bell et implémenter des portes non-locales par échange d'intrication, le cadre réduit la profondeur du circuit tout en maintenant la contrainte linéaire de plus proches voisins.",
    "D": "Il exploite les qubits pendants (positions temporairement inactives dans le réseau linéaire pendant les portes agissant sur des régions non-adjacentes) comme ancillas d'extraction de syndrome pour la détection d'erreurs concurrente, permettant l'exécution d'opérations multi-qubits avec tolérance aux pannes intégrée comparé aux portes non-protégées entre plus proches voisins. En utilisant stratégiquement ces positions temporairement inutilisées pour surveiller les vérifications de parité en parallèle avec le calcul, le cadre réduit les taux d'erreur logiques tout en maintenant la contrainte de connectivité linéaire.",
    "solution": "B"
  },
  {
    "id": 233,
    "question": "Comment le Mécanisme d'Attention Quantique (QAM) améliore-t-il les modèles d'apprentissage quantique ?",
    "A": "Il attribue dynamiquement des poids d'importance aux différents états quantiques d'entrée à travers des scores d'attention appris, permettant au modèle de concentrer les ressources de calcul sur les caractéristiques les plus pertinentes tout en supprimant le bruit et l'information non pertinente. Cette emphase sélective améliore l'efficacité de l'extraction de caractéristiques et permet au circuit quantique de prioriser de manière adaptative les canaux d'information en fonction de la tâche spécifique de classification ou de régression.",
    "B": "Il implémente des transformations requête-clé-valeur entraînables à travers des circuits quantiques paramétrés où les scores d'attention émergent de la mesure de la fidélité entre états de requête et de clé, créant une pondération adaptative des états de valeur basée sur le recouvrement des états quantiques. Ce mécanisme permet l'amplification sélective des caractéristiques quantiques pertinentes tout en atténuant les canaux d'information non pertinents par interférence destructive. Cependant, le calcul des scores d'attention nécessite d'effectuer des tests de permutation ou d'autres protocoles d'estimation de fidélité qui consomment des qubits ancillaires et ajoutent une profondeur de circuit linéaire au nombre de têtes d'attention, ce qui peut introduire une disparition du gradient dans le calcul du score d'attention lui-même lorsque le nombre de caractéristiques dépasse environ 2^(d/3), où d est le budget de profondeur de circuit disponible avant que la décohérence ne domine.",
    "C": "Il introduit des rotations contrôlées multi-qubits paramétrées qui modulent le flux d'information entre les couches d'encodeur et de décodeur en fonction de motifs d'attention appris, où les poids d'attention sont encodés comme des angles de rotation déterminés par les produits scalaires entre les amplitudes des états de requête et de clé. Le mécanisme amplifie sélectivement les caractéristiques pertinentes par interférence quantique constructive des états auxquels on prête attention tout en supprimant l'information non pertinente via interférence destructive. Cependant, l'extraction des scores d'attention nécessite de mesurer les valeurs d'espérance d'observables non-commutatives (spécifiquement, les composantes X et Y nécessaires pour calculer les poids d'attention à valeurs complexes), ce qui nécessite des exécutions de circuit séparées pour chaque observable et augmente le nombre total de mesures d'un facteur égal à la dimension de la tête d'attention, limitant fondamentalement l'approche aux espaces d'attention de faible dimension à l'ère NISQ où les budgets de mesures contraignent la précision statistique.",
    "D": "Il applique une sélection adaptative de caractéristiques quantiques en implémentant des portes paramétriques pondérées par attention qui modulent la force de couplage entre différents registres de qubits encodant les caractéristiques d'entrée, où les scores d'attention contrôlent les angles de rotation des portes RY qui déterminent avec quelle force chaque caractéristique d'entrée contribue à la représentation cachée. Cela crée une importance dynamique des caractéristiques par manipulation d'états quantiques, permettant au modèle de concentrer les ressources de calcul sur l'information pertinente. Les poids d'attention sont implémentés comme des paramètres entraînables dans le circuit quantique qui sont optimisés pendant l'entraînement par descente de gradient sur la fonction de perte classique, mais cette approche nécessite que les scores d'attention restent limités dans [-π, π] pour maintenir l'implémentabilité des portes, ce qui contraint la gamme dynamique d'importance des caractéristiques et peut causer des effets de saturation où les caractéristiques très pertinentes ne peuvent pas être suffisamment amplifiées par rapport au bruit lorsque leur importance réelle dépasse cette plage angulaire.",
    "solution": "A"
  },
  {
    "id": 234,
    "question": "Parmi les méthodes suivantes, laquelle est la plus efficace pour réduire les erreurs de diaphonie (crosstalk) en informatique quantique ?",
    "A": "L'application d'impulsions de portes plus fortes permet à l'opération du qubit ciblé de dominer sur les termes de couplage parasites, noyant efficacement les signaux de diaphonie par simple avantage d'amplitude. En augmentant la fréquence de Rabi du champ de contrôle au-delà de la force de couplage entre qubits voisins, on peut s'assurer que la transition cible est entraînée beaucoup plus rapidement que les transitions indésirables ne peuvent s'accumuler, supprimant ainsi la diaphonie à des niveaux négligeables sans nécessiter d'ingénierie d'impulsions sophistiquée.",
    "B": "Atteindre une isolation complète nécessiterait d'éliminer tous les Hamiltoniens de couplage entre qubits, ce qui non seulement va à l'encontre du but de construire un processeur quantique (puisque les portes à deux qubits reposent sur des interactions contrôlées) mais est aussi physiquement irréalisable étant donné que les systèmes quantiques interagissent intrinsèquement par des champs électromagnétiques, des modes de phonons ou d'autres mécanismes de couplage.",
    "C": "Les techniques de mise en forme d'impulsions qui contrôlent précisément les opérations de qubits et minimisent les interactions non intentionnelles représentent l'approche la plus efficace. En concevant des formes d'onde de contrôle avec des enveloppes lisses, une sélectivité en fréquence et des séquences de portes soigneusement chronométrées, ces méthodes peuvent supprimer les excitations hors résonance de qubits voisins tout en maintenant une haute fidélité sur le qubit cible. Des techniques avancées comme les impulsions DRAG (derivative removal by adiabatic gate) annulent activement les transitions indésirables qui causent la diaphonie, atteignant des fidélités de portes dépassant 99,9% dans les systèmes supraconducteurs et à ions piégés modernes.",
    "D": "La conception de topologies de qubits hautement connectées où chaque qubit se couple à de nombreux voisins dilue la diaphonie à travers le réseau par des effets de moyennage statistique, réduisant les points chauds d'erreurs localisés et améliorant les fidélités des portes individuelles.",
    "solution": "C"
  },
  {
    "id": 235,
    "question": "Quel est l'objectif principal de la simulation Hamiltonienne en informatique quantique ?",
    "A": "Implémenter des décompositions de formules de produit (expansions de Trotter-Suzuki) qui approximent les opérateurs d'évolution temporelle e^(-iHt) en décomposant des Hamiltoniens composites H = Σ_k H_k en séquences d'exponentielles plus simples e^(-iH_k·dt), permettant la simulation quantique numérique de dynamiques continues. Cette décomposition convertit l'évolution différentielle en séquences de portes discrètes, avec une erreur de Trotter évoluant comme O((dt)^2) pour la décomposition de premier ordre, ce qui permet fondamentalement aux ordinateurs quantiques de modéliser des systèmes physiques malgré leur fonctionnement par portes unitaires discrètes.",
    "B": "Modéliser l'évolution temporelle de systèmes quantiques sous des Hamiltoniens physiques, ce qui permet l'étude de processus dynamiques en chimie, physique de la matière condensée et science des matériaux en implémentant l'opérateur unitaire e^(-iHt) sur un ordinateur quantique pour simuler comment les états quantiques évoluent selon l'équation de Schrödinger.",
    "C": "Préparer des états thermiques de Gibbs ρ = e^(-βH)/Z pour des systèmes quantiques à température inverse β en implémentant l'évolution en temps imaginaire e^(-τH) à travers des séquences de portes probabilistes, puis en convertissant en dynamique temporelle réelle. Cela permet des calculs de propriétés d'équilibre comme la capacité thermique et la susceptibilité magnétique dans les systèmes de matière condensée. Les propriétés spectrales du Hamiltonien assurent la convergence vers l'état fondamental lorsque τ→∞, fournissant une borne supérieure variationnelle sur l'énergie fondamentale même avec des circuits de profondeur finie limités par la décohérence.",
    "D": "Calculer les valeurs d'espérance ⟨ψ|O|ψ⟩ d'observables en exploitant l'évolution dans le formalisme de Heisenberg O(t) = e^(iHt)O e^(-iHt), qui transforme les opérateurs indépendants du temps en opérateurs dépendants du temps sans faire évoluer l'état lui-même. Cette approche réduit la profondeur de circuit de O(t/ε) comparé à l'évolution de Schrödinger pour une précision ε, puisque les opérateurs observables ont typiquement une localité inférieure aux états complets du système. Les algorithmes d'estimation de phase extraient ensuite les valeurs propres des opérateurs évolués, permettant des calculs de spectroscopie et d'énergie de l'état fondamental par dynamique basée sur les opérateurs plutôt que sur les états.",
    "solution": "B"
  },
  {
    "id": 236,
    "question": "Quel est le compromis principal lors du choix entre un chemin long à haute fidélité et un chemin court à faible fidélité ?",
    "A": "Les chemins plus longs à travers le réseau quantique nécessitent de synchroniser plusieurs nœuds intermédiaires, chacun introduisant des délais de communication classique pour les protocoles d'échange d'intrication et les mesures d'états de Bell. Bien que ces chemins puissent offrir davantage de ressources physiques en qubits, la latence cumulative résultant de la messagerie classique séquentielle peut dominer le temps de distribution de bout en bout, forçant un choix entre disposer de nombreux qubits disponibles lentement ou de moins de qubits livrés rapidement via des sauts directs courts.",
    "B": "La température et la vitesse des portes évoluent de manière inversement proportionnelle à la distance du réseau en raison des exigences de cohérence sur des canaux de transmission étendus, imposant des conditions de fonctionnement plus froides et des fréquences d'opération plus lentes pour la connectivité longue distance.",
    "C": "Pureté de l'intrication versus débit : les chemins longs à haute fidélité délivrent des états intriqués de meilleure qualité avec une cohérence supérieure et des taux d'erreur plus faibles, mais nécessitent plus de temps et de ressources pour les protocoles de purification. Les chemins courts à faible fidélité offrent une distribution plus rapide et un débit plus élevé mais sacrifient la qualité de l'état, exigeant une correction d'erreurs plus agressive en aval. Le compromis équilibre la vitesse opérationnelle par rapport à la qualité de l'intrication distribuée.",
    "D": "Les chemins de communication quantique plus longs accumulent davantage de bruit de canal et de décohérence, nécessitant des codes de correction d'erreurs quantiques progressivement plus robustes avec des facteurs de redondance plus élevés pour maintenir la fidélité des qubits logiques. Les chemins plus courts subissent moins d'interférences environnementales mais peuvent toujours contenir des liaisons ou nœuds défectueux, exigeant une détection d'erreurs robuste sans la charge complète de correction mise à l'échelle de la distance. Le compromis consiste à décider d'investir les ressources du circuit dans la correction des erreurs de transmission accumulées sur les longues routes ou dans le renforcement contre les défaillances localisées sur les topologies compactes.",
    "solution": "C"
  },
  {
    "id": 237,
    "question": "Quelle technique d'attaque spécifique peut déterminer la structure d'un calcul quantique par observation passive ?",
    "A": "En surveillant les durées précises des opérations de portes quantiques individuelles et en mesurant les intervalles entre les événements de mesure, un adversaire peut construire une empreinte temporelle de l'architecture du circuit, puisque différents types de portes nécessitent des temps d'exécution caractéristiquement différents sur la plupart des plateformes matérielles quantiques.",
    "B": "Les processeurs quantiques modernes utilisent une électronique de contrôle classique qui génère des signatures de puissance distinctes lors de l'exécution de différents types d'opérations de portes, les portes à deux qubits nécessitant généralement des impulsions micro-ondes d'amplitude plus élevée et donc une consommation de puissance instantanée plus importante que les portes à un qubit. Un attaquant ayant accès à des traces de consommation d'énergie échantillonnées avec une résolution nanoseconde peut appliquer des techniques d'analyse différentielle de puissance pour distinguer les types de portes, identifier des motifs de circuits répétés, et déduire des propriétés structurelles telles que la profondeur du circuit, les schémas de connectivité des qubits, et la présence de sous-routines algorithmiques spécifiques comme les transformées de Fourier quantiques.",
    "C": "L'analyse des motifs de fuite micro-ondes exploite le rayonnement électromagnétique inévitablement émis lors des opérations de portes quantiques, car les impulsions de contrôle appliquées aux qubits supraconducteurs génèrent des signatures spectrales caractéristiques qui se propagent au-delà du blindage cryogénique et peuvent être capturées par des antennes sensibles positionnées près du réfrigérateur à dilution, permettant aux adversaires de corréler les motifs de fréquence détectés avec des séquences de portes spécifiques.",
    "D": "Les qubits supraconducteurs fonctionnent à des températures millikelvin à l'intérieur de réfrigérateurs à dilution, et chaque opération de porte dissipe une petite quantité d'énergie mesurable sous forme de chaleur dans le bain thermique. En plaçant des détecteurs bolométriques sensibles à des emplacements stratégiques sur les étages thermiques du réfrigérateur, un adversaire peut surveiller de minuscules fluctuations de température avec une résolution temporelle microseconde pour reconstruire la séquence de portes et la topologie du circuit.",
    "solution": "C"
  },
  {
    "id": 238,
    "question": "Que réalise la décomposition de Shannon quantique dans la synthèse de circuits quantiques ?",
    "A": "Décompose toute opération unitaire à n qubits en une séquence hiérarchique de rotations à un qubit et de portes contrôlées à deux qubits par factorisation récursive basée sur la décomposition de Schmidt. La méthode réduit systématiquement la dimension du problème en extrayant des opérateurs de rotation multiplexés à chaque niveau tout en préservant la transformation unitaire globale à une phase globale près. Cela fournit une preuve d'existence constructive que le calcul quantique universel peut être réalisé avec un ensemble fini de portes, bien que la décomposition entraîne une mise à l'échelle exponentielle du nombre de portes.",
    "B": "Décompose toute opération unitaire à n qubits en une séquence hiérarchique de rotations à un qubit et de portes CNOT à deux qubits par factorisation récursive. La décomposition procède en réduisant systématiquement la taille du problème, en extrayant des opérations contrôlées à chaque niveau tout en préservant la transformation unitaire globale. Cela fournit une preuve constructive que le calcul quantique universel peut être réalisé avec un ensemble fini de portes.",
    "C": "Décompose les unitaires à n qubits en circuits de profondeur optimale utilisant des portes à un qubit et des opérations CNOT en exploitant les bornes théoriques de l'information dérivées du théorème de capacité de canal de Shannon. La décomposition minimise la profondeur du circuit plutôt que le nombre de portes en parallélisant stratégiquement les opérations commutatives sur les couches de qubits. Cette adaptation quantique de la théorie classique de Shannon fournit les bornes de profondeur les plus serrées connues pour synthétiser des unitaires arbitraires, prouvant que la profondeur évolue polynomialement avec le nombre de qubits pour les transformations génériques.",
    "D": "Factorise des unitaires arbitraires à n qubits en rotations à un qubit multiplexées interconnectées par des portes intriquantes à deux qubits via une décomposition cosinus-sinus appliquée récursivement. La procédure isole systématiquement les paramètres angulaires de chaque couche de qubit tout en maintenant la structure unitaire par des rotations de Givens dans des espaces de dimension supérieure. Cela permet la synthèse de toute porte quantique en utilisant uniquement des primitives de rotation et CNOT, bien que le prétraitement classique pour calculer les paramètres de décomposition nécessite une mémoire classique exponentielle pour stocker les facteurs unitaires intermédiaires.",
    "solution": "B"
  },
  {
    "id": 239,
    "question": "De quoi les modèles génératifs quantiques (QGM) ont-ils besoin pour représenter avec précision la distribution des données ?",
    "A": "Une profondeur d'ansatz suffisante et des circuits quantiques paramétrés où l'expressivité évolue avec les couches du circuit et la connectivité des portes intriquantes, garantissant que la variété variationnelle contient les distributions cibles via des transformations unitaires.",
    "B": "Suffisamment de données d'entraînement et accès au matériel quantique",
    "C": "Un alignement de noyau avec les données classiques via des cartes de caractéristiques quantiques et des bases de mesure optimisées par tomographie d'ombre classique pour garantir que les statistiques d'échantillonnage de la règle de Born correspondent aux distributions empiriques.",
    "D": "Une estimation efficace du gradient par des règles de décalage de paramètres ou des méthodes de différences finies combinées avec des stratégies d'atténuation des plateaux stériles, permettant la convergence vers les distributions cibles par des procédures d'optimisation variationnelle.",
    "solution": "B"
  },
  {
    "id": 240,
    "question": "Dans le contexte de la cryptographie post-quantique, quelle approche cryptanalytique avancée constitue actuellement la plus grande menace pour les schémas basés sur les réseaux euclidiens ? Considérez que les attaquants peuvent combiner plusieurs techniques plutôt que de s'appuyer sur un seul algorithme, et que les implémentations pratiques introduisent souvent des vulnérabilités au-delà des hypothèses de dureté mathématique. Le paysage des menaces inclut à la fois des algorithmes purement quantiques et des stratégies hybrides classique-quantique.",
    "A": "Les attaques hybrides quantiques combinant la réduction de réseaux avec la recherche quantique exploitent la synergie entre le prétraitement classique de type BKZ et l'accélération quadratique de recherche de Grover, où les algorithmes classiques réduisent la qualité de la base à un niveau quasi-optimal et la recherche quantique complète efficacement l'étape d'optimisation finale. L'approche menace les paramètres choisis pour une sécurité classique de 128 bits en réduisant effectivement les niveaux de sécurité à environ 64 bits quantiques, rendant les schémas de réseaux actuellement déployés vulnérables une fois que des ordinateurs quantiques de taille modérée avec plusieurs milliers de qubits logiques deviennent disponibles. Les exigences en mémoire restent gérables par rapport aux approches purement quantiques, et la technique représente la menace la plus immédiate car elle combine des algorithmes de réduction classiques matures avec des capacités quantiques à court terme réalisables, nécessitant des augmentations de paramètres défensifs qui impactent significativement les performances et les tailles de clés dans tous les principaux candidats NIST basés sur les réseaux.",
    "B": "Les attaques avancées par canaux auxiliaires ciblant l'échantillonnage gaussien discret et les implémentations de transformée en théorie des nombres extraient les secrets de réseaux par analyse combinée de chronométrage, de puissance et électromagnétique, exploitant le fait que les implémentations à temps constant restent difficiles pour l'échantillonnage par rejet et les opérations en virgule flottante requises dans l'échantillonnage gaussien. Ces attaques menacent les systèmes déployés immédiatement puisqu'elles ne nécessitent aucune ressource quantique et ciblent la structure algorithmique plutôt que mathématique, forçant des reconceptions complètes d'implémentation avec des pénalités de performance substantielles dues aux contre-mesures de masquage et de brassage. Contrairement aux approches purement cryptanalytiques qui affectent abstraitement les paramètres de sécurité, les vulnérabilités par canaux auxiliaires permettent la récupération de clés à partir de dispositifs réels aujourd'hui dans tous les principaux candidats NIST basés sur les réseaux incluant Kyber, Dilithium et FALCON, en faisant la menace pratique la plus immédiate bien qu'elles soient adressables par l'ingénierie plutôt que de nécessiter des changements d'hypothèses de dureté mathématique.",
    "C": "Les algorithmes de criblage quantique dérivés de l'énumération de réseaux atteignent une complexité de 2^(0,2570d + o(d)) pour les problèmes de vecteur le plus court en dimension d en combinant la recherche de Grover avec des techniques classiques de fusion de listes du crible de Nguyen-Vidick, représentant une amélioration quantique sous-exponentielle par rapport à la complexité classique de 2^(0,2925d) mais nécessitant des architectures de mémoire à accès aléatoire quantique qui restent non réalisées. L'approche menace les paramètres choisis pour la sécurité à long terme en réduisant les niveaux de sécurité effectifs plus que la simple application de Grover à BKZ, forçant des augmentations de dimension de réseau et de taille de module qui impactent significativement les performances. L'implémentabilité à court terme dépasse les approches purement quantiques car les exigences QRAM évoluent comme O(2^(0,2d)) plutôt que de maintenir une superposition complète, et la technique représente une menace croissante à mesure que la technologie de mémoire quantique progresse, nécessitant des augmentations de paramètres défensifs dans tous les principaux candidats NIST basés sur les réseaux au cours des prochaines décennies.",
    "D": "Les algorithmes quantiques généralisés pour les problèmes de réseaux dans le pire des cas dérivés d'approximations polynomiales du problème de vecteur le plus court atteignent un avantage quantique par amplification d'amplitude appliquée aux procédures d'échantillonnage classiques, réduisant la complexité d'échantillonnage de 2^O(n) à 2^O(√n) pour des facteurs d'approximation γ = n^c. L'approche menace les hypothèses de sécurité sous-tendant les réductions du pire cas au cas moyen qui justifient la dureté du problème Learning With Errors, sapant potentiellement les fondements théoriques de schémas comme Kyber et Dilithium plutôt que d'attaquer directement leurs instances. L'implémentation nécessite de maintenir des états quantiques proportionnels à la dimension du réseau sur des milliers de portes, dépassant les temps de cohérence actuels mais restant plus proche des capacités à court terme que les algorithmes de factorisation à l'échelle de Shor. Cela représente la menace la plus fondamentale car elle remet en question les hypothèses de dureté computationnelle elles-mêmes plutôt que de cibler des choix de paramètres spécifiques, nécessitant potentiellement des fondements mathématiques entièrement nouveaux pour la cryptographie post-quantique basée sur les réseaux.",
    "solution": "A"
  },
  {
    "id": 241,
    "question": "Quelle méthodologie d'attaque avancée peut compromettre la sécurité des protocoles d'empreinte quantique ?",
    "A": "Les protocoles d'empreinte quantique encodent des données classiques en états quantiques exponentiellement plus courts par des fonctions de hachage qui transforment des chaînes de N bits en états de log(N) qubits, mais ces fonctions de hachage quantiques sont vulnérables aux attaques par collision de type paradoxe des anniversaires. Plus précisément, un adversaire peut préparer une grande base de données d'empreintes quantiques précalculées et effectuer une recherche de collision améliorée par Grover en temps O(2^(n/3)) plutôt que O(2^(n/2)) classiquement, où n est la longueur de l'empreinte. En trouvant deux entrées distinctes qui produisent des empreintes orthogonales avec un produit scalaire élevé (quasi-collisions), l'adversaire peut forger des messages qui passent le test d'égalité quantique même lorsque les données classiques sous-jacentes diffèrent, compromettant les garanties d'intégrité du protocole.",
    "B": "Le test SWAP, utilisé dans l'empreinte quantique pour déterminer si deux états quantiques sont identiques en mesurant un qubit de contrôle après des opérations SWAP contrôlées, présente des vulnérabilités d'implémentation dues à des fidélités de portes imparfaites et à une imprécision temporelle. Un adversaire peut exploiter ces faiblesses en injectant du bruit calibré pendant les portes SWAP contrôlées qui décale systématiquement les statistiques de mesure — par exemple, en ajoutant une petite rotation au qubit de contrôle qui biaise les résultats vers un rapport d'égalité même pour des empreintes distinctes. En ajustant soigneusement cette injection basée sur des informations de timing divulguées ou une analyse par canaux auxiliaires des impulsions de contrôle de portes, l'attaquant peut provoquer des rapports d'égalité faux positifs avec une probabilité significativement supérieure au taux d'erreur conçu du protocole.",
    "C": "La discrimination d'états approximative utilisant des mesures généralisées permet à un adversaire de distinguer partiellement des empreintes quantiques non orthogonales avec une probabilité dépassant les bornes de sécurité prévues du protocole.",
    "D": "Dans les implémentations à états cohérents de l'empreinte quantique, où les empreintes sont encodées sous forme d'impulsions cohérentes faibles |α⟩ avec une amplitude α << 1, un adversaire peut effectuer une détection homodyne ou hétérodyne sur les états transmis pour extraire des informations d'amplitude et de phase. En mesurant de manière répétée les composantes de quadrature X = (a + a†)/2 et P = (a - a†)/2i à travers de nombreuses exécutions du protocole avec la même empreinte, l'analyse statistique des distributions de quadrature révèle l'amplitude complexe α, effectuant efficacement une tomographie d'état. Puisque la sécurité de l'empreinte repose sur le théorème de non-clonage empêchant la copie d'amplitude, cette attaque par analyse d'amplitude contourne les protections quantiques en utilisant des statistiques de mesure plutôt que le clonage, permettant la reconstruction des valeurs d'empreinte.",
    "solution": "C"
  },
  {
    "id": 242,
    "question": "Quel est l'objectif principal de l'introduction de la régularisation de variance dans l'entraînement des réseaux de neurones quantiques ?",
    "A": "Un entraînement plus stable est obtenu en atténuant la concentration exponentielle des gradients qui se produit dans les espaces de paramètres de haute dimension, où la régularisation de variance pénalise le moment second de la distribution des gradients pour empêcher l'optimiseur d'échantillonner des mises à jour de paramètres situées dans les queues d'une distribution de bruit à queue lourde. En contraignant la variance des estimations de gradient à travers différentes bases de mesure, la méthode garantit que la matrice d'information de Fisher empirique reste bien conditionnée tout au long de l'entraînement, ce qui stabilise la convergence même lorsque le paysage de perte présente les propriétés spectrales caractéristiques du phénomène de plateau stérile.",
    "B": "Augmenter l'expressivité du circuit quantique en imposant une dispersion minimale dans le spectre de valeurs propres de l'unitaire paramétré, puisque la régularisation de variance empêche efficacement l'effondrement du circuit en opérateurs de rang faible qui ne couvrent qu'un petit sous-espace de l'espace de Hilbert total. Cette contrainte sur la concentration spectrale provient de la pénalisation des configurations de paramètres où des applications répétées de portes produisent des unitaires avec des phases propres regroupées, forçant ainsi l'ansatz à maintenir une diversité suffisante dans son action sur les états de base computationnelle et permettant l'approximation d'une classe plus large d'unitaires cibles à travers l'algèbre de Lie dynamique générée par les portes paramétrées.",
    "C": "Réduire la variance de mesure dans les valeurs d'espérance empêche les estimations de gradient de devenir trop bruitées pendant les mises à jour de paramètres, ce qui permet à l'optimiseur de converger de manière plus fiable même lorsque le bruit de grenaille est significatif. En pénalisant les fluctuations dans les observables mesurées, la régularisation de variance garantit que le signal d'entraînement reste suffisamment fort pour guider le processus d'optimisation vers de meilleures solutions sans être submergé par le bruit statistique provenant d'un échantillonnage fini.",
    "D": "Améliorer la génération d'intrication à travers le registre quantique en introduisant un terme de régularisation qui pénalise explicitement les états séparables dans la variété variationnelle, où la régularisation de variance calcule la pureté moyenne de toutes les matrices de densité réduites bipartites et ajoute une pénalité proportionnelle aux écarts par rapport à l'état maximalement mélangé. Ce mécanisme ajuste dynamiquement le pouvoir d'intrication effectif des portes paramétrées en modifiant le paysage de perte pour favoriser les configurations hautement intriquées, ce qui élargit à son tour la capacité expressive de l'ansatz en garantissant que le circuit explore des corrélations véritablement quantiques plutôt que de rester confiné à des mélanges probabilistes classiques pendant l'entraînement.",
    "solution": "C"
  },
  {
    "id": 243,
    "question": "Dans la conception de circuits quantiques pour les tâches d'apprentissage automatique, pourquoi les portes de rotation RX, RY et RZ sont-elles couramment utilisées ?",
    "A": "Elles fournissent des paramètres ajustables qui permettent un encodage flexible des données classiques dans des états quantiques et facilitent l'optimisation basée sur le gradient pendant l'entraînement. Les angles de rotation servent de paramètres variationnels qui peuvent être ajustés par rétropropagation ou règles de décalage de paramètres, permettant au circuit quantique d'apprendre des motifs complexes dans les données tout en maintenant la différentiabilité pour les algorithmes d'optimisation.",
    "B": "Ces rotations à un qubit génèrent l'algèbre de Lie complète su(2) qui permet un contrôle universel des états de qubits individuels, tandis que leur paramétrisation par des angles continus les rend naturellement compatibles avec les méthodes d'optimisation basées sur le gradient. La structure différentiable lisse des portes permet un calcul efficace des gradients de paramètres via la règle de décalage de paramètres, et leur combinaison avec des opérations d'intrication produit des ansätze variationnels expressifs pour l'apprentissage.",
    "C": "Les portes de rotation fournissent une base complète pour les opérations à un qubit tout en maintenant la compatibilité avec les méthodes d'optimisation par gradient naturel quantique qui exploitent la structure géométrique du circuit. Leur paramétrisation continue permet une atténuation efficace des plateaux stériles par entraînement couche par couche, et la dépendance trigonométrique des valeurs d'espérance aux angles de rotation permet un calcul analytique des gradients sans approximations par différences finies.",
    "D": "Elles forment un ensemble de portes universel pour les opérations à un qubit dont les relations de commutation garantissent que les circuits quantiques résultants satisfont les propriétés de fermeture de l'algèbre de Lie requises pour une optimisation variationnelle efficace. La paramétrisation exponentielle par angles de rotation fournit une régularisation naturelle contre le surajustement en contraignant la variété variationnelle à un sous-ensemble compact du groupe unitaire, tandis que leur structure de produit tensoriel permet des mises à jour de paramètres parallèles à travers plusieurs qubits.",
    "solution": "A"
  },
  {
    "id": 244,
    "question": "Lequel des éléments suivants est un piège courant dans les pratiques d'évaluation comparative en apprentissage automatique quantique ?",
    "A": "Ne jamais comparer avec des références classiques, ce qui laisse les résultats quantiques flotter dans le vide sans contexte significatif. Rapporter des métriques de précision absolues ou des vitesses de convergence ne signifie rien à moins que les chercheurs ne démontrent que les méthodes d'apprentissage automatique classiques comme les forêts aléatoires, les SVM ou les réseaux de neurones ne peuvent pas égaler ou dépasser la performance quantique sur des ensembles de données identiques en utilisant des ressources computationnelles et un temps d'entraînement comparables.",
    "B": "Utiliser des ensembles de données synthétiques spécifiquement construits pour présenter des structures qui correspondent au biais inductif du modèle quantique — par exemple, générer des données à partir de circuits quantiques ou incorporer des données classiques via un encodage d'amplitude qui crée artificiellement la géométrie d'espace de Hilbert que l'algorithme quantique exploite. Cette conception circulaire garantit le succès de l'approche quantique par construction plutôt que de démontrer un avantage réel, car les modèles classiques optimisés pour la distribution de données originale surpasseraient s'ils étaient testés sur des échantillons réels dépourvus de cette structure quantique artificielle.",
    "C": "Tester exclusivement sur des ensembles de données équilibrés où les populations de classes sont artificiellement égalisées, ce qui masque l'effondrement de performance qui se produit lorsque les modèles quantiques rencontrent le déséquilibre de classes omniprésent dans les applications réelles. Les classificateurs quantiques variationnels présentent un biais sévère envers les classes majoritaires lorsque les probabilités a priori sont biaisées car la règle de Born pondère naturellement les résultats de mesure par l'amplitude au carré, et les objectifs d'entraînement standard n'incorporent pas de pénalités sensibles aux coûts — ainsi les métriques de précision équilibrée rapportées surestiment systématiquement la performance de généralisation dans les scénarios de déploiement pratiques.",
    "D": "Rapporter les résultats uniquement de la meilleure graine aléatoire après avoir exécuté plusieurs entraînements indépendants, ce qui capitalise sur les fluctuations stochastiques dans la navigation des plateaux stériles plutôt que de refléter un comportement algorithmique reproductible. Puisque les circuits quantiques variationnels présentent une sensibilité chaotique à l'initialisation dans les espaces de paramètres de haute dimension, présenter sélectivement le meilleur résultat gonfle la performance perçue tout en dissimulant les modes d'échec typiques où la plupart des graines convergent vers des minima locaux sous-optimaux avec des courbes d'entraînement qui n'échappent jamais aux gradients proches de zéro.",
    "solution": "A"
  },
  {
    "id": 245,
    "question": "Dans le contexte de la mémoire quantique tolérante aux fautes, considérons un qubit logique encodé utilisant un code stabilisateur de type CSS où les portes transversales sont limitées au groupe de Clifford. Supposons qu'un adversaire peut choisir adaptivement quels qubits physiques subissent des erreurs X versus Z après avoir observé les résultats de syndrome des cycles précédents de correction d'erreurs. Quelle est la distinction principale entre la façon dont une erreur de basculement de bit (X) versus une erreur de basculement de phase (Z) se propage à travers les cycles d'extraction de syndrome ultérieurs lorsque le code utilise des mesures de stabilisateurs X et Z séparées ?",
    "A": "Les erreurs X anticommutent avec les stabilisateurs de type Z et sont donc détectées en mesurant ces générateurs, tandis que les erreurs Z anticommutent avec les stabilisateurs de type X et déclenchent ces mesures de syndrome ; cependant, pendant l'extraction de syndrome elle-même, une erreur X sur un qubit de données peut se propager à travers les portes CNOT pour corrompre les qubits ancillaires utilisés pour mesurer les stabilisateurs de type X, provoquant l'interférence des deux canaux d'erreur chaque fois que les circuits de mesure de syndrome partagent des ressources physiques ou des chevauchements temporels dans le calendrier d'extraction.",
    "B": "Les erreurs de basculement de bit sont détectées exclusivement par des vérifications de parité impliquant des produits d'opérateurs de Pauli Z sur les qubits de données, mais elles peuvent se propager aux qubits ancillaires pendant les mesures de stabilisateurs X si le circuit d'extraction utilise des CNOT avec les qubits de données comme contrôles, créant des erreurs corrélées à travers les deux types de syndrome. Les erreurs de basculement de phase déclenchent des violations de stabilisateurs X et se propagent de manière similaire pendant l'extraction de stabilisateurs Z lorsque les qubits de données agissent comme cibles CNOT, provoquant une diaphonie de syndrome qui couple les deux canaux de correction nominalement indépendants.",
    "C": "Les erreurs X basculent l'état de base computationnelle des qubits physiques et sont détectées en mesurant les stabilisateurs de type Z, tandis que les erreurs Z introduisent des décalages de phase relatifs et sont capturées par les mesures de stabilisateurs de type X ; parce que les codes CSS implémentent ces deux circuits d'extraction de syndrome de manière indépendante en utilisant des registres ancillaires et des séquences de mesure séparés, les deux canaux d'erreur restent découplés tout au long du processus de correction sans interférence mutuelle.",
    "D": "Les erreurs de basculement de bit se propagent à travers l'extraction de syndrome en se répandant le long des chaînes CNOT où les qubits de données erronés servent de qubits de contrôle, ce qui se produit pendant les mesures de stabilisateurs Z mais pas pendant les mesures de stabilisateurs X en raison de l'orientation des CNOT dans le circuit d'extraction. Les erreurs de basculement de phase se propagent lorsque les qubits de données erronés agissent comme cibles CNOT pendant les mesures de stabilisateurs X, mais cette propagation est supprimée pendant l'extraction de stabilisateurs Z car les portes de Hadamard précédant ces mesures convertissent les erreurs de phase en basculements de bit qui se propagent dans la direction opposée.",
    "solution": "C"
  },
  {
    "id": 246,
    "question": "Dans un réseau quantique typique, vous devez établir de l'intrication entre des nœuds distants tout en gérant des temps de cohérence limités et des opérations locales imparfaites. Plusieurs chemins peuvent exister, chacun avec des caractéristiques de fidélité et des besoins en ressources différents. Compte tenu de ces contraintes, quelle est la complexité computationnelle fondamentale pour trouver des routes d'intrication optimales ?",
    "A": "Le problème se ramène à un min-cost max-flow lorsqu'il est formulé avec des capacités d'arêtes représentant les taux de génération d'intrication et des coûts reflétant l'inverse de la fidélité, mais le couplage entre la sélection de chemins et l'allocation des ressources de purification aux nœuds intermédiaires introduit des contraintes non linéaires qui brisent la sous-modularité requise pour que les algorithmes d'approximation gloutons fournissent des garanties de performance bornées.",
    "B": "Le problème est NP-difficile car la sélection optimale de chemins sous des seuils de fidélité et des contraintes de ressources implique des choix de chemins disjoints qui interagissent par le biais de goulots d'étranglement de purification partagés et de taux de génération d'intrication limités.",
    "C": "Le routage dans les réseaux quantiques admet un schéma d'approximation en temps polynomial (PTAS) en exploitant le fait que les fonctions réalistes de dégradation de la fidélité sont sous-modulaires sous concaténation — la perte marginale de fidélité due à l'ajout d'un swap supplémentaire diminue avec la longueur du chemin — permettant une approche de programmation dynamique avec élagage de l'espace d'états qui ne conserve que les solutions partielles Pareto-optimales à chaque nœud, atteignant une approximation (1+ε) en temps O(n³/ε²).",
    "D": "La structure en pas de temps discrets imposée par les temps de cohérence finis permet une formulation sous forme de graphe en couches où chaque couche représente un tour d'opération de swap, mais trouver des routes optimales nécessite de résoudre une variante de flot multi-commodités où différentes requêtes de paires de Bell se disputent des ressources de purification partagées, ce qui est résolvable en temps polynomial via des méthodes de points intérieurs uniquement lorsque les rendements de purification sont modélisés comme des fonctions concaves de la fidélité d'entrée — une approximation qui échoue pour les protocoles de distillation réalistes.",
    "solution": "B"
  },
  {
    "id": 247,
    "question": "Pourquoi les erreurs de mesure sont-elles particulièrement difficiles à corriger en informatique quantique ?",
    "A": "Les erreurs de mesure introduisent une incertitude dans les résultats d'extraction de syndromes qui se propage à travers la chaîne d'inférence de l'algorithme de décodage, car les syndromes obtenus à partir de mesures défectueuses n'indiquent plus de manière fiable quelles erreurs se sont produites sur les qubits de données pendant les opérations quantiques précédentes. Si les qubits de syndrome produisent des résultats incorrects avec une probabilité p_m, le décodeur doit distinguer entre les scénarios où un syndrome propre n'indique aucune erreur de données et un syndrome inversé masquant une véritable erreur de données. Cette ambiguïté se compose à travers plusieurs tours d'extraction de syndromes dans les protocoles tolérants aux pannes, obligeant le décodeur à maintenir des distributions de probabilité sur un nombre exponentiellement grand d'histoires d'erreurs plutôt que d'identifier de manière déterministe une seule configuration d'erreur la plus probable.",
    "B": "Les erreurs induites par la mesure corrompent la chaîne de bits classiques extraite des registres quantiques après l'exécution de toutes les portes de calcul, et comme l'information quantique ne peut pas être clonée, il n'y a aucun moyen de vérifier le résultat de la mesure contre des copies redondantes de l'état quantique non mesuré. Contrairement aux erreurs de portes qui s'accumulent pendant l'exécution du circuit où les codes stabilisateurs peuvent les détecter et les corriger par des mesures de syndromes en milieu de circuit, les erreurs de mesure n'apparaissent qu'après que l'état quantique a été irréversiblement projeté, nécessitant soit l'exécution répétée de l'algorithme entier pour recueillir suffisamment de statistiques pour un vote majoritaire à travers plusieurs exécutions, soit le déploiement de techniques classiques d'atténuation d'erreurs qui utilisent des matrices de confusion calibrées pour inférer de manière probabiliste le véritable état pré-mesure.",
    "C": "La lecture défectueuse se produit à l'étape finale après l'application de toutes les portes quantiques, donc vous avez besoin de mesures répétées redondantes ou d'astuces d'atténuation statistique classiques pour la détecter. Contrairement aux erreurs de portes qui se produisent en milieu de circuit où les opérations ultérieures peuvent propager des syndromes vers des qubits ancillaires pour correction, les erreurs de mesure n'apparaissent que lors de l'extraction du résultat computationnel final, nécessitant des techniques de post-traitement telles que le vote majoritaire à travers plusieurs tours de mesure identiques ou le décodage par maximum de vraisemblance basé sur des matrices de confusion de lecture calibrées pour inférer l'état pré-mesure le plus probable à partir des résultats classiques bruités.",
    "D": "Les erreurs de mesure violent les hypothèses du théorème de seuil tolérant aux pannes en créant des motifs d'erreurs corrélés à travers plusieurs qubits qui partagent des circuits de lecture, car dans la plupart des implémentations physiques, plusieurs qubits de données sont mesurés en utilisant des lignes de contrôle partagées ou des étages d'amplification multiplexés qui peuvent subir une mauvaise calibration simultanée. Lorsqu'une impulsion électromagnétique transitoire ou un événement de saturation d'amplificateur affecte le matériel de lecture, cela induit des erreurs de mesure sur des qubits spatialement proches dans le même circuit d'extraction de syndromes, créant des corrélations d'erreurs que les codes de surface supposent indépendantes. Ces défaillances de mesure spatialement corrélées peuvent former des chaînes d'erreurs dépassant la distance du code, défaisant la capacité du décodeur à distinguer les erreurs de faible poids corrigibles des erreurs de poids élevé incorrigibles.",
    "solution": "C"
  },
  {
    "id": 248,
    "question": "Pourquoi est-il nécessaire d'appliquer des changements de base spécifiques avant d'effectuer des mesures sans démolition de produits de Pauli arbitraires ?",
    "A": "Les portes de couplage d'ancillas nécessitent une conjugaison d'opérateur dans la base propre de Z pour les circuits d'extraction de syndromes CNOT",
    "B": "Les qubits physiques mesurent nativement la base computationnelle ; les portes de Hadamard et de phase font pivoter X/Y sous forme mesurable",
    "C": "L'appareillage de mesure se couple aux états propres d'énergie ; les rotations de Clifford mappent les opérateurs cibles sur l'observable Z",
    "D": "Le matériel ne mesure nativement que σ_z ; l'extraction de syndromes basée sur CNOT nécessite que tous les opérateurs de Pauli soient tournés sous forme Z",
    "solution": "D"
  },
  {
    "id": 249,
    "question": "Quelle est la signification des conditions de Knill-Laflamme dans la correction d'erreurs quantiques ?",
    "A": "Ces conditions fournissent des critères nécessaires et suffisants pour qu'un code quantique corrige avec succès les erreurs arbitraires d'un ensemble d'erreurs spécifié — en exigeant que les opérateurs d'erreur préservent l'espace de code ou déplacent les états corrigibles vers des sous-espaces orthogonaux et distinguables, les conditions garantissent que les mesures de syndromes peuvent identifier et inverser de manière unique toutes les erreurs corrigibles sans perturber l'information logique encodée.",
    "B": "Ces conditions fournissent des critères nécessaires pour la correctabilité d'un code quantique en exigeant que les opérateurs d'erreur produisent des syndromes distinguables lorsqu'ils agissent sur les états de code — cependant, la preuve de suffisance nécessite l'hypothèse supplémentaire que les opérations de récupération commutent avec les stabilisateurs du code, une contrainte automatiquement satisfaite pour les codes stabilisateurs mais nécessitant une vérification explicite pour les codes de sous-systèmes où les libertés de jauge permettent des cartes de récupération logiquement équivalentes qui peuvent ne pas préserver la base de mesure d'extraction de syndromes utilisée pendant la phase initiale de détection d'erreurs.",
    "C": "Le cadre de Knill-Laflamme établit des conditions nécessaires et suffisantes pour la correctabilité des erreurs en exigeant que les opérateurs d'erreur préservent la fidélité de l'espace de code ou projettent les états corrigibles dans des sous-espaces de syndromes avec un chevauchement nul — cette exigence d'orthogonalité garantit que l'effondrement de mesure de syndrome n'introduit pas de décohérence supplémentaire dans le sous-espace logique, bien que les conditions supposent des mesures projectives et doivent être modifiées pour des mesures continues faibles ou des protocoles adaptatifs où la rétroaction de mesure effondre partiellement les états de superposition avant que la détermination complète du syndrome ne soit achevée.",
    "D": "Ces conditions spécifient des critères nécessaires et suffisants pour la correctabilité en exigeant que les produits scalaires entre les états de code transformés par erreur satisfassent des relations d'orthogonalité spécifiques — cependant, la formulation originale s'applique uniquement aux ensembles d'erreurs discrets et nécessite une modification pour les canaux d'erreurs continus, où les conditions doivent être remplacées par des contraintes de norme d'opérateur sur les intégrales de chevauchement d'opérateurs de Kraus pour garantir que la carte de correction d'erreurs reste préservant la trace même lorsque les mesures de syndromes distinguent incomplètement entre les amplitudes d'erreur distribuées de manière continue dans la boule d'erreur corrigible.",
    "solution": "A"
  },
  {
    "id": 250,
    "question": "Quel principe de sécurité est violé lorsque la synthèse approximative de circuits quantiques est compromise ?",
    "A": "La confidentialité, car le processus de synthèse approximative divulgue nécessairement des informations sur l'unitaire cible à travers la sélection de séquences de portes et d'angles de rotation, qui peuvent être rétro-conçues par un adversaire surveillant l'étape de compilation. Cette fuite est inhérente à tout procédure d'optimisation qui équilibre la fidélité contre le nombre de portes, car l'évaluation de la fonction de coût révèle des propriétés structurelles de la transformation protégée.",
    "B": "La non-répudiation, puisque la synthèse approximative introduit intrinsèquement une incertitude dans la chaîne de provenance des opérations quantiques — si le circuit implémenté diffère de l'unitaire spécifié par une erreur bornée epsilon, alors ni l'émetteur ni le récepteur ne peuvent prouver cryptographiquement quelle transformation exacte a été appliquée. Cette ambiguïté dans la fidélité des portes compromet toute tentative d'établir un enregistrement infalsifiable des opérations quantiques, rendant impossible de tenir les parties responsables des écarts par rapport au protocole.",
    "C": "L'intégrité, puisque la synthèse approximative introduit des erreurs bornées qui s'accumulent à travers le circuit, permettant potentiellement à un adversaire d'injecter de petites perturbations qui se composent en écarts significatifs de la transformation unitaire prévue. Lorsque les séquences de portes sont optimisées pour la réduction de profondeur, l'approximation résultante crée une fenêtre de vulnérabilité où les modifications des opérations intermédiaires restent non détectées jusqu'à la vérification finale de fidélité, moment auquel le résultat computationnel a déjà été corrompu.",
    "D": "La disponibilité",
    "solution": "C"
  },
  {
    "id": 251,
    "question": "Pourquoi la topologie Zephyr du D-Wave Advantage nécessite-t-elle de nouvelles heuristiques d'incorporation (embedding) par rapport à Chimera ?",
    "A": "La connectivité de la cellule unitaire a changé, donc les distributions de longueurs de chaînes et la disponibilité des coupleurs diffèrent — ce qui modifie les fonctions de coût pour le minor-embedding. Spécifiquement, les nœuds de degré 15 de Zephyr versus les nœuds de degré 6 de Chimera altèrent le compromis entre longueur de chaîne et densité de couplage inter-chaînes, exigeant que les heuristiques équilibrent ces objectifs concurrents différemment lors de la projection de graphes logiques sur la topologie matérielle.",
    "B": "La connectivité de la cellule unitaire est passée de cellules biparties K_{4,4} à des nœuds de degré 15, ce qui modifie la taille maximale de clique incorporable sans chaînes de 4 à 8, changeant fondamentalement l'objectif d'incorporation de la minimisation de longueur de chaîne à l'optimisation de couverture par cliques. Puisque des qubits logiques plus grands peuvent maintenant être incorporés comme chaînes simples au sein de cellules unitaires, les heuristiques doivent prioriser le placement intra-cellule plutôt que le routage inter-cellules, inversant la hiérarchie de fonctions de coût qui fonctionnait pour la structure de connectivité plus sparse de Chimera.",
    "C": "La connectivité de la cellule unitaire a transité vers une configuration d'échelle de Möbius non-planaire où les coupleurs se croisant créent des contraintes topologiques sur les chemins de routage des chaînes, signifiant que les incorporations doivent maintenant satisfaire des conditions de classe d'homologie pour éviter d'introduire des erreurs logiques provenant de motifs de couplage géométriquement frustrés. Cette obstruction topologique nécessite des heuristiques qui calculent les groupes de cohomologie du graphe logique pour assurer l'incorporabilité, remplaçant le problème purement combinatoire de recherche de mineur de Chimera par un problème de topologie algébrique où le placement de chaînes doit respecter les cycles fondamentaux dans la structure de complexe cellulaire du graphe matériel.",
    "D": "La connectivité de la cellule unitaire incorpore des sommets de degré impair qui brisent la propriété de couplage biparti parfait de Chimera, ce qui signifie que la réduction standard du minor-embedding de graphe au couplage pondéré maximal dans les graphes bipartis ne s'applique plus directement. Puisque les nœuds de degré 15 de Zephyr créent une séquence de degrés irrégulière incompatible avec le théorème du mariage de Hall, les heuristiques doivent maintenant résoudre le problème de b-matching plus général où les capacités des sommets varient à travers la topologie, nécessitant des algorithmes d'optimisation combinatoire pondérée plutôt que les procédures de couplage en temps polynomial suffisantes pour la structure uniforme de degré 6 de Chimera.",
    "solution": "A"
  },
  {
    "id": 252,
    "question": "Pourquoi le taux de clé secrète se dégrade-t-il avec la distance dans les systèmes de distribution quantique de clés ?",
    "A": "La complexité du chiffrement augmente linéairement avec la longueur de la liaison, car des distances de transmission plus longues nécessitent des codes de correction d'erreurs plus sophistiqués pour maintenir la sécurité contre les tentatives d'écoute. Chaque kilomètre supplémentaire de fibre ou de transmission en espace libre nécessite des tours supplémentaires d'amplification de confidentialité et de réconciliation d'information, consommant davantage de matériel de clé brute et laissant moins disponible pour la clé secrète finale.",
    "B": "Les protocoles de correction d'erreurs deviennent instables à hauts débits, particulièrement lorsque le taux d'erreur quantique brut dépasse certains seuils qui rendent impossible la distillation de matériel de clé sécurisé sans consommer plus de bits de clé qu'il n'en est généré.",
    "C": "La génération de clés dépend de la ligne de visée satellite, qui devient de plus en plus difficile à maintenir à mesure que la distance entre les stations terrestres augmente au-delà de la limite d'horizon. Les contraintes géométriques de la courbure terrestre signifient que les chemins optiques directs ne sont disponibles que pour des distances relativement courtes, forçant les systèmes QKD à utiliser soit des satellites relais soit des liaisons en espace libre.",
    "D": "La perte de photons augmente avec la distance de transmission dans les fibres optiques et les canaux en espace libre, réduisant le taux auquel les événements de détection valides se produisent et diminuant ainsi le débit de matériel de clé brute disponible pour la distillation.",
    "solution": "D"
  },
  {
    "id": 253,
    "question": "Pourquoi la coloration d'un graphe de commutation aide-t-elle à minimiser le surcoût de SWAP lors de la projection de circuits ?",
    "A": "Les sommets de même couleur identifient des portes qui commutent et peuvent donc être réordonnées ou exécutées en parallèle sans changer la sémantique du circuit, éliminant le besoin de SWAPs pour résoudre des dépendances artificielles — mais la coloration doit utiliser exactement χ(G) couleurs où χ est le nombre chromatique, car utiliser plus de couleurs fragmente les classes de commutation et force le compilateur à insérer des instructions de barrière qui synchronisent l'exécution à travers les frontières de couleur, ce qui augmente le surcoût de SWAP en empêchant l'ordonnanceur d'exploiter la flexibilité complète des réordonnancements commutatifs.",
    "B": "Les sommets de même couleur représentent des portes qui commutent entre elles, signifiant qu'elles peuvent être exécutées en parallèle ou réordonnées librement sans affecter la correction du circuit, donc elles n'ont pas besoin de portes SWAP supplémentaires insérées pour résoudre des conflits d'ordonnancement ou satisfaire des contraintes de connectivité sur la topologie matérielle quantique.",
    "C": "Les sommets de même couleur correspondent à des portes commutantes qui peuvent être librement réordonnées sans altérer la sortie du circuit, donc elles tolèrent un ordonnancement flexible qui évite d'insérer des SWAPs — cependant, l'algorithme de coloration doit respecter la structure de cône causal du circuit, signifiant que les portes opérant sur des qubits dans le cône de lumière futur de l'autre ne peuvent pas partager une couleur même si leurs opérateurs commutent algébriquement, car le réordonnancement temporel de telles portes viole l'ordre partiel du circuit et peut introduire par inadvertance des opérations SWAP durant la phase de projection.",
    "D": "Les portes assignées à des couleurs identiques commutent sous composition et peuvent être ordonnancées dans n'importe quel ordre ou exécutées simultanément, supprimant le besoin de SWAPs pour imposer de fausses dépendances de données — mais cela ne tient que lorsque la coloration du graphe de commutation utilise des nombres chromatiques d'intervalles plutôt que des nombres chromatiques ordinaires, car la coloration de graphe standard ignore les contraintes d'ordonnancement temporel implicites dans les circuits quantiques, et seule la coloration d'intervalles (qui assigne des couleurs aux cliques maximales dans le graphe de chevauchement d'intervalles) identifie correctement les ensembles de portes dont la commutativité permet un réordonnancement sans SWAP sur des architectures avec connectivité de qubit limitée.",
    "solution": "B"
  },
  {
    "id": 254,
    "question": "Une équipe de recherche implémente VQE pour trouver l'énergie de l'état fondamental d'un hamiltonien moléculaire. Ils remarquent qu'après avoir compilé leur circuit ansatz vers l'ensemble de portes natif du matériel, les estimations d'énergie finale convergent lentement et se bloquent souvent. Pendant ce temps, un collègue suggère qu'ils devraient se concentrer sur les hyperparamètres de l'optimiseur classique plutôt que sur le circuit quantique lui-même. Les algorithmes hybrides quantique-classiques tels que VQE utilisent des boucles d'optimisation classiques principalement pour :",
    "A": "Ajuster les paramètres du circuit de manière itérative afin que la valeur d'espérance mesurée de l'état ansatz, lorsqu'évaluée par rapport à l'hamiltonien moléculaire, atteigne un minimum correspondant à l'énergie de l'état fondamental grâce à des méthodes de recherche basées sur le gradient ou sans gradient qui explorent le paysage des paramètres.",
    "B": "Raffiner les paramètres variationnels en minimisant la fonctionnelle d'énergie à travers des campagnes de mesure itératives, où chaque cycle évalue les valeurs d'espérance de l'hamiltonien à des points de paramètres proposés et met à jour ces paramètres via descente de gradient ou méthodes de simplexe pour naviguer vers des régions de plus basse énergie de la variété ansatz jusqu'à ce que les critères de convergence soient satisfaits.",
    "C": "Mettre à jour les paramètres ansatz en évaluant les gradients de fonction de coût calculés à partir de mesures par différences finies des valeurs d'espérance d'énergie, appliquant des schémas de taux d'apprentissage et des termes de momentum pour accélérer la convergence vers des points stationnaires où l'état variationnel approxime l'état propre fondamental de l'hamiltonien cible au sein du sous-espace ansatz.",
    "D": "Optimiser les angles de rotation dans le circuit quantique paramétré en effectuant des recherches sans gradient à travers l'espace de paramètres classique, évaluant la valeur d'espérance d'énergie à chaque ensemble de paramètres candidats à travers des mesures quantiques répétées et sélectionnant des mises à jour de paramètres qui diminuent de manière monotone l'énergie mesurée jusqu'à atteindre un minimum local ou global.",
    "solution": "A"
  },
  {
    "id": 255,
    "question": "Qu'est-ce qui distingue les codes stabilisateurs \"propres\" des codes \"sous-systèmes\" ?",
    "A": "Les codes sous-systèmes partitionnent l'espace de code en qubits logiques plus qubits de jauge où les degrés de liberté de jauge ne stockent pas d'information mais permettent l'extraction de syndrome à travers des opérateurs de corps plus faible, fournissant une flexibilité supplémentaire dans les calendriers de mesure et permettant à certaines erreurs d'être ignorées si elles n'affectent que les sous-systèmes de jauge plutôt que l'information logique encodée.",
    "B": "Les codes stabilisateurs propres exigent que tous les générateurs de stabilisateurs agissent trivialement sur le sous-espace logique à travers la construction du centralisateur, imposant une orthogonalité stricte entre les supports de code et de stabilisateur, tandis que les codes sous-systèmes relâchent cette exigence en introduisant des opérateurs de jauge qui commutent avec les stabilisateurs mais peuvent agir non-trivialement sur un sous-système de jauge. Cette distinction affecte la complexité du décodeur car les erreurs de jauge n'ont pas besoin d'être corrigées, réduisant le poids effectif des opérateurs d'extraction de syndrome.",
    "C": "La différence clé réside dans la façon dont les codes factorisent leur espace de Hilbert complet : les codes propres le décomposent en sous-espaces code ⊗ erreur où tous les stabilisateurs commutent, alors que les codes sous-systèmes ajoutent un facteur de jauge donnant code ⊗ jauge ⊗ erreur, mais les deux types nécessitent toujours de mesurer le groupe de stabilisateurs complet pour extraire les syndromes. La liberté de jauge dans les codes sous-systèmes se manifeste dans la dégénérescence de syndrome où plusieurs chaînes d'erreur produisent des résultats de mesure identiques qui correspondent à des corrections logiquement équivalentes.",
    "D": "Les codes stabilisateurs propres définissent leurs opérateurs logiques comme éléments du normalisateur qui commutent avec tous les stabilisateurs, créant un groupe de Pauli logique unique, tandis que les codes sous-systèmes permettent des opérateurs logiques qui ne commutent avec les stabilisateurs qu'à des transformations de jauge près. Cela permet aux codes sous-systèmes d'implémenter des portes transversales à travers des rotations de secteur de jauge qui violeraient les contraintes de distance dans les codes propres, bien que l'extraction de syndrome nécessite toujours de mesurer le même nombre de générateurs de stabilisateurs que dans les codes propres de distance équivalente.",
    "solution": "A"
  },
  {
    "id": 256,
    "question": "L'écart de complexité entre quantique et classique pour la commutativité de groupe provient du fait que la marche quantique peut :",
    "A": "Détecter les relations non commutatives après avoir évalué moins de produits des générateurs du groupe que ne nécessiteraient les algorithmes classiques d'échantillonnage aléatoire. La marche quantique exploite les motifs d'interférence en superposition sur les éléments du groupe pour sonder efficacement la structure des commutateurs, lui permettant d'identifier les violations de commutativité avec un nombre quadratiquement inférieur d'opérations de groupe par rapport aux approches classiques.",
    "B": "Évaluer les relations de commutateurs sur plusieurs paires de générateurs simultanément grâce au parallélisme quantique sur la structure de présentation du groupe, mais l'avantage fondamental provient en réalité de l'estimation de phase appliquée à la représentation régulière plutôt que de l'interférence dans le graphe de Cayley. En encodant les générateurs comme opérateurs unitaires agissant sur une base de calcul indexée par les éléments du groupe, la marche quantique extrait l'information globale sur les commutateurs à travers le spectre de l'opérateur combiné, tandis que les algorithmes classiques doivent sonder les relations de commutation locales séquentiellement sans accès à cette structure spectrale.",
    "C": "Exploiter l'interférence quantique pour détecter la non-commutativité par évaluation superposée de produits de générateurs, mais le mécanisme d'accélération diffère subtilement de l'amplification d'amplitude standard : la marche quantique construit une superposition cohérente sur les chemins dans le graphe de Cayley, où l'interférence destructive se produit spécifiquement sur les boucles fermées correspondant à des commutateurs triviaux, tandis que l'interférence constructive amplifie les chemins représentant des relations de commutateurs non triviales. Ce motif d'interférence émerge après O(√n) étapes plutôt que O(n) car les chemins pertinents ont une longueur algébrique qui évolue avec le nombre de générateurs, et non l'ordre du groupe.",
    "D": "Sonder la structure des commutateurs en implémentant l'estimation de phase quantique sur la matrice de représentation par permutation du groupe, qui encode les relations de commutativité dans ses dégénérescences de valeurs propres pouvant être extraites quadratiquement plus rapidement que l'analyse spectrale classique. L'idée clé est que les générateurs qui commutent partagent nécessairement des bases propres simultanées dans toute représentation fidèle, donc détecter les chevauchements d'espaces propres par des opérations contrôlées révèle la structure complète des commutateurs sans calculer explicitement les produits de générateurs, réduisant la complexité de O(n²) comparaisons classiques à O(n) requêtes quantiques grâce à des mesures parallélisées d'espaces propres.",
    "solution": "A"
  },
  {
    "id": 257,
    "question": "Comment l'augmentation de l'ordre du développement de Trotter-Suzuki affecte-t-elle la précision de la simulation hamiltonienne numérique pour un temps total fixe ?",
    "A": "Les formules de produit d'ordre supérieur réduisent polynomialement le terme d'erreur de Trotter dominant — de l'ordre (Δt)² à (Δt)⁴, (Δt)⁶, et au-delà — ce qui signifie que beaucoup moins de tranches temporelles sont nécessaires pour atteindre la même précision globale pour un temps d'évolution total fixe. Cette amélioration permet une discrétisation temporelle plus grossière tout en maintenant la fidélité de simulation, réduisant le nombre total de portes et la profondeur du circuit par rapport aux décompositions d'ordre inférieur.",
    "B": "Les formules de Trotter d'ordre supérieur réduisent l'erreur de troncation locale par pas de temps de (Δt)² à (Δt)⁴ ou (Δt)⁶, mais cette amélioration est partiellement compensée par un nombre accru de portes dans chaque pas de Trotter — les formules du quatrième ordre nécessitent environ 5× plus d'exponentielles que le second ordre. Pour les hamiltoniens avec des normes élevées ou de nombreux termes non commutatifs, l'erreur cohérente accumulée de ces portes supplémentaires peut dominer l'erreur de troncation réduite, conduisant à une précision globale pire à moins que les fidélités des portes ne dépassent 99,9 %. Le point de basculement dépend de manière critique de la structure de l'hamiltonien et des caractéristiques de bruit du matériel.",
    "C": "Passer à des ordres de Trotter supérieurs élimine systématiquement les erreurs induites par les commutateurs en incorporant plus de termes du développement de Baker-Campbell-Hausdorff, mais cette annulation ne s'applique qu'au développement de Magnus de l'hamiltonien effectif — elle ne supprime pas les erreurs provenant des termes hamiltoniens non commutatifs eux-mêmes. Pour les systèmes fortement non commutatifs, les formules d'ordre supérieur amplifient en réalité la croissance de norme dans l'opérateur d'erreur car la procédure de symétrisation compose les erreurs de phase sur plusieurs exponentielles imbriquées. Cet effet devient significatif lorsque ∥[H_i, H_j]∥t dépasse l'unité, ce qui fait que les méthodes du quatrième ordre sont moins performantes que le second ordre pour une taille de pas de temps fixe.",
    "D": "Les formules de produit d'ordre supérieur atteignent une meilleure précision en construisant des exponentielles de matrice approximatives avec de meilleures propriétés de troncation des séries de Taylor, mais elles échangent fondamentalement l'erreur de Trotter contre une profondeur de circuit accrue puisque chaque ordre nécessite exponentiellement plus de termes dans la décomposition symétrisée. Spécifiquement, la formule du m-ième ordre nécessite O(2^m) facteurs exponentiels pour annuler les termes d'erreur par extrapolation de Richardson, ce qui signifie que la trotterisation du sixième ordre demande ~64 exponentielles individuelles de termes hamiltoniens par pas de temps comparé à 2 pour le premier ordre. Bien que l'erreur locale diminue comme (Δt)^m, le surcoût en portes rend les ordres supérieurs au quatrième ordre impraticables pour la plupart des matériels quantiques.",
    "solution": "A"
  },
  {
    "id": 258,
    "question": "Pourquoi placer des coupures à travers des régions de faible intrication minimise-t-il le surcoût classique dans la découpe de circuit ?",
    "A": "Les fils transportant une faible entropie d'intrication permettent une reconstruction approximative utilisant moins de mesures d'états de Bell entre sous-circuits, car la décomposition de Schmidt d'une bipartition faiblement intriquée contient moins de coefficients significatifs — réduisant le nombre de termes classiques qui doivent être suivis pendant la recombinaison par quasi-probabilité.",
    "B": "Une faible intrication à travers une coupure implique moins de résultats de mesure corrélés entre les sous-circuits séparés, réduisant le nombre de termes de probabilité conjointe qui doivent être sommés classiquement pendant la reconstruction — cette réduction exponentielle du fardeau de post-traitement classique rend les coupures à faible intrication computationnellement efficaces.",
    "C": "Les coupures à faible intrication minimisent le surcoût car le protocole de découpe de fil nécessite d'échantillonner à partir de distributions de quasi-probabilité dont la taille du support évolue exponentiellement avec la dimension de liaison χ de la coupure — une intrication plus faible correspond à un χ plus petit, réduisant directement le nombre d'échantillons Monte Carlo nécessaires pour une estimation précise des valeurs d'espérance.",
    "D": "Les coupures à travers des liaisons à faible intrication produisent des décompositions de quasi-probabilité avec des coefficients plus proches de l'unité en magnitude, réduisant le facteur de surcoût statistique (la somme des valeurs absolues de tous les poids) qui détermine la complexité d'échantillonnage — cela découle directement de la relation entre l'entropie d'intrication et la norme ℓ₁ de la décomposition.",
    "solution": "B"
  },
  {
    "id": 259,
    "question": "En quoi le modèle d'erreur par canal d'effacement diffère-t-il du canal de dépolarisation dans la correction d'erreur quantique ?",
    "A": "Les canaux d'effacement produisent des erreurs où l'information quantique est perdue vers un troisième sous-espace orthogonal (souvent un état de fuite excité |2⟩ au-delà de la base de calcul {|0⟩,|1⟩}), avec un indicateur ou un résultat de mesure révélant explicitement quels qubits ont subi des effacements sans faire s'effondrer l'état quantique logique, permettant une correction ciblée par des opérations de récupération appliquées uniquement aux emplacements d'erreur connus. Les canaux de dépolarisation appliquent plutôt des opérateurs de Pauli stochastiques (X, Y, Z) uniformément avec probabilité p/3 chacun, où à la fois l'emplacement et le type d'erreur restent cachés jusqu'à ce que des mesures d'extraction de syndrome soient effectuées. Cette distinction affecte fondamentalement les performances du code : la correction d'effacement nécessite une distance d pour protéger contre d-1 erreurs à emplacement connu par récupération déterministe, tandis que les erreurs de dépolarisation exigent une distance d pour corriger ⌊(d-1)/2⌋ erreurs à emplacement inconnu nécessitant un décodage de syndrome probabiliste.",
    "B": "Dans les canaux d'effacement, le mécanisme d'erreur signale quels qubits spécifiques ont décohéré en les faisant transiter vers un état de vide connu |∅⟩ orthogonal au sous-espace de calcul, permettant aux protocoles de correction d'erreur d'identifier les qubits défaillants par des mesures de syndrome qui détectent l'absence d'information sans perturber les données quantiques préservées sur d'autres qubits. Cela permet la récupération en remplaçant l'information perdue en utilisant la redondance des qubits encodés restants. Les canaux de dépolarisation appliquent des rotations de Pauli aléatoires (X, Y, Z chacune avec probabilité p/4, plus l'identité avec probabilité 1-3p/4) uniformément sur tous les qubits, avec des erreurs se produisant à des emplacements inconnus et dans des bases de Pauli inconnues, nécessitant une extraction de syndrome pour déduire à la fois les positions et les types d'erreur à partir de mesures de stabilisateur qui projettent sur des sous-espaces de code, nécessitant des algorithmes de décodage par maximum de vraisemblance pour identifier le motif d'erreur le plus probable.",
    "C": "Pour les erreurs d'effacement, vous savez quel emplacement de qubit spécifique a échoué grâce à des mesures d'indicateur ou à une surveillance environnementale qui révèle la position de l'erreur sans perturber l'information quantique sur d'autres qubits, permettant des opérations de récupération ciblées qui n'ont qu'à restaurer l'état quantique perdu à l'emplacement connu. En revanche, pour les erreurs de dépolarisation, à la fois l'emplacement où une erreur s'est produite et le type spécifique d'erreur de Pauli (X, Y ou Z) restent complètement inconnus, nécessitant des mesures de syndrome qui extraient l'information d'erreur sans révéler l'état quantique sous-jacent et des algorithmes de décodage plus complexes pour identifier et corriger le motif d'erreur inconnu à partir des syndromes de stabilisateur mesurés.",
    "D": "Les erreurs d'effacement se produisent lorsque les qubits décohèrent dans un état défaillant détectable marqué par un indicateur ancillaire qui signale l'emplacement de l'erreur par une mesure projective sur un sous-espace orthogonal hors de la base de calcul, permettant au protocole de correction d'erreur de savoir précisément quels qubits nécessitent une récupération sans rien apprendre sur le contenu informationnel quantique protégé. Cela permet un décodage plus simple puisque seuls d-1 effacements nécessitent une distance de correction d. Les canaux de dépolarisation appliquent chaque opérateur de Pauli (I, X, Y, Z) avec probabilité égale p/4, créant des erreurs où ni l'emplacement spatial ni le type de Pauli n'est connu jusqu'à ce que les mesures de syndrome extraient des informations partielles par des vérifications de stabilisateur. Cependant, les deux canaux préservent également le théorème de non-clonage : les indicateurs d'effacement révèlent les positions d'erreur mais jamais l'état quantique lui-même, tandis que les syndromes révèlent les motifs d'erreur modulo les stabilisateurs mais jamais le mot de code logique.",
    "solution": "C"
  },
  {
    "id": 260,
    "question": "Considérez un scénario d'informatique quantique en nuage où un adversaire a un accès physique au centre de données mais ne peut pas accéder directement au processeur quantique ou à son électronique de contrôle immédiate. L'adversaire souhaite apprendre des informations sur les calculs effectués par les utilisateurs légitimes. Quel vecteur d'attaque est le plus réaliste compte tenu de ces contraintes, et pourquoi fonctionne-t-il malgré l'isolation physique du matériel quantique ? Supposons que l'adversaire puisse déployer des équipements de mesure sensibles dans l'installation mais doit rester à au moins 10 mètres du réfrigérateur à dilution. Quel principe physique fondamental rend cette attaque réalisable, et quel artefact computationnel spécifique l'adversaire ciblerait-il pour maximiser l'extraction d'information tout en minimisant le risque de détection ?",
    "A": "L'analyse des fluctuations thermiques cryogéniques fonctionne car les opérations quantiques génèrent des signatures thermiques mesurables qui se propagent à travers l'infrastructure de refroidissement de l'installation, permettant à un adversaire de reconstruire les motifs de calcul à partir de données de séries temporelles thermiques collectées aux points d'accès du liquide de refroidissement. Chaque opération de porte dissipe une quantité d'énergie caractéristique dans la chambre de mélange, et différents algorithmes quantiques produisent des profils thermiques distincts basés sur leur composition de portes et leur séquence d'exécution. En surveillant la température du mélange hélium-3/hélium-4 aux retours de l'échangeur de chaleur avec des capteurs à résolution millikelvin, un attaquant peut appliquer une analyse de Fourier pour extraire les composantes de fréquence dominantes correspondant à des types de portes spécifiques.",
    "B": "Tomographie d'état quantique effectuée par des qubits sondes intriqués qui ont été précédemment préparés et insérés dans le système lors d'un compromis de chaîne d'approvisionnement, permettant une lecture à distance des états computationnels. Ces qubits sondes restent dormants et maximalement intriqués avec un système de référence externe contrôlé par l'adversaire, et ils deviennent corrélés avec les qubits computationnels de l'utilisateur par des hamiltoniens de couplage parasite qui sont toujours présents dans les systèmes multi-qubits.",
    "C": "La fuite électromagnétique des impulsions de contrôle est le vecteur principal — les impulsions RF pilotant les portes quantiques rayonnent des bandes latérales détectables qui sont corrélées avec les séquences de portes, et celles-ci peuvent être capturées à distance avec des antennes sensibles positionnées à 10+ mètres. Le timing des impulsions, la structure de fréquence et les motifs de modulation divulguent la structure algorithmique même sans récupérer les formes d'onde parfaites. Cette exploitation d'émanations électromagnétiques non intentionnelles représente une attaque par canal auxiliaire réaliste qui fonctionne par des principes physiques RF standard sans nécessiter d'accès au processeur quantique lui-même ou à son environnement cryogénique.",
    "D": "Extraction de données de calibration par analyse du trafic réseau, car les paramètres de calibration téléchargés vers le système de contrôle quantique contiennent suffisamment d'informations sur l'hamiltonien pour reconstruire les algorithmes utilisateur à partir des séquences d'impulsions optimales. Les ordinateurs quantiques nécessitent un étalonnage fréquent pour tenir compte de la dérive de fréquence des qubits et de l'évolution de la diaphonie, et ces routines de calibration téléchargent des matrices détaillées de fidélité des portes à un et deux qubits vers le serveur de contrôle.",
    "solution": "C"
  },
  {
    "id": 261,
    "question": "Quelle contre-mesure spécifique au niveau matériel protège le mieux contre les attaques par canal auxiliaire électromagnétique sur les ordinateurs quantiques ?",
    "A": "Filtrage des lignes de contrôle utilisant des réseaux LC passifs multi-étages qui atténuent sélectivement les émissions électromagnétiques dans les bandes de fréquences les plus susceptibles d'être interceptées tout en préservant l'intégrité du signal pour les impulsions de contrôle elles-mêmes. En mettant en œuvre des filtres coupe-bande soigneusement conçus à chaque étage de la chaîne de contrôle — depuis l'électronique à température ambiante jusqu'à la chambre de mélange — l'architecture filtrée crée 60 à 80 dB d'atténuation dans les gammes GHz où les équipements d'écoute classiques fonctionnent le plus efficacement.",
    "B": "Mise en forme d'impulsions différentielles, où chaque signal de contrôle est divisé en composantes complémentaires positives et négatives qui sont acheminées par des lignes de transmission parallèles et recombinées uniquement au niveau du qubit cible.",
    "C": "Isolation par cage de Faraday, qui entoure physiquement le processeur quantique et son électronique de contrôle d'une enceinte conductrice continue qui bloque les champs électromagnétiques externes et empêche les émissions électromagnétiques internes de s'échapper. Le blindage métallique mis à la terre crée une surface équipotentielle qui force les champs électriques variables dans le temps à se terminer sur la cage plutôt que de se propager dans l'espace libre, tandis que les courants de Foucault induits dans le conducteur génèrent des champs magnétiques qui s'opposent et annulent les variations du champ magnétique interne, atténuant ainsi les composantes électriques et magnétiques du rayonnement électromagnétique sur un large spectre de fréquences.",
    "D": "Signaux de contrôle à spectre étalé, qui modulent les impulsions de contrôle des qubits sur une large bande passante en utilisant des séquences de saut de fréquence pseudo-aléatoires synchronisées avec une clé cryptographique inconnue des attaquants potentiels. Cette technique, empruntée aux communications militaires sécurisées, garantit que toute fuite électromagnétique est distribuée sur des centaines de mégahertz de spectre, réduisant la densité spectrale de puissance du signal en dessous du plancher de bruit à toute fréquence individuelle qu'un adversaire pourrait surveiller.",
    "solution": "C"
  },
  {
    "id": 262,
    "question": "Quelle approche technique précise fournit les garanties de sécurité les plus fortes pour les puzzles clients contre les adversaires quantiques ?",
    "A": "Les schémas de preuve de travail basés sur les réseaux euclidiens combinent la difficulté des problèmes de vecteur le plus court avec des exigences de compromis temps-espace, forçant les adversaires à maintenir une grande mémoire quantique tout en effectuant des réductions séquentielles de base de réseau — cette double contrainte empêche théoriquement à la fois les accélérations de Grover et les attaques quantiques parallèles en goulotant le calcul par la bande passante mémoire plutôt que par le nombre de portes. La réduction de sécurité aux problèmes de réseau dans le pire cas fournit une résistance quantique, tandis que le produit temps-espace reste invariant sous optimisation quantique, rendant cette approche asymptotiquement sécurisée contre les solveurs classiques et quantiques.",
    "B": "Les fonctions exigeantes en mémoire atteignent la résistance quantique en obligeant les attaquants à maintenir des états quantiques cohérents sur d'énormes tableaux de mémoire proportionnels à la taille du problème, forçant effectivement la décohérence avant que le calcul ne se termine. Les constructions Argon2 ou scrypt, lorsqu'elles sont paramétrées avec des coûts mémoire dépassant la RAM quantique disponible (typiquement >10^6 qubits pour une sécurité significative), créent un goulot d'étranglement de ressources fondamental qui persiste même sous l'algorithme de Grover, puisque l'accélération quadratique ne peut surmonter le surcoût mémoire exponentiel requis pour maintenir la superposition sur l'ensemble de l'espace d'adressage pendant les accès mémoire séquentiels.",
    "C": "Les fonctions à délai vérifiable avec vérifiabilité trappe imposent un calcul intrinsèquement séquentiel grâce à des comptes d'itération précisément calibrés que la parallélisation quantique ne peut contourner, tout en maintenant des voies de vérification efficaces. La structure cryptographique empêche l'accélération de Grover en liant chaque étape de calcul au résultat de son prédécesseur par des transformations non inversibles.",
    "D": "Les puzzles basés sur le hachage exploitant des primitives cryptographiques résistantes aux attaques quantiques connues peuvent être facilement adaptés en augmentant les paramètres de difficulté pour compenser l'accélération quadratique de Grover, bien que cela nécessite de doubler la longueur effective de sortie. Les fonctions de hachage standard comme SHA-3, lorsqu'elles sont configurées avec des sorties de 384 bits, forcent les adversaires quantiques à effectuer environ 2^192 opérations — un seuil computationnellement infaisable qui maintient des marges de sécurité pratiques bien dans l'ère post-quantique, rendant le déploiement relativement simple.",
    "solution": "C"
  },
  {
    "id": 263,
    "question": "Pourquoi les circuits quantiques aléatoires conduisent-ils à des distributions de sortie de Porter-Thomas ?",
    "A": "Les circuits aléatoires génèrent des conceptions de circuits quantiques approximatifs (t-designs) pour un t suffisamment grand, faisant correspondre la fonction génératrice des moments des probabilités de sortie à celle des unitaires Haar-aléatoires. Cependant, cette convergence nécessite une profondeur de circuit évoluant comme O(n² log n) pour n qubits, ce qui signifie que les circuits peu profonds produisent des distributions sous-Porter-Thomas avec un excès de kurtosis qui les distingue du véritable comportement chaotique jusqu'à ce que le temps de brouillage soit atteint.",
    "B": "Les circuits aléatoires profonds approximent les unitaires Haar-aléatoires, faisant suivre aux probabilités de sortie une distribution exponentielle — une signature du brouillage chaotique. Cette universalité émerge parce que des portes aléatoires suffisamment profondes répandent l'intrication sur tous les qubits, faisant explorer uniformément l'espace de Hilbert à la fonction d'onde.",
    "C": "L'évolution unitaire aléatoire maximise l'entropie de von Neumann des matrices de densité réduites, forçant la décomposition de Schmidt des bipartitions en états maximalement mélangés où tous les coefficients de Schmidt deviennent égaux. Cette maximisation d'entropie implique directement que les probabilités des résultats de mesure doivent suivre les statistiques de Porter-Thomas car la distribution exponentielle est la distribution à entropie maximale soumise à la contrainte de normalisation sur les amplitudes de probabilité, indépendamment de la séquence de portes spécifique appliquée.",
    "D": "Les circuits profonds mettent en œuvre une thermalisation effective de l'état quantique en couplant chaque qubit à un environnement implicite formé par tous les autres qubits, poussant le système vers un ensemble de Gibbs à température infinie où tous les états de base sont également peuplés. La distribution de Porter-Thomas émerge comme la projection microcanonique de l'ensemble canonique lors de la mesure d'un sous-système, exactement analogue aux distributions de vitesse de Maxwell-Boltzmann résultant de l'équilibration thermique en mécanique statistique classique.",
    "solution": "B"
  },
  {
    "id": 264,
    "question": "Pourquoi les interconnexions photoniques sont-elles considérées comme avantageuses dans les systèmes informatiques quantiques distribués ?",
    "A": "Tous les nœuds deviennent des unités de traitement interchangeables sans contraintes de routage — En établissant des liens photoniques entre chaque paire de nœuds dans une topologie maillée entièrement connectée, le système élimine complètement le besoin d'algorithmes de routage de qubits, puisque tout qubit logique peut être instantanément téléporté vers n'importe quel emplacement physique à travers le réseau d'intrication. Cela supprime le surcoût computationnel associé à l'insertion de portes SWAP et permet à la compilation de circuits de traiter le système distribué comme un seul processeur monolithique avec une connectivité uniforme.",
    "B": "Ils préservent les états quantiques en transit indéfiniment, réduisant le besoin de qubits de mémoire aux nœuds intermédiaires et permettant des topologies de réseau arbitraires sans préoccupations de décohérence.",
    "C": "Rendre le réseau compatible classique grâce au codage numérique de bits — Les interconnexions photoniques exploitent le multiplexage par répartition en longueur d'onde pour convertir l'information quantique en signaux binaires classiques pouvant traverser l'infrastructure de fibre optique standard sans nécessiter de températures cryogéniques ou de conditions de vide. Ce processus de codage numérique transforme les états de superposition en séquences de bits robustes utilisant des codes correcteurs d'erreurs similaires à ceux des télécommunications classiques, permettant aux réseaux quantiques de s'intégrer de manière transparente avec l'architecture dorsale Internet existante.",
    "D": "Liens flexibles et accordables pour le partage d'intrication à la demande — Les interconnexions photoniques permettent l'établissement dynamique d'intrication entre des nœuds distants grâce à l'émission contrôlable de photons, la transmission à travers des fibres optiques à faibles pertes, et des mesures d'états de Bell basées sur l'interférence, fournissant les corrélations quantiques nécessaires aux algorithmes quantiques distribués et aux protocoles de communication basés sur la téléportation.",
    "solution": "D"
  },
  {
    "id": 265,
    "question": "Quel est l'avantage principal du recuit quantique pour les problèmes d'optimisation en apprentissage automatique ?",
    "A": "L'effet tunnel quantique à travers les barrières d'énergie fournit un mécanisme pour échapper aux minima locaux qui piégeraient les optimiseurs classiques basés sur le gradient, permettant théoriquement la découverte d'optima globaux dans des paysages de perte non convexes. Pendant le calendrier de recuit, le système peut traverser par effet tunnel les barrières de potentiel avec des probabilités qui évoluent favorablement par rapport à l'activation thermique, particulièrement pour les problèmes avec des surfaces d'énergie rugueuses présentant de nombreux optima locaux séparés par des barrières élevées.",
    "B": "Le processus de recuit quantique présente une robustesse au bruit inhérente car les fluctuations thermiques et la décohérence environnementale aident en fait le système à explorer le paysage d'énergie plus complètement, fonctionnant effectivement comme des perturbations bénéfiques plutôt que comme des erreurs. Les recuits quantiques actuels maintiennent la cohérence quantique tout au long de la trajectoire d'optimisation entière même à des températures de fonctionnement autour de 15 millikelvins, ce qui élimine complètement le besoin de protocoles de correction d'erreurs coûteux.",
    "C": "Les problèmes d'optimisation binaire non contrainte quadratique (QUBO) survenant en apprentissage automatique — tels que le clustering, la sélection de caractéristiques et la régression parcimonieuse — se mappent directement au hamiltonien d'Ising natif implémenté dans le matériel de recuit quantique. Cette correspondance naturelle élimine le besoin de décompositions de portes complexes ou de compilation de circuits, permettant aux praticiens de formuler les objectifs d'optimisation comme des fonctions d'énergie que le recuit minimise par son évolution physique, fournissant une interface simple entre la structure du problème d'apprentissage automatique et les capacités du matériel quantique.",
    "D": "Les recuits quantiques démontrent une résilience exceptionnelle aux sources de bruit systématiques et aléatoires pendant l'opération, avec des temps de décohérence qui dépassent largement la durée typique du calendrier de recuit.",
    "solution": "C"
  },
  {
    "id": 266,
    "question": "La mise en œuvre d'une régularisation par variance dans les QNN nécessite-t-elle des évaluations supplémentaires de circuits quantiques ?",
    "A": "L'estimation de la variance nécessite de calculer les moments d'ordre deux en mesurant l'observable au carré ⟨Ô²⟩, ce qui nécessite un circuit modifié où l'on implémente des applications contrôlées de l'opérateur de mesure Ô conditionnées par des qubits ancillaires qui calculent effectivement les valeurs espérées des produits d'opérateurs. Ce circuit de mesure auxiliaire doit être évalué séparément du circuit d'estimation de la moyenne ⟨Ô⟩, doublant le nombre de programmes quantiques exécutés par itération d'entraînement mais fournissant les deux statistiques nécessaires pour la régularisation par variance via la formule Var(Ô) = ⟨Ô²⟩ - ⟨Ô⟩².",
    "B": "Aucune évaluation supplémentaire si le circuit produit des statistiques suffisantes comme plusieurs échantillons de mesure indépendants, à partir desquels la moyenne et la variance peuvent être estimées simultanément en utilisant des formules statistiques standard appliquées aux données collectées. Le même budget de mesures fournit à la fois la valeur espérée pour la fonction de perte et l'estimation de variance pour la régularisation.",
    "C": "Le calcul de la variance nécessite le protocole de test de Hadamard pour extraire les composantes imaginaires des amplitudes quantiques correspondant aux termes croisés dans la distribution de sortie. On construit un circuit auxiliaire avec un qubit ancillaire supplémentaire en superposition qui contrôle si l'unitaire original ou son inverse est appliqué, puis la mesure de l'ancillaire dans la base X fournit les parties réelle et imaginaire de ⟨ψ|Û|ψ⟩. L'exécution de ce circuit modifié parallèlement au circuit de mesure original permet l'extraction de la variance à partir du motif d'interférence encodé dans les statistiques de l'ancillaire.",
    "D": "La variance peut être extraite d'un seul ensemble d'échantillons de mesure par post-traitement des résultats sous forme de chaînes de bits via des protocoles de tomographie par ombres classiques qui reconstruisent les fonctions de corrélation du second ordre à partir de mesures de Pauli aléatoires. On modifie le circuit pour ajouter des unitaires de Clifford aléatoires avant la mesure, on collecte les ombres classiques, puis on utilise des estimateurs médiane-des-moyennes pour calculer simultanément ⟨Ô⟩ et ⟨Ô²⟩ à partir des mêmes données d'ombre sans évaluations quantiques supplémentaires, exploitant le fait que le twirling de Clifford préserve l'information des moments tout en réduisant la surcharge de mesures.",
    "solution": "B"
  },
  {
    "id": 267,
    "question": "Dans un environnement de laboratoire où vous essayez d'implémenter un algorithme quantique sur un processeur supraconducteur avec une architecture de qubits fixe, vous constatez que certaines portes à deux qubits ne peuvent pas être appliquées directement entre des paires arbitraires de qubits. Quelle est la contrainte matérielle la plus courante responsable de cette limitation ?",
    "A": "La diaphonie par micro-ondes entre les lignes de contrôle limite la fidélité des portes pour les paires de qubits non adjacentes, car les tonalités de pilotage hors résonance fuient par couplage capacitif vers les qubits spectateurs positionnés le long du chemin de propagation du signal. Les erreurs de phase induites par la diaphonie s'accumulent de manière quadratique avec le nombre de qubits intermédiaires entre les paires cibles, rendant les portes directes réalisables uniquement pour les plus proches voisins où un routage minimal se produit.",
    "B": "L'architecture de couplage physique restreint l'application des portes aux qubits plus proches voisins uniquement, car le couplage capacitif ou inductif entre qubits supraconducteurs décroît rapidement avec la séparation spatiale sur la puce.",
    "C": "La mise en forme des impulsions de flux pour les portes de couplage ajustables nécessite que les qubits individuels soient accordés en fréquence dans une fenêtre déterminée par l'inductance mutuelle et l'anharmonicité du coupleur. Les paires de qubits non adjacentes ont généralement des différences de fréquence dépassant 200 MHz en raison de la variation de fabrication, et la trajectoire d'accord adiabatique nécessaire pour les mettre en résonance sans peupler les états de fuite devient plus longue que T₂, limitant les portes directes aux qubits proches ayant des fréquences de transition naturellement similaires.",
    "D": "Les canaux de désintégration Purcell parasites couplent chaque qubit à son résonateur de lecture dédié, et l'application de portes à deux qubits entre paires distantes nécessite un désaccord simultané des résonateurs pour supprimer le déphasage induit par la mesure pendant l'opération de porte. Le matériel de contrôle ne peut moduler qu'un nombre fini de fréquences de résonateurs en parallèle en raison des limites de bande passante des générateurs de formes d'onde arbitraires, contraignant les opérations de portes simultanées aux qubits partageant des réseaux de résonateurs couplés au sein d'un graphe de connectivité local.",
    "solution": "B"
  },
  {
    "id": 268,
    "question": "Quel est l'objectif principal de la technique d'optimisation par fusion de rotations dans la compilation de circuits quantiques ?",
    "A": "Elle aligne l'orientation physique des qubits avec le champ magnétique terrestre pendant la calibration en ajustant les angles de rotation pour compenser les interférences géomagnétiques, qui peuvent induire des déphasages parasites dans les circuits supraconducteurs. L'optimisation identifie les portes de rotation qui peuvent être réorientées pour annuler le flux magnétique environnemental traversant le dispositif, créant effectivement une configuration d'annulation de champ.",
    "B": "Elle garantit que toutes les rotations de qubits se produisent simultanément pour une synchronisation améliorée sur l'ensemble du dispositif, ce qui est critique lors du traitement d'opérations d'intrication multi-qubits qui nécessitent un timing précis. En parallélisant les portes de rotation sur différents qubits, la technique minimise la profondeur totale du circuit et réduit l'impact des erreurs de diaphonie qui résultent de l'application séquentielle de portes.",
    "C": "La technique convertit systématiquement les séquences de rotations arbitraires en séquences composées exclusivement d'opérations de Clifford (portes Hadamard, CNOT et Phase), qui peuvent ensuite être simulées efficacement classiquement en utilisant le théorème de Gottesman-Knill. Cette conversion est réalisée en approximant chaque angle de rotation continu à l'équivalent Clifford le plus proche, échangeant une petite perte de fidélité de porte contre des améliorations exponentielles de la surcharge de simulation classique.",
    "D": "Combiner des rotations consécutives autour du même axe en une seule porte",
    "solution": "D"
  },
  {
    "id": 269,
    "question": "Quel est le rôle principal des changements de référentiel dans la planification d'impulsions de Qiskit ?",
    "A": "Les changements de référentiel implémentent des rotations virtuelles sur l'axe Z en mettant à jour la référence de phase de l'oscillateur local plutôt qu'en appliquant des impulsions micro-ondes physiques, éliminant la surcharge temporelle pour les portes de phase dans la base computationnelle. Cependant, ils doivent être soigneusement synchronisés avec le système de suivi de phase global pour éviter l'accumulation de dérive de référentiel sur les circuits profonds. Chaque mise à jour de référentiel décale l'angle de phase du référentiel tournant pour les impulsions de pilotage ultérieures sur ce qubit, nécessitant que le compilateur maintienne un accumulateur de phase qui suit la rotation virtuelle totale appliquée. Ce mécanisme échange la durée d'impulsion physique contre une surcharge de comptabilité classique, mais introduit des exigences subtiles de cohérence de phase lorsque plusieurs qubits partagent des lignes de contrôle multiplexées en fréquence dans le réseau de distribution micro-ondes du réfrigérateur à dilution.",
    "B": "Les changements de référentiel permettent le branchement conditionnel en temps réel dans les planifications d'impulsions en sélectionnant dynamiquement entre des modèles d'impulsions précompilés basés sur les résultats de mesure en milieu de circuit, implémentant le flux de contrôle nécessaire pour les algorithmes quantiques adaptatifs. Lorsqu'un résultat de mesure arrive pendant l'exécution de la planification, l'instruction de changement de référentiel met à jour un registre interne qui détermine quelle forme d'onde d'impulsion ultérieure est chargée depuis le tampon mémoire du générateur de formes d'onde arbitraires. Cette sélection conditionnelle d'impulsions se produit avec une latence sous-microseconde, permettant aux protocoles comme la correction d'erreurs quantiques d'appliquer des opérations de récupération dépendantes du syndrome dans le temps de cohérence du qubit, bien que le mécanisme nécessite une gestion soigneuse des dépendances de registres classiques pour éviter d'introduire des variations temporelles déterministes qui pourraient fuir de l'information.",
    "C": "Mise à jour de phase virtuelle — essentiellement, cela décale simplement le référentiel pour les impulsions suivantes, ce qui donne une rotation Z sans consommer de temps de pilotage. Au lieu d'envoyer une impulsion micro-ondes réelle pour implémenter une porte de phase, le système de contrôle met simplement à jour l'angle de phase du référentiel tournant utilisé pour définir les enveloppes d'impulsions ultérieures. Cette approche est instantanée et élimine la surcharge temporelle et les erreurs potentielles associées aux rotations Z physiques, rendant les changements de référentiel essentiels pour une compilation efficace de planification d'impulsions et l'optimisation de portes.",
    "D": "Les changements de référentiel implémentent des mélangeurs définis par logiciel pour la conversion ascendante à bande latérale unique des enveloppes d'impulsions en bande de base vers la fréquence de pilotage du qubit, remplaçant les modulateurs IQ matériels par un traitement du signal numérique qui applique des transformées de Hilbert dans le compilateur d'impulsions. Lorsque le planificateur rencontre un changement de référentiel, il met à jour le noyau de multiplication exponentielle complexe utilisé pour le mélange hétérodyne du segment de forme d'onde suivant, décalant effectivement la fréquence porteuse par le décalage de phase spécifié. Cette approche de mélange numérique fournit une résolution de fréquence sous-hertz pour adresser les qubits individuels dans les régions spectrales encombrées, bien qu'elle nécessite de maintenir la continuité de phase à travers les limites d'impulsions par une interpolation soigneuse de la forme d'onde de l'oscillateur local pour éviter les fuites spectrales qui piloteraient des transitions hors résonance.",
    "solution": "C"
  },
  {
    "id": 270,
    "question": "Quelle est l'idée principale du « transport assisté par déphasage » dans les modèles de biologie quantique ?",
    "A": "Un bruit modéré perturbe l'interférence destructive, augmentant l'efficacité du transport au-delà de l'évolution purement cohérente en ouvrant des voies classiquement interdites",
    "B": "Le déphasage environnemental crée un transfert de population incohérent entre les états propres d'énergie qui contourne les effets de gel quantique de Zénon, permettant aux excitations d'échapper aux pièges locaux plus rapidement que ne le permet le saut cohérent pur",
    "C": "Un faible déphasage brise la symétrie de renversement temporel dans la dynamique de Lindblad, induisant un flux d'énergie dirigé vers des états puits de plus basse énergie via un terme effectif non hermitien qui imite le couplage optimal de guide d'ondes",
    "D": "Les fluctuations thermiques modulent dynamiquement les énergies des sites à des taux correspondant aux forces de couplage inter-sites, créant un effet tunnel assisté par résonance qui améliore le transport via des mécanismes de résonance stochastique sans nécessiter de cohérence de longue durée",
    "solution": "A"
  },
  {
    "id": 271,
    "question": "Considérez un solveur variationnel quantique d'état propre utilisé pour entraîner un circuit quantique paramétré pour une tâche de classification, où le paysage de coût est fortement non-convexe avec de nombreux minima locaux séparés par de hautes barrières classiques. L'entraînement est bloqué dans un bassin sous-optimal, et la descente de gradient standard ne progresse pas. Quelle approche peut potentiellement aider le réseau neuronal quantique à échapper à ce minimum local pendant l'entraînement ?",
    "A": "Les protocoles de recuit quantique appliqués à l'optimisation des paramètres fonctionnent en réduisant lentement les termes de champ transverse qui permettent au système d'explorer le paysage énergétique de manière plus large avant de se stabiliser dans des minima plus profonds. En initialisant les paramètres dans une superposition sur de nombreuses configurations et en évoluant de manière adiabatique vers l'état fondamental de l'hamiltonien de coût, le circuit peut traverser les barrières par effet tunnel quantique pendant le processus de recuit lui-même, se localisant finalement dans un optimum global une fois que le champ transverse disparaît. Cela exploite directement la nature quantique des paramètres eux-mêmes, pas seulement l'architecture du circuit.",
    "B": "Le recuit simulé classique avec termes de moment fournit un mécanisme d'échappement basé sur la température qui accepte occasionnellement des mouvements ascendants, permettant à l'optimiseur de sortir de manière probabiliste des bassins peu profonds et d'explorer les régions voisines de l'espace des paramètres.",
    "C": "L'exploitation de l'effet tunnel quantique à travers les barrières dans l'espace des paramètres permet à la fonction d'onde de l'optimiseur de pénétrer des régions classiquement interdites, échantillonnant des configurations de l'autre côté des barrières énergétiques sans avoir à les franchir. Cet effet fondamentalement quantique permet d'explorer des régions déconnectées du paysage de coût que les méthodes basées sur le gradient ne peuvent atteindre, car l'amplitude de probabilité peut s'étendre à travers les barrières même lorsque la trajectoire classique serait réfléchie, découvrant potentiellement des minima plus profonds qui sont séparés de l'emplacement actuel par des murs de potentiel prohibitivement hauts.",
    "D": "N'importe laquelle de ces stratégies pourrait fonctionner selon l'architecture spécifique du circuit et la structure du problème, car elles fournissent chacune des mécanismes différents pour explorer le paysage de coût au-delà de l'information locale du gradient. Le recuit quantique gère l'effet tunnel dans l'espace des paramètres, les méthodes basées sur le moment ajoutent une dynamique classique qui peut franchir les discontinuités, et les approches hybrides combinent les deux paradigmes pour maximiser la probabilité d'échapper aux bassins peu profonds. Le choix optimal nécessite souvent des tests empiriques sur la géométrie particulière de la surface de perte rencontrée dans votre tâche de classification.",
    "solution": "C"
  },
  {
    "id": 272,
    "question": "Dans le contexte des algorithmes d'estimation de phase quantique qui utilisent des qubits auxiliaires pour extraire des informations sur les valeurs propres d'un opérateur unitaire, quelle est la relation fondamentale entre la transformée de Fourier quantique continue (qui agit sur des états de superposition arbitraires dans un espace de Hilbert de dimension infinie) et la transformée de Fourier quantique discrète (qui est implémentée sur un registre fini de n qubits) lorsque toutes deux sont restreintes à opérer sur des états de base de calcul |j⟩ où j varie sur les indices autorisés ?",
    "A": "Parce que la transformée de Fourier quantique discrète opère sur un espace de Hilbert fini de dimension 2^n tandis que la version continue s'étend sur un espace de dimension infinie, l'implémentation discrète introduit nécessairement des artéfacts d'échantillonnage et des effets de fuite spectrale. Ces erreurs de troncature se manifestent comme des biais systématiques d'estimation de phase qui diminuent en O(1/n), ce qui signifie qu'atteindre une extraction de valeur propre de haute précision nécessite des registres auxiliaires exponentiellement grands. Ce n'est qu'asymptotiquement, lorsque n diverge, que la transformée discrète converge vers la limite continue idéalisée.",
    "B": "La transformée de Fourier quantique continue existe purement comme une abstraction mathématique utilisée dans les analyses théoriques et les preuves, sans aucune réalisation physique directe possible sur un matériel quantique fini. En revanche, la transformée discrète constitue le circuit réellement implémentable composé de portes Hadamard et de rotations de phase contrôlées.",
    "C": "Lorsqu'elle est restreinte aux états de base de calcul, la transformée de Fourier quantique discrète implémente exactement la version continue sur ce sous-espace fini sans aucune erreur d'approximation, quelle que soit la taille du registre. L'équivalence mathématique tient car les deux transformées appliquent des facteurs de phase identiques e^(2πijk/N) aux amplitudes des états de base, la version discrète restreignant simplement le domaine aux indices j ∈ {0,1,...,2^n-1}. Cette correspondance exacte permet aux algorithmes d'estimation de phase quantique d'extraire des informations sur les valeurs propres avec une précision limitée uniquement par la taille du registre, et non par une erreur de discrétisation inhérente à la transformée elle-même.",
    "D": "Alors que la transformée de Fourier quantique continue est définie pour des états quantiques arbitraires incluant des superpositions complexes sur tous les éléments de base, la transformée discrète implémentée via des circuits quantiques standards repose sur l'application séquentielle de portes de phase conditionnelles qui ne fonctionnent correctement que lorsque l'entrée est un état de base de calcul classique.",
    "solution": "C"
  },
  {
    "id": 273,
    "question": "La dimension de Vapnik-Chervonenkis (VC) est occasionnellement utilisée dans la recherche en apprentissage automatique quantique pour :",
    "A": "Comparer la capacité d'apprentissage des modèles quantiques versus classiques en quantifiant l'expressivité des circuits quantiques paramétrés par rapport aux réseaux neuronaux classiques, où la dimension VC mesure le nombre maximal de points de données qui peuvent être brisés (correctement classifiés dans tous les étiquetages possibles) et fournit ainsi une borne théorique rigoureuse sur la performance de généralisation déterminée par la complexité d'échantillonnage évoluant comme O(VC/ε²) pour une tolérance d'erreur ε, indépendamment des algorithmes d'entraînement spécifiques ou des fonctions de perte, permettant des comparaisons architecturales équitables.",
    "B": "Comparer la capacité d'apprentissage des modèles quantiques versus classiques en quantifiant l'expressivité des circuits quantiques paramétrés par rapport aux réseaux neuronaux classiques, où la dimension VC mesure le nombre maximal de points de données qui peuvent être brisés (correctement classifiés dans tous les étiquetages possibles) et fournit ainsi une borne théorique rigoureuse sur la performance de généralisation indépendamment des algorithmes d'entraînement spécifiques ou des fonctions de perte.",
    "C": "Comparer la complexité statistique des opérateurs de mesure quantiques en quantifiant la dimension effective de l'espace de Hilbert accessible via des éléments POVM paramétrés, où la dimension VC mesure le nombre maximal de résultats de mesure qui peuvent être distingués sur tous les états quantiques possibles et fournit ainsi des bornes sur la complexité d'échantillonnage pour l'apprentissage des canaux quantiques. Des dimensions VC plus élevées indiquent une expressivité de mesure plus riche, permettant aux modèles quantiques d'extraire plus d'information classique à partir de moins de requêtes quantiques que les mesures projectives, ce qui est critique pour les méthodes à noyaux quantiques où la base de mesure détermine la performance de classification.",
    "D": "Comparer la performance de généralisation des circuits quantiques en quantifiant la dimension effective de l'espace des paramètres par rapport à la taille de l'ensemble d'entraînement, où la dimension VC mesure le nombre de directions orthogonales dans le paysage de perte qui peuvent être optimisées indépendamment (liée au rang de la matrice d'information de Fisher) et fournit ainsi des bornes sur le risque de surapprentissage indépendamment des algorithmes d'entraînement spécifiques. Lorsque la dimension VC dépasse la taille de l'ensemble de données de plus que des facteurs logarithmiques, le modèle quantique est prouvé dans le régime surparamétré où les plateaux stériles deviennent exponentiellement improbables, rendant une dimension VC plus élevée souhaitable pour l'entraînabilité plutôt que pour la généralisation.",
    "solution": "B"
  },
  {
    "id": 274,
    "question": "Dans le contexte des méthodes à noyaux quantiques et de l'apprentissage de circuits, les chercheurs ont observé que le nombre de conditionnement de la matrice de noyau joue un rôle critique dans la détermination de la performance du modèle sur des données non vues. Lorsque la matrice de noyau devient mal conditionnée — c'est-à-dire lorsque ses valeurs propres s'étendent sur plusieurs ordres de grandeur — cette propriété mathématique a un impact direct et mesurable sur :",
    "A": "La performance de généralisation. Spécifiquement, les noyaux mal conditionnés conduisent à des modèles qui sont extrêmement sensibles au bruit dans les données d'entraînement, résultant en une faible robustesse lors de l'évaluation sur les ensembles de test. La dispersion des valeurs propres amplifie effectivement les petites perturbations pendant le processus d'apprentissage, ce qui se manifeste par du surapprentissage et une précision prédictive dégradée sur de nouveaux exemples.",
    "B": "Le temps de relaxation physique des qubits pendant les cycles répétés de préparation d'état, car la dispersion des valeurs propres dans la matrice de Gram du noyau module directement les canaux de décroissance T₁ par rétroaction sur l'appareil de mesure. Les grands nombres de conditionnement correspondent à un couplage résonant entre les modes propres du noyau et les bains de phonons environnementaux, accélérant les taux de décohérence proportionnellement au logarithme du rapport spectral et réduisant ainsi la fenêtre de cohérence effective disponible pour les évaluations de circuit ultérieures.",
    "C": "Les exigences de calibration de fréquence des impulsions micro-ondes, car les matrices de noyau mal conditionnées introduisent de la diaphonie entre les lignes de contrôle qui décale les fréquences de résonance des qubits.",
    "D": "L'empreinte mémoire classique des circuits quantiques transpilés, particulièrement lors de l'utilisation de réseaux SWAP sur des topologies de connectivité linéaire, car les nombres de conditionnement élevés forcent le compilateur à insérer des qubits auxiliaires supplémentaires pour stabiliser la précision numérique pendant l'inversion de la matrice de noyau. Chaque ordre de grandeur dans la dispersion des valeurs propres nécessite environ log₂(κ) qubits supplémentaires pour la correction d'erreurs dans le protocole de tomographie par ombres classiques, gonflant exponentiellement la consommation de RAM pendant la simulation de circuit et le post-traitement.",
    "solution": "A"
  },
  {
    "id": 275,
    "question": "Quelle propriété des systèmes quantiques fournit potentiellement une voie vers un apprentissage automatique plus efficace en termes d'échantillons ?",
    "A": "Les corrélations amplifiées par l'intrication, qui permettent aux systèmes quantiques de capturer des dépendances multi-variables qui nécessiteraient un nombre exponentiel de paramètres classiques pour être représentées explicitement, réduisant ainsi le nombre d'échantillons d'entraînement nécessaires pour apprendre des distributions jointes complexes. En encodant les corrélations directement dans la structure d'intrication d'un état quantique, le modèle peut généraliser à partir de moins d'exemples car il représente implicitement des relations que les modèles classiques doivent apprendre via des données extensives.",
    "B": "L'interférence quantique permettant une convergence plus rapide pendant l'optimisation en amplifiant de manière constructive les chemins vers des configurations de paramètres optimales tout en annulant de manière destructive les trajectoires sous-optimales dans le paysage de perte. Ce phénomène permet aux méthodes basées sur le gradient d'échapper aux minima locaux plus efficacement que les approches classiques, car les motifs d'interférence guident le processus d'optimisation le long de directions de recherche améliorées quantiquement qui échantillonnent l'espace des paramètres plus efficacement.",
    "C": "La capacité de représenter des distributions de probabilité avec moins de paramètres grâce à la compression d'état quantique, où un système de n qubits peut encoder 2^n amplitudes en utilisant seulement 2n paramètres réels après prise en compte de la normalisation et de la phase globale. Cette compression exponentielle signifie que les modèles quantiques peuvent représenter des distributions hautement complexes sur de grands espaces discrets en utilisant un nombre de paramètres qui évolue logarithmiquement avec la taille du support de la distribution.",
    "D": "Toutes les réponses ci-dessus",
    "solution": "D"
  },
  {
    "id": 276,
    "question": "Quel est l'objectif de la mesure en milieu de circuit dans l'informatique quantique ?",
    "A": "La mesure en milieu de circuit permet des protocoles de commutation dynamique de code où le processeur quantique passe d'un code de correction d'erreurs à un autre pendant l'exécution de l'algorithme, en fonction de l'évaluation en temps réel des processus d'erreur physiques qui dominent actuellement l'environnement de bruit. En mesurant les qubits de syndrome à des étapes intermédiaires et en analysant leurs corrélations statistiques, le système de contrôle détermine si les erreurs de basculement de bit ou de basculement de phase sont plus répandues, puis reconfigure l'ensemble de générateurs de stabilisateurs en conséquence pour optimiser la distance du code contre le canal d'erreur identifié, maintenant ainsi la fidélité computationnelle tout au long des algorithmes quantiques étendus.",
    "B": "Extraire des résultats de mesure partiels tout en poursuivant le calcul sur les qubits non mesurés — permet la réutilisation adaptative de qubits, le recyclage d'ancilles, le branchement conditionnel et l'extraction de syndrome en temps réel pour les protocoles de correction d'erreurs quantiques, permettant une rétroaction classique pour guider les séquences de portes ultérieures en fonction des résultats intermédiaires sans terminer l'ensemble de l'algorithme quantique.",
    "C": "La mesure en milieu de circuit implémente un protocole obligatoire de gestion de l'entropie requis lorsque les circuits quantiques dépassent un seuil de profondeur critique où l'entropie d'intrication à travers les coupes biparties approche des valeurs maximales S ≈ n log 2 pour des sous-systèmes à n-qubits. En effectuant des mesures projectives sur des sous-ensembles stratégiques de qubits au point médian du circuit, l'algorithme réduit le rang de Schmidt de l'état quantique global, empêchant la croissance exponentielle de la complexité de simulation classique et permettant à l'électronique de contrôle du processeur quantique de maintenir une représentation efficace de la fonction d'onde sous forme d'état produit de matrices pour le suivi d'erreurs en temps réel.",
    "D": "La mesure en milieu de circuit fournit des capacités de téléportation quantique essentielles pour distribuer l'information quantique à travers des registres de qubits spatialement séparés au sein de l'architecture du processeur. En mesurant des paires d'ancilles intriquées dans la base de Bell à des profondeurs de circuit intermédiaires et en appliquant des corrections de Pauli conditionnelles basées sur les résultats de mesure classiques, le protocole transfère des états quantiques entre des qubits distants sans portes de couplage direct, contournant les contraintes de connectivité limitée des qubits dans les architectures de plus proches voisins. Ce transfert d'état basé sur la mesure réduit le nombre de portes nécessaires par rapport aux cascades de portes SWAP d'environ 40% pour les opérations typiques de chirurgie de réseau s'étendant sur plus de trois couches de qubits.",
    "solution": "B"
  },
  {
    "id": 277,
    "question": "Quelle est la représentation générale de l'état d'un qubit ?",
    "A": "Un mélange probabiliste de |0⟩ et |1⟩ avec des coefficients réels p₀ et p₁, où p₀ + p₁ = 1, représente le qubit comme une distribution de probabilité classique sur les états de base computationnelle. Cette formulation capture la nature statistique des résultats de mesure quantique en traitant le qubit comme étant définitivement dans l'état |0⟩ avec probabilité p₀ ou définitivement dans l'état |1⟩ avec probabilité p₁, plutôt que dans une superposition cohérente.",
    "B": "Un produit tensoriel des états |0⟩ et |1⟩ sans superposition, exprimé comme |0⟩ ⊗ |1⟩, représente le qubit en combinant les deux états de base par l'opération de produit tensoriel plutôt que par combinaison linéaire. Cette formulation traite le qubit comme un système composite occupant simultanément les deux états de base computationnelle dans des facteurs tensoriels séparés, encodant ainsi les deux possibilités dans un seul objet mathématique.",
    "C": "α|0⟩ + β|1⟩ où α, β sont des amplitudes complexes satisfaisant la condition de normalisation |α|² + |β|² = 1, représentant une superposition quantique cohérente qui capture à la fois les amplitudes de probabilité et la phase relative.",
    "D": "Un état intriqué de |0⟩ et |1⟩ combinés sans utiliser de nombres complexes, représentant l'unité fondamentale d'information quantique dans un espace de Hilbert purement à valeurs réelles. Cette formulation restreint les coefficients de superposition aux nombres réels α, β ∈ ℝ, éliminant le degré de liberté de phase associé aux amplitudes complexes. En exigeant α|0⟩ + β|1⟩ avec α, β réels et α² + β² = 1, la représentation confine l'état du qubit à un sous-espace réel de la sphère de Bloch.",
    "solution": "C"
  },
  {
    "id": 278,
    "question": "Dans un déploiement d'installation sécurisée s'étendant sur trois zones métropolitaines, vous implémentez un système QKD indépendant du dispositif de mesure entre institutions financières. Le système utilise des nœuds relais non fiables pour effectuer des mesures d'état de Bell, et toutes les parties vérifient la sécurité par analyse statistique des corrélations de mesure. Après six mois de fonctionnement, un adversaire ayant un accès physique aux stations relais mais pas aux dispositifs terminaux prétend avoir extrait des bits de clé. Quelle vulnérabilité sophistiquée existe dans l'implémentation de la distribution quantique de clés indépendante du dispositif de mesure qui rendrait cette attaque réalisable ?",
    "A": "L'adversaire peut manipuler subtilement les références temporelles et les signaux de synchronisation d'horloge entre les stations géographiquement distribuées, créant des corrélations temporelles artificielles dans les résultats de mesure qui passent les tests standard d'inégalité CHSH mais fuient de l'information partielle sur la clé brute à travers des fenêtres de mesure soigneusement conçues qui exploitent les contraintes de causalité relativiste inhérentes aux protocoles multi-parties, permettant la reconstruction de bits de clé à partir des données de correction d'erreurs annoncées publiquement lorsqu'elles sont combinées avec une connaissance précise des délais de propagation.",
    "B": "L'adversaire exploite des attaques de remappage de phase aux interfaces de séparateur de faisceau du relais où les modes spatiaux entrants des deux points terminaux sont combinés pour l'analyse d'état de Bell. En introduisant une biréfringence contrôlée par des modulateurs stress-optiques précisément alignés positionnés le long des 10 derniers mètres de fibre avant le relais, l'adversaire crée des déphasages dépendants de la polarisation (Δφ ≈ π/180 par cycle de mesure) qui biaisent systématiquement quels états de Bell sont projetés avec succès. Sur six mois de statistiques accumulées, l'analyse de Fourier de ces corrélations codées en phase révèle des motifs périodiques synchronisés avec les annonces de choix de base des dispositifs terminaux, fuyant environ 0,03 bits par paire de photons transmis à travers des corrélations de canal auxiliaire qui survivent à l'étape d'amplification de confidentialité mais deviennent extractibles lorsqu'elles sont croisées avec les bits de parité de correction d'erreurs publiés pendant le post-traitement classique.",
    "C": "L'efficacité de détection finie de l'analyseur d'état de Bell — typiquement η ≈ 0,45 pour les photodiodes à avalanche commerciales fonctionnant aux longueurs d'onde télécom — crée une faille de post-sélection où les résultats de mesure sont annoncés uniquement lorsque des détections de coïncidence se produisent aux deux ports de sortie. Un adversaire avec accès au relais peut exploiter ceci en implémentant une stratégie sophistiquée d'interception-renvoi qui effectue d'abord des mesures de Bell partielles utilisant des interféromètres déséquilibrés avec des rapports de division asymétriques (par exemple, 70:30 au lieu de 50:50). En analysant quel bras d'interféromètre produit des taux de comptage plus élevés corrélés avec les données de réconciliation de base annoncées publiquement par les stations terminales sur des périodes d'observation étendues, l'adversaire reconstruit de l'information partielle sur les états de polarisation des photons pré-mesure. Combiné avec la connaissance des rapports d'extinction finis dans les modulateurs de polarisation des points terminaux (typiquement 20-25 dB plutôt que l'extinction infinie idéale), ceci permet l'estimation par maximum de vraisemblance des bits de clé bruts avec environ 12% de probabilité de succès par impulsion transmise.",
    "D": "L'appareil de mesure d'état de Bell du relais emploie des séparateurs de faisceau polarisants avec des rapports d'extinction finis (typiquement 1000:1 plutôt qu'une suppression infinie idéale), créant une fuite petite mais systématique de photons polarisés orthogonalement vers des ports de sortie nominalement bloqués. Lorsqu'elle est couplée avec les variations d'efficacité de couplage dépendantes de la longueur d'onde inhérentes aux interfaces optiques fibre-vers-espace libre au relais — où les coefficients de réflexion de Fresnel varient d'environ 3% à travers la largeur de bande spectrale de 5 nm des sources de photons pratiques — ces imperfections génèrent des corrélations entre les résultats de mesure et la distribution de longueur d'onde physique au sein de chaque paire de photons. Un adversaire avec accès au relais surveille ces statistiques de détection résolues en longueur d'onde en utilisant des spectromètres haute résolution et les corrèle avec les motifs temporels dans les courants d'injection des diodes laser des points terminaux, qui présentent un glissement de fréquence dépendant de la température qui se couple au profil de dispersion de fibre accumulé sur des distances métropolitaines.",
    "solution": "A"
  },
  {
    "id": 279,
    "question": "Comment les réseaux de neurones convolutifs quantiques (QCNNs) contribuent-ils à la correction d'erreurs quantiques ?",
    "A": "Ils augmentent l'intrication à travers toutes les couches du circuit quantique, ce qui empêche la perte d'information en créant une redondance que la correction d'erreurs classique peut ensuite exploiter par post-traitement des résultats de mesure et extraction de syndrome.",
    "B": "Les QCNNs implémentent des transformations unitaires apprises qui remplacent entièrement les mesures de stabilisateurs, utilisant des circuits quantiques paramétrés pour projeter directement les états quantiques corrompus vers l'espace du code sans effondrer la superposition. Cette approche sans mesure préserve la cohérence quantique tout au long de la correction d'erreurs en traitant la correction comme une rotation continue dans l'espace de Hilbert plutôt qu'un protocole discret syndrome-puis-récupération.",
    "C": "Plutôt que d'effectuer la correction d'erreurs sur du matériel quantique, les QCNNs s'exécutent sur des clusters GPU classiques pour simuler l'évolution de circuits quantiques bruités avec une précision suffisante pour que la sortie de simulation classique puisse être utilisée directement à la place du calcul quantique, remplaçant effectivement le coût de correction d'erreurs quantiques onéreux par une inférence classique rapide.",
    "D": "Prédire et corriger les erreurs à l'aide de l'apprentissage automatique",
    "solution": "D"
  },
  {
    "id": 280,
    "question": "Dans le contexte des limitations matérielles de l'ère NISQ où les fidélités de portes fluctuent à travers les cycles de calibration et où les graphes de connectivité de qubits imposent un coût de routage non trivial, pourquoi les méthodes de compilation dynamique sont-elles particulièrement utiles par rapport aux approches de compilation statique anticipée qui fixent toutes les décompositions de portes et les mappages de qubits avant l'exécution ?",
    "A": "Les techniques de compilation dynamique s'adaptent continuellement aux données de performance d'exécution en surveillant les taux d'erreur en temps réel et les fidélités de portes pendant l'exécution du circuit, permettant au compilateur de réoptimiser les décompositions de portes, les mappages de qubits et les stratégies d'atténuation d'erreurs à la volée. Cette approche suit efficacement les fluctuations de bruit dépendantes du temps et la dérive du dispositif entre les calibrations, permettant au système d'ajuster les choix de transpilation pour favoriser les qubits physiques et les implémentations de portes les plus performants à chaque instant, améliorant ainsi substantiellement la fidélité globale de sortie du circuit par rapport aux mappages statiques qui deviennent sous-optimaux à mesure que les caractéristiques matérielles évoluent.",
    "B": "La compilation dynamique exploite la synthèse de portes juste-à-temps en reportant la décomposition des rotations de qubit unique arbitraires dans des ensembles de portes natifs jusqu'à immédiatement avant l'exécution, moment auquel elle accède aux données de calibration les plus récentes pour sélectionner des paramètres d'impulsion et des durées de porte optimaux. Cette approche suit les temps de cohérence et les taux d'erreur de portes dépendants du temps qui dérivent entre les cycles de calibration, permettant au compilateur d'ajuster continuellement les stratégies de décomposition pour minimiser l'erreur accumulée, améliorant ainsi substantiellement la fidélité du circuit par rapport aux approches statiques qui reposent sur des données de calibration potentiellement obsolètes de la phase de pré-compilation.",
    "C": "La compilation dynamique emploie des stratégies d'allocation de qubits adaptatives qui réaffectent les mappages logique-vers-physique de qubits entre sous-circuits en fonction de la surveillance en temps réel des taux d'erreur de portes à deux qubits et du coût de swap le long de différents chemins de routage à travers le graphe de connectivité. En profilant continuellement quelles paires de qubits physiques présentent actuellement les erreurs CNOT les plus faibles et en ajustant les placements de portes ultérieurs en conséquence, cette approche exploite la variabilité temporelle de la performance matérielle qui se produit entre les calibrations, atteignant une meilleure fidélité de circuit que la compilation statique qui s'engage sur un mappage fixe avant d'observer les caractéristiques d'erreur d'exécution.",
    "D": "La compilation dynamique utilise la caractérisation d'erreurs en ligne à travers des séquences de benchmarking randomisé entrelacées exécutées entre les couches de circuit, construisant des modèles statistiques des processus de bruit actuels qui informent la sélection des protocoles d'atténuation d'erreurs et des politiques de planification de portes. Ce profilage de bruit en temps réel permet au compilateur de détecter la dégradation des temps de cohérence et les motifs de diaphonie au fur et à mesure qu'ils émergent pendant l'exécution, ajustant dynamiquement les décisions de compilation ultérieures pour contourner les qubits et implémentations de portes qui se détériorent, maintenant ainsi une fidélité de circuit plus élevée que les approches statiques qui ne peuvent pas répondre aux variations de performance intra-exécution.",
    "solution": "A"
  },
  {
    "id": 281,
    "question": "Quel est l'objectif de la construction 'miter' pour la vérification d'équivalence dans le ZX-calcul ?",
    "A": "La construction miter effectue le produit tensoriel des deux circuits avec leurs conjugués hermitiens respectifs, puis trace sur les registres de sortie pour former une quantité scalaire dont la représentation ZX est simplifiée en utilisant les règles de fusion d'araignées et de complémentation locale — l'équivalence fonctionnelle est vérifiée lorsque ce scalaire se réduit à la dimension de l'espace de Hilbert, confirmant que les circuits implémentent le même unitaire à une phase globale près.",
    "B": "Le miter connecte les fils de sortie d'un diagramme ZX directement aux fils d'entrée du second diagramme après application de l'opération dague, créant un diagramme fermé dont la valeur scalaire est calculée en appliquant de manière répétée les règles de pivot et de complémentation locale du ZX-calcul — lorsque les diagrammes sont fonctionnellement équivalents, ces réécritures réduisent la structure à un facteur de phase scalaire égal à la dimension du système.",
    "C": "Le miter compose un circuit avec l'inverse de l'autre pour former un diagramme combiné, puis applique les règles de réécriture du ZX-calcul pour simplifier le résultat — si les diagrammes sont équivalents, la simplification produit l'opération identité, confirmant l'égalité fonctionnelle.",
    "D": "Le miter forme un état ancillaire de paire de Bell connecté via des portes CNOT aux lignes de qubits correspondantes dans les deux circuits, puis effectue une post-sélection sur les mesures ancillaires — l'équivalence est établie lorsque les règles de réécriture ZX pour la copie de nœuds araignées à travers la structure ancillaire produisent des signatures de stabilisateurs correspondantes pour tous les résultats de mesure, ce qui peut être vérifié en réduisant les deux branches à la forme normale d'état graphe.",
    "solution": "C"
  },
  {
    "id": 282,
    "question": "Les réseaux de neurones convolutifs quantiques (QCNNs) offrent des avantages en extraction de caractéristiques, classification et traitement de l'information. Cependant, ils font également face à des défis majeurs. Laquelle des affirmations suivantes décrit le mieux à la fois leurs avantages et leurs limitations ?",
    "A": "Les opérations de pooling hiérarchiques compressent les états quantiques de manière efficace, mais l'entraînement nécessite un surcoût de mesures exponentiel pour estimer les gradients avec précision.",
    "B": "Grâce au parallélisme quantique, les QCNNs traitent des espaces de caractéristiques exponentiellement grands en superposition, permettant une convolution simultanée sur toutes les régions spatiales des données d'entrée au sein d'une seule évaluation de circuit. Cela accélère considérablement la génération de cartes de caractéristiques par rapport aux convolutions classiques. Cependant, le surcoût en qubits physiques croît exponentiellement avec la dimensionnalité de l'entrée car chaque caractéristique de données supplémentaire nécessite des qubits dédiés pour la préparation d'état, et les techniques actuelles de correction d'erreur ne peuvent pas compresser efficacement ces représentations, rendant le traitement d'images à grande échelle impraticable sur les dispositifs à court terme.",
    "C": "Les QCNNs exploitent des circuits quantiques paramétriques avec beaucoup moins de paramètres entraînables que les CNNs classiques — atteignant souvent une compression de paramètres de 10× à 100× — en encodant l'information dans des espaces de Hilbert de haute dimension où un seul angle de rotation peut représenter des transformations non linéaires complexes. Pourtant, cette compacité s'accompagne d'un coût prohibitif : l'implémentation de la correction d'erreur tolérante aux fautes pour chaque couche nécessite des circuits d'extraction de syndrome avec un surcoût en ancilles qui évolue en O(d³) pour des codes de distance d, et les cycles de correction concaténés nécessaires pour les architectures QCNN profondes poussent le nombre total de qubits au-delà de 10⁶ même pour des tâches de classification modestes.",
    "D": "Les QCNNs exploitent l'intrication quantique pour capturer les corrélations non locales dans les données de manière plus efficace que les détecteurs de caractéristiques classiques, permettant une reconnaissance de motifs supérieure dans les ensembles de données structurées tels que les configurations moléculaires ou les systèmes de spin sur réseau où des corrélations quantiques à longue portée existent naturellement. Cette extraction de caractéristiques basée sur l'intrication fournit des avantages représentationnels exponentiels pour certaines classes de problèmes. Cependant, le défi omniprésent de la décohérence et des erreurs de portes sur le matériel NISQ actuel dégrade sévèrement ces corrélations quantiques lors de l'évaluation de réseaux profonds, provoquant la dissipation de la ressource d'intrication avant d'atteindre la couche de mesure, ce qui limite fondamentalement la profondeur et la précision pratiques réalisables dans les implémentations QCNN réelles.",
    "solution": "D"
  },
  {
    "id": 283,
    "question": "Pourquoi la description des ordinateurs NISQ comme systèmes à deux niveaux avec les états d'énergie |0⟩ et |1⟩ est-elle considérée comme une abstraction ?",
    "A": "Les qubits sont physiquement restreints à deux états par conception grâce à une ingénierie soigneuse de la structure des niveaux d'énergie, et tous les niveaux d'énergie supérieurs qui pourraient exister dans le système physique sous-jacent sont rendus inaccessibles par de grands écarts d'énergie et des règles de sélection qui empêchent les transitions en dehors du sous-espace computationnel. Ce confinement strict à deux niveaux est maintenu même sous des champs d'excitation forts, rendant l'abstraction essentiellement exacte plutôt qu'approximative.",
    "B": "La description à deux niveaux n'est qu'une approximation rendue nécessaire par les qubits NISQ sujets aux erreurs qui manquent de cohérence suffisante — des qubits vraiment idéaux avec une isolation parfaite seraient de véritables systèmes à deux niveaux qui ne fuiteraient jamais vers des états supérieurs. Une fois que les ordinateurs quantiques tolérants aux fautes seront développés avec une correction d'erreur appropriée, l'abstraction ne sera plus nécessaire car le matériel imposera un comportement strictement à deux niveaux par suppression active de toute transition de fuite.",
    "C": "Les qubits physiques réels possèdent inévitablement des niveaux d'énergie supplémentaires au-delà des états computationnels |0⟩ et |1⟩, et ces états de niveau supérieur peuvent être accédés par inadvertance lors des opérations de portes, particulièrement sous des excitations micro-ondes intenses ou des impulsions hors résonance, conduisant à des erreurs de fuite qui dégradent la fidélité du circuit.",
    "D": "Les ordinateurs quantiques sont fondamentalement contraints par le principe de superposition à traiter au plus deux états orthogonaux par qubit physique, quelle que soit la façon dont le qubit est physiquement implémenté. Tenter d'accéder à un troisième état violerait la nature binaire de l'information quantique telle que décrite par la représentation de la sphère de Bloch, qui ne peut accommoder que deux états de base plus leurs combinaisons linéaires — par conséquent, le modèle à deux niveaux n'est pas une abstraction mais une limite physique stricte.",
    "solution": "C"
  },
  {
    "id": 284,
    "question": "Pourquoi la compilation randomisée peut-elle atténuer certaines attaques cachées au niveau des impulsions mais échouer contre les détournements d'amplitude d'excitation paramétrique ?",
    "A": "La compilation randomisée permute les séquences de portes et modifie ainsi l'ordonnancement temporel des impulsions de contrôle appliquées à chaque qubit, ce qui perturbe les canaux cachés dépendant du timing qui reposent sur des motifs d'arrivée d'impulsions prévisibles, mais les modulations d'amplitude d'excitation paramétrique agissent globalement sur toute la puce via des lignes de flux partagées et injectent donc des signaux cohérents dans tous les qubits simultanément indépendamment de la façon dont les séquences de portes individuelles sont réordonnées, ce qui signifie que l'enveloppe d'amplitude malveillante de l'attaquant persiste uniformément à travers chaque exécution randomisée et ne peut pas être décorrélée par des permutations au niveau des portes.",
    "B": "La compilation randomisée mélange la décomposition logique des portes en séquences Clifford, permutant l'ordre dans lequel les opérations élémentaires sont appliquées pour créer de la diversité entre les exécutions de circuit, mais ce réordonnancement opère purement au niveau des portes et laisse les formes d'impulsion analogiques sous-jacentes — y compris leurs profils d'amplitude, modulations de phase et fonctions d'enveloppe — complètement inchangées, ce qui signifie qu'un attaquant qui compromet les amplitudes d'excitation paramétrique peut toujours injecter des signaux malveillants qui persistent à travers toutes les compilations randomisées.",
    "C": "La compilation randomisée introduit des opérateurs de Pauli stochastiques qui moyennent les erreurs cohérentes causées par des erreurs de calibration d'impulsion systématiques, supprimant efficacement les canaux cachés qui exploitent les imperfections de portes déterministes, mais les détournements d'amplitude d'excitation paramétrique modifient les termes hamiltoniens gouvernant les interactions à deux qubits par manipulation directe du biais du coupleur, et ces perturbations au niveau hamiltonien commutent avec le twirling de Pauli appliqué pendant la compilation randomisée, permettant aux modulations d'amplitude de l'attaquant de rester cohérentes et non affectées par le protocole de randomisation.",
    "D": "La compilation randomisée applique des opérations de twirling qui transforment les erreurs d'impulsion cohérentes en bruit de dépolarisation en moyennant sur des cadres de Pauli aléatoires, ce qui neutralise les canaux cachés reposant sur l'accumulation d'erreurs cohérentes, mais les attaques d'amplitude d'excitation paramétrique exploitent le régime adiabatique où des rampes d'amplitude lentes induisent des transitions entre états propres d'énergie sans générer suffisamment de composantes haute fréquence, et puisque la condition adiabatique garantit que ces transitions restent cohérentes en phase à travers toutes les décompositions de portes randomisées, la modulation d'amplitude malveillante ne peut pas être convertie en bruit incohérent par aucun protocole de twirling.",
    "solution": "B"
  },
  {
    "id": 285,
    "question": "Supposons que vous analysiez une architecture computationnelle où des circuits Clifford sont augmentés d'un petit nombre d'états magiques en entrée, mais les circuits eux-mêmes restent non adaptatifs (c'est-à-dire que toutes les portes sont fixées avant la mesure). Certaines recherches affirment que ceux-ci devraient rester classiquement simulables efficacement puisque « les circuits Clifford sont faciles ». Pourquoi ce raisonnement est-il erroné, et qu'est-ce qui rend de tels circuits généralement difficiles à simuler malgré leur structure Clifford non adaptative ?",
    "A": "Les circuits Clifford non adaptatifs avec états magiques restent difficiles car les états magiques se situent en dehors du normalisateur du groupe de Clifford, donc leur représentation en stabilisateurs nécessite le suivi d'un nombre exponentiel de mises à jour de cadre de Pauli qui ne peuvent pas être compressées en utilisant le tableau de Gottesman-Knill — chaque état magique contribue des termes non stabilisateurs qui se multiplient à travers le circuit.",
    "B": "Le raisonnement échoue car les états magiques introduisent des phases non Clifford qui créent une ambiguïté de base computationnelle dans la simulation de Gottesman-Knill — tandis que les circuits Clifford purs mappent les états stabilisateurs vers des états stabilisateurs avec des mesures de Pauli déterministes, les états magiques ont des valeurs propres de stabilisateurs indéfinies nécessitant que le simulateur se branche exponentiellement sur les enregistrements de mesure possibles.",
    "C": "Les états magiques brisent la structure de stabilisateur, et même un nombre constant peut promouvoir des circuits autrement faciles à un échantillonnage #P-difficile car les états magiques injectent des amplitudes non stabilisatrices qui se propagent à travers les portes Clifford, détruisant la représentation classique efficace qui rend les circuits Clifford purs traitables.",
    "D": "Les circuits Clifford préservent le rang de stabilisateur, mais les états magiques augmentent ce rang multiplicativement à chaque application de porte — même un seul état magique force le simulateur classique à maintenir une superposition sur 2^k tableaux de stabilisateurs où k croît linéairement avec la profondeur du circuit, car la conjugaison Clifford d'états non stabilisateurs génère des superpositions de projecteurs de stabilisateurs.",
    "solution": "C"
  },
  {
    "id": 286,
    "question": "Qu'est-ce qui est nécessaire pour que l'apprentissage par transfert quantique (QTL) fonctionne efficacement ?",
    "A": "L'intrication quantique seule constitue le composant essentiel et suffisant pour transférer les représentations apprises entre modèles quantiques, car les corrélations non locales codées dans les états intriqués transportent naturellement l'information pertinente de la tâche source vers la tâche cible sans nécessiter d'étiquettes de données classiques. La structure d'intrication elle-même encode les motifs appris, et en préservant ces corrélations pendant le processus de transfert par des transformations unitaires appropriées, le modèle cible hérite directement des connaissances du modèle source à travers la ressource d'intrication partagée.",
    "B": "L'apprentissage par transfert quantique nécessite fondamentalement un ordinateur quantique entièrement corrigé d'erreurs pour fonctionner, car les représentations transférées doivent maintenir une cohérence parfaite lorsqu'elles se propagent du modèle source pré-entraîné vers le modèle cible, et toute décohérence durant ce processus de transfert corromprait irrémédiablement les caractéristiques quantiques apprises. Sans qubits logiques tolérants aux fautes, les erreurs accumulées durant l'étape de transfert de paramètres dépasseraient le seuil de fidélité nécessaire pour préserver les mappages de caractéristiques classiques vers quantiques encodés.",
    "C": "Des données étiquetées pour améliorer la généralisabilité du modèle et permettre un transfert efficace des connaissances des tâches sources vers les tâches cibles",
    "D": "Un large ensemble de données classiques est obligatoire pour pré-entraîner les modèles quantiques avant que tout apprentissage par transfert puisse avoir lieu, puisque le système quantique doit d'abord apprendre les représentations de caractéristiques classiques par une exposition extensive à des exemples étiquetés dans le domaine source. Les paramètres du circuit quantique doivent être initialisés en intégrant des motifs de données classiques à travers des époques d'entraînement répétées sur des millions d'échantillons, construisant graduellement les représentations quantiques internes.",
    "solution": "C"
  },
  {
    "id": 287,
    "question": "Dans une architecture distribuée réaliste, quel facteur limite le plus la fréquence d'exécution des portes distantes ?",
    "A": "Le taux de génération réussie d'intrication entre nœuds distants, qui dépend de processus probabilistes comme la transmission de photons à travers des canaux avec pertes et la création de paires de Bell annoncées qui réussissent typiquement avec une probabilité décroissant exponentiellement avec la distance, contraignant fondamentalement la fréquence des opérations distantes",
    "B": "La limitation principale provient des temps de cohérence finis de l'intrication stockée à chaque nœud — les paires de Bell générées décohèrent avant de pouvoir être consommées pour la téléportation de portes lorsque la latence nœud-à-nœud excède la durée de vie du stockage d'intrication. Puisque les portes distantes nécessitent la distribution d'intrication suivie d'une communication classique des résultats de mesure avant l'achèvement de la porte, la latence aller-retour doit rester dans la fenêtre de décohérence, créant une contrainte temporelle stricte qui limite les taux de portes distantes réalisables",
    "C": "La latence de communication classique pour transmettre les résultats de mesure devient le goulot d'étranglement car les protocoles de portes distantes nécessitent un échange d'information de syndrome avant que la correction d'erreurs puisse reconstruire l'état téléporté. Lorsque les nœuds sont séparés par des distances nécessitant des temps aller-retour à l'échelle de la milliseconde, et que les taux de décohérence exigent un achèvement des portes à l'échelle de la microseconde, la limitation de la vitesse de la lumière crée un écart temporel insurmontable qui limite la fréquence des opérations distantes indépendamment des fidélités des portes locales ou des capacités de génération d'intrication",
    "D": "La contrainte fondamentale émerge des implications du théorème de non-clonage pour la consommation d'intrication : chaque opération de porte distante consomme irréversiblement une paire de Bell par effondrement de mesure, et générer une intrication de remplacement nécessite des processus physiques soumis aux contraintes d'incertitude de Heisenberg sur les taux de préparation d'états. Cette limitation de mécanique quantique borne la fréquence soutenable de portes distantes à l'inverse du temps de génération d'intrication, qui évolue avec la racine carrée du coefficient de perte de photons dépendant de la distance dans les implémentations en fibre optique",
    "solution": "A"
  },
  {
    "id": 288,
    "question": "Quel protocole avancé offre la sécurité la plus forte pour le transfert inconscient quantique ?",
    "A": "Les protocoles du modèle de stockage quantique borné atteignent une sécurité inconditionnelle en exploitant la capacité limitée de mémoire quantique de l'adversaire — spécifiquement, si l'adversaire ne peut pas stocker plus qu'un certain nombre de qubits entre les tours de protocole, la sécurité théorique de l'information peut être prouvée même sans hypothèses de calcul. Cette approche a été démontrée expérimentalement et fournit des garanties de sécurité pratiques lorsque les parties honnêtes peuvent transmettre de l'information quantique plus rapidement que l'adversaire ne peut la traiter et la stocker, en faisant un candidat convaincant pour le déploiement réel.",
    "B": "Les hypothèses de stockage bruité exploitent le fait que tout dispositif de stockage quantique réaliste introduira de la décohérence et des erreurs au fil du temps, permettant aux protocoles de garantir la sécurité en forçant l'adversaire à stocker des états quantiques suffisamment longtemps pour que le bruit détruise l'avantage informationnel.",
    "C": "Les protocoles de transfert inconscient indépendants du dispositif, qui atteignent la sécurité sans faire confiance aux dispositifs quantiques en utilisant les violations d'inégalités de Bell pour certifier la présence d'intrication quantique authentique et l'absence de canaux latéraux qui pourraient fuir de l'information vers l'une ou l'autre partie.",
    "D": "Les protocoles d'engagement de bit relativistes qui exploitent la séparation spatio-temporelle pour empêcher la triche par l'une ou l'autre partie durant la phase de transfert peuvent être étendus au transfert inconscient en faisant placer au transmetteur les deux messages possibles à des emplacements causalement déconnectés, garantissant que le choix du receveur de quel message récupérer ne peut être connu de l'émetteur qu'après la fin de la phase d'engagement.",
    "solution": "C"
  },
  {
    "id": 289,
    "question": "Qu'implique le fait que le taux d'erreurs logiques soit supprimé plus rapidement que l'augmentation des ressources en qubits physiques ?",
    "A": "Le système opère dans le régime super-seuil où le coût de la correction d'erreurs évolue de manière sous-exponentielle avec la distance du code. Cela se produit lorsque les taux d'erreurs physiques se situent légèrement au-dessus du seuil de tolérance aux fautes, permettant aux premiers niveaux de concaténation de supprimer les erreurs plus rapidement que la croissance polynomiale des ressources, bien que cet avantage sature à des distances de code plus élevées avant qu'une suppression exponentielle véritable ne soit atteinte.",
    "B": "Le décodeur opère dans le régime de maximum de vraisemblance où les résultats de mesure de syndrome se regroupent près de la classe d'erreur de poids minimal, créant une réduction effective des taux d'erreurs logiques par moyennage statistique sur des cycles de syndrome répétés. Ce comportement de pseudo-seuil imite la suppression d'erreurs véritable mais reflète l'efficacité du décodeur plutôt qu'une mise à l'échelle véritablement tolérante aux fautes, le distinguant du fonctionnement sous-seuil.",
    "C": "Le système opère dans le régime sous-seuil où la correction d'erreurs quantique procure un bénéfice net. Cela signifie que le taux d'erreurs physiques est inférieur au seuil de tolérance aux fautes, permettant à chaque couche supplémentaire de correction d'erreurs (qui nécessite plus de qubits physiques) de produire une suppression d'erreurs logiques exponentiellement meilleure, démontrant que l'ordinateur quantique peut se mettre à l'échelle avec succès vers un fonctionnement tolérant aux fautes.",
    "D": "Le modèle d'erreurs physiques satisfait l'approximation de dépolarisation au niveau du circuit où les erreurs de portes se produisent uniformément sur tous les qubits physiques, causant une diminution du taux d'erreurs logiques plus rapide que ne le prédirait la borne de capacité du code. Cela se produit parce que les circuits d'extraction de syndrome, lorsque les taux d'erreurs sont spatialement uniformes, suppriment naturellement les erreurs de poids deux par redondance de mesure, créant une suppression apparemment super-exponentielle jusqu'à ce que des corrélations spatiales émergent.",
    "solution": "C"
  },
  {
    "id": 290,
    "question": "Comment l'algorithme de Shor gère-t-il le cas où la valeur mesurée dans le premier registre ne conduit pas à la période correcte ?",
    "A": "L'algorithme repose sur plusieurs exécutions indépendantes du circuit quantique combinées à un post-traitement classique des résultats de mesure pour extraire la période avec une probabilité élevée",
    "B": "Le développement en fractions continues appliqué au rapport des résultats de mesure approxime la période véritable même lorsque la phase mesurée est légèrement décalée, mais cette étape de post-traitement classique nécessite plusieurs essais pour distinguer les candidats de période authentiques des facteurs parasites.",
    "C": "La transformée de Fourier quantique concentre l'amplitude de probabilité sur les multiples entiers de l'inverse de la période, donc les mesures individuelles donnent des multiples de période nécessitant un calcul du plus grand commun diviseur sur plusieurs exécutions pour isoler la période fondamentale de manière fiable.",
    "D": "L'algorithme de Shor emploie une étape de filtrage classique qui teste chaque valeur mesurée contre la condition d'exponentiation modulaire, rejetant les résultats qui ne satisfont pas la contrainte de périodicité et répétant la sous-routine quantique jusqu'à ce qu'un diviseur de période valide émerge des statistiques de mesure.",
    "solution": "A"
  },
  {
    "id": 291,
    "question": "Quels sont les défis clés dans l'entraînement et l'optimisation des algorithmes d'apprentissage automatique quantique (QML) ?",
    "A": "Le bruit quantique provenant de la décohérence et des erreurs de porte nécessite une atténuation sophistiquée des erreurs pendant l'entraînement, mais une fois atténué, l'espace de Hilbert exponentiellement grand élimine entièrement les problèmes de disparition du gradient, les plateaux stériles dans le paysage de perte deviennent navigables grâce aux méthodes de gradient naturel quantique qui exploitent la métrique de Fubini-Study, la profondeur de circuit limitée sur les dispositifs NISQ est surmontée par les effets de concentration des paramètres, et les goulots d'étranglement classiques pour le calcul du gradient sont résolus via les règles de décalage de paramètre permettant des mises à jour efficaces pendant les itérations d'entraînement.",
    "B": "Le bruit quantique et les erreurs matérielles nécessitent des stratégies d'atténuation des erreurs comparables aux techniques de régularisation classiques, les plateaux stériles émergent dans le paysage de perte mais sont principalement causés par des minima locaux plutôt que par des gradients exponentiellement évanescents nécessitant une compilation consciente du matériel au lieu d'optimiseurs avancés, la profondeur de circuit limitée sur les dispositifs NISQ contraint l'expressivité du modèle de manière similaire aux réseaux classiques peu profonds, et les coûts de simulation classique pour le calcul du gradient évoluent polynomialement avec le nombre de qubits lors de l'utilisation de méthodes par différences finies permettant des mises à jour pratiques des paramètres pendant les itérations d'entraînement.",
    "C": "Le bruit quantique provenant des erreurs de porte et de la décohérence crée un bruit de mesure qui évolue inversement avec la profondeur du circuit, rendant paradoxalement les modèles plus profonds plus entraînables, les plateaux stériles dans les circuits variationnels sont contournés par la surparamétrisation qui augmente exponentiellement le signal de gradient avec le nombre de paramètres, la profondeur de circuit limitée sur les dispositifs NISQ est compensée par l'avantage du noyau quantique dans l'espace des caractéristiques, et la simulation classique exploite les contractions de réseaux tensoriels atteignant une mise à l'échelle sous-exponentielle pour les ansätze structurés permettant le calcul du gradient pour des systèmes de taille modérée pendant les itérations d'entraînement.",
    "D": "Le bruit quantique et les erreurs matérielles créent des difficultés d'optimisation importantes nécessitant des stratégies sophistiquées d'atténuation des erreurs, les plateaux stériles dans le paysage de perte rendent l'entraînement basé sur le gradient inefficace pour de nombreux circuits variationnels nécessitant des algorithmes d'optimisation avancés, la profondeur de circuit limitée sur les dispositifs NISQ contraint l'expressivité du modèle, et les goulots d'étranglement de simulation classique pour le calcul du gradient entravent les mises à jour efficaces des paramètres pendant les itérations d'entraînement.",
    "solution": "D"
  },
  {
    "id": 292,
    "question": "Les états de photons lourds stockés dans des résonateurs non linéaires agissent comme ancilles pour les codes bosoniques car ils offrent quel avantage ?",
    "A": "L'immunité au bruit de flux découlant de leur nature purement basée sur la charge, qui élimine le canal de déphasage dominant qui limite les temps de cohérence des transmons dans les architectures supraconductrices modernes. Contrairement aux qubits accordables en flux qui se couplent aux fluctuations de champ magnétique provenant de systèmes à deux niveaux dans le substrat, les résonateurs à photons lourds fonctionnent exclusivement par des interactions capacitives insensibles au bruit 1/f provenant des vortex piégés, permettant des opérations d'ancille déterministes même en présence de gradients de champ magnétique environnementaux.",
    "B": "Chaque mode peut encoder simultanément plusieurs qubits logiques via la base du nombre d'occupation, augmentant considérablement le taux de code par dispositif physique et réduisant la surcharge matérielle requise pour la correction d'erreurs. En exploitant l'espace de Hilbert de dimension infinie de l'oscillateur harmonique, un seul résonateur à photons lourds peut stocker en parallèle un registre entier de syndrome de stabilisateur.",
    "C": "La compatibilité directe avec des excitations de haute puissance, éliminant le besoin d'atténuateurs dans la chaîne cryogénique et simplifiant ainsi l'infrastructure du réfrigérateur à dilution. Parce que les modes de photons lourds ont une masse effective plus grande et donc une susceptibilité réduite à la population de photons thermiques, ils peuvent tolérer des amplitudes d'excitation micro-ondes plusieurs ordres de grandeur plus fortes que les impulsions de contrôle transmon typiques, permettant des opérations de porte plus rapides sans saturer la non-linéarité de Kerr ou induire des transitions indésirables vers des niveaux d'énergie supérieurs dans le spectre.",
    "D": "Des temps de cohérence (T1) plus longs par rapport aux transmons, ce qui est critique pour les cycles de correction d'erreurs. L'anharmonicité réduite et la sensibilité plus faible aux pertes diélectriques dans le régime des photons lourds signifient que ces modes ancilles peuvent maintenir l'information quantique pendant des durées qui dépassent les durées de vie typiques des transmons par des facteurs de deux à cinq, fournissant une stabilité suffisante pour l'extraction de syndrome multi-tours.",
    "solution": "D"
  },
  {
    "id": 293,
    "question": "Pourquoi le concept d'erreur intrinsèque par Clifford (EPC) est-il utile pour l'évaluation comparative des compilateurs ?",
    "A": "La métrique EPC intrinsèque isole les performances du compilateur de la dérive de calibration dépendante du temps en mesurant uniquement les propriétés structurelles des circuits compilés via des séquences Clifford aléatoires — puisque les opérations Clifford forment un sous-groupe efficacement simulable, le post-traitement classique peut déconvoluer les taux d'erreur observés pour extraire la contribution intrinsèque des choix de compilation tels que la planification des portes et l'optimisation de la profondeur du circuit, permettant des comparaisons équitables entre compilateurs multi-plateformes qui restent valables même lorsque les systèmes comparés présentent des ensembles de portes natives ou des topologies de connectivité de qubits différents, bien que cette approche suppose des modèles de bruit markoviens qui peuvent ne pas capturer les corrélations d'erreur non locales.",
    "B": "L'évaluation comparative Clifford aléatoire avec extraction EPC fournit une métrique spécifique au compilateur qui capture les performances à travers la sélection de portes, l'optimisation de disposition et la simplification algébrique — cependant, la sensibilité de la métrique à la qualité du compilateur dépend de manière critique de la distribution de poids des portes Clifford dans les séquences aléatoires, car les compilateurs optimisés pour des types de portes spécifiques montrent des scores EPC artificiellement améliorés lorsque les séquences d'évaluation sur-représentent ces portes, nécessitant une conception de séquence soigneuse qui correspond à la distribution de charge de travail d'application attendue pour garantir que l'EPC extrait reflète les performances réalistes du compilateur plutôt que des artefacts de réglage spécifiques à l'évaluation.",
    "C": "L'exécution de séquences Clifford aléatoires et l'extraction de l'erreur intrinsèque par Clifford capture les performances globales du compilateur à travers plusieurs dimensions d'optimisation — y compris la planification intelligente des portes pour minimiser le temps d'inactivité sur les qubits spectateurs, le routage efficace des qubits qui réduit la surcharge SWAP sur les topologies contraintes, et la simplification algébrique qui élimine les opérations Clifford redondantes — fournissant ainsi une métrique d'évaluation holistique et réaliste qui reflète l'efficacité totale du compilateur au-delà du simple comptage de portes, révélant comment les stratégies de compilation classiques impactent la fidélité réelle des circuits quantiques.",
    "D": "Le cadre EPC intrinsèque quantifie l'efficacité du compilateur en mesurant l'erreur moyenne par porte à travers des séquences Clifford aléatoires, isolant la qualité de compilation logique de la calibration de porte physique — cette métrique révèle avec quel succès le compilateur exploite les relations de commutation et les opportunités de fusion de portes pour réduire le nombre total de portes, bien que son utilité dépende de l'hypothèse que toutes les portes Clifford contribuent également à l'erreur de circuit, une approximation qui s'effondre lorsque le compilateur achemine sélectivement les opérations à travers des paires de qubits à erreur plus faible ou planifie préférentiellement les portes pendant les fenêtres de calibration optimales, causant des mesures EPC qui confondent l'intelligence du compilateur avec l'hétérogénéité matérielle à moins que des contrôles supplémentaires normalisent les variations d'erreur spatio-temporelles.",
    "solution": "C"
  },
  {
    "id": 294,
    "question": "Quelle technique spécifique peut détecter les modifications non autorisées dans le matériel de contrôle quantique ?",
    "A": "L'empreinte de canaux auxiliaires capture la consommation d'énergie, les émissions électromagnétiques et les modèles de temporisation de l'électronique de contrôle pour construire des signatures matérielles uniques, permettant la détection de modifications de micrologiciel ou de substitutions de composants qui altèrent les caractéristiques opérationnelles.",
    "B": "La tomographie d'état quantique reconstruit la matrice densité complète des états de sortie en mesurant les valeurs d'espérance sur un ensemble informationnellement complet d'observables, puis applique l'estimation du maximum de vraisemblance pour extraire la représentation d'état physique. En comparant les états reconstruits aux prédictions théoriques de la pile de contrôle non modifiée, les déviations indiquent une altération matérielle ou une corruption du micrologiciel.",
    "C": "Le hachage du matériel de contrôle intègre des sommes de contrôle cryptographiques dans les tables de définition d'impulsions micro-ondes et les flux binaires FPGA, vérifiant l'intégrité avant chaque séquence expérimentale en comparant les configurations d'exécution aux valeurs de référence signées par le fabricant. Les correctifs de micrologiciel non autorisés ou les paramètres de calibration modifiés produisent des incompatibilités de hachage, signalant une altération potentielle dans la chaîne de contrôle.",
    "D": "L'évaluation comparative aléatoire avec des portes Clifford intercalées injecte des opérations de test entre les couches de circuit compilées, mesurant la fidélité moyenne de séquence sur de nombreuses instances aléatoires. L'analyse statistique révèle si les erreurs de porte ont changé par rapport à la caractérisation de référence, indiquant des formes d'onde de contrôle modifiées ou des forces de couplage de qubits altérées introduites par un matériel compromis.",
    "solution": "A"
  },
  {
    "id": 295,
    "question": "Les décodeurs qui exploitent l'autocorrélation dans les séries temporelles de syndrome peuvent surpasser les décodeurs statiques car les motifs corrélés indiquent quelle propriété du bruit ?",
    "A": "L'indépendance complète des canaux d'erreur X et Z est révélée lorsque les autocorrélations de syndrome décroissent rapidement à zéro en quelques cycles d'extraction de syndrome, confirmant que les erreurs de bit-flip et de phase-flip se produisent par des mécanismes non corrélés qui échantillonnent indépendamment de leurs distributions de bruit respectives. Cela permet aux décodeurs de réseaux tensoriels de factoriser le problème de décodage en sous-problèmes d'erreur classique et quantique séparés, chacun résolu avec des algorithmes spécialisés optimisés pour les stabilisateurs X ou Z, atteignant des accélérations exponentielles en évitant de suivre les corrélations entre types d'erreur à travers l'historique de syndrome combiné.",
    "B": "La dominance du bruit de mesure sur les erreurs physiques réelles est confirmée lorsque les autocorrélations de syndrome montrent une mise à l'échelle caractéristique en 1/√N avec le nombre de mesures répétées N, indiquant que la source principale d'incertitude de syndrome provient du bruit de projection quantique dans la lecture de stabilisateur plutôt que de processus d'erreur cohérents s'accumulant sur les qubits de données. Cela permet à de simples filtres de seuil de distinguer les erreurs réelles (qui produisent des changements de syndrome persistants) des fluctuations de mesure transitoires (qui s'équilibrent en moyenne), permettant aux décodeurs de réduire considérablement la charge de calcul en écartant les séquences de syndrome dont la variance temporelle correspond à la signature du bruit de mesure.",
    "C": "La persistance temporelle des processus d'erreur sous-jacents qu'une hypothèse markovienne manquerait, révélant que les erreurs actuelles dépendent de l'historique d'erreur passé par des mécanismes corrélés. Cela permet aux décodeurs sophistiqués de construire des modèles probabilistes incorporant des effets de mémoire, améliorant considérablement la précision de correction en prédisant les emplacements d'erreur probables basés sur les motifs de syndrome plutôt qu'en traitant chaque cycle d'extraction comme indépendant.",
    "D": "Les rotations unitaires de la base d'erreur laissent les mesures de syndrome invariantes car les valeurs propres de stabilisateur sont préservées sous conjugaison unitaire, ce qui signifie que les autocorrélations dans la série temporelle de syndrome reflètent directement les rotations entre différents sous-espaces d'erreur (X, Y, Z) pilotées par l'évolution hamiltonienne. Lorsque les décodeurs observent des pics d'autocorrélation périodiques à des fréquences correspondant aux échelles d'énergie caractéristiques du système, cela indique que les erreurs cyclent à travers différents types de Pauli via une dynamique cohérente, et tenir compte de ces rotations par une transformation de base de décodeur dépendante du temps permet des stratégies de correction qui suivent le cadre d'erreur rotatif plutôt que de traiter chaque extraction de syndrome comme indépendante.",
    "solution": "C"
  },
  {
    "id": 296,
    "question": "Quel type d'attaque peut exploiter les contrôles au niveau des impulsions dans un système quantique multi-locataire pour perturber des qubits éloignés ?",
    "A": "Dans les architectures avec coupleurs ajustables (comme les systèmes gmon ou fluxonium), un attaquant qui obtient un accès API au niveau root peut reconfigurer les points de polarisation des coupleurs pour établir des interactions directes à deux qubits entre leurs qubits alloués et les qubits victimes situés à plusieurs sites de réseau de distance. En ajustant dynamiquement les paramètres hamiltoniens du coupleur — spécifiquement la force de couplage g_{ij} et le désaccord Δ — l'attaquant crée effectivement de nouvelles arêtes dans le graphe de connectivité qui n'étaient pas présentes dans la topologie publiée du dispositif.",
    "B": "En injectant du code malveillant dans la pile de compilation du fournisseur cloud, un attaquant peut réécrire les opérateurs de mesure de Pauli appliqués aux qubits victimes lors de la lecture, faisant pivoter effectivement la base de mesure de Z à X ou Y sans altérer l'état quantique lui-même. Cette manipulation au niveau logiciel force l'expérience de la victime à mesurer la mauvaise observable entièrement, effondrant les superpositions le long d'axes orthogonaux à la base de calcul prévue et divulguant ainsi des informations sur les phases qui auraient dû rester cachées.",
    "C": "Un attaquant ayant une proximité physique avec le réfrigérateur à dilution peut introduire des interférences électromagnétiques pulsées directement dans les lignes de contrôle micro-ondes ou le câblage de polarisation DC, contournant entièrement les abstractions logicielles de la plateforme cloud. Ces signaux injectés se couplent aux qubits victimes par des lignes de transmission partagées ou des câbles coaxiaux insuffisamment blindés, causant des erreurs de déphasage ou de basculement de bit même lorsque l'attaquant ne détient aucune allocation légitime sur le processeur quantique. L'attaque exploite la nature analogique du contrôle de qubit : parce que les impulsions de contrôle sont des formes d'onde continues plutôt que des commandes numériques discrètes, tout bruit EM dans la bande de fréquence pertinente (typiquement 4–8 GHz pour les transmons) sera indiscernable des tonalités de pilotage légitimes et ne peut donc pas être filtré par des schémas d'authentification classiques.",
    "D": "Déployer des séquences d'impulsions personnalisées soigneusement conçues sur des qubits contrôlés par l'attaquant qui génèrent une diaphonie non intentionnelle à travers des couplages résiduels toujours actifs dans l'hamiltonien du dispositif, permettant à l'adversaire d'induire des erreurs de phase ou des rotations non intentionnelles sur des qubits victimes situés à plusieurs sites de réseau de distance sans nécessiter de connectivité directe.",
    "solution": "D"
  },
  {
    "id": 297,
    "question": "Pourquoi l'échange de jetons dans les protocoles d'informatique quantique distribuée doit-il tenir compte des fenêtres de synchronisation de portes ?",
    "A": "Les protocoles de génération d'intrication distribuée tels que les schémas photoniques annoncés produisent des paires de Bell avec une gigue temporelle héritée des événements de détection probabilistes, nécessitant des fenêtres de synchronisation pour garantir que les deux nœuds consomment les paires partagées dans les limites du temps de décohérence. Sans coordination, un nœud peut détenir sa moitié d'une paire intriquée pendant que le qubit du nœud partenaire subit une relaxation, détruisant les corrélations avant que les protocoles de téléportation de porte distribuée ne se terminent. L'échange de jetons impose l'alignement temporel des calendriers de consommation.",
    "B": "Les protocoles d'échange de jetons encodent les graphes de dépendance de portes dans des flux de métadonnées classiques qui spécifient quelles opérations distribuées doivent être complétées avant que les portes ultérieures puissent s'exécuter. Puisque la téléportation quantique nécessite que les résultats de mesure se propagent entre les nœuds avant que les unitaires de correction ne soient appliqués, les fenêtres de synchronisation garantissent que la latence de communication classique ne dépasse pas le temps de cohérence des qubits en attente, prévenant les erreurs induites par décohérence dans les circuits distribués qui dépendent du maintien de l'intrication à travers les délais de communication.",
    "C": "Les protocoles de réseau quantique implémentent des seaux de jetons qui régulent le rythme auquel les nœuds consomment les ressources intriquées partagées, garantissant que les taux de génération de paires de Bell restent équilibrés sur tous les liens réseau. Sans synchronisation, les nœuds avec une distillation d'intrication plus rapide épuiseraient leur allocation de jetons pendant que les nœuds partenaires prennent du retard, créant des décalages temporels où les qubits restent inactifs au-delà de leurs limites de cohérence T2 en attendant que les partenaires rattrapent leur retard, dégradant ainsi la fidélité globale du circuit distribué.",
    "D": "La latence de communication classique doit correspondre aux exigences temporelles afin que les paires de Bell partagées arrivent aux deux nœuds dans leurs fenêtres de temps de cohérence, garantissant que les ressources intriquées restent viables pour les opérations de porte distribuées ultérieures. Sans synchronisation, la décohérence détruit les corrélations avant que les protocoles de téléportation ne puissent se terminer, causant l'échec du calcul distribué en raison de l'expiration des liens quantiques entre les nœuds.",
    "solution": "D"
  },
  {
    "id": 298,
    "question": "Pourquoi la réduction du nombre de paramètres dans les HQNNs est-elle particulièrement importante dans le contexte de l'informatique quantique ?",
    "A": "Moins de paramètres réduisent directement le coût d'estimation du gradient puisque chaque paramètre nécessite plusieurs évaluations de circuit utilisant des règles de décalage de paramètre ou des différences finies, et sur les dispositifs NISQ avec des budgets de coups limités et un bruit d'échantillonnage élevé, évaluer les gradients pour des centaines de paramètres consomme des ressources de mesure prohibitives, rendant la réduction de paramètres essentielle pour atteindre la convergence dans des contraintes expérimentales pratiques où le total d'exécutions de circuit doit rester traitable compte tenu des limitations d'accès matériel et des échelles de temps de décohérence.",
    "B": "La réduction de paramètres traite principalement l'entraînabilité en atténuant les phénomènes de plateau stérile : les études empiriques montrent que les ansätze sur-paramétrés avec des paramètres dépassant O(n²) dans les systèmes à n qubits présentent des gradients qui s'annulent exponentiellement en raison d'effets de concentration de mesure, tandis que les architectures efficaces en paramètres avec O(n) ou O(n log n) paramètres maintiennent une échelle de gradient polynomiale, permettant la convergence d'optimisation là où les circuits fortement paramétrés rencontreraient des paysages de perte plats indépendamment de la stratégie d'initialisation ou du choix d'optimiseur, rendant l'économie de paramètres essentielle pour accéder à un signal de gradient significatif.",
    "C": "Réduire les paramètres minimise la profondeur du circuit, ce qui est crucial en raison de la décohérence quantique, puisque moins de paramètres correspondent généralement à des circuits moins profonds avec moins de couches de portes qui terminent l'exécution avant que le bruit accumulé ne détruise la cohérence quantique et ne rende les résultats de calcul peu fiables.",
    "D": "Des nombres de paramètres plus faibles réduisent la susceptibilité aux erreurs d'estimation de gradient induites par le bruit : sur le matériel actuel, chaque gradient de paramètre nécessite O(1/ε²) coups pour atteindre une précision ε, et le bruit de mesure se compose à travers les dimensions de paramètres, donc réduire les paramètres de P à P/k améliore le rapport signal sur bruit du gradient d'un facteur √k sous des budgets de coups fixes, permettant une optimisation fiable sur des dispositifs bruyants où l'estimation de gradient en haute dimension serait dominée par des fluctuations d'échantillonnage qui obscurcissent les vraies directions de gradient et empêchent la convergence.",
    "solution": "C"
  },
  {
    "id": 299,
    "question": "L'initialisation à valeur zéro est généralement évitée dans les circuits variationnels car elle :",
    "A": "Crée des circuits où toutes les portes de rotation deviennent des opérations d'identité initialement, éliminant la génération d'intrication dans la première itération d'entraînement et forçant l'optimiseur à commencer à partir d'un état produit. Cela retarde l'exploration des régions intriquées où les solutions optimales résident typiquement, nécessitant des époques d'optimisation supplémentaires pour échapper au sous-espace séparable et causant souvent une convergence prématurée vers des minima locaux dans le régime classiquement simulable avant d'atteindre les régions d'avantage quantique.",
    "B": "Piège l'optimiseur dans des régions de symétrie plates où les gradients s'annulent et le paysage de paramètres devient dégénéré, empêchant l'exploration efficace de l'espace de solution et conduisant souvent à des échecs de convergence ou à des minima sous-optimaux",
    "C": "Produit des configurations de paramètres où les symétries du circuit causent l'annulation identique des gradients de la fonction de coût en raison de contributions positives et négatives égales provenant de chemins de calcul parallèles, créant des plateaux artificiels non liés aux plateaux stériles",
    "D": "Induit un masquage de gradient où les dérivées de paramètres s'annulent en raison d'arrangements de portes symétriques autour d'angles de rotation nuls, créant des points stationnaires fallacieux qui satisfont les conditions d'optimalité du premier ordre sans représenter de véritables extrema du paysage de coût",
    "solution": "B"
  },
  {
    "id": 300,
    "question": "Dans les implémentations pratiques de distribution de clés quantiques couvrant des distances métropolitaines (50-100 km), où les utilisateurs légitimes Alice et Bob doivent établir des clés sécurisées tout en faisant face à des pertes de canal réalistes et à une écoute potentielle, quel est le compromis principal lors de l'utilisation de nœuds de confiance simplifiés par rapport aux architectures de nœuds de confiance complets ?",
    "A": "Les nœuds de confiance simplifiés modifient fondamentalement le modèle de sécurité en nécessitant des paires quantiques intriquées pré-partagées pour l'authentification continue à chaque point de relais intermédiaire, plutôt que de s'appuyer uniquement sur des canaux classiques authentifiés pour la réconciliation de base publique.",
    "B": "La perte cumulée de photons à travers plusieurs sauts dans les réseaux de nœuds simplifiés augmente de près d'un ordre de grandeur par rapport à la transmission directe à distance totale équivalente, principalement parce que ces nœuds manquent de mémoire quantique et de capacités d'échange d'intrication requises pour une véritable fonctionnalité de répéteur quantique.",
    "C": "Les nœuds de confiance simplifiés introduisent un compromis critique entre sécurité et complexité d'implémentation dans lequel ils tolèrent significativement moins de bruit et nécessitent de meilleures conditions de canal par rapport aux nœuds de confiance complets qui peuvent effectuer des mesures quantiques intermédiaires et un traitement classique — cette sensibilité accrue au bruit nécessite des protocoles de correction d'erreurs plus sophistiqués, des seuils d'efficacité de détecteur plus élevés et des limites plus strictes sur les comptages de photons de fond pour maintenir le même taux de génération de clés effectif, restreignant ainsi la flexibilité de déploiement dans des environnements métropolitains réels avec des conditions atmosphériques variables, des imperfections de fibre, des comptages sombres de détecteur et d'autres déficiences pratiques qui créent des taux d'erreur de bits quantiques élevés qui doivent être maintenus en dessous des limites plus strictes imposées par les marges de tolérance d'erreur réduites de l'architecture de nœud simplifiée.",
    "D": "Puisque les nœuds simplifiés ne peuvent pas effectuer de vérification directe d'état quantique sur les photons passants sans détruire l'information de clé, ils doivent plutôt s'appuyer sur des schémas de vérification de résultats de mesure basés sur des certificats classiques où chaque nœud signe numériquement et transmet les statistiques de détecteur aux points terminaux pour validation rétrospective.",
    "solution": "C"
  },
  {
    "id": 301,
    "question": "Qu'est-ce que la fidélité de porte en informatique quantique ?",
    "A": "La précision avec laquelle une porte quantique physiquement implémentée correspond à l'opération de porte théorique idéale, quantifiant le recouvrement entre la transformation réellement appliquée aux états quantiques et l'évolution unitaire attendue. Cette métrique, généralement exprimée comme un nombre entre 0 et 1, capture toutes les sources d'erreur incluant la décohérence, les imperfections de contrôle et la diaphonie, ce qui en fait l'indicateur fondamental de performance pour la qualité des portes.",
    "B": "La distance de trace entre le canal quantique implémenté et l'opération unitaire idéale, moyennée sur tous les états purs d'entrée tirés uniformément de la sphère de Bloch selon la mesure de Haar. Cette métrique capture les erreurs cohérentes systématiques et le bruit incohérent stochastique, quantifiant la qualité de la porte à travers la distinguabilité minimale entre les opérations réelles et cibles. Exprimée entre 0 et 1, elle prend en compte la décohérence, les erreurs de contrôle et les fuites, ce qui en fait la métrique d'évaluation standard pour la performance des portes.",
    "C": "La fidélité de processus entre le canal quantique réel et la porte unitaire cible, calculée en moyennant la fidélité d'état sur tous les états d'entrée pondérés par la mesure de Haar, puis en prenant le conjugué complexe du résultat avant normalisation. Cela capture la façon dont l'opération implémentée préserve la cohérence quantique à travers tout l'espace d'états, tenant compte à la fois des erreurs unitaires et des mécanismes de décohérence. La métrique varie de 0 à 1 et quantifie directement la qualité de la porte incluant la diaphonie et les imperfections de contrôle.",
    "D": "La distance de norme diamant entre le superopérateur implémenté et la porte unitaire idéale, maximisée sur tous les états d'entrée possibles incluant ceux intriqués avec un système auxiliaire de dimension égale. Cette mesure de fidélité dans le pire des cas capture toutes les sources d'erreur—décohérence, diaphonie et imperfections de contrôle—en quantifiant la distinguabilité maximale atteignable par n'importe quel protocole quantique. Exprimée entre 0 et 1, elle fournit la métrique de certification fondamentale pour la qualité des portes dans les architectures d'informatique quantique tolérante aux fautes.",
    "solution": "A"
  },
  {
    "id": 302,
    "question": "Dans un système quantique distribué, quelle est la fonction principale de l'ordonnanceur de circuit ?",
    "A": "Surveiller la qualité des liens d'intrication et demander de manière préventive de nouvelles paires de Bell lorsque les prédictions de fidélité tombent en dessous des seuils opérationnels, en utilisant des données de tomographie en temps réel provenant d'échanges récents pour prévoir quelles connexions se dégraderont avant l'exécution des prochaines portes distantes, maintenant ainsi un tampon de ressources de haute fidélité qui empêche les blocages de circuit dus à l'épuisement de l'intrication à travers la topologie du réseau distribué",
    "B": "Réorganiser dynamiquement les portes distantes en fonction de la disponibilité actuelle des liens intriqués entre les nœuds et déterminer quelles opérations à deux qubits sont réellement prêtes à être exécutées compte tenu des ressources d'intrication distribuées, des contraintes de latence de communication et des chaînes de dépendances dans la structure du circuit",
    "C": "Partitionner le circuit quantique en sous-circuits indépendants pouvant s'exécuter en parallèle sur des nœuds distribués tout en respectant le taux de génération d'intrication entre nœuds comme contrainte, puis résoudre un problème d'optimisation pour minimiser le temps d'exécution total en chevauchant les opérations de portes locales avec la latence requise pour la distribution d'intrication distante, mettant effectivement en pipeline les phases de calcul et de communication pour saturer la bande passante disponible",
    "D": "Coordonner le timing des opérations unitaires locales avec l'arrivée d'informations de syndrome non locales provenant de nœuds voisins de sorte que les cycles de correction d'erreur restent causalement cohérents malgré les délais réseau variables, implémentant un protocole de synchronisation d'horloge logique qui garantit que les mesures de stabilisateurs se terminent dans l'ordre relatif approprié sur tous les nœuds même lorsque les temps d'exécution des portes physiques diffèrent entre processeurs quantiques hétérogènes",
    "solution": "B"
  },
  {
    "id": 303,
    "question": "Dans le contexte du décodage de codes de surface et d'autres codes topologiques à grande échelle, les chercheurs ont exploré l'adaptation des méthodes de groupe de renormalisation de matrice de densité (DMRG) issues de la physique de la matière condensée. La motivation provient de la croissance exponentielle de la complexité de décodage classique à mesure que la distance de code augmente. Étant donné une mesure de syndrome sur un code de surface de distance d avec conditions aux limites, quel avantage computationnel spécifique les décodeurs basés sur DMRG offrent-ils par rapport aux approches de décodage exact par maximum de vraisemblance ?",
    "A": "Le décodage de code de surface basé sur DMRG reformule le motif de syndrome comme un hamiltonien classique effectif où l'état fondamental correspond à la configuration d'erreur de poids minimal cohérente avec les syndromes observés, puis utilise des balayages itératifs de réseau de tenseurs pour optimiser variationnellement la représentation de chaîne d'erreur, atteignant une complexité temporelle polynomiale en distance de code en restreignant la structure d'intrication des configurations d'erreur candidates à celles représentables avec une dimension de liaison χ, qui capture les chaînes d'erreur localement corrélées typiques des modèles de bruit physiques tout en écartant systématiquement exponentiellement de nombreuses configurations de haute intrication qui domineraient les recherches par maximum de vraisemblance.",
    "B": "Les décodeurs basés sur DMRG exploitent l'observation que les configurations d'erreur probables dans les codes de surface peuvent être efficacement représentées comme des états produits matriciels le long de chaînes d'erreur unidimensionnelles, ce qui réduit les exigences mémoire d'une échelle exponentielle en distance de code à une échelle polynomiale grâce à des contractions de réseaux de tenseurs structurées qui capturent les corrélations essentielles dans la distribution d'erreur tout en écartant exponentiellement de nombreuses configurations improbables qui domineraient les approches exactes par maximum de vraisemblance",
    "C": "Le décodeur DMRG construit une représentation d'opérateur produit matriciel de l'opérateur de projection de syndrome qui impose la cohérence entre les syndromes observés et les chaînes d'erreur candidates, puis effectue des contractions de tenseurs séquentielles le long de la frontière spatiale du réseau pour calculer les probabilités marginales pour chaque violation de stabilisateur de plaquette, permettant une propagation de croyance efficace à travers l'espace de code où le coût computationnel évolue polynomialement avec la distance de code parce que les dimensions de liaison de tenseur restent bornées lorsqu'elles sont restreintes aux configurations d'erreur satisfaisant la contrainte homologique que les chaînes d'erreur forment des boucles fermées ou se terminent aux frontières, contrairement aux recherches par maximum de vraisemblance qui doivent énumérer toutes les configurations de chaîne topologiquement distinctes.",
    "D": "Les méthodes DMRG transforment le problème de décodage de syndrome en une contraction de réseau de tenseurs où chaque bit de syndrome correspond à un indice de tenseur dans un réseau dont la valeur de contraction égale la probabilité de l'erreur de maximum de vraisemblance étant donné les résultats de mesure, mais plutôt que d'effectuer une contraction exacte (exponentiellement coûteuse), l'approche DMRG utilise des décompositions en valeurs singulières séquentielles le long d'un chemin unidimensionnel à travers le réseau de code bidimensionnel pour approximer la contraction en ne conservant que les χ plus grandes valeurs singulières à chaque étape, où χ est choisi tel que l'erreur de troncature reste en dessous du taux d'erreur physique du matériel quantique, atteignant ainsi un décodage approximatif par maximum de vraisemblance en temps polynomial avec dégradation de précision contrôlable.",
    "solution": "B"
  },
  {
    "id": 304,
    "question": "Quelle est la relation entre le choix d'ansatz de circuit et l'occurrence de plateaux arides dans les réseaux de neurones quantiques ?",
    "A": "Les ansätze efficaces en matériel avec structure globale, qui maximisent l'utilisation des ensembles de portes natifs et minimisent les frais de compilation, ont démontré dans plusieurs études concentrer la variance du gradient de manière exponentielle à mesure que la profondeur du circuit augmente, parce que l'intrication de type aléatoire qu'ils génèrent à travers tous les qubits crée un paysage de coût qui devient exponentiellement plat dans l'espace de paramètres de haute dimension.",
    "B": "La stratégie d'initialisation compte — une mauvaise initialisation des paramètres augmente significativement la probabilité que les gradients s'évanouissent exponentiellement à travers le paysage. Lorsque les paramètres sont échantillonnés uniformément à partir de plages qui ne respectent pas la structure de l'algèbre de Lie sous-jacente au circuit, l'état initial résultant explore une région plate de la fonction de coût où les magnitudes de gradient évoluent comme O(1/2^n) avec le nombre de qubits.",
    "C": "Les ansätze spécifiques au problème qui encodent la connaissance du domaine aident à éviter une croissance inutile de l'intrication, les structures d'intrication limitées contraignent la variance du gradient en restreignant la connectivité aux voisinages locaux, et les stratégies d'initialisation intelligentes qui respectent la structure de l'algèbre de Lie sous-jacente peuvent retarder l'apparition de gradients exponentiellement évanescents, formant ensemble une approche d'atténuation à plusieurs volets.",
    "D": "Les structures d'intrication restreintes qui limitent la connectivité entre qubits aux voisinages locaux ou aux topologies arborescentes empêchent le système d'explorer le plein espace de Hilbert, ce qui à son tour contraint la fonction de coût à une variété de dimension inférieure où les gradients restent bornés loin de zéro. Cette approche échange l'expressivité contre l'entraînabilité : en interdisant l'intrication à longue portée, le circuit ne peut plus représenter certains états cibles hautement intriqués.",
    "solution": "C"
  },
  {
    "id": 305,
    "question": "Quel rôle joue la communication classique dans le processus de téléportation quantique ?",
    "A": "Envoie deux bits classiques encodant le résultat de mesure de Bell afin que le récepteur sache laquelle des quatre corrections de Pauli possibles appliquer à sa moitié de la paire intriquée, la transformant en l'état cible qui était initialement possédé par l'émetteur.",
    "B": "Transmet le résultat de mesure de Bell à deux bits obtenu en projetant le qubit de l'émetteur et sa ressource intriquée sur la base de Bell, permettant au récepteur de déterminer quelle unitaire locale du groupe de Pauli {I, X, Z, XZ} doit être appliquée pour décoder sa particule dans l'état téléporté attendu.",
    "C": "Communique le résultat de la mesure conjointe de l'émetteur sur le qubit d'entrée et sa moitié de la ressource intriquée, transmettant lequel des quatre états de Bell a été observé afin que le récepteur puisse effectuer l'opération conditionnelle correspondante pour faire pivoter son qubit dans l'état cible qui réplique l'information quantique originale.",
    "D": "Délivre les résultats de mesure de la projection de l'émetteur dans la base de Bell, fournissant deux bits qui spécifient quelle correction dépendante de la base le récepteur doit exécuter sur son qubit intriqué pour compléter le protocole de téléportation en transformant son état conditionnel en une réplique exacte du qubit original qui a été mesuré.",
    "solution": "A"
  },
  {
    "id": 306,
    "question": "Quelle approche est la plus couramment utilisée pour implémenter la classification quantique avec des circuits quantiques paramétrés ?",
    "A": "Application d'opérations unitaires d'encodage de données suivies de circuits variationnels paramétrés entraînés par descente de gradient et mesure d'observables dépendant des classes, où les portes paramétrées sont optimisées pour mapper les caractéristiques d'entrée encodées vers des états quantiques dont les statistiques de mesure distinguent entre les classes, permettant l'apprentissage supervisé par rétropropagation de fonctions de coût calculées à partir des résultats de mesure et des étiquettes classiques.",
    "B": "Encodage de données par modulation d'amplitude paramétrée et mesure d'observables de Pauli distinguant les classes — cette approche prépare chaque vecteur de caractéristiques d'entrée en appliquant des portes de rotation paramétrées avec des angles proportionnels aux valeurs des caractéristiques, construisant un état quantique où les amplitudes encodent les données normalisées, puis mesure un ensemble d'opérateurs de Pauli commutants dont les valeurs d'espérance sont introduites dans une fonction discriminante classique. Les décisions de classification sont extraites de ces vecteurs de valeurs d'espérance en calculant des frontières de décision dans l'espace des résultats de mesure, permettant l'apprentissage supervisé par ajustement des poids des observables de lecture.",
    "C": "Cartes de caractéristiques quantiques basées sur des noyaux avec classification par machines à vecteurs de support — le problème de classification est formulé en plongeant les données classiques dans l'espace de Hilbert quantique via une carte de caractéristiques unitaire fixe U(x) qui encode l'entrée x comme un état quantique, puis en calculant les produits scalaires ⟨ψ(x)|ψ(x')⟩ entre états encodés pour construire une matrice de noyau. Ce noyau quantique quantifie la similarité entre points de données via des mesures de chevauchement basées sur l'interférence, permettant aux machines à vecteurs de support classiques de trouver des hyperplans de séparation optimaux dans l'espace de caractéristiques quantique sans entraînement explicite de circuits quantiques paramétrés.",
    "D": "Branchement quantique séquentiel basé sur des mesures avec propagation adaptative — cette méthode construit la classification en effectuant des mesures par couches d'observables dépendant des caractéristiques où chaque résultat de mesure conditionne les portes paramétrées suivantes via un contrôle classique de propagation. Chaque mesure projette sur un sous-espace correspondant à des décisions de classification partielles, et l'algorithme affine la classe prédite par des projections séquentielles jusqu'à atteindre un état de classification final. Les portes paramétrées entre mesures sont optimisées pour maximiser la séparabilité des états induite par les mesures, permettant l'apprentissage supervisé par entraînement des angles de rotation conditionnels.",
    "solution": "A"
  },
  {
    "id": 307,
    "question": "Quel est le principal avantage de concevoir des circuits quantiques avec une structure cyclique ?",
    "A": "La réutilisation des paramètres à travers les couches permet un entraînement et une optimisation efficaces des algorithmes quantiques variationnels en réduisant le nombre total de paramètres indépendants devant être ajustés, ce qui diminue la dimensionnalité du paysage d'optimisation classique et accélère la convergence. Lorsque les paramètres des portes se répètent périodiquement à travers la profondeur du circuit, les optimiseurs basés sur le gradient rencontrent moins de minima locaux et le problème des plateaux arides est atténué, car les corrélations entre couches créent des géométries structurées de fonctions de coût qui guident la recherche vers les optima globaux de manière plus fiable que les circuits profonds initialisés aléatoirement.",
    "B": "La dérive de calibration matérielle est naturellement atténuée car les séquences de portes périodiques permettent un étalonnage en temps réel à chaque limite de cycle, où la mesure répétée du même état logique permet le suivi de la dérive via un contrôle statistique de processus. Lorsque des couches paramétrées identiques se répètent avec une période L, les déviations de fidélité des portes entre cycles peuvent être détectées en comparant les valeurs d'espérance d'observables invariants cycliquement, permettant aux protocoles de recalibration adaptatifs de compenser les erreurs systématiques avant qu'elles ne s'accumulent au-delà des seuils de correction d'erreur, particulièrement important pour maintenir la précision de calcul lors de solveurs d'états propres variationnels nécessitant des centaines d'évaluations de circuit à travers des balayages de paramètres.",
    "C": "Les effets de découplage dynamique émergent automatiquement de l'application périodique de séquences de portes, où les symétries d'inversion temporelle dans des blocs de circuit répétés créent un contrôle Bang-Bang effectif qui supprime le bruit de basse fréquence. Lorsque des couches unitaires sont appliquées cycliquement avec une structure de signes alternés, les perturbations environnementales s'annulent en moyenne sur chaque période via une interférence destructive dans le référentiel de basculement, étendant les temps de cohérence sans ingénierie explicite d'impulsions. Cette propriété d'autocorrection devient particulièrement précieuse dans les algorithmes variationnels où la même structure de circuit est évaluée à plusieurs reprises, car les canaux de bruit systématiques sont intrinsèquement filtrés par la symétrie translationnelle de l'architecture cyclique.",
    "D": "La complexité de contraction des réseaux de tenseurs est réduite grâce aux conditions aux limites périodiques qui permettent des représentations en états de produits matriciels avec une dimension de liaison évoluant logarithmiquement plutôt qu'exponentiellement en profondeur de circuit. Lorsque les couches de portes se répètent avec une période L, le formalisme de matrice de transfert permet la simulation classique exacte des valeurs d'espérance en diagonalisant l'opérateur d'évolution à L étapes une fois et en l'élevant à une puissance, évitant la croissance exponentielle de dimension de liaison qui afflige la simulation de circuits génériques, ce qui s'avère essentiel pour valider les performances des algorithmes NISQ par étalonnage classique et débogage des implémentations matérielles avant déploiement sur des processeurs quantiques.",
    "solution": "A"
  },
  {
    "id": 308,
    "question": "Pourquoi l'informatique quantique distribuée est-elle considérée comme une approche évolutive pour les algorithmes quantiques, et quels défis introduit-elle ?",
    "A": "Les architectures distribuées donnent accès à des nombres totaux de qubits beaucoup plus importants en fédérant plusieurs processeurs quantiques, rendant possibles des instances de problèmes auparavant insolubles. Cependant, la limitation fondamentale est que pratiquement tous les algorithmes quantiques connus — du factorisation de Shor à la recherche de Grover en passant par les solveurs d'états propres variationnels — ont été conçus en supposant une connectivité totale des qubits au sein d'un dispositif monolithique. Par conséquent, presque tous les algorithmes utiles nécessitent une refonte architecturale complète pour décomposer les opérations en portes locales uniquement qui ne couplent jamais des qubits résidant sur différents nœuds physiques, ce qui augmente considérablement la profondeur des circuits et élimine souvent entièrement l'avantage quantique.",
    "B": "Les processeurs distribués exécutent des circuits quantiques en parallèle en découpant temporellement les opérations à travers des unités de traitement quantique indépendantes, multipliant efficacement le débit de calcul par le nombre de nœuds. Le principal défi est le maintien de la cohérence de phase à travers tous les processeurs via des signaux d'horloge synchronisés avec une précision sub-nanoseconde, car même de petits décalages temporels entre nœuds accumulent une décohérence qui dégrade la fidélité de l'ensemble du calcul distribué.",
    "C": "C'est évolutif car vous ajoutez des nœuds au lieu d'entasser plus de qubits sur une seule puce, bien que la correction d'erreur à travers plusieurs processeurs reste le problème le plus difficile à résoudre car les codes n'ont pas été conçus à l'origine pour des systèmes spatialement séparés.",
    "D": "Les architectures distribuées permettent l'évolutivité en agrégeant les qubits à travers plusieurs processeurs quantiques physiquement séparés plutôt qu'en exigeant toutes les ressources de calcul au sein d'un seul dispositif monolithique, qui fait face à des limites de fabrication fondamentales. Cependant, le goulot d'étranglement critique émerge dans l'établissement et le maintien de l'intrication à longue portée entre qubits sur différents nœuds, car les algorithmes quantiques nécessitent généralement une connectivité totale. La communication d'états quantiques entre processeurs exige soit des qubits volants à travers des canaux optiques (introduisant des pertes de photons) soit des protocoles d'échange d'intrication (consommant des portes et du temps supplémentaires), tous deux dégradant significativement la fidélité et la profondeur des circuits.",
    "solution": "D"
  },
  {
    "id": 309,
    "question": "Dans le développement d'algorithmes quantiques pour l'optimisation réelle de portefeuilles financiers, une équipe de recherche doit équilibrer l'avantage quantique théorique contre les limitations matérielles actuelles. Ils comparent différentes approches : l'une utilise une implémentation entièrement tolérante aux fautes de l'estimation d'amplitude quantique nécessitant 10^6 qubits logiques, une autre emploie QAOA sur des dispositifs NISQ avec ~100 qubits bruités, et une troisième propose un algorithme variationnel hybride qui délègue la plupart des calculs de manière classique. L'équipe doit décider quelle approche est la plus viable pour un déploiement dans 3-5 ans, considérant que les taux d'erreur se situent actuellement à 10^-3 par porte et les temps de cohérence autour de 100 microsecondes. Quel facteur est le plus critique pour déterminer si une approche quantique surpassera les méthodes d'optimisation classiques comme les solveurs de programmation en nombres entiers mixtes pour des portefeuilles avec 500-1000 actifs ?",
    "A": "La capacité d'encoder la matrice de covariance complète dans des états quantiques sans approximation, car tout prétraitement classique qui réduit la taille du problème éliminera l'avantage quantique avant même que l'algorithme ne s'exécute. Si des techniques de réduction de dimensionnalité comme l'ACP ou des méthodes de matrices creuses sont appliquées pour rendre le problème traitable pour le matériel quantique, alors le problème effectif résolu devient suffisamment petit pour que les solveurs classiques puissent le gérer efficacement.",
    "B": "L'existence d'une borne inférieure prouvée sur la complexité de l'algorithme classique pour cette classe de problèmes, établissant qu'aucun algorithme classique ne peut résoudre l'optimisation de portefeuille avec N actifs plus rapidement qu'un temps exponentiel dans le pire cas. Sans une telle preuve de difficulté, toute difficulté classique observée pourrait simplement refléter les limitations des heuristiques actuelles plutôt que des barrières de calcul fondamentales, ce qui signifie qu'un algorithme classique suffisamment astucieux pourrait émerger et égaler les performances quantiques.",
    "C": "Si la profondeur du circuit QAOA évolue mieux que O(n^2) avec le nombre d'actifs, car même avec le parallélisme quantique, les circuits profonds sur les dispositifs NISQ décohéreront avant de produire des résultats utiles étant donné les temps de cohérence actuels de ~100 microsecondes et les temps de porte de ~100 nanosecondes, limitant les circuits pratiques à des profondeurs inférieures à 1000 portes. La comparaison avec les solveurs classiques doit tenir compte du temps réel incluant les exécutions répétées de circuits pour l'optimisation des paramètres — généralement des milliers d'itérations — et le surcoût classique du traitement des résultats de mesure, du calcul des gradients et de la mise à jour des paramètres variationnels entre les tirs, qui peut dominer le temps d'exécution total et annuler les accélérations quantiques théoriques si le rapport profondeur-de-circuit-sur-taille-du-problème devient défavorable.",
    "D": "À quelle vitesse la correction d'erreur quantique atteint le seuil où les taux d'erreur logiques tombent en dessous de 10^-6, ce qui est le minimum nécessaire pour les applications financières qui exigent des résultats précis à six décimales pour la conformité réglementaire. Étant donné que les sorties d'optimisation de portefeuille doivent être certifiées selon les normes institutionnelles, tout taux d'erreur supérieur à ce seuil nécessitera des étapes de vérification classiques qui consomment plus de temps que l'algorithme quantique n'en économise.",
    "solution": "C"
  },
  {
    "id": 310,
    "question": "Qu'est-ce qu'une erreur de diaphonie en informatique quantique ?",
    "A": "Une erreur de couplage parasite où les impulsions de contrôle micro-ondes destinées à piloter les transitions dans un qubit cible fuient à travers des désadaptations d'impédance et des limites d'isolation de coupleur directionnel dans les résonateurs de lecture de qubits adjacents, créant des décalages Stark AC hors résonance qui font tourner les états de qubits voisins d'angles non désirés proportionnels au carré du rapport de désaccord. Cet encombrement spectral dans les architectures multiplexées en fréquence conduit à une accumulation de phase conditionnelle décrite par des termes Hamiltoniens ZZ résiduels ∝σᶻ⊗σᶻ, se manifestant comme des interactions intriquantes non intentionnelles pendant des opérations nominalement à un seul qubit. Les erreurs corrélées résultantes violent l'hypothèse d'erreur indépendante sous-jacente à la plupart des codes de correction d'erreur quantique, nécessitant une atténuation par mise en forme d'impulsion, découplage dynamique ou optimisations de compilateur tenant compte de la diaphonie.",
    "B": "Une erreur où un qubit interagit involontairement avec un qubit voisin via des mécanismes de couplage résiduel toujours actifs tels que des chemins capacitifs ou inductifs parasites, conduisant à des changements non désirés cohérents ou incohérents dans son état quantique. Cette interaction parasite peut se manifester comme une accumulation de phase conditionnelle indésirable, des termes de couplage ZZ parasites dans l'Hamiltonien, ou une fuite d'impulsions de contrôle destinées à un qubit dans le spectre encombré en fréquence de qubits adjacents, dégradant finalement les fidélités des portes et introduisant des erreurs corrélées qui compliquent la correction d'erreur.",
    "C": "Un mécanisme de couplage cohérent où un échange d'énergie résonant entre qubits adjacents se produit via des interactions Jaynes-Cummings fixes médiées par des résonateurs de ligne de transmission partagés, implémentant des portes iSWAP ou √iSWAP non intentionnelles pendant les périodes d'inactivité lorsque les qubits sont garés à leurs fréquences d'interaction. Ce couplage d'échange résiduel accumule une phase conditionnelle φ=∫J(t)dt pendant les temps d'inactivité des qubits, où J(t) représente l'intensité de couplage dépendant du temps modulée par des éléments de coupleur accordables en flux. La génération d'intrication résultante entre qubits de calcul et spectateurs crée une fuite hors de l'espace de code protégé, se manifestant comme des rotations systématiques corrélées à travers plusieurs qubits qui ne peuvent pas être corrigées par des codes stabilisateurs standards.",
    "D": "Un canal de décohérence où l'interférence électromagnétique provenant de courants de polarisation variant dans le temps dans des lignes de flux supraconductrices se couple aux Hamiltoniens de contrôle des qubits via l'inductance mutuelle, injectant un bruit 1/f de basse fréquence qui module les fréquences de transition des qubits et cause un déphasage au-delà des limites T₂* intrinsèques. Cette diaphonie classique se manifeste lorsque des impulsions de courant destinées à accorder la fréquence d'un qubit via sa boucle SQUID génèrent un flux magnétique traversant les boucles de qubits adjacentes, créant des décalages de fréquence corrélés qui font tourner les états des qubits de manière dépendante du temps. La nature stochastique de ces fluctuations de flux introduit des erreurs non-Markoviennes avec des temps de corrélation comparables aux durées de porte, nécessitant une caractérisation via des protocoles d'étalonnage randomisé entrelacé qui mesurent les fidélités de porte à deux qubits conditionnées par des opérations simultanées à un seul qubit.",
    "solution": "B"
  },
  {
    "id": 311,
    "question": "Considérez un algorithme de mappage de qubits tenant compte du bruit, conçu pour optimiser les fidélités des portes sur un processeur supraconducteur de 127 qubits présentant des caractéristiques T1 et T2 variant dans le temps. L'algorithme utilise des données d'étalonnage historiques pour prédire les affectations optimales de qubits pour un circuit donné. Lequel des scénarios suivants dégraderait le plus significativement les performances de l'algorithme et pourquoi ?",
    "A": "Une augmentation du nombre total de qubits physiques disponibles sur le processeur, ce qui élargit l'espace de recherche mais offre davantage d'options à haute fidélité pour les portes à deux qubits critiques — Bien que la complexité combinatoire du mappage de qubits évolue exponentiellement avec la taille du processeur, les algorithmes heuristiques modernes utilisant une recherche guidée par apprentissage automatique ou une optimisation génétique peuvent naviguer efficacement dans des espaces de solutions plus grands en exploitant les motifs de localité présents dans les circuits quantiques typiques.",
    "B": "Des circuits constitués entièrement de portes de Clifford, puisque celles-ci possèdent des règles de décomposition bien établies et le problème de mappage se réduit à un isomorphisme de graphes avec des heuristiques polynomiales connues — La structure algébrique particulière des opérations de Clifford permet de les simuler efficacement en utilisant le formalisme des stabilisateurs, ce qui fournit des modèles exacts de propagation d'erreurs que l'algorithme de mappage peut exploiter lors de l'optimisation.",
    "C": "Des caractéristiques de bruit matériel qui fluctuent sur des échelles de temps comparables ou plus rapides que le temps d'exécution du circuit, rendant les données d'étalonnage historiques peu fiables pour prédire les performances actuelles du dispositif — Lorsque les temps de cohérence des qubits, les fidélités des portes et les erreurs de lecture varient significativement pendant ou entre les exécutions de circuits, les prédictions de l'algorithme de mappage basées sur des mesures passées deviennent inexactes, conduisant à des affectations de qubits sous-optimales qui échouent à éviter les régions actuellement dégradées du processeur et résultant en des taux d'erreur globaux plus élevés que ce que des stratégies adaptatives en temps réel alternatives permettraient d'obtenir.",
    "D": "Intégration avec des méthodes de contraction de réseaux de tenseurs pour le découpage de circuits, ce qui peut réduire la profondeur effective du circuit mais introduit un surcoût classique supplémentaire dans le pipeline de compilation — Lorsque l'algorithme de mappage s'interface avec des stratégies de décomposition en réseaux de tenseurs qui partitionnent de grands circuits en sous-circuits plus petits, il doit maintenant optimiser les affectations de qubits non seulement pour une seule exécution monolithique mais à travers plusieurs tranches qui peuvent avoir des préférences de placement conflictuelles.",
    "solution": "C"
  },
  {
    "id": 312,
    "question": "Quelle est la signification des conditions de Knill-Laflamme dans la théorie de la correction d'erreurs quantiques ?",
    "A": "Définir les exigences énergétiques minimales pour la mise en œuvre de la correction d'erreurs quantiques en quantifiant le coût thermodynamique de l'inversion des processus de décohérence.",
    "B": "Elles établissent une borne supérieure sur le nombre de qubits physiques nécessaires pour tout code de correction d'erreurs quantiques, qui dépend de la distance du code et du nombre de qubits logiques protégés contre la décohérence environnementale.",
    "C": "Prouver que des états arbitraires inconnus ne peuvent pas être clonés, ce qui signifie que la correction d'erreurs quantiques doit fonctionner différemment des schémas de redondance classiques. Les conditions formalisent cette contrainte de non-clonage en montrant que toute tentative de copier l'information quantique pour la détection d'erreurs perturbe nécessairement l'état protégé.",
    "D": "Nécessaires et suffisantes pour corriger un ensemble d'erreurs donné — ces conditions fournissent la caractérisation mathématique complète permettant de déterminer quand un code quantique peut détecter et corriger avec succès des erreurs spécifiques sans perturber l'information logique encodée. Plus précisément, elles stipulent qu'un code C peut corriger les erreurs de l'ensemble E si et seulement si les éléments de matrice ⟨i|E†_a E_b|j⟩ sont indépendants des états de base du code |i⟩, |j⟩ pour tous les opérateurs d'erreur E_a, E_b dans E. Ce critère capture élégamment l'exigence selon laquelle les syndromes d'erreur doivent être extractibles sans révéler quoi que ce soit sur l'information quantique protégée elle-même, fournissant à la fois un test pratique de viabilité du code et un fondement théorique pour concevoir de nouveaux schémas de correction d'erreurs sur des modèles d'erreur arbitraires.",
    "solution": "D"
  },
  {
    "id": 313,
    "question": "Dans un protocole de distribution quantique de clés indépendant du dispositif fonctionnant sur un canal avec pertes ayant une efficacité de détection η = 0,82 et une valeur CHSH observée S = 2,31, vous suspectez que la taille finie du bloc (n = 10^6 tours) limite votre taux de clé sécurisée. Le taux de clé brute avant amplification de confidentialité est de 1,2 × 10^5 bits. Quelle technique spécifique traite le plus efficacement les effets de clé finie dans ce régime pour maximiser la longueur de clé sécurisée extractible ?",
    "A": "Les fonctions de hachage universelles pour l'amplification de confidentialité, car elles sont des extracteurs optimaux démontrés pour le post-traitement classique indépendamment de la taille du bloc et peuvent extraire essentiellement toute la min-entropie disponible de la clé brute même lorsque n est relativement modeste, comme le montre le lemme du hachage résiduel.",
    "B": "Les techniques d'estimation de min-entropie, qui donnent de meilleures bornes sur l'information de l'adversaire lorsque vous disposez de statistiques limitées en utilisant des inégalités de concentration spécifiquement adaptées aux corrélations quantiques plutôt que des bornes classiques du pire cas. Des techniques avancées comme le théorème d'accumulation d'entropie permettent de suivre la min-entropie tour par tour et de l'agréger d'une manière beaucoup moins pessimiste que l'application des bornes de Hoeffding au bloc complet, récupérant typiquement 40-60% du matériel de clé qui serait perdu avec des corrections de taille finie trop conservatrices.",
    "C": "Les cadres de sécurité composable qui fournissent des bornes strictes de taille finie sur l'écart par rapport à la sécurité idéale, vous permettant de calculer des paramètres précis de correction et de secret en fonction de n et de la probabilité d'échec. Ces cadres emploient des inégalités de concentration optimisées pour les corrélations quantiques afin d'estimer les intervalles de confiance autour des statistiques observées comme la valeur CHSH, puis propagent ces incertitudes à travers la preuve de sécurité pour déterminer combien de clé doit être sacrifiée pour l'amplification de confidentialité. En utilisant des bornes de queue plus strictes spécifiques aux violations d'inégalités de Bell plutôt que des inégalités de Hoeffding génériques, les cadres composables récupèrent significativement plus de clé sécurisée que les analyses asymptotiques.",
    "D": "La génération de nombres aléatoires quantiques pour étendre le matériel de clé brute avant l'amplification de confidentialité, augmentant efficacement artificiellement votre taille d'échantillon en utilisant une source d'entropie quantique certifiée pour générer de l'aléatoire supplémentaire indépendant qui peut être XORé avec les bits de clé brute. Cette technique, parfois appelée expansion d'aléatoire quantique, permet de démarrer à partir de la clé brute relativement petite vers un pool beaucoup plus grand de bits aléatoires de haute qualité qui apparaissent statistiquement indépendants de tout artefact de taille finie dans les données DIQKD originales.",
    "solution": "C"
  },
  {
    "id": 314,
    "question": "Pourquoi les solutions matérielles sont-elles considérées pour le post-traitement de la distribution quantique de clés (QKD) ?",
    "A": "Réduction de latence pour les protocoles à variables continues — les accélérateurs matériels dédiés (FPGA, ASIC personnalisés) permettent une réconciliation de modulation gaussienne en temps réel grâce au décodage parallèle de syndromes de codes LDPC multidimensionnels, traitant les mesures de quadrature à des taux (gigaéchantillons par seconde) que les implémentations logicielles ne peuvent soutenir, ce qui est essentiel car les systèmes CV-QKD génèrent des données gaussiennes corrélées nécessitant une réconciliation immédiate avant que les effets de décohérence ne s'accumulent, rendant les solutions matérielles nécessaires pour maintenir le flux de clés continu requis pour les communications sécurisées à large bande passante sans introduire de délais de traitement qui compromettraient la synchronisation.",
    "B": "Débit de calcul et efficacité énergétique — les accélérateurs matériels dédiés (FPGA, ASIC) peuvent exécuter les protocoles de réconciliation d'information et d'amplification de confidentialité plusieurs ordres de grandeur plus rapidement que les processeurs génériques tout en consommant moins d'énergie par bit traité, ce qui est critique car les systèmes QKD à haute vitesse génèrent du matériel de clé brute à des taux (mégabits par seconde) qui submergent les implémentations logicielles, créant des goulots d'étranglement qui limiteraient autrement le taux pratique de génération de clés sécurisées et rendraient le post-traitement en temps réel infaisable.",
    "C": "Sécurité améliorée grâce au traitement physiquement isolé — les modules matériels avec conception à isolation totale empêchent les fuites par canaux auxiliaires pendant l'amplification de confidentialité en isolant l'étape d'extraction d'aléatoire des systèmes connectés au réseau, garantissant que les valeurs intermédiaires des fonctions de hachage universelles ne résident jamais dans la mémoire générique où les attaques par canal temporel de cache ou les vulnérabilités d'exécution spéculative pourraient exposer des informations partielles sur la clé. Cette séparation physique est critique car le post-traitement implique de manipuler la clé tamisée brute avant la compression finale, créant des fenêtres où les canaux auxiliaires de calcul pourraient théoriquement fuir de l'information vers des adversaires ayant un accès physique à l'infrastructure classique supportant la liaison QKD.",
    "D": "Chronométrage déterministe pour les preuves de sécurité composable — les implémentations matérielles fournissent une exécution précise au cycle près du protocole de correction d'erreurs en cascade, garantissant que le nombre réel d'échanges de parité correspond à l'analyse théorique utilisée dans les bornes de sécurité à clé finie, ce qui est critique car les cadres de sécurité composable nécessitent une comptabilité précise de l'information révélée pendant la réconciliation. Les implémentations logicielles introduisent une latence variable et des chemins d'exécution non déterministes qui créent une incertitude dans le nombre exact de bits divulgués, forçant des estimations conservatrices qui réduisent le taux final de clé sécurisée en dessous de ce que le QBER mesuré supporterait théoriquement avec des caractéristiques de chronométrage garanties.",
    "solution": "B"
  },
  {
    "id": 315,
    "question": "Comment les HQNN se comportent-ils par rapport aux modèles classiques comme TF-IDF et LSTM dans les tâches d'appariement d'entités ?",
    "A": "Les réseaux de neurones quantiques hybrides atteignent une précision comparable aux références classiques tout en utilisant significativement moins de paramètres entraînables — nécessitant typiquement seulement 20-40% du nombre de paramètres requis par les architectures LSTM équivalentes pour atteindre des scores F1 similaires sur les benchmarks standards d'appariement d'entités, démontrant une efficacité paramétrique supérieure qui se traduit par une convergence d'entraînement plus rapide et un surapprentissage réduit sur les ensembles de données plus petits.",
    "B": "Les réseaux de neurones quantiques hybrides démontrent une efficacité paramétrique relative aux références classiques, nécessitant environ 60-80% des paramètres requis par les architectures LSTM pour atteindre des scores F1 légèrement inférieurs — cet avantage provient de cartes de caractéristiques quantiques qui encodent implicitement les corrélations non linéaires, bien que l'écart de performance absolu reste dans les 1-2% sur les benchmarks standards, suggérant que la réduction de paramètres se fait au prix de pertes mineures de capacité représentationnelle qui deviennent négligeables uniquement sur des ensembles de données d'appariement d'entités hautement structurés.",
    "C": "Les architectures quantiques-neuronales hybrides égalent la précision des références classiques tout en utilisant moins de paramètres, typiquement 30-50% des comptes LSTM équivalents — cependant, cette efficacité se manifeste principalement lors de l'inférence plutôt que lors de l'entraînement, car les gradients de paramètres quantiques nécessitent significativement plus de coups de mesure par étape de mise à jour pour atteindre une variance d'estimation de gradient comparable, résultant en des temps d'entraînement plus longs malgré le nombre réduit de paramètres et rendant les gains d'efficacité pratiques dépendants des capacités de taux de coups du matériel.",
    "D": "Les réseaux de neurones quantiques hybrides atteignent des scores F1 comparables aux références LSTM tout en utilisant 25-45% moins de paramètres entraînables — mais cette efficacité paramétrique dérive principalement des couches d'embedding classiques plutôt que des composants quantiques, puisque les circuits quantiques variationnels apportent des améliorations d'expressivité négligeables par rapport aux caractéristiques de Fourier aléatoires lorsque la profondeur du circuit reste en dessous du seuil d'intrication requis pour un avantage quantique véritable, faisant de l'efficacité observée une conséquence de la réduction agressive de dimensionnalité dans l'architecture hybride plutôt que des bénéfices de calcul quantique.",
    "solution": "A"
  },
  {
    "id": 316,
    "question": "Quelle caractéristique unique fournirait un Protocole de Réseau Privé Virtuel Quantique ?",
    "A": "Un VPN quantique établirait une sécurité théorique de l'information pour les points d'extrémité du tunnel grâce à une distribution quantique de clés à variables continues s'exécutant sur la même infrastructure de fibre optique que le trafic internet classique, avec des garanties de sécurité dérivées des relations d'incertitude de Heisenberg qui empêchent la mesure précise simultanée d'observables de quadrature conjuguées. Contrairement aux VPN conventionnels dont la sécurité dépend d'hypothèses de calcul sur les problèmes de logarithme discret ou de courbe elliptique, la confidentialité du protocole quantique reste prouvablement sécurisée contre des adversaires disposant de ressources de calcul arbitraires car les tentatives d'écoute introduisent nécessairement des erreurs de déplacement dans l'espace des phases détectables par les statistiques de mesure homodyne. Cependant, l'implémentation pratique exige que les deux points d'extrémité du tunnel possèdent des canaux classiques authentifiés établis par des secrets pré-partagés ou des autorités de certification de confiance — sans cette couche d'authentification, le protocole quantique ne peut empêcher les attaques de l'homme du milieu où un adversaire établit des sessions QKD indépendantes avec chaque point d'extrémité tout en se faisant passer pour eux l'un envers l'autre.",
    "B": "Des garanties de sécurité enracinées dans les lois fondamentales de la physique quantique plutôt que dans des hypothèses de dureté computationnelle, ce qui signifie que la confidentialité du protocole reste prouvablement sécurisée même contre des adversaires disposant de ressources de calcul classique ou quantique illimitées. Contrairement aux VPN conventionnels qui reposent sur des problèmes comme la factorisation d'entiers ou les logarithmes discrets qui peuvent être vulnérables à de futures percées algorithmiques ou à des ordinateurs quantiques exécutant l'algorithme de Shor, un VPN quantique exploite des principes physiques tels que la perturbation par la mesure et le théorème de non-clonage pour détecter les tentatives d'écoute avec une certitude théorique de l'information. Cela fournit une sécurité inconditionnelle à long terme pour les transmissions de données sensibles, éliminant le besoin de faire confiance au fait que certains problèmes mathématiques resteront insolubles alors que les capacités de calcul progressent sur des décennies.",
    "C": "Le protocole VPN quantique mettrait en œuvre une vérification de sécurité indépendante du dispositif où les points d'extrémité communicants n'ont pas besoin de faire confiance à leurs propres implémentations matérielles quantiques, utilisant des violations d'inégalités de Bell pour certifier la présence de corrélations quantiques authentiques immunisées contre la manipulation d'équipement. Cela répond à une vulnérabilité critique dans les architectures VPN classiques où des accélérateurs cryptographiques compromis ou des générateurs de nombres aléatoires piégés peuvent silencieusement fuiter du matériel de clé. Le protocole fonctionne en faisant partager aux points d'extrémité des paires de photons intriqués et effectuer des mesures séparées de type espace dont les corrélations statistiques bornent l'information accessible à tout espion via l'inégalité CHSH. La sécurité est garantie par la violation observable des bornes de corrélation classiques plutôt que par des hypothèses sur le comportement du dispositif, fournissant une protection même lorsque le matériel quantique est fabriqué par des fournisseurs potentiellement adverses. Cependant, le protocole nécessite un canal classique authentifié de confiance pour l'étape finale d'amplification de confidentialité.",
    "D": "Les protocoles VPN quantiques fourniraient des garanties de confidentialité persistante renforcées par l'irréversibilité physique des mesures quantiques : une fois que le matériel de clé est généré par distribution quantique de clés et utilisé pour chiffrer une session VPN, un adversaire qui compromet ultérieurement un point d'extrémité ne peut pas déchiffrer rétroactivement les sessions passées car les états quantiques qui ont généré ces clés ont été irréversiblement effondrés par la mesure et n'existent plus dans aucun système physique. Cela contraste avec les VPN classiques utilisant l'échange de clés Diffie-Hellman éphémère, où la confidentialité persistante dépend de l'hypothèse computationnelle que les logarithmes discrets restent difficiles — si cette hypothèse échoue à l'avenir (par exemple, par des algorithmes quantiques ou des percées mathématiques), les transcriptions de session stockées deviennent vulnérables. La sécurité du protocole quantique repose plutôt sur le théorème de non-clonage empêchant l'adversaire d'avoir conservé des copies parfaites des états quantiques mesurés, fournissant une confidentialité persistante théorique de l'information qui reste valide indépendamment des capacités de calcul futures.",
    "solution": "B"
  },
  {
    "id": 317,
    "question": "La distinction d'éléments reste difficile pour les ordinateurs quantiques dans le cas le pire parce que :",
    "A": "L'ordonnancement adversarial des entrées peut forcer tout algorithme quantique dans un régime où l'amplification d'amplitude échoue à distinguer les motifs de collision des fluctuations aléatoires, nécessitant des étapes de vérification classique qui dominent le temps d'exécution",
    "B": "La détection de collision nécessite de comparer toutes les paires éventuellement, et bien que les algorithmes de marche quantique trouvent des collisions en O(N^(2/3)) requêtes, la structure de sous-groupe caché nécessaire pour une meilleure accélération n'existe pas pour les problèmes de collision arbitraires",
    "C": "Aucune structure connue à exploiter au-delà des collisions elles-mêmes, ce qui signifie que les algorithmes quantiques ne peuvent pas tirer parti de motifs spécifiques au problème ou de régularités mathématiques",
    "D": "L'oracle de comparaison d'éléments doit préserver la réversibilité tout en révélant l'information de collision, créant un compromis fondamental où les techniques de retour de phase ne peuvent extraire que O(√N) bits de données de collision par superposition de requête",
    "solution": "C"
  },
  {
    "id": 318,
    "question": "Dans les codes de surface standard de distance 3, chaque générateur de stabilisateur nécessite quatre portes à deux qubits pour la mesure (un CNOT par qubit de données dans le support du générateur). Les codes de distance 5 nécessitent naïvement de mesurer des stabilisateurs de poids 5 avec cinq CNOT chacun, mais les schémas basés sur des drapeaux atteignent des nombres de portes plus faibles. Comparés aux circuits de syndrome standard, les schémas basés sur des drapeaux pour les codes de distance 5 réduisent le nombre de portes à deux qubits principalement en faisant quoi ?",
    "A": "Employant des modes ancilla à variables continues implémentés dans des cavités micro-ondes à facteur de qualité élevé qui peuvent absorber des erreurs multi-qubits corrélées via des protocoles de correction d'erreur bosonique basés sur des encodages GKP.",
    "B": "Encodant les valeurs propres de stabilisateur directement dans des décalages de fréquence de qubit protégés par des techniques d'ingénierie hamiltonienne soigneusement conçues qui cartographient les attentes d'opérateur de Pauli sur des séparations d'énergie mesurables, puis en utilisant uniquement des rotations à un qubit et des impulsions micro-ondes résonnantes pour les lire spectroscopiquement, ce qui élimine complètement le besoin d'interactions CZ ou CNOT à deux qubits explicites pendant les rondes d'extraction de syndrome tout en préservant l'information complète du stabilisateur.",
    "C": "Remplaçant de nombreuses portes intriquantes standard par des corrections de propagation classiques purement classiques qui sont dérivées de l'analyse de motifs dans les résultats de mesure répétés sur plusieurs rondes d'extraction de syndrome.",
    "D": "Un seul qubit ancilla surveille simultanément plusieurs emplacements de défauts de générateur de stabilisateur — lorsque cette ancilla drapeau se déclenche, vous savez qu'une erreur nocive s'est produite, vous permettant d'utiliser moins de portes tout en maintenant la tolérance aux pannes grâce à des protocoles de re-mesure conditionnelle qui s'activent uniquement lorsque les drapeaux indiquent une propagation potentielle d'erreur de poids 2 provenant de défauts de porte unique.",
    "solution": "D"
  },
  {
    "id": 319,
    "question": "En quoi le concept de capacité de canal quantique diffère-t-il fondamentalement de son équivalent classique en théorie de l'information ?",
    "A": "La capacité quantique présente une non-additivité due à l'intrication entre les utilisations de canal : l'information cohérente (formule de capacité quantique) peut augmenter de manière superlinéaire lorsque les canaux sont utilisés conjointement plutôt qu'indépendamment. Cela contraste avec l'information mutuelle classique, qui est toujours additive car les corrélations classiques obéissent à l'inégalité de traitement de données sans amélioration de ressources quantiques partagées. Cependant, prouver la superadditivité nécessite de construire des codes explicites exploitant cet effet, ce qui reste un problème ouvert pour la plupart des canaux au-delà de contre-exemples spécialisés comme le canal de dépolarisation combiné avec des canaux d'effacement.",
    "B": "Non-additivité : la capacité pour plusieurs utilisations peut dépasser la somme des capacités individuelles. Contrairement à la capacité de Shannon classique où l'utilisation conjointe de n canaux donne exactement n fois la capacité d'utilisation unique, les canaux quantiques présentent une superadditivité due à des protocoles assistés par intrication qui débloquent des corrélations indisponibles aux encodages d'état produit.",
    "C": "Les canaux quantiques prennent en charge plusieurs notions de capacité distinctes (capacité classique, capacité quantique, capacité classique assistée par intrication) qui peuvent différer arbitrairement, alors que les canaux classiques ont une capacité unique donnée par l'information mutuelle du canal maximisée sur les distributions d'entrée. La capacité quantique Q nécessite d'optimiser l'information cohérente I(A⟩B) = S(B) - S(AB), qui peut être négative pour les canaux dégradables où l'environnement apprend plus que le récepteur, forçant Q = 0 malgré une capacité classique non nulle. Les effets de taille finie apparaissent comme des corrections O(√log N/N) provenant de mesures d'information quantique à un coup plutôt que la concentration O(1/N) que les codes classiques atteignent.",
    "D": "Les canaux quantiques présentent une capacité dépendante de la mesure où les résultats dépendent du choix de base de mesure du récepteur, contrairement aux canaux classiques avec transmission d'information indépendante de la base. La borne de Holevo χ ≤ S(ρ) - ΣᵢpᵢS(ρᵢ) montre que l'information accessible est toujours inférieure à l'entropie de von Neumann transmise, créant un écart fondamental entre capacité quantique et classique égal à la discorde quantique de l'ensemble de l'encodeur. Cet écart disparaît uniquement pour les états commutants où [ρᵢ, ρⱼ] = 0, ce qui fait que les canaux quantiques se réduisent à des canaux classiques lorsque tous les états transmis sont simultanément diagonalisables dans une base propre partagée.",
    "solution": "B"
  },
  {
    "id": 320,
    "question": "Dans des scénarios d'apprentissage automatique quantique fédéré impliquant plusieurs parties non fiables qui doivent entraîner collaborativement un modèle sans révéler leurs ensembles de données quantiques individuels, quelle approche fournit les garanties de sécurité théoriques les plus fortes tout en maintenant une faisabilité computationnelle pour les dispositifs quantiques à court terme ?",
    "A": "Des mécanismes de confidentialité différentielle qui ajoutent du bruit quantique soigneusement calibré aux mises à jour de gradient à chaque ronde d'apprentissage fédéré, ce qui fournit une borne mathématiquement rigoureuse sur la fuite d'information mais peut dégrader substantiellement la précision du modèle dans des espaces de paramètres de haute dimension où l'amplitude de bruit requise pour la confidentialité croît avec le nombre de paramètres.",
    "B": "Les preuves quantiques à divulgation nulle de connaissance permettent à chaque partie participante de démontrer cryptographiquement que son ensemble de données quantiques local satisfait certaines propriétés et que ses contributions de gradient ont été calculées correctement selon la fonction de perte convenue, sans révéler aucune information sur les états quantiques réels dans leur ensemble de données, bien que les étapes de génération et de vérification de preuve introduisent une surcharge computationnelle significative qui peut être prohibitive pour les dispositifs NISQ actuels.",
    "C": "En adaptant le chiffrement entièrement homomorphe au cadre quantique, chaque partie peut chiffrer son ensemble de données quantiques local et ses calculs de gradient en utilisant un schéma de chiffrement compatible quantique qui permet d'appliquer des portes quantiques arbitraires directement aux états quantiques chiffrés, avec des gradients chiffrés agrégés sur un serveur central sans déchiffrement, bien que l'implémentation d'opérations quantiques homomorphes tolérantes aux pannes nécessite des surcharges de correction d'erreur dépassant les capacités à court terme.",
    "D": "Protocoles de calcul multipartite sécurisé",
    "solution": "D"
  },
  {
    "id": 321,
    "question": "Qu'implique « l'universalité adiabatique » pour les ordinateurs quantiques adiabatiques ?",
    "A": "Que tout calcul quantique peut être incorporé dans l'évolution de l'état fondamental en encodant les portes logiques comme des passages adiabatiques entre sous-espaces dégénérés de hamiltoniens intermédiaires H(s), avec la condition adiabatique garantissant que les transitions diabatiques restent exponentiellement supprimées et l'état fondamental final encodant la sortie du circuit avec un surcoût polynomial en qubits ancillaires et en temps d'évolution total.",
    "B": "Que tout algorithme quantique basé sur des portes peut être simulé en encodant le calcul dans l'évolution de l'état fondamental d'un hamiltonien dépendant du temps H(t), avec seulement un surcoût polynomial dans le nombre d'opérations par rapport au modèle de circuit.",
    "C": "Que tout problème d'optimisation peut être résolu en temps polynomial en construisant un hamiltonien dont l'état fondamental encode la solution, à condition que le programme d'interpolation respecte la condition adiabatique exigeant que le temps d'évolution dépasse l'inverse du carré de l'écart spectral minimal — l'universalité garantit que l'échelle de l'écart est au pire polynomiale en taille de problème pour toutes les instances dans BQP.",
    "D": "Que la simulation classique d'algorithmes adiabatiques est réalisable efficacement en utilisant l'échantillonnage de Monte Carlo par intégrale de chemin sur des hamiltoniens interpolants H(s), car l'universalité implique que l'évolution reste dans une variété de taille polynomiale d'états à faible intrication et le recouvrement de l'état fondamental avec les états produits reste borné inférieurement par l'inverse polynomial de la taille du système, permettant aux méthodes de trajectoires quasi-classiques d'approximer la dynamique du recuit quantique.",
    "solution": "B"
  },
  {
    "id": 322,
    "question": "Dans le contexte de la correction d'erreur quantique approximative, comment la condition de Knill-Laflamme doit-elle être modifiée ?",
    "A": "La condition stricte de Knill-Laflamme PE†ᵢEⱼP = αᵢⱼP exige que les opérateurs d'erreur mappent l'espace du code dans des sous-espaces mutuellement orthogonaux avec des constantes de proportionnalité αᵢⱼ purement réelles et indépendantes de la distance, mais pour la correction quantique d'erreur approximative ceci est assoupli pour permettre des coefficients à valeurs complexes PE†ᵢEⱼP = αᵢⱼP où Im(αᵢⱼ) ≤ ε, permettant de petites composantes imaginaires qui brisent l'hermiticité du canal d'erreur tout en maintenant la distinguabilité du syndrome, à condition que la phase accumulée pendant la détection d'erreur reste bornée en dessous de π/4, ce qui garantit une fidélité de récupération dépassant 1 - 2ε² pour les événements d'erreur unique.",
    "B": "La condition stricte de Knill-Laflamme PE†ᵢEⱼP = αᵢⱼP exige que l'extraction de syndrome distingue parfaitement toutes les paires d'erreur corrigeables par des projections orthogonales, mais pour la correction quantique d'erreur approximative cette exigence d'orthogonalité est affaiblie en PE†ᵢEⱼP = αᵢⱼP + βᵢⱼQ où Q projette sur le complément orthogonal de l'espace du code et ||βᵢⱼ|| ≤ ε, permettant de petites composantes de fuite qui couplent l'espace du code à des états de plus haute énergie pendant la correction d'erreur, à condition que la probabilité totale de fuite reste en dessous du pseudo-seuil du code déterminé par le rapport du temps de mesure du syndrome à T₁.",
    "C": "La condition stricte d'orthogonalité de Knill-Laflamme PE†ᵢEⱼP = αᵢⱼP, qui exige que les opérateurs d'erreur mappent l'espace du code vers des sous-espaces mutuellement orthogonaux avec des constantes de proportionnalité exactes, est assouplie en PE†ᵢEⱼP ≈ αᵢⱼP où les coefficients hermitiens αᵢⱼ doivent seulement satisfaire l'égalité approximative dans une tolérance d'erreur spécifiée ε, permettant aux espaces de code qui satisfont presque les critères de correction d'erreur d'atteindre quand même une suppression des taux d'erreur logiques proportionnelle au carré du taux d'erreur physique, à condition que les déviations de l'orthogonalité exacte restent bornées en dessous d'un seuil dépendant de la distance qui évolue avec le poids minimal du code.",
    "D": "La condition stricte de Knill-Laflamme PE†ᵢEⱼP = αᵢⱼP, qui exige une proportionnalité exacte pour toutes les paires d'opérateurs d'erreur dans l'ensemble corrigeable, est modifiée en PE†ᵢEⱼP = αᵢⱼP + δᵢⱼ où δᵢⱼ représente des perturbations bornées satisfaisant ||δᵢⱼ|| ≤ ε/d² avec d étant la distance du code, mais de manière critique les constantes de proportionnalité αᵢⱼ doivent rester exactement identiques (αᵢⱼ = α pour tout i,j) pour préserver l'opération de récupération universelle, alors qu'assouplir cette contrainte de récupération universelle éliminerait la capacité du code à corriger des erreurs arbitraires dans l'ensemble corrigeable, même approximativement.",
    "solution": "C"
  },
  {
    "id": 323,
    "question": "Quelle vulnérabilité spécifique une attaque par réordonnancement quantique exploite-t-elle ?",
    "A": "Les variations temporelles dans le moment où les opérateurs de mesure sont appliqués par rapport à la chronologie de décohérence des qubits individuels, exploitant le fait que l'effondrement de la mesure n'est pas instantané au niveau matériel. En chronométrant soigneusement les impulsions de mesure pour qu'elles se produisent pendant des états transitoires ou immédiatement après des opérations de porte spécifiques, un adversaire peut biaiser les résultats de mesure vers des valeurs propres particulières, effectuant efficacement une attaque par canal auxiliaire par manipulation contrôlée de la rétroaction de mesure sur l'état quantique.",
    "B": "Le surcoût computationnel introduit par les codes de correction d'erreur, qui crée des fenêtres temporelles pendant les cycles d'extraction de syndrome où des séquences de portes adverses peuvent être insérées sans détection. En exploitant la latence entre les mesures de stabilisateur et l'application de correction, les attaquants peuvent injecter des opérations malveillantes qui semblent faire partie du protocole normal de correction d'erreur.",
    "C": "Les contraintes implicites d'ordonnancement imposées par les relations de commutation de portes et les dépendances causales entre opérations, où le réordonnancement de portes non-commutatives peut altérer les résultats de mesure. Un attaquant manipule le planificateur pour permuter les portes de manières qui préservent la structure superficielle du circuit mais violent la séquence d'opérateurs prévue du circuit quantique, conduisant à des erreurs cohérentes qui s'accumulent multiplicativement à travers la profondeur du circuit sans déclencher les mécanismes traditionnels de détection d'erreur.",
    "D": "Des mappages incohérents de qubits entre la représentation logique du circuit et la topologie matérielle physique, où l'allocation de qubits du compilateur ne parvient pas à maintenir des assignations stables à travers différentes passes de compilation ou étapes d'optimisation. Cela crée des opportunités pour un attaquant de manipuler la fonction de mappage de sorte que les portes destinées à un qubit physique soient redirigées vers un autre, exploitant l'écart entre les étiquettes de qubits abstraites et les adresses matérielles concrètes pour injecter des opérations qui semblent valides dans la couche logique mais s'exécutent sur des qubits non prévus.",
    "solution": "D"
  },
  {
    "id": 324,
    "question": "Quelle amélioration dans la conception de l'authentification aide à contrecarrer les attaques par rejeu sur les canaux classiques QKD basés sur satellite sans surcharge importante de bande passante ?",
    "A": "Des numéros de séquence temporels incorporés directement dans les métadonnées de charge utile photonique via encodage de polarisation sur les mêmes photons QKD. Chaque message authentifié porte des horodatages croissants de manière monotone modulés sur des modes de polarisation orthogonaux, fusionnant l'authentification et la distribution de clés dans une seule couche optique.",
    "B": "Des transmissions de balise RF hors bande fournissant des valeurs de somme de contrôle synchrones dérivées de mesures de turbulence atmosphérique, qui sont intrinsèquement imprévisibles et partagées entre la station au sol et le satellite. En corrélant les paramètres environnementaux avec chaque message classique, le système génère des étiquettes fraîches sans consommer de matériel de clé QKD.",
    "C": "Des signatures numériques à clé publique basées sur des réseaux post-quantiques comme Dilithium ou Falcon appliquées à chaque paquet classique, utilisant la clé privée du satellite avec vérification par la station au sol. Ces signatures résistantes au quantique assurent la non-répudiation et empêchent le rejeu avec environ 1-3 kilo-octets de surcharge par paquet.",
    "D": "Des MAC de hachage universel à usage unique sans état chaînés via évolution de clé secrète, où chaque étiquette d'authentification est générée en utilisant une portion fraîche de la clé secrète dérivée de QKD combinée avec des fonctions de hachage cryptographiques. La construction de hachage universel garantit que chaque message reçoit une étiquette unique et imprévisible qui ne peut pas être réutilisée par un adversaire tentant un rejeu. En maintenant un flux de clés monotone sans nécessiter d'état synchronisé entre le satellite et la station au sol au-delà du secret partagé, cette approche atteint une authentification forte avec une surcharge minimale — typiquement 128-256 bits par trame authentifiée quelle que soit la longueur du message.",
    "solution": "D"
  },
  {
    "id": 325,
    "question": "Quel est un inconvénient principal de l'utilisation de portes hautement expressives comme la porte B dans les charges de travail quantiques standard ?",
    "A": "Elles sont excessives pour la plupart des opérations, donc vous vous retrouvez avec plus de portes qu'une décomposition sur mesure n'en nécessiterait — l'expressivité excessive signifie que vous utilisez un ensemble de portes universelles là où des séquences spécialisées de portes natives (comme Clifford+T) atteindraient la même opération logique avec moins de ressources physiques et de meilleures caractéristiques d'erreur.",
    "B": "Elles sont excessives pour la plupart des opérations, donc vous vous retrouvez avec des nombres de portes plus élevés qu'une décomposition sur mesure n'en nécessiterait — l'expressivité excessive signifie que vous appliquez des portes d'un espace de paramètres continu là où des séquences de portes discrètes (comme Clifford+T) atteindraient des opérations logiques équivalentes avec de meilleures propriétés de tolérance aux pannes, car les protocoles de distillation d'états magiques sont optimisés pour des ensembles de portes discrets et ne peuvent pas gérer efficacement les unitaires paramétrés de manière continue, forçant le compilateur à arrondir les paramètres de porte B à des valeurs discrètes proches et perdant l'avantage théorique de l'universalité continue.",
    "C": "Elles sont excessives pour la plupart des opérations, donc vous vous retrouvez avec des circuits plus profonds que des décompositions optimisées n'en nécessiteraient — l'expressivité excessive signifie que vous utilisez des portes en dehors de la hiérarchie de Clifford où des mises à jour spécialisées de cadre de Pauli et des règles de commutation de portes pourraient réduire substantiellement la profondeur du circuit. Puisque les portes B ne préservent pas la structure de stabilisateur, chaque application force le compilateur à casser le chemin rapide de simulation de Clifford et à revenir au suivi de vecteur d'état à coût exponentiel pendant les passes d'optimisation, empêchant le compilateur d'appliquer les optimisations peephole standard qui exploitent la conjugaison de Clifford pour fusionner les couches adjacentes.",
    "D": "Elles sont excessives pour la plupart des opérations, donc vous vous retrouvez avec de moins bonnes performances limitées par la cohérence que des séquences de portes ciblées n'atteindraient — l'expressivité excessive signifie que vous implémentez des unitaires de la variété complète SU(4) où une décomposition de Cartan en séquences minimales de portes natives (comme des séquences de portes de cross-résonance en écho) se terminerait plus rapidement et accumulerait moins d'erreur de phase. Les portes B nécessitent des procédures de calibration plus longues car leur espace de paramètres continu rend la pré-calibration de toutes les instances possibles impraticable, forçant la génération d'impulsions juste-à-temps qui introduit une latence de compilation proportionnelle au degré d'expressivité de la porte.",
    "solution": "A"
  },
  {
    "id": 326,
    "question": "La recherche de triangles dans les graphes creux reste difficile pour les marches quantiques car :",
    "A": "Les matrices d'adjacence creuses provoquent une réduction de l'écart discriminant dans l'opérateur de marche quantique à jeton, proportionnellement inverse au degré moyen, réduisant l'avantage spectral effectif de quadratique à sous-quadratique lorsque la densité du graphe diminue en dessous du seuil de percolation. Cette dégradation du temps de mélange se produit car la séparation des valeurs propres de l'opérateur de marche dépend de la conductance du graphe, qui diminue dans les graphes creux où les voisinages locaux deviennent arborescents, empêchant l'amplification d'amplitude d'atteindre son plein accélération quadratique. Bien que les marches quantiques maintiennent des avantages théoriques de complexité de requête dans le modèle oracle en interrogeant seulement O(n^(1.3)) arêtes comparé à Ω(n^(1.5)) classique, le temps d'exécution concret souffre lorsque les propriétés spectrales se dégradent, rendant l'approche quantique moins convaincante pour les instances creuses malgré le maintien d'une supériorité asymptotique.",
    "B": "Le nombre d'arêtes potentielles est déjà beaucoup plus petit dans les graphes creux comparé aux graphes denses, ce qui signifie qu'il y a moins de triangles à trouver et que la réduction de l'espace de recherche diminue les gains de temps absolus réalisables grâce à l'accélération quantique même lorsque l'avantage quadratique est maintenu. Les algorithmes classiques peuvent exploiter des structures de données spécifiques à la rareté comme les listes d'adjacence pour atteindre une performance quasi-optimale évoluant avec le nombre d'arêtes réelles plutôt que d'arêtes potentielles, réduisant l'écart entre les approches classiques et quantiques. Bien que les marches quantiques offrent toujours des avantages asymptotiques dans le modèle de complexité de requête, les améliorations pratiques de temps réel deviennent marginales lorsque le nombre d'arêtes est petit, rendant l'approche quantique moins convaincante pour les instances de graphes creux malgré sa supériorité théorique dans l'analyse du pire cas.",
    "C": "Les graphes creux nécessitent des implémentations de marche quantique utilisant des techniques de détection comprimée pour représenter efficacement les O(m) arêtes en mémoire quantique, où m << n^2, mais le processus de mesure nécessaire pour vérifier l'existence de triangles introduit une décohérence proportionnelle au taux de compression. Les architectures QRAM standard supposent des encodages de graphes denses avec Θ(n^2) cellules mémoire adressables, créant une surcharge lorsque la plupart des entrées disparaissent, et les approches de hachage par compartiments pour stocker uniquement les arêtes présentes ne peuvent pas être interrogées de manière cohérente sans effondrer les superpositions par déréférencement de pointeur classique. Cette tension fondamentale entre représentation creuse économe en espace et modèles d'accès quantique cohérent limite l'avantage quantique pratique, rendant l'approche de marche quantique moins efficace pour les instances creuses malgré le maintien d'une supériorité théorique de complexité de requête dans les modèles oracle idéalisés.",
    "D": "Les bornes de complexité oracle pour la recherche de triangles supposent que les requêtes d'arêtes peuvent être effectuées en temps unitaire, mais les oracles de graphes creux nécessitent nécessairement un temps de requête Ω(log n) pour spécifier laquelle des m << n^2 arêtes est accédée par adressage binaire des listes d'adjacence, multipliant le coût effectif de requête par un facteur logarithmique. Cette surcharge d'adressage érode l'accélération quadratique de la marche quantique de O(n^(1.3)) requêtes d'arêtes à O(n^(1.3) log n) temps lors de la prise en compte des coûts d'accès aux structures de données creuses, tandis que les algorithmes classiques utilisant des dispositions de listes d'adjacence efficaces en cache connaissent des facteurs logarithmiques plus petits en raison de la localité spatiale. L'écart du modèle de complexité entre les requêtes d'arêtes à coût unitaire et les modèles d'accès mémoire réalistes désavantage particulièrement les approches quantiques dans les régimes creux où la poursuite de pointeurs domine le calcul.",
    "solution": "B"
  },
  {
    "id": 327,
    "question": "Pour mesurer un stabilisateur de poids six sur du matériel à connectivité limitée, les protocoles tolérants aux fautes le décomposent généralement en quelle séquence ?",
    "A": "Une cascade structurée en arbre utilisant quatre portes CNOT dans la première couche pour coupler six qubits de données en trois qubits ancillaires intermédiaires, suivie de deux CNOT pour combiner ces ancilles en un bit de syndrome final, réduisant la profondeur du circuit à log(6) ≈ 3 couches au prix de nécessiter trois qubits ancillaires plutôt qu'un.",
    "B": "Une chaîne séquentielle de portes CNOT à deux qubits entre un qubit ancillaire et chacun des six qubits de données successivement, suivie de la mesure de l'ancille pour extraire l'information de parité.",
    "C": "Trois mesures de poids deux effectuées en parallèle utilisant des qubits ancillaires séparés, une pour chaque paire de qubits de données, suivies d'un XOR classique des trois bits de syndrome pour reconstruire la parité de poids six tout en maintenant une séparation spatiale pour empêcher les erreurs en crochet de se propager entre les circuits de mesure.",
    "D": "Deux mesures séquentielles de stabilisateur de poids trois où la première ancille se couple aux qubits de données {1,2,3} et la seconde à {4,5,6}, leur produit déterminant la valeur propre de poids six. Cette factorisation maintient la tolérance aux fautes car les erreurs en crochet ne peuvent propager que des erreurs de données de poids deux au sein de chaque sous-ensemble plutôt que des erreurs de poids trois sur tout le support.",
    "solution": "B"
  },
  {
    "id": 328,
    "question": "Dans le contexte de l'apprentissage automatique quantique, quelle est une caractéristique de l'algorithme HHL qui limite son applicabilité pratique ?",
    "A": "La sortie est un état quantique représenté sous forme d'amplitudes dans un espace de Hilbert de haute dimension plutôt que des données classiques accessibles par lecture conventionnelle. L'extraction d'informations classiques complètes sur ce vecteur solution nécessiterait un nombre exponentiel de mesures pour reconstruire toutes les amplitudes avec une précision raisonnable, annulant l'accélération quantique.",
    "B": "L'accélération exponentielle ne se matérialise que pour des matrices structurées spécifiques, particulièrement celles qui sont creuses et bien conditionnées avec des propriétés spectrales favorables. Les matrices denses ou les systèmes avec des nombres de conditionnement qui évoluent exponentiellement effacent l'avantage quantique, car le temps d'exécution de l'algorithme dépend polynomialement du nombre de conditionnement. De plus, les matrices résultant de la discrétisation de problèmes continus manquent souvent de la structure requise, et même lorsque la structure existe, vérifier ces propriétés classiquement peut nécessiter un effort de calcul comparable à la résolution du système original.",
    "C": "L'algorithme nécessite une préparation efficace de l'état d'entrée encodant le vecteur du membre de droite, ce qui peut être exponentiellement difficile pour des vecteurs de données classiques arbitraires. Charger n nombres classiques dans n amplitudes de qubit demande généralement un temps linéaire en 2^n, submergeant complètement toute accélération quantique. Bien que des structures de données spécialisées ou des encodages spécifiques au problème puissent parfois être préparés efficacement, comme des états représentant des fonctions lisses ou des sorties de calculs quantiques antérieurs, le goulot d'étranglement de préparation d'état reste la limitation pratique dominante pour la plupart des systèmes linéaires du monde réel rencontrés dans les applications d'apprentissage automatique.",
    "D": "Toutes les réponses ci-dessus",
    "solution": "D"
  },
  {
    "id": 329,
    "question": "Qu'est-ce que l'\"information douce\" dans le contexte de la correction d'erreurs quantiques ?",
    "A": "Des données de mesure qui incluent des estimations de confiance ou de probabilité pour chaque résultat de syndrome, plutôt que des valeurs binaires dures, fournissant au décodeur des informations analogiques sur la fiabilité de la mesure qui permettent aux algorithmes de décodage probabilistes de pondérer les bits de syndrome selon leur fidélité et de distinguer entre les détections de haute confiance et de faible confiance lors de l'inférence de la chaîne d'erreur la plus probable.",
    "B": "Les corrélations de syndrome extraites de mesures de stabilisateur répétées—spécifiquement, l'information obtenue en suivant les corrélations temporelles entre les tours de syndrome consécutifs pour identifier les événements de détection persistants versus transitoires. En corrélant les bits de syndrome à travers plusieurs cycles de mesure, l'information douce distingue les véritables violations de stabilisateur des erreurs de mesure, produisant des données de syndrome probabilistes où l'estimation de fiabilité de chaque bit reflète sa cohérence temporelle. Cela permet au décodeur de sous-pondérer les événements de détection isolés probablement causés par des fautes de mesure plutôt que des erreurs de données.",
    "C": "L'information de syndrome partielle provenant de la lecture d'ancille sans démolition—résultats de mesure auxiliaires obtenus par des protocoles quantiques sans démolition qui révèlent les valeurs propres de stabilisateur tout en préservant la cohérence de l'état logique. Parce que ces mesures extraient des données de syndrome sans effondrer complètement les états ancillaires, elles fournissent des bits de syndrome probabilistes avec des amplitudes analogiques correspondant au degré d'intrication entre les qubits ancillaires et de données. Le décodeur interprète ces lectures à valeurs continues comme des informations de syndrome pondérées par la confiance, permettant des algorithmes de décodage à décision douce qui tiennent compte de l'effondrement partiel induit par la mesure.",
    "D": "Les estimations de syndrome continues provenant d'opérateurs de mesure paramétrés—résultats de mesure obtenus en utilisant des angles de lecture ajustables où chaque bit de syndrome résulte d'une observable de Pauli paramétrée plutôt qu'un générateur de stabilisateur fixe. En ajustant les angles de base de mesure selon des tables de consultation pré-calibrées, le décodeur reçoit des valeurs de syndrome modulées par des estimations de fidélité de lecture dérivées des paramètres de réglage de contrôle. Cela produit des informations de syndrome douces où les statistiques de mesure analogiques encodent des niveaux de confiance déterminés par l'angle de rotation de l'opérateur de mesure par rapport à la base de calcul.",
    "solution": "A"
  },
  {
    "id": 330,
    "question": "Pourquoi les portes non commutatives sont-elles essentielles pour maintenir l'entraînabilité dans les circuits quantiques en couches ?",
    "A": "Les portes non commutatives empêchent l'annulation du gradient en garantissant que les décalages de paramètres se propagent à travers le circuit d'une manière qui préserve la sensibilité des résultats de mesure aux variations de paramètres. Lorsque les portes commutent, le circuit peut être effectivement réordonné et simplifié, conduisant souvent à des gradients exponentiellement évanescents (plateaux arides) parce que le paysage de paramètres devient plat. La non-commutativité maintient la structure riche et interdépendante de l'espace des paramètres, permettant aux informations de gradient significatives d'atteindre la fonction de coût et permettant une optimisation efficace des algorithmes quantiques variationnels.",
    "B": "Les portes non commutatives garantissent que les gradients de paramètres calculés via la règle de décalage de paramètre maintiennent une variance finie en empêchant la formation de sous-circuits de Clifford qui effondreraient le paysage de fonction de coût en une structure constante par morceaux. Lorsque des portes consécutives commutent, elles peuvent être fusionnées analytiquement par fusion d'opérateurs, ce qui réduit le nombre effectif de paramètres indépendants et provoque la concentration du vecteur gradient dans un sous-espace de dimension inférieure. Cet effondrement dimensionnel induit directement des phénomènes de plateau aride en créant des magnitudes de dérivées exponentiellement petites. La non-commutativité préserve la structure de rang complet de la matrice d'information de Fisher, maintenant le nombre de conditionnement nécessaire pour une optimisation stable basée sur le gradient dans les ansätze variationnels profonds.",
    "C": "Les séquences de portes non commutatives empêchent l'interférence destructive des contributions de gradient provenant de différentes régions de paramètres en maintenant des crochets de Lie non nuls entre les couches successives du circuit. Lorsque les portes commutent, la représentation adjointe de l'algèbre de Lie du circuit devient abélienne, ce qui force tous les termes de gradient d'ordre supérieur (calculés via des commutateurs imbriqués dans l'expansion de Baker-Campbell-Hausdorff) à s'annuler identiquement. Cette élimination des corrections d'ordre supérieur provoque le développement de régions exponentiellement plates connues sous le nom de plateaux arides dans la fonction de coût. La non-commutativité garantit que les commutateurs imbriqués restent non triviaux, permettant au flux de gradient d'incorporer des corrélations multi-paramètres qui encodent des informations géométriques sur la courbure du paysage de coût.",
    "D": "Les portes non commutatives maintiennent l'entraînabilité en garantissant que la dimension effective du groupe unitaire généré par les couches paramétrées évolue exponentiellement avec la profondeur du circuit plutôt que linéairement. Lorsque les portes commutent, elles génèrent un sous-groupe abélien dont la dimension égale le nombre de paramètres, créant une variété de solution restreinte avec une mesure approchant zéro dans l'espace SU(2^n) complet. Cette variété restreinte présente des phénomènes de concentration de mesure où les gradients de fonction de coût s'annulent exponentiellement avec la taille du système. La non-commutativité brise cette structure abélienne, permettant à la famille unitaire paramétrée de former un groupe de Lie non abélien dont le volume exponentiellement plus grand empêche la concentration de gradient et préserve l'expressivité nécessaire pour que les algorithmes d'optimisation trouvent des solutions non triviales.",
    "solution": "A"
  },
  {
    "id": 331,
    "question": "Quelles contraintes rencontrent les autoencodeurs quantiques ?",
    "A": "Les autoencodeurs quantiques nécessitent une correction d'erreur impeccable au niveau de chaque porte car tout événement de décohérence pendant le circuit d'encodage brouille irrémédiablement la représentation compressée, sans possibilité de récupération par redondance ou techniques classiques d'atténuation d'erreur, puisque même une seule interaction parasite avec un photon ou une fluctuation thermique provoquant une erreur de phase sur un qubit se propage à travers les portes d'intrication pour corrompre entièrement l'état encodé.",
    "B": "Les autoencodeurs quantiques opèrent dans un régime purement quantique où toute interface avec des données classiques viole fondamentalement le théorème de non-clonage, rendant impossible l'encodage d'informations classiques dans des états quantiques sans détruire les propriétés de superposition requises pour la compression.",
    "C": "La dimensionnalité de l'espace latent dans les autoencodeurs quantiques croît exponentiellement avec la taille d'entrée en raison de la structure de produit tensoriel des espaces de Hilbert multi-qubits, nécessitant 2^n qubits pour encoder même des ensembles de données modestes de n caractéristiques classiques, créant un surcoût paradoxal où la compression d'un ensemble de données classiques de 100 dimensions nécessiterait plus de 10^30 qubits rien que pour représenter la couche d'entrée de l'encodeur.",
    "D": "Les limitations matérielles telles que le nombre restreint de qubits et leur connectivité, le bruit environnemental provenant de la décohérence et des erreurs de porte, et le surcoût des codes de correction d'erreur quantique qui augmentent considérablement les besoins en ressources pour une opération tolérante aux fautes.",
    "solution": "D"
  },
  {
    "id": 332,
    "question": "Quelle contrainte de planification résulte des topologies de plus proches voisins lorsque deux portes CX partagent un qubit commun ?",
    "A": "Le qubit partagé doit exécuter une séquence de découplage dynamique entre les interactions successives pour supprimer le couplage ZZ résiduel du coupleur accordable. Lorsqu'un qubit participe à des portes CX consécutives, le bruit de charge sur le coupleur induit des interactions cohérentes permanentes qui accumulent des erreurs de phase proportionnelles au temps d'attente, nécessitant l'insertion d'impulsions d'écho de Hahn qui augmentent la séparation effective entre portes de ~40ns à ~120ns pour maintenir une fidélité supérieure à 99%.",
    "B": "Le qubit partagé ne peut exécuter qu'une seule interaction à deux qubits à la fois, créant une contrainte de sérialisation fondamentale. Lorsqu'un qubit participe à une porte CX, il ne peut pas simultanément s'engager dans une autre opération à deux qubits, forçant le planificateur à séquencer ces portes temporellement plutôt que de les exécuter en parallèle.",
    "C": "La topologie de couplage impose une contrainte de commutation exigeant que la seconde CX anticommute avec la première lorsqu'elles partagent un qubit de contrôle mais commute lorsqu'elles partagent une cible. Cela provient du fait que l'activation simultanée de deux impulsions de flux adressant le même qubit crée une interférence destructive dans le sous-espace |11⟩ pour les paires partageant le contrôle mais une interférence constructive pour les paires partageant la cible, forçant le compilateur à insérer des portes d'identité qui complètent le planning jusqu'à ce que le rôle du qubit partagé s'inverse, ajoutant typiquement 2-3 couches de portes pour maintenir la structure correcte du groupe stabilisateur.",
    "D": "Les deux portes CX doivent utiliser la même orientation contrôle-cible par rapport au qubit partagé pour maintenir la cohérence de phase micro-onde à travers la séquence de portes. Inverser la direction nécessiterait de reprogrammer la référence de phase de l'oscillateur local en cours d'exécution, introduisant une dérive de calibration qui corrompt l'angle de rotation conditionnelle jusqu'à 15°, donc le planificateur impose une cohérence directionnelle en sérialisant toute paire où le qubit partagé bascule entre les rôles de contrôle et de cible.",
    "solution": "B"
  },
  {
    "id": 333,
    "question": "Dans le contexte des schémas de chiffrement homomorphe quantique permettant le calcul sur des états quantiques chiffrés, quel vecteur d'attaque pose la menace la plus sévère au maintien de la confidentialité computationnelle tout en préservant la capacité d'effectuer des opérations de porte arbitraires sur des données chiffrées sans déchiffrement ? Considérons que l'adversaire a accès à toutes les sorties computationnelles intermédiaires mais pas aux clés de chiffrement.",
    "A": "En effectuant une tomographie d'état quantique sur les états intermédiaires chiffrés après chaque couche computationnelle, un adversaire peut reconstruire la matrice de densité complète des données chiffrées et exploiter les corrélations entre les valeurs d'espérance de Pauli du texte en clair et les statistiques de mesure du texte chiffré. Bien que les mesures individuelles apparaissent aléatoires, l'agrégation de millions d'exécutions de circuit identiques permet une estimation par maximum de vraisemblance pour récupérer des informations structurelles—telles que les motifs de connectivité des qubits, les relations de phase relatives et les amplitudes de superposition—qui révèlent collectivement jusqu'à 40% de l'entropie du texte en clair original à travers des moments statistiques d'ordre supérieur.",
    "B": "Les protocoles de chiffrement entièrement homomorphe quantique nécessitent des opérations de rechiffrement périodiques (changement de clé) après avoir accumulé un nombre seuil d'évaluations de portes pour empêcher l'accumulation de bruit, et ces procédures de changement de clé impliquent l'évaluation d'un circuit quantique qui applique des opérateurs de Pauli pondérés par les bits de clé secrète. Si un adversaire accède aux états de sortie bruités immédiatement après le changement de clé—par des canaux auxiliaires temporels ou lecture de mémoire—alors l'analyse en composantes principales de la matrice de covariance du bruit peut isoler les dépendances linéaires entre les éléments de clé secrète, exposant environ log₂(d) bits d'information de clé par cycle de changement pour un système de d qubits.",
    "C": "L'évaluation homomorphe des portes non-Clifford, en particulier la porte T, nécessite l'injection d'états magiques à travers des circuits de téléportation de porte où les résultats de mesure doivent être communiqués classiquement pour compléter l'opération de porte chiffrée. Ces résultats de mesure, bien qu'individuellement aléatoires, présentent des dépendances statistiques sur le contenu logique des données chiffrées lorsqu'ils sont agrégés sur de nombreuses évaluations de portes T. Un adversaire ayant accès à ces enregistrements de mesure peut appliquer des techniques d'analyse différentielle de puissance empruntées à la cryptanalyse par canaux auxiliaires, corrélant les distributions de résultats de mesure avec des valeurs de texte en clair hypothétiques pour reconstruire progressivement l'information quantique sous-jacente par une stratégie d'attaque à texte clair choisi impliquant des superpositions d'entrée spécialement conçues.",
    "D": "La profondeur du circuit augmente linéairement avec le nombre d'opérations homomorphes, causant un surcoût polynomial dans les exigences de fidélité des portes.",
    "solution": "D"
  },
  {
    "id": 334,
    "question": "Quel algorithme classique est le plus couramment utilisé dans l'étape finale de l'algorithme de Shor ?",
    "A": "L'élimination de Gauss sur les corps finis est employée pour résoudre le système de congruences linéaires qui résultent de multiples mesures de période, traitant chaque sortie QFT comme une équation de contrainte. En réduisant ce système à la forme échelonnée, nous isolons la vraie période du bruit introduit par les statistiques de mesure quantique, filtrant efficacement les périodicités parasites qui ne correspondent pas à l'ordre réel de l'exponentiation modulaire.",
    "B": "Le théorème des restes chinois est appliqué pour reconstruire la période à partir de ses résidus modulaires sur plusieurs exécutions indépendantes de la sous-routine quantique, chacune effectuée avec différentes bases aléatoires. En combinant ces résultats partiels via CRT, nous obtenons la période globale avec une grande confiance, parallélisant efficacement l'étape de recherche de période sur plusieurs exécutions de circuit quantique puis fusionnant classiquement les résultats.",
    "C": "Le test de primalité de Miller-Rabin est invoqué après la transformée de Fourier quantique pour vérifier que le candidat de période mesuré est bien premier avec le module, assurant que le développement en fraction continue produira un facteur valide. Cette vérification probabiliste s'exécute en temps polynomial et confirme que la période r satisfait la condition de coprimalité requise pour que le post-traitement classique extraie des diviseurs non triviaux de N.",
    "D": "L'algorithme d'Euclide pour calculer les plus grands diviseurs communs est appliqué pour extraire les facteurs de la période trouvée par la sous-routine quantique. Après que la transformée de Fourier quantique produit une période candidate r, nous calculons pgcd(a^(r/2) ± 1, N) où a est la base choisie et N est le nombre à factoriser. Cette procédure classique arithmétique en temps polynomial identifie efficacement les diviseurs non triviaux en exploitant la structure multiplicative révélée par la période, complétant la factorisation avec haute probabilité en quelques étapes arithmétiques classiques seulement.",
    "solution": "D"
  },
  {
    "id": 335,
    "question": "Qu'est-ce qui définit la classe de complexité IQP (Instantaneous Quantum Polynomial-time) ?",
    "A": "Des circuits quantiques constitués d'unitaires diagonaux qui commutent entre eux, implémentables en profondeur constante à travers une seule couche de portes parallèles suivie d'une mesure en base computationnelle—considérés difficiles à simuler classiquement malgré la simplicité architecturale",
    "B": "IQP consiste en des circuits quantiques avec des couches unitaires diagonales de taille polynomiale et profondeur constante dans la base X—diagonales dans toute base fixée—qui peuvent être implémentées comme des rotations commutantes simultanées. La difficulté classique provient de l'échantillonnage de la distribution de sortie, qui est liée au calcul de permanents de matrices sur certaines matrices structurées, un problème considéré intraitable pour les ordinateurs classiques malgré la structure peu profonde du circuit quantique",
    "C": "Cette classe capture les circuits quantiques composés de couches de portes diagonales à deux qubits commutantes dans la base computationnelle, implémentables en profondeur logarithmique lorsque les contraintes de localité des portes sont assouplies. La caractéristique définissante est que toutes les portes commutent globalement, permettant un réordonnancement arbitraire, mais la distribution de sortie reste difficile à échantillonner classiquement en raison de motifs d'interférence constructive qui émergent des relations de phase diagonales sur l'ensemble du registre",
    "D": "IQP englobe les circuits quantiques construits à partir de portes Clifford augmentées d'un nombre polynomial de portes T arrangées en couches de profondeur constante, où tous les éléments non-Clifford sont positionnés pour agir simultanément dans la couche finale. La difficulté de simulation classique dérive de la théorie des ressources d'états magiques : alors que les circuits Clifford sont simulables efficacement, l'ajout même de couches de portes T de profondeur constante crée des motifs d'interférence dont l'échantillonnage précis nécessiterait conjecturalement des ressources classiques exponentielles",
    "solution": "A"
  },
  {
    "id": 336,
    "question": "Le dropout quantique, implémenté en retirant de manière probabiliste des portes paramétrées pendant l'entraînement, vise à :",
    "A": "Régulariser le circuit quantique variationnel et prévenir le surapprentissage sur les données d'entraînement, fonctionnant de manière analogue au dropout dans les réseaux de neurones classiques où la désactivation aléatoire de neurones force le modèle à apprendre des caractéristiques robustes qui ne dépendent d'aucun paramètre unique.",
    "B": "Atténuer les plateaux stériles en introduisant des perturbations stochastiques au paysage de coût pendant l'optimisation, exploitant le fait que les portes aléatoirement retirées réduisent la profondeur effective du circuit et augmentent la variance du gradient à chaque étape d'entraînement, permettant à l'optimiseur d'échapper aux régions plates où les gradients de la règle de décalage de paramètre s'annulent exponentiellement avec le nombre de qubits.",
    "C": "Régulariser le circuit quantique en imposant une moyenne d'ensemble sur les sous-structures pendant l'entraînement, similaire au dropout classique forçant l'apprentissage de caractéristiques robustes, mais diffère de manière critique en ce que le dropout quantique préserve l'ensemble complet de paramètres tandis que le dropout classique masque les poids — ici toutes les portes restent entraînables et le retrait probabiliste crée un ensemble implicite de topologies partageant les paramètres.",
    "D": "Réduire le coût de mesure en entraînant le circuit à être invariant sous le retrait de portes, fonctionnant de manière analogue au dropout classique mais ciblant le coût de mesure plutôt que la généralisation — le circuit entraîné produit des valeurs d'espérance stables même lorsqu'il est évalué avec moins de mesures par porte parce que l'entraînement avec des opérations manquantes force un ajustement compensatoire des paramètres qui réduit la sensibilité au bruit de grenaille.",
    "solution": "A"
  },
  {
    "id": 337,
    "question": "Comment le concept de difficulté du syndrome impacte-t-il la performance du décodeur en correction d'erreur quantique ?",
    "A": "Les syndromes qui violent la borne de distance minimale du code nécessitent des décodeurs avec capacité de retour en arrière pour résoudre l'ambiguïté en testant séquentiellement plusieurs hypothèses de correction. Lorsque la difficulté du syndrome dépasse un seuil — c'est-à-dire que l'erreur de poids minimal cohérente avec le syndrome a un poids approchant d/2 — les algorithmes d'appariement basés sur des graphes produisent des égalités entre des appariements parfaits de poids égaux, et le décodeur doit énumérer ces solutions dégénérées pour identifier quelle correction préserve l'état logique. Des décodeurs avancés comme le décodage par statistiques ordonnées ou les méthodes de Monte Carlo séquentielles deviennent nécessaires dans ce régime, car ils peuvent explorer l'espace de solutions au-delà du premier minimum local et agréger des preuves à travers plusieurs tentatives d'appariement pour sélectionner des corrections qui maintiennent les relations de commutation logiques avec le groupe de stabilisateurs.",
    "B": "Les syndromes avec plusieurs configurations d'erreur probables nécessitent des décodeurs plus sophistiqués capables de gérer l'ambiguïté en évaluant des hypothèses d'erreur concurrentes avec des probabilités similaires. Lorsque la difficulté du syndrome est élevée — c'est-à-dire que plusieurs configurations d'erreur distinctes auraient pu produire le syndrome observé avec une vraisemblance comparable — l'appariement parfait de poids minimal simple peut échouer car il s'engage sur une seule interprétation d'erreur sans tenir compte de cette dégénérescence. Des décodeurs plus avancés comme la propagation de croyance, les classificateurs à réseaux de neurones ou les décodeurs de maximum de vraisemblance deviennent nécessaires pour atteindre une performance de correction optimale, car ils peuvent raisonner de manière probabiliste sur l'espace des configurations d'erreur candidates et sélectionner des corrections qui minimisent les taux d'erreur logique attendus plutôt que de simplement faire correspondre le poids du syndrome.",
    "C": "Les syndromes correspondant à des erreurs de poids élevé près de la frontière du code nécessitent des décodeurs avec un raisonnement spatial amélioré pour éviter les échecs de correction dus aux effets de bord. Lorsque la difficulté du syndrome est élevée — c'est-à-dire que le motif de syndrome présente des défauts regroupés près des frontières du réseau où existent moins de chemins de correction — les décodeurs standard en volume qui supposent l'invariance par translation échouent car ils surestiment le nombre de chaînes d'erreur indépendantes qui auraient pu produire le syndrome de frontière. Des décodeurs plus sophistiqués avec une conscience explicite de la frontière, tels que les méthodes de groupe de renormalisation ou les décodeurs à réseau de tenseurs, deviennent nécessaires pour gérer cette dégénérescence géométrique, car ils peuvent tenir compte de la flexibilité de correction réduite près des bords et ajuster leurs estimations de vraisemblance d'erreur en fonction de la proximité de la périphérie du code où moins de générateurs de stabilisateurs contraignent l'espace d'erreur.",
    "D": "Les syndromes présentant des corrélations temporelles à travers des cycles de mesure consécutifs nécessitent des décodeurs avec mémoire pour suivre la dynamique de propagation des erreurs et résoudre l'ambiguïté des motifs répétés. Lorsque la difficulté du syndrome est élevée — c'est-à-dire que les mêmes emplacements de défaut s'activent à travers plusieurs cycles d'extraction de syndrome — les décodeurs sans mémoire à injection unique qui traitent chaque cycle indépendamment échouent car ils ne peuvent pas distinguer les défauts matériels persistants des erreurs stochastiques transitoires avec des signatures de syndrome similaires. Des décodeurs plus avancés incorporant des modèles de Markov cachés, des réseaux de neurones récurrents ou un filtrage bayésien deviennent nécessaires dans ce régime, car ils peuvent intégrer l'historique du syndrome au fil du temps pour déduire si les motifs récurrents proviennent de processus de bruit corrélés ou de répétitions d'erreur coïncidentes, sélectionnant des corrections qui tiennent compte de la structure temporelle du processus d'erreur plutôt que de traiter chaque cycle comme statistiquement indépendant.",
    "solution": "B"
  },
  {
    "id": 338,
    "question": "À quoi fait référence l'erreur de porte en informatique quantique ?",
    "A": "L'erreur de porte englobe les dommages physiques permanents au qubit dus à une énergie excessive d'opération de porte, où l'application répétée de portes quantiques dégrade progressivement les propriétés de cohérence du système quantique par chauffage cumulatif ou formation de défauts du réseau. Chaque opération de porte dépose une petite quantité d'énergie dans le substrat du qubit, et après des milliers d'applications de portes, les dommages accumulés se manifestent sous forme de décohérence irréversible ou de décalages de la fréquence de transition du qubit qui le rendent inutilisable pour tout calcul quantique ultérieur.",
    "B": "L'erreur de porte fait spécifiquement référence aux situations où une porte quantique échoue complètement à s'exécuter, laissant le qubit gelé dans son état original au lieu d'appliquer la transformation unitaire prévue, ce qui provoque l'arrêt du calcul à cette étape. Ce type d'échec de porte catastrophique se produit lorsque les signaux de contrôle ne parviennent pas à atteindre le qubit ou lorsque le système décohère temporairement pendant la fenêtre d'opération de la porte, résultant en une opération d'identité effective qui préserve l'état d'entrée inchangé tandis que le reste du circuit continue à s'exécuter comme si la porte avait été appliquée.",
    "C": "L'erreur de porte décrit un artefact de mesure où le système de contrôle de porte quantique effectue incorrectement une mesure projective prématurée de l'état du qubit avant d'appliquer l'opération prévue, effondrant la superposition puis appliquant la porte à la valeur de bit maintenant classique. Cette erreur de pré-mesure provient de la diaphonie entre les lignes de contrôle de porte et l'appareil de mesure, provoquant l'activation des circuits de lecture pendant l'exécution de la porte.",
    "D": "Implémentation imparfaite des portes quantiques causant une déviation de l'unitaire appliqué par rapport à la transformation cible idéale",
    "solution": "D"
  },
  {
    "id": 339,
    "question": "Comment le routage priorisé améliore-t-il les performances pour les tâches à haute priorité ?",
    "A": "Alloue les canaux quantiques de plus haute fidélité et les chemins d'intrication les plus courts aux applications critiques en temps, reportant les requêtes de priorité inférieure jusqu'à ce que les ressources premium deviennent disponibles.",
    "B": "La priorisation impose une précédence temporelle dans les opérations d'échange d'intrication en planifiant les mesures de paires de Bell à haute priorité aux nœuds intermédiaires avant les échanges de basse priorité, ce qui augmente statistiquement la fidélité conditionnelle des liens prioritaires puisque les mesures antérieures se terminent avant que le déphasage accumulé dû aux temps de cohérence de mémoire finis ne dégrade les états intriqués stockés, réallouant effectivement la ressource de fidélité dépendante du temps des tâches de fond vers les requêtes urgentes sans nécessiter de matériel physique supplémentaire.",
    "C": "Le trafic à haute priorité reçoit un accès exclusif aux paires de Bell récemment générées dont la fidélité ne s'est pas encore dégradée en dessous du seuil de distillation, tandis que les connexions de priorité inférieure se voient attribuer des paires intriquées plus anciennes qui ont subi une décohérence partielle mais restent au-dessus de la fidélité minimale utilisable — cette stratification temporelle exploite la décroissance continue de l'intrication stockée pour créer une hiérarchie de priorité naturelle sans préemption explicite, car les tâches de fond attendent de manière adaptative que les ressources fraîches de haute fidélité vieillissent dans la plage acceptable pour leurs exigences de qualité de service assouplies.",
    "D": "Les algorithmes de routage sensibles à la priorité calculent des allocations de chemins Pareto-optimales qui maximisent une somme pondérée d'utilités par tâche où les poids reflètent les niveaux de priorité, mais parce que les protocoles de distillation simultanés sur des chemins qui se chevauchent créent des régions réalisables non convexes dans l'espace objectif fidélité-versus-latence, trouver le véritable optimum nécessite de résoudre un programme quadratique en nombres entiers mixtes dont l'écart de relaxation croît avec la taille du réseau — les implémentations pratiques utilisent des heuristiques gloutonnes qui attribuent séquentiellement les requêtes de plus haute priorité en premier, acceptant une sous-optimalité pour les niveaux inférieurs afin de maintenir des décisions de planification en temps polynomial.",
    "solution": "A"
  },
  {
    "id": 340,
    "question": "Qu'est-ce qu'un modèle causal quantique ?",
    "A": "Un cadre qui étend les méthodologies d'inférence causale classique aux systèmes quantiques, incorporant les caractéristiques uniques de la mécanique quantique telles que la superposition, l'intrication et la contextualité. Il fournit des outils mathématiques pour représenter et analyser les relations causales entre événements quantiques tout en respectant les corrélations non classiques qui violent les inégalités de Bell, permettant un traitement rigoureux de la causalité dans les scénarios où les effets quantiques dominent.",
    "B": "Un cadre qui applique l'inférence causale classique aux processus de mesure quantique en représentant chaque observable comme un nœud dans un graphe acyclique dirigé, avec des arêtes encodant les dépendances conditionnelles entre résultats de mesure. Il incorpore des caractéristiques quantiques comme la superposition et l'intrication à travers des tables de probabilité conditionnelle modifiées qui tiennent compte de la contextualité, permettant l'analyse des relations causales dans les expériences quantiques tout en respectant le principe de non-signalement plutôt que les violations des inégalités de Bell.",
    "C": "Un cadre étendant les réseaux bayésiens classiques aux systèmes quantiques en représentant les états quantiques comme des distributions de probabilité sur des modèles à variables cachées qui reproduisent les corrélations quantiques. Il fournit des outils mathématiques pour analyser les relations causales entre événements quantiques à travers des mécanismes réalistes locaux, permettant de traiter la non-localité apparente comme provenant de corrélations préexistantes encodées dans la préparation initiale de l'état quantique plutôt que d'influences dynamiques.",
    "D": "Un cadre qui généralise les modèles causaux structurels classiques aux processus quantiques en incorporant des algèbres de probabilité non commutatives et en représentant les interventions comme des applications complètement positives et préservant la trace sur les opérateurs de densité. Il fournit des outils mathématiques pour analyser les relations causales tout en respectant les contraintes de non-clonage quantique et le principe d'incertitude de Heisenberg, permettant un traitement rigoureux de la causalité à travers des matrices de processus qui satisfont des conditions de séparabilité causale.",
    "solution": "A"
  },
  {
    "id": 341,
    "question": "Quelle vulnérabilité sophistiquée existe dans les techniques d'atténuation des erreurs des ordinateurs quantiques à court terme ?",
    "A": "L'extrapolation à bruit nul repose sur l'amplification délibérée du bruit du circuit en insérant des paires de portes identité ou des opérations d'étirement d'impulsion pour générer des points de données à différents niveaux de bruit, mais si le processus d'amplification du bruit introduit des erreurs non uniformes qui évoluent de manière non linéaire avec le facteur d'étirement — par exemple, si les fidélités des portes à deux qubits se dégradent plus rapidement que les portes à un qubit sous étirement, ou si la relaxation thermique commence à dominer les erreurs cohérentes aux échelles de bruit plus élevées — alors la forme fonctionnelle supposée lors de l'extrapolation polynomiale devient invalide. Un adversaire exploitant cela peut injecter un bruit ciblé qui semble linéaire aux faibles échelles mais qui s'incurve de manière imprévisible aux échelles supérieures, provoquant la convergence de l'extrapolation à bruit nul vers des résultats systématiquement biaisés qui semblent statistiquement significatifs mais encodent des informations contrôlées par l'attaquant.",
    "B": "La compilation randomisée atténue les erreurs cohérentes en moyennant sur des décompositions de portes aléatoires, convertissant efficacement le bruit cohérent en canaux de Pauli stochastiques, mais les portes de twirling aléatoires de Haar elles-mêmes sont sujettes à des erreurs d'implémentation qui peuvent introduire un échantillonnage biaisé dans l'ensemble de twirling. Plus précisément, si un adversaire peut subtilement biaiser le générateur de nombres aléatoires ou exploiter les limitations d'ensemble de portes fini qui empêchent une véritable uniformité sur le groupe de Clifford, certains canaux d'erreur de Pauli deviennent surreprésentés tandis que d'autres sont sous-échantillonnés. Ce biais d'échantillonnage signifie que le modèle de bruit effectif du circuit compilé dévie du canal dépolarisant prévu, permettant aux erreurs cohérentes structurées de survivre partiellement au processus de randomisation et de passer à travers l'atténuation des erreurs, biaisant finalement les sorties algorithmiques dans des directions prévisibles.",
    "C": "La manipulation des paramètres d'extrapolation dans les schémas d'extrapolation à bruit nul permet aux adversaires de biaiser le modèle de bruit ajusté en influençant subtilement les coefficients polynomiaux par injection ciblée d'erreurs corrélées à des facteurs d'échelle de bruit spécifiques.",
    "D": "Le twirling de Pauli symétrise le bruit en conjuguant les opérations avec des portes de Pauli aléatoires, convertissant les erreurs cohérentes arbitraires en canaux de Pauli diagonaux sous l'hypothèse que le groupe de twirling agit transitivement sur l'espace d'erreur. Cependant, cette technique présente des faiblesses systématiques lorsqu'elle est appliquée à des erreurs ayant une structure de symétrie inhérente — par exemple, si le bruit physique présente un alignement d'axe préférentiel dû aux directions des champs de contrôle ou à la géométrie de couplage environnemental. Dans de tels cas, le twirling de Pauli ne parvient pas à randomiser complètement l'erreur car certains opérateurs de Pauli commutent avec les canaux d'erreur dominants, laissant intacts les composants cohérents. Un adversaire conscient de ces faiblesses de suppression d'erreurs symétriques peut concevoir des processus de bruit alignés avec les symétries de twirling, permettant aux erreurs cohérentes ciblées de persister à travers la couche d'atténuation.",
    "solution": "C"
  },
  {
    "id": 342,
    "question": "Pourquoi les qubits ancillaires sont-ils utiles dans les implémentations de portes arithmétiques par rétroaction de phase ?",
    "A": "Les qubits ancillaires permettent des rotations de phase contrôlées qui dépendent simultanément des états de base computationnelle de plusieurs qubits de données, ce qui est essentiel pour implémenter la propagation de retenue dans les additionneurs quantiques. En intriquant les ancilles avec des positions de bits spécifiques dans le registre arithmétique, la phase acquise par l'ancille encode des informations sur les conditions de débordement sans effondrer la superposition. Cela permet aux résultats arithmétiques de s'accumuler de manière cohérente dans la phase de l'ancille, qui peut ensuite se répercuter sur les qubits de contrôle pour compléter l'opération de manière unitaire.",
    "B": "Ils stockent temporairement les informations de retenue, permettant l'accumulation cohérente de phase sans mesurer les chiffres intermédiaires, préservant ainsi l'unitarité requise pour le calcul quantique.",
    "C": "Dans l'arithmétique par rétroaction de phase, les qubits ancillaires agissent comme des cibles de phase qui accumulent des rotations proportionnelles au résultat arithmétique, qui peuvent ensuite être lues par des mesures d'interférence sans mesurer directement le registre de données. En encodant la somme ou le produit dans la phase relative entre les états |0⟩ et |1⟩ de l'ancille plutôt que dans les états de base computationnelle, le résultat arithmétique devient accessible par des mesures en base de Hadamard qui préservent la cohérence quantique. Cette stratégie d'encodage de phase réduit le nombre de portes multi-contrôlées requises par rapport aux implémentations arithmétiques en états de base.",
    "D": "Les qubits ancillaires facilitent la décomposition d'opérations contrôlées multi-qubits en séquences de portes à un et deux qubits en servant de cibles de contrôle intermédiaires dans une structure de portes en cascade. Pour les opérations arithmétiques nécessitant des contrôles sur de nombreux bits simultanément (comme vérifier si un registre dépasse un seuil), les ancilles permettent de factoriser la logique de contrôle en un arbre de portes CNOT plutôt que de nécessiter une seule porte avec de nombreux contrôles. Cette factorisation préserve les relations de phase nécessaires pour l'arithmétique cohérente tout en maintenant une profondeur de circuit logarithmique par rapport à la taille du registre.",
    "solution": "B"
  },
  {
    "id": 343,
    "question": "Dans le contexte du développement matériel de l'informatique quantique, considérez un système de qubits supraconducteurs où vous concevez la prochaine génération de processeurs corrigés d'erreurs. Votre équipe évalue s'il faut implémenter des autoencodeurs quantiques dans la couche de compression de votre hiérarchie de mémoire quantique. L'architecte en chef soutient que la correction d'erreurs par code de surface standard est suffisante, tandis que vous soupçonnez que les autoencodeurs introduisent des vulnérabilités uniques. Pourquoi la correction d'erreurs est-elle particulièrement importante pour les autoencodeurs quantiques par rapport à d'autres circuits quantiques ?",
    "A": "Les autoencodeurs quantiques fonctionnent intrinsèquement en compressant l'information quantique dans un sous-espace de dimension plus petite, mais cette compression augmente en réalité la résilience aux erreurs en réduisant le nombre de qubits physiques exposés au bruit environnemental.",
    "B": "Ils intègrent des transferts basés sur la téléportation entre les couches, et ces protocoles sont notoirement fragiles — nécessitant des paires de Bell intriquées qui doivent être générées, distribuées et consommées dans des fenêtres temporelles de l'ordre de la nanoseconde. Chaque étape de téléportation introduit deux mesures projectives plus une surcharge de communication classique, créant plusieurs points où les erreurs de phase peuvent s'accumuler sans être détectées.",
    "C": "La compression repose sur le maintien de motifs d'interférence quantique précis sur plusieurs qubits simultanément — une interférence qui dépend de relations de phase exactes entre les états de base computationnelle. Contrairement aux circuits plus simples où les erreurs affectent les opérations locales indépendamment, les erreurs d'autoencodeur se propagent à travers la représentation compressée et amplifient leur impact sur la sortie décodée. Puisque vous compressez l'information dans moins de qubits, il n'y a pas de redondance pour amortir le bruit, ce qui signifie que même de petites dérives de phase corrompent la variété encodée et produisent des résultats aberrants après décodage.",
    "D": "La décohérence les frappe plus durement car les états compressés sont plus fragiles — lorsque vous encodez n qubits dans k<<n qubits latents, le volume de l'espace de Hilbert diminue exponentiellement, ne laissant presque aucune marge d'erreur. La représentation compressée existe sur une variété de faible dimension intégrée dans l'espace complet, et tout événement de décohérence fait dériver le vecteur d'état hors de cette variété vers des régions qui se décodent en résultats aberrants.",
    "solution": "C"
  },
  {
    "id": 344,
    "question": "Que se passe-t-il dans l'algorithme de Grover si l'oracle ne marque aucune solution ?",
    "A": "Les amplitudes subissent des oscillations périodiques qui reviennent exactement à la superposition uniforme après avoir complété un cycle de Grover complet, car l'opérateur d'inversion par rapport à la moyenne préserve l'état uniforme comme point fixe lorsqu'aucun état marqué n'existe. Cependant, les mesures intermédiaires pendant les cycles partiels donnent des distributions non uniformes, avec une masse de probabilité se concentrant temporairement sur les états les plus éloignés de l'amplitude moyenne arithmétique, créant une structure apparente qui ne disparaît qu'après la complétion de multiples entiers de π√N/4 itérations.",
    "B": "L'opérateur de diffusion devient singulier car l'inversion par rapport à la moyenne nécessite de calculer l'amplitude moyenne entre les sous-espaces marqués et non marqués, et avec zéro état marqué, le calcul rencontre une condition de division par zéro dans le mécanisme de rétroaction de phase. Les implémentations modernes gèrent cela en détectant les réponses nulles de l'oracle dans O(√N) itérations via des sous-routines d'estimation d'amplitude qui mesurent l'écart de valeur propre de l'opérateur de Grover, permettant une terminaison anticipée avant que les instabilités numériques ne corrompent l'état quantique.",
    "C": "L'algorithme détecte cela en surveillant l'accumulation de phase globale après chaque itération de Grover : lorsqu'aucune solution n'existe, la phase acquise par la composante de superposition uniforme se stabilise à exactement π après √N itérations, ce qui peut être mesuré en utilisant des techniques interférométriques qui comparent l'état évolué à une copie de référence de la superposition uniforme initiale, déclenchant des protocoles de terminaison anticipée qui évitent de gaspiller davantage de ressources quantiques sur des instances de recherche insatisfaisables.",
    "D": "L'état final reste proche de la superposition uniforme, car le processus d'amplification d'amplitude n'a aucun état marqué vers lequel concentrer la masse de probabilité, résultant en des mesures qui continuent de produire des résultats uniformément aléatoires de l'espace de recherche même après le nombre standard d'itérations de Grover.",
    "solution": "D"
  },
  {
    "id": 345,
    "question": "Considérez une implémentation de code de surface sur un matériel où les temps T1 des qubits physiques varient d'un ordre de grandeur à travers la puce, et vous compilez un circuit logique qui nécessite de déplacer des états encodés entre des patchs de code distants. Le compilateur peut choisir entre une chaîne SWAP directe (nombre minimal de portes) et une stratégie de routage bidirectionnel qui déplace temporairement les qubits à travers des régions de meilleure qualité avant d'atteindre la cible. Pourquoi l'approche bidirectionnelle réduit-elle parfois l'erreur globale du circuit malgré l'ajout de plus de portes ?",
    "A": "Les cycles d'extraction de syndrome requis pendant le transport d'état accumulent moins d'erreurs lorsque les qubits physiques dans les circuits de mesure ont des temps de cohérence plus longs, et le routage bidirectionnel peut planifier les segments les plus sujets aux erreurs pour qu'ils se produisent dans des régions où les qubits ancillaires ont des valeurs T1 plus élevées, réduisant la source d'erreur dominante même si le nombre total de SWAP augmente en passant par des patchs intermédiaires de haute qualité.",
    "B": "Le routage à travers des régions de qubits physiques de meilleure qualité peut réduire l'erreur de décohérence accumulée plus que les portes SWAP supplémentaires ne l'augmentent, en particulier lorsque les qubits de faible cohérence accumuleraient autrement des erreurs d'inactivité pendant de longues chaînes de transport.",
    "C": "Les chemins bidirectionnels créent des opportunités pour des opérations de chirurgie de réseau qui fusionnent et divisent les patchs de code à des emplacements intermédiaires où la connectivité est meilleure, convertissant les chaînes SWAP en une séquence de déformations de patch qui nécessitent moins de portes physiques totales car la chirurgie de réseau parallélise le mouvement de données logiques sur plusieurs cycles d'extraction de syndrome plutôt que de déplacer séquentiellement les qubits.",
    "D": "Les portes SWAP entre qubits de haute et basse qualité présentent des profils d'erreur asymétriques où le mécanisme de bruit dominant est la relaxation d'énergie depuis l'état excité, qui se produit principalement sur le qubit à T1 plus faible. Le routage bidirectionnel exploite cette asymétrie en garantissant que le qubit de faible qualité reste dans l'état fondamental pendant la plupart des SWAP, convertissant efficacement les erreurs de portes à deux qubits en erreurs d'effacement que les codes de surface gèrent plus efficacement que les erreurs de Pauli.",
    "solution": "B"
  },
  {
    "id": 346,
    "question": "Quel est le principal compromis lors de l'adaptation des codes de surface pour tolérer la perte d'atomes ?",
    "A": "Les taux d'erreur logique se dégradent car les emplacements d'effacement restent incertains entre les événements de détection.",
    "B": "Le décodage devient plus complexe et les mesures de stabilisateurs perdent leur espacement uniforme.",
    "C": "Les seuils de capacité du code diminuent lorsque les poids des syndromes deviennent non uniformes d'un cycle à l'autre.",
    "D": "Le surcoût en ancillas augmente car la détection de perte nécessite des qubits dédiés uniquement à la mesure.",
    "solution": "B"
  },
  {
    "id": 347,
    "question": "Dans les schémas de partage de secret quantique incorporant la correction d'erreurs, quelle vulnérabilité fondamentale émerge de l'interaction entre les deux couches d'encodage ? Considérez un scénario où un adversaire a accès à la fois au canal quantique et à l'information classique auxiliaire provenant du protocole de correction d'erreurs, et peut effectuer des attaques cohérentes sur des sous-ensembles de parts pendant la phase de reconstruction.",
    "A": "La structure du code Reed-Solomon quantique pour le partage crée intrinsèquement des vulnérabilités algébriques car la base d'interpolation polynomiale utilisée pour encoder les parts établit des relations linéaires déterministes entre n'importe quelles k parts, et ces relations persistent comme sous-espaces invariants même après l'application de la correction d'erreurs quantique, ce qui signifie qu'un adversaire qui obtient k-1 parts plus l'accès aux syndromes d'erreurs peut effectivement reconstruire des coefficients polynomiaux partiels en résolvant le système sous-déterminé avec les données de syndrome comme contraintes supplémentaires, contournant la sécurité théorique de seuil puisque l'information de syndrome n'est pas indépendante au sens de la théorie de l'information du secret partagé dans la construction Reed-Solomon.",
    "B": "La manipulation du seuil de reconstruction des parts exploite le fait que lorsque les adversaires corrompent stratégiquement des parts juste en dessous du seuil de reconstruction, les parties honnêtes sont forcées d'invoquer les mécanismes de redondance dans la couche de correction d'erreurs, et ce processus d'invocation expose intrinsèquement des informations structurelles sur le secret à travers les motifs d'erreur spécifiques qui déclenchent la correction.",
    "C": "Les propriétés de distance du code stabilisateur deviennent exploitables lorsque le seuil de partage de secret k et la distance de correction d'erreurs d satisfont k > (n-d)/2, créant un écart mathématique où un adversaire peut injecter précisément d/2 erreurs dans des parts spécifiques de sorte qu'elles survivent au processus de correction d'erreurs mais biais systématiquement l'état reconstruit de manière détectable.",
    "D": "La fuite d'information via le syndrome d'erreur se produit parce que les mesures de syndrome projettent nécessairement l'état partagé sur un sous-espace, et un espion surveillant ces syndromes classiques peut apprendre des informations partielles sur le secret à travers des corrélations statistiques, en particulier lorsque les motifs de syndrome se répètent à travers plusieurs tentatives de reconstruction ou lorsque la distance du code est à peine suffisante pour le taux d'erreur attendu, puisque les données de syndrome ne sont pas uniformément aléatoires mais reflètent la distribution d'erreur réelle qui elle-même porte de faibles corrélations avec la structure du secret encodé à travers le choix des générateurs de stabilisateurs.",
    "solution": "D"
  },
  {
    "id": 348,
    "question": "Quelle est la relation théorique entre la profondeur d'un circuit quantique et la complexité des fonctions qu'il peut exprimer ?",
    "A": "La profondeur du circuit est liée à la complexité de la fonction par la croissance de l'entropie d'intrication à travers les bipartitions du registre de qubits : chaque couche peut augmenter l'entropie d'intrication d'au plus O(min(k, n-k)) pour une coupe à k qubits, et l'entropie maximale évolue comme S ≤ min(dt, n/2·log(2)) où d est la profondeur et t est le nombre de portes intriquantes par couche. Puisque de nombreuses fonctions complexes — en particulier celles qui apparaissent dans les algorithmes quantiques comme la factorisation de Shor ou la simulation quantique de systèmes à plusieurs corps — nécessitent de générer des états avec une intrication étendue à travers plusieurs partitions simultanément, la profondeur doit évoluer au moins logarithmiquement avec la complexité de la fonction pour les architectures à connexion locale, bien que la relation précise dépende de la possibilité de paralléliser le circuit de la fonction ou s'il nécessite des opérations intrinsèquement séquentielles.",
    "B": "La relation profondeur-expressivité découle de la structure d'algèbre de Lie des ensembles de portes quantiques : chaque couche de portes k-locales génère des éléments de crochets de commutateurs successivement plus élevés dans l'algèbre du groupe de Pauli, avec une profondeur d permettant d'accéder à des commutateurs imbriqués d'ordre O(d). Puisque l'implémentation de fonctions correspondant à des opérateurs de Pauli de poids élevé — qui représentent des corrélations complexes entre de nombreux qubits — nécessite de générer ces opérateurs à travers des séquences de relations de commutateurs, la profondeur du circuit doit croître au moins polynomialement avec le poids de la décomposition de Pauli de la fonction cible. Ceci est distinct de la profondeur de circuit classique car les portes quantiques génèrent des groupes de Lie continus plutôt qu'une logique booléenne discrète, nécessitant une analyse minutieuse des propriétés d'accessibilité au sein de la variété du groupe.",
    "C": "La profondeur du circuit est directement liée à la complexité des fonctions implémentables, avec des augmentations exponentielles de l'expressivité possibles avec des augmentations linéaires de profondeur. Comme chaque couche de portes peut créer de nouvelles structures d'intrication et des corrélations entre qubits, l'ajout de plus de couches permet au circuit d'approximer des mappages de plus en plus complexes de l'entrée vers la sortie. Cette relation d'échelle est soutenue par des travaux théoriques montrant que des circuits plus profonds peuvent implémenter des polynômes de degré supérieur et des fonctions booléennes plus complexes, bien que des barrières pratiques comme le bruit et les temps de cohérence limitent la profondeur réalisable sur les dispositifs à court terme.",
    "D": "La relation théorique entre profondeur et expressivité est gouvernée par la capacité du circuit à générer des t-designs : les circuits de profondeur d peuvent implémenter des t-designs approximatifs avec t ≈ O(d/n) pour n qubits avec placement aléatoire de portes, ce qui signifie qu'ils peuvent reproduire les t premiers moments statistiques de la mesure de Haar sur les matrices unitaires. Puisque les fonctions complexes nécessitent une correspondance de moments d'ordre élevé pour se distinguer des unitaires aléatoires — en particulier celles implémentant des fonctions pseudo-aléatoires ou des fonctions à sens unique pertinentes pour la cryptographie quantique — la profondeur doit évoluer comme d ≈ O(tn) pour exprimer des fonctions de complexité caractérisée par l'ordre de t-design. Ce cadre explique pourquoi une profondeur polynomiale suffit pour de nombreux algorithmes quantiques mais une profondeur exponentielle serait nécessaire pour implémenter des permutations véritablement aléatoires.",
    "solution": "C"
  },
  {
    "id": 349,
    "question": "Quelle condition détermine l'équivalence de deux circuits avec des nombres différents de qubits ?",
    "A": "Les circuits produisent des états de sortie identiques lorsque les qubits ancillas sont initialisés à |+⟩ et ensuite tracés, avec un accord requis jusqu'à un facteur de phase globale non pertinent qui n'a aucune conséquence observable. L'initialisation |+⟩ assure un témoin d'intrication maximal pour vérifier l'équivalence à travers différents registres d'ancillas.",
    "B": "Les opérateurs unitaires doivent satisfaire |Tr(U†V)|² = d où d est la dimension de l'espace de Hilbert partagé sur lequel ils agissent tous les deux, indiquant que le produit scalaire de Hilbert-Schmidt atteint sa valeur maximale et les transformations sont équivalentes à une phase globale physiquement non pertinente près.",
    "C": "Les circuits produisent des états de sortie identiques lorsque les qubits ancillas sont initialisés à |0⟩ et ensuite tracés, avec un accord requis jusqu'à un facteur de phase globale non pertinent qui n'a aucune conséquence observable.",
    "D": "Les circuits implémentent la même application complètement positive et préservant la trace lorsque les qubits ancillas sont initialisés à |0⟩ et leurs états finaux sont éliminés via la trace partielle, avec une équivalence valable à une phase globale près. Cette formulation via les applications CPTP gère naturellement le registre d'ancillas en le traitant comme partie d'un environnement étendu qui se couple au système logique mais dont les degrés de liberté sont finalement tracés, garantissant que l'équivalence de circuit respecte la sémantique opérationnelle des canaux quantiques même lorsque les nombres de qubits intermédiaires diffèrent.",
    "solution": "C"
  },
  {
    "id": 350,
    "question": "De quelle manière fondamentale les plateaux arides induits par le bruit diffèrent-ils du phénomène standard de plateau aride dans les circuits quantiques paramétrés ? Considérez que les plateaux arides standard proviennent de la concentration exponentielle des gradients due à l'expressivité, tandis que le bruit introduit un mécanisme distinct. Comment l'interaction entre la profondeur du circuit, la localité de la fonction de coût et le bruit matériel modifie-t-elle les conditions dans lesquelles les gradients s'annulent ?",
    "A": "Les plateaux arides induits par le bruit émergent même dans des circuits peu profonds avec des fonctions de coût locales, car le bruit matériel supprime directement les signaux de gradient indépendamment de l'expressivité du circuit ou de la structure d'intrication globale. Contrairement aux plateaux arides standard qui dépendent fondamentalement de la profondeur du circuit et de l'utilisation d'observables globales, la disparition du gradient induite par le bruit se produit par un mécanisme physique distinct où la décohérence et les erreurs de porte corrompent la propagation d'information dépendante des paramètres à travers le circuit, rendant les défis d'entraînabilité inévitables même lorsque des stratégies d'atténuation traditionnelles comme la réduction de la profondeur du circuit ou la localité des observables sont employées.",
    "B": "Les plateaux arides induits par le bruit proviennent de la dynamique non unitaire introduite par les canaux de décohérence qui brisent la structure préservant l'information des circuits quantiques paramétrés, causant la décroissance exponentielle des signaux de gradient avec une échelle de longueur caractéristique déterminée par le rapport de la fidélité de porte à la profondeur du circuit. Spécifiquement, le bruit dépolarisant avec un taux d'erreur p crée un facteur de suppression de gradient effectif d'environ (1-4p/3)^L où L est la profondeur du circuit, faisant disparaître les gradients même pour des circuits peu profonds avec des observables locales lorsque p dépasse un seuil proche de 1/(4L). Cependant, ce mécanisme diffère fondamentalement des plateaux arides standard car il dépend du taux de bruit local du circuit plutôt que de l'entropie d'intrication globale, ce qui signifie que des techniques d'atténuation d'erreur spatialement localisées comme le découplage dynamique appliqué aux portes portant des paramètres peuvent restaurer l'information de gradient sans nécessiter une refonte complète du circuit, à condition que les portes atténuées en erreur atteignent des fidélités satisfaisant (1-4p_mitigated/3)^L > n^(-1/2) où n est le nombre de qubits.",
    "C": "La distinction fondamentale réside dans la dynamique temporelle de l'information de gradient : les plateaux arides standard représentent une propriété statique de l'architecture unitaire du circuit où la variance du gradient évolue comme O(2^(-n)) dès le départ, tandis que les plateaux induits par le bruit émergent dynamiquement lorsque l'information cohérente du gradient se dégrade pendant l'exécution du circuit à un taux déterminé par le temps de déphasage T_2 par rapport à la durée de la porte. Pour des circuits avec un temps d'exécution total τ_circuit et un temps T_2 moyen, les signaux de gradient survivent seulement lorsque τ_circuit < T_2 · ln(n)/2, créant un seuil dépendant de la profondeur mais indépendant du taux de bruit. Ce mécanisme temporel signifie que même des circuits profonds et hautement expressifs peuvent éviter les plateaux arides induits par le bruit s'ils sont exécutés suffisamment rapidement, suggérant que l'accélération par la parallélisation des couches de portes (réduisant τ_circuit sans changer la profondeur du circuit L) peut restaurer l'entraînabilité — une stratégie d'atténuation fondamentalement différente de la refonte de circuit requise pour les plateaux arides standard.",
    "D": "Les plateaux arides induits par le bruit présentent une mise à l'échelle qualitativement différente avec la taille du système car la décohérence affecte préférentiellement les cohérences hors-diagonale qui encodent la sensibilité aux paramètres, tandis que les plateaux arides standard proviennent du brouillage uniforme de l'information quantique à travers tous les secteurs de l'espace de Hilbert. Spécifiquement, les canaux d'amortissement d'amplitude avec un taux γ suppriment les gradients comme O(e^(-γLn/2)) où L est la profondeur et n est le nombre de qubits, créant une suppression doublement exponentielle qui combine la profondeur du circuit et la taille du système multiplicativement plutôt que la mise à l'échelle purement exponentielle O(2^(-n)) des plateaux standard. Cette différence fondamentale signifie que les plateaux induits par le bruit deviennent sévères même à des tailles de système modestes (n≈20) avec des circuits peu profonds (L≈10) à des taux d'erreur réalistes (γ≈0.001), tandis que les plateaux standard ne dominent généralement pas jusqu'à n>50 à toute profondeur fixée, faisant du phénomène induit par le bruit l'obstacle principal à l'entraînabilité pour les dispositifs à court terme bien qu'il soit mécaniquement distinct de la concentration de gradient pilotée par l'expressivité.",
    "solution": "A"
  },
  {
    "id": 351,
    "question": "Pourquoi l'algorithme de Grover est-il inefficace pour les clés cryptographiques courtes ?",
    "A": "Les synchronisations de portes Toffoli deviennent un goulot d'étranglement dans les implémentations de Grover pour les clés courtes car le circuit oracle nécessite des opérations Toffoli parallèles sur plusieurs qubits pour calculer la condition de recherche, mais les architectures quantiques actuelles manquent de la connectivité tous-à-tous nécessaire pour exécuter ces portes simultanément sans réseaux SWAP.",
    "B": "La profondeur du circuit évolue quadratiquement avec la taille de l'entrée dans l'algorithme de Grover car chaque itération nécessite des appels oracle contrôlés dont la profondeur d'implémentation croît en O(n²) pour des clés de n bits en raison des constructions en cascade de portes Toffoli nécessaires pour évaluer le prédicat de recherche.",
    "C": "Les petites clés ne peuvent pas être encodées en utilisant une compilation Clifford uniquement car l'opérateur de diffusion de Grover nécessite intrinsèquement des portes non-Clifford pour réaliser la réflexion de phase négative autour de l'amplitude moyenne, et les implémentations tolérantes aux fautes de ces portes via la distillation d'états magiques deviennent impraticables pour des espaces de clés inférieurs à environ 2⁶⁴ entrées.",
    "D": "Le surcoût de correction d'erreur nécessaire pour maintenir la cohérence tout au long des itérations de Grover annule l'accélération quantique théorique pour les petits espaces de clés, car le nombre de qubits logiques et de cycles de mesure de syndrome nécessaires pour se protéger contre la décohérence dépasse l'avantage computationnel obtenu par la réduction de complexité en requêtes √N dans les implémentations de taille pratique.",
    "solution": "D"
  },
  {
    "id": 352,
    "question": "Qu'est-ce qu'un circuit Clifford en informatique quantique ?",
    "A": "Un circuit quantique où toutes les portes proviennent du groupe de Clifford—Hadamard, Phase, CNOT—qui forme un sous-groupe fini du groupe unitaire et normalise le groupe de Pauli, ce qui signifie que la conjugaison de Clifford transforme les opérateurs de Pauli en opérateurs de Pauli, une propriété exploitée par le théorème de Gottesman-Knill pour une simulation classique efficace via des tableaux de stabilisateurs.",
    "B": "Un circuit quantique composé exclusivement de portes du groupe de Clifford—à savoir les portes Hadamard, Phase et CNOT—qui peut être simulé efficacement sur des ordinateurs classiques en utilisant le théorème de Gottesman-Knill.",
    "C": "Un circuit comprenant des portes qui préservent la base de calcul sous conjugaison, spécifiquement les opérations Pauli-X, Pauli-Z et CNOT, permettant une simulation classique car ces portes transforment les états de base en états de base sans superposition et l'évolution peut être suivie de manière déterministe en utilisant la propagation de chaînes de bits plutôt que des amplitudes de vecteurs d'état nécessitant une mémoire exponentielle.",
    "D": "Un circuit quantique construit à partir de portes qui stabilisent des états maximalement intriqués sous application répétée, incluant Hadamard, porte S et CNOT, qui permettent une simulation classique en temps polynomial car les opérations de Clifford préservent le rang des matrices de densité réduites et la structure d'intrication peut être représentée de manière compacte en utilisant un nombre logarithmique de bits classiques par qubit via le formalisme des stabilisateurs.",
    "solution": "B"
  },
  {
    "id": 353,
    "question": "Dans les systèmes d'identité décentralisée post-quantiques, considérez un scénario où les utilisateurs doivent prouver leur appartenance à un ensemble de credentials sans révéler quel credential spécifique ils détiennent, tout en garantissant que les credentials révoqués ne peuvent pas être utilisés même si la liste de révocation est mise à jour de manière asynchrone sur les nœuds du réseau. Le système doit rester sécurisé contre les adversaires quantiques ayant accès à la fois aux informations de canal auxiliaire classiques et à la capacité d'effectuer des attaques hors ligne sur les transcriptions de protocole interceptées. Quelle approche technique fournit les garanties combinées les plus fortes pour la confidentialité, la révocabilité et la sécurité post-quantique dans ce modèle de menace ?",
    "A": "Les architectures d'identité auto-souveraine utilisant des chaînes de signatures post-quantiques emploient une signature séquentielle où chaque credential devient partie d'une chaîne cryptographique ancrée à un engagement d'identité genesis, fournissant une authenticité résistante aux attaques quantiques via des signatures basées sur les réseaux euclidiens comme Dilithium ou Falcon. Bien que cette approche empêche la falsification de credentials sous attaque quantique, la traçabilité entre credentials successifs crée intrinsèquement un graphe temporel qui peut être exploité par analyse de trafic, et la nature monolithique de la structure de chaîne exige que les utilisateurs présentent des portions substantielles de leur historique de credentials pour prouver un seul attribut, compromettant fondamentalement les garanties de non-traçabilité.",
    "B": "Les accumulateurs à arbre de Merkle basés sur des fonctions de hachage pour la gestion de révocation construisent des preuves d'appartenance résistantes aux attaques quantiques en faisant en sorte que chaque utilisateur maintienne un chemin d'authentification de sa feuille de credential à la racine de l'accumulateur, mis à jour via des journaux en ajout seul qui empêchent l'antidatage des révocations. Bien que les signatures SPHINCS+ ou XMSS assurent l'intégrité post-quantique des mises à jour de l'accumulateur, le protocole de vérification exige intrinsèquement que les utilisateurs divulguent leur index de feuille exact pour valider le chemin d'authentification, révélant directement quel credential ils détiennent de l'ensemble et défaisant complètement l'anonymat, bien que l'efficacité computationnelle reste excellente avec une taille de preuve en O(log n).",
    "C": "Les preuves d'identité à divulgation nulle de connaissance résistantes aux attaques quantiques construites sur des hypothèses de réseaux euclidiens, permettant à la fois la divulgation d'attributs et la vérification de révocation sans lier les instances de preuve, combinant des signatures en anneau pour l'anonymat avec des accumulateurs cryptographiques pour une vérification efficace du statut de révocation tout en maintenant la sécurité contre les attaques quantiques grâce aux hypothèses de difficulté sous-jacentes basées sur les réseaux euclidiens.",
    "D": "Les credentials anonymes basés sur les réseaux euclidiens avec protocoles de divulgation sélective exploitent la difficulté de ring-LWE pour construire des schémas de signature résistants aux attaques quantiques où les utilisateurs peuvent prouver la possession d'attributs signés sans révéler la signature complète de l'émetteur, utilisant typiquement des protocoles de Stern transformés par Fiat-Shamir pour des preuves à divulgation nulle de connaissance. Lorsqu'ils sont combinés avec des accumulateurs cryptographiques basés sur des arbres de Merkle avec des fonctions de hachage post-quantiques, cela permet une vérification efficace de révocation, bien que les mises à jour de témoins d'accumulateur doivent être synchronisées sur tous les détenteurs de credentials chaque fois que des révocations se produisent, et les preuves d'appartenance à l'ensemble divulguent par inadvertance l'époque de l'accumulateur via des variations de taille de preuve, créant des vulnérabilités d'oracle temporel.",
    "solution": "D"
  },
  {
    "id": 354,
    "question": "Laquelle des affirmations suivantes concernant la topologie de réseau en informatique quantique distribuée est la plus exacte ?",
    "A": "Pour toute opération impliquant des qubits distribués sur des processeurs séparés—qu'il s'agisse d'appliquer une porte CNOT entre des qubits distants ou de les mesurer dans une base intriquée—le système nécessite absolument des liens d'intrication physiques directs connectant ces processeurs spécifiques. Sans de telles connexions point-à-point dédiées, il n'existe aucun canal quantique par lequel les corrélations quantiques nécessaires peuvent être établies pour exécuter l'opération, car acheminer l'information quantique à travers des nœuds intermédiaires violerait le théorème de non-clonage et dégraderait la fidélité en dessous des seuils utiles pour la plupart des algorithmes distribués.",
    "B": "En l'absence de canaux de transmission de qubits physiques ou de ressources d'intrication pré-partagées connectant directement deux processeurs, la seule approche viable est la coordination classique où les résultats de mesure d'un nœud sont transmis via des liens réseau conventionnels pour informer les opérations effectuées à l'autre nœud.",
    "C": "L'échange d'intrication crée une connectivité virtuelle entre des nœuds non adjacents en effectuant des mesures d'états de Bell sur des qubits intermédiaires, étendant efficacement les corrélations quantiques à travers la topologie du réseau sans nécessiter de liens physiques directs entre chaque paire de processeurs",
    "D": "Le théorème de non-clonage impose des contraintes architecturales strictes sur les réseaux quantiques distribués en exigeant que tous les processeurs maintiennent une connectivité physique directe avec tous les autres processeurs du système. Cette topologie entièrement connectée devient nécessaire car tenter d'acheminer l'information quantique à travers des nœuds intermédiaires exigerait que ces nœuds créent des copies de l'état quantique à des fins de transfert, ce qui viole directement l'interdiction fondamentale de cloner des états quantiques arbitraires—par conséquent, les protocoles distribués tolérants aux fautes impliquant des opérations logiques multi-qubits couvrant plusieurs processeurs ne peuvent fonctionner que lorsque chaque paire possible de nœuds partage un lien dédié de génération d'intrication, augmentant substantiellement la complexité matérielle à mesure que le réseau évolue.",
    "solution": "C"
  },
  {
    "id": 355,
    "question": "Dans l'évaluation de formules quantiques via des marches quantiques sur des formules équilibrées, qu'établit réellement la borne stricte prouvée concernant les performances de l'algorithme par rapport à toutes les approches quantiques possibles et aux méthodes classiques randomisées ?",
    "A": "Aucun algorithme quantique ne peut évaluer de telles formules asymptotiquement plus rapidement, établissant l'optimalité de l'approche par marche quantique à des facteurs constants près.",
    "B": "Les formules non équilibrées nécessitent invariablement exactement deux fois le nombre de requêtes par rapport aux formules équilibrées car la marche quantique doit parcourir chaque branche non équilibrée séparément plutôt que d'explorer les deux côtés de chaque porte en superposition, et l'asymétrie empêche l'interférence constructive entre les chemins computationnels—cette pénalité d'un facteur deux est prouvée stricte par des bornes inférieures explicites dérivées de méthodes adversariales appliquées aux structures d'arbres non équilibrées dans le pire cas, où un sous-arbre a une profondeur logarithmique tandis que l'autre a une profondeur linéaire.",
    "C": "Les algorithmes classiques randomisés atteignent en réalité la même complexité en requêtes que l'algorithme de marche quantique pour les formules équilibrées lorsqu'ils sont amortis sur de nombreuses évaluations, car une stratégie d'échantillonnage aléatoire soigneusement conçue peut prioriser les variables à forte influence et élaguer adaptivement les sous-arbres en fonction des résultats intermédiaires—l'idée clé est que les formules équilibrées ont une complexité en requêtes espérée de O(√n) sous un arbre de décision randomisé optimal qui exploite la concentration de la mesure dans les distributions de produits à haute dimension.",
    "D": "La borne stricte s'applique spécifiquement uniquement lorsque la formule est construite exclusivement à partir de portes NAND plutôt que de portes OR ou AND, car le mécanisme de retour de phase utilisé dans l'algorithme de marche quantique dépend de manière critique de la propriété auto-duale de NAND sous négation. De plus, la borne est prouvée stricte uniquement dans le cas restreint où la profondeur de la formule évolue logarithmiquement avec la taille d'entrée—une propriété garantie pour les arbres binaires équilibrés mais violée par des structures de graphes plus touffues ou plus générales.",
    "solution": "A"
  },
  {
    "id": 356,
    "question": "Que permet de déterminer l'entropie min quantique dans la distribution quantique de clés ?",
    "A": "L'entropie min fournit une borne inférieure stricte sur l'entropie de von Neumann conditionnelle de la clé conditionnée par l'information quantique auxiliaire de l'adversaire, ce qui détermine la quantité d'amplification de confidentialité nécessaire par hachage universel pour comprimer la clé en bits prouvablement sûrs.",
    "B": "Elle caractérise l'information de l'espion dans le pire des cas en bornant inférieurement l'imprévisibilité des résultats de mesure du point de vue de l'adversaire, ce qui détermine la quantité d'extraction d'aléa requise pour distiller du matériel de clé inconditionnellement sûr à partir de la clé tamisée, en tenant compte des corrélations quantiques.",
    "C": "Elle quantifie le nombre de bits de clé prouvablement sûrs extractibles qui peuvent être dérivés d'un état quantique mesuré, en tenant compte de l'information maximale possible de l'adversaire sur le matériel de clé brute",
    "D": "L'entropie min borne l'information mutuelle maximale entre la clé brute d'Alice et le système quantique d'Ève en quantifiant l'entropie de Shannon minimale sur toutes les stratégies de mesure possibles qu'Ève pourrait employer, déterminant ainsi le taux de clé sûre après correction d'erreurs et amplification de confidentialité dans les régimes de longueur de clé finie.",
    "solution": "C"
  },
  {
    "id": 357,
    "question": "Pourquoi les nœuds de confiance simplifiés doivent-ils occasionnellement effectuer une distribution quantique de clés locale ?",
    "A": "Pour atténuer les effets de taille finie dans l'estimation des paramètres par le biais de sessions de QKD locales périodiques qui génèrent de nouveaux échantillons statistiques pour mettre à jour les paramètres d'amplification de confidentialité, garantissant que les estimations d'erreur de phase accumulées restent précises alors que le nœud de confiance traite plusieurs canaux simultanés dont les statistiques de corrélation violeraient autrement les preuves de sécurité contre les attaques collectives.",
    "B": "Pour renouveler leur réserve de clés de communication classique authentifiée par le biais de sessions de QKD locales périodiques qui reconstituent les clés symétriques utilisées pour les protocoles d'authentification, garantissant que les clés d'authentification compromises ne se propagent pas en vulnérabilités à travers l'infrastructure du réseau de nœuds de confiance.",
    "C": "Pour vérifier l'étalonnage de l'efficacité des détecteurs par des mesures de QKD locales qui comparent les taux de détection attendus et observés, car les nœuds de confiance doivent maintenir des statistiques de détection de photons uniques précises pour prévenir les attaques par canal auxiliaire exploitant les vulnérabilités d'aveuglement des détecteurs qui pourraient compromettre la capacité du nœud à relayer correctement les états quantiques.",
    "D": "Pour prévenir l'épuisement du tampon de clés par des sessions de QKD locales programmées qui maintiennent des réserves adéquates de matériel de clé inconditionnellement sûr pour le chiffrement par masque jetable des messages classiques inter-nœuds, car la sécurité théorique de l'information du protocole de nœud de confiance nécessite une disponibilité continue de bits de clé frais pour authentifier les métadonnées classiques des signaux quantiques transmis.",
    "solution": "B"
  },
  {
    "id": 358,
    "question": "Dans la compilation pour pièges à ions, qu'est-ce qui motive l'insertion d'impulsions π de découplage de modes spectateurs ?",
    "A": "Elles refocalisent l'intrication non intentionnelle avec les modes de mouvement hors résonance, protégeant la fidélité des opérations XX cibles en moyennant efficacement les couplages indésirables qui surviennent lorsque le schéma d'adressage laser ne peut pas isoler parfaitement un seul mode de mouvement. Ces impulsions π créent un effet d'écho de spin qui annule les phases accumulées des modes spectateurs.",
    "B": "Elles suppriment l'intrication indésirable avec les modes de mouvement hors résonance en appliquant des retournements de spin rapides qui moyennent le hamiltonien de couplage à zéro par une séquence de découplage dynamique. Ces impulsions π interrompent l'évolution sous les interactions de modes spectateurs, empêchant l'accumulation de phase qui réduirait autrement la pureté de la porte quantique à deux qubits cible en créant des corrélations indésirables entre les degrés de liberté computationnels et de mouvement.",
    "C": "Elles éliminent les déplacements Stark des faisceaux laser hors résonance en créant une séquence d'impulsions symétrique dans le temps où l'accumulation de phase Stark AC pendant la première moitié de la porte est exactement annulée par une accumulation de signe opposé pendant la seconde moitié. Ces impulsions π inversent le signe du déplacement lumineux différentiel ressenti par chaque qubit, garantissant que les fluctuations d'intensité des faisceaux d'adressage spectateurs n'introduisent pas d'erreurs de phase conditionnelle dans l'opération d'intrication cible.",
    "D": "Elles atténuent le chauffage des modes de mouvement spectateurs en inversant l'effet de l'opérateur de création de phonons au point médian de la séquence de porte, implémentant effectivement un train de Carr-Purcell qui supprime le chauffage anormal. Ces impulsions π créent une interférence destructive entre les processus de chauffage dans les première et seconde moitiés de la porte, préservant l'occupation de l'état fondamental de mouvement requise pour les opérations de Mølmer-Sørensen à haute fidélité malgré le bruit de champ électrique ambiant couplé aux modes spectateurs.",
    "solution": "A"
  },
  {
    "id": 359,
    "question": "Quelle méthodologie d'attaque avancée cible les effets de taille finie dans les implémentations de distribution quantique de clés ?",
    "A": "L'exploitation des frontières de taille de bloc tire parti du fait que les systèmes pratiques de QKD doivent partitionner les flux de clés continus en blocs discrets pour l'analyse statistique d'échantillons finis. Un adversaire synchronise soigneusement son intervention pour cibler les frontières entre blocs consécutifs, où les protocoles de réconciliation transitent entre différents codes de correction d'erreurs optimisés pour des longueurs de bloc variables. En induisant des erreurs corrélées précisément à ces points de transition, Ève peut créer des anomalies statistiques qui apparaissent comme du bruit légitime dans les blocs individuels mais s'accumulent systématiquement aux frontières.",
    "B": "La manipulation d'intervalles de confiance exploite l'incertitude statistique inhérente à l'estimation des taux d'erreur de bits quantiques à partir d'échantillons finis en introduisant stratégiquement des erreurs qui élargissent les bornes de confiance d'Alice et Bob. Lorsque les parties légitimes calculent leurs statistiques d'erreur, elles doivent choisir entre des bornes conservatrices qui gaspillent trop de matériel de clé en amplification de confidentialité ou des bornes agressives qui risquent d'accepter des clés compromises. Un adversaire surveille le canal de réconciliation public pour apprendre quels estimateurs statistiques sont utilisés, puis injecte des erreurs avec des corrélations temporelles soigneusement ajustées qui maximisent la variance de l'estimateur sans augmenter sa moyenne au-delà du seuil d'abandon.",
    "C": "L'interférence dans l'estimation des paramètres cible la façon dont Alice et Bob estiment les taux d'erreur à partir d'échantillons limités, les forçant à accepter des clés avec une amplification de confidentialité insuffisante. En manipulant le processus d'échantillonnage statistique pendant la phase de mesure du taux d'erreur de bits quantiques, un adversaire peut biaiser la distribution d'erreur observée vers l'extrémité inférieure de ce qui déclencherait un abandon, conduisant les parties légitimes à sous-estimer le gain d'information d'Ève. Cette exploitation repose sur l'incertitude inhérente aux statistiques d'échantillons finis où les estimations doivent être faites à partir de milliers plutôt que d'échanges de photons infinis.",
    "D": "L'amplification de fluctuation statistique cible la variance d'échantillonnage inhérente à la QKD à clé finie en exploitant la mise à l'échelle en racine carrée de l'écart-type avec la taille d'échantillon. Dans les implémentations réalistes limitées à 10^6-10^9 échanges de photons par établissement de clé, les fluctuations aléatoires du taux d'erreur observé peuvent atteindre plusieurs écarts-types au-dessus du bruit de canal moyen. Un attaquant synchronise son écoute clandestine pour coïncider avec des fluctuations positives naturelles du taux d'erreur de bits quantiques, puis ajoute une petite perturbation supplémentaire qui semble cohérente avec le niveau de bruit déjà élevé. Cette technique cache efficacement le gain d'information d'Ève dans les bornes d'incertitude statistique qu'Alice et Bob doivent accepter lorsqu'ils travaillent avec des données finies.",
    "solution": "C"
  },
  {
    "id": 360,
    "question": "Laquelle des approches suivantes est valide pour atténuer les plateaux stériles dans les réseaux de neurones quantiques ?",
    "A": "Des structures d'ansatz spécifiques au problème qui incorporent des symétries, des lois de conservation ou d'autres connaissances du domaine pour contraindre l'unitaire paramétré à une variété de dimension inférieure alignée avec le paysage de la fonction de coût. Par exemple, dans les applications de chimie quantique, les ansätze qui préservent le nombre de particules et les symétries de spin restreignent l'espace de recherche aux états physiquement pertinents, évitant les régions de l'espace de Hilbert où les gradients disparaissent en raison de non-pertinence plutôt que de concentration exponentielle.",
    "B": "L'entraînement par couches, où le circuit est optimisé de manière incrémentale en entraînant d'abord un sous-circuit peu profond puis en ajoutant des couches supplémentaires une à la fois tout en gelant ou en ajustant finement les paramètres précédemment optimisés. Cette stratégie garantit qu'à chaque étape de l'entraînement, le problème d'optimisation actif n'implique qu'un sous-ensemble de l'espace de paramètres complet, empêchant la suppression exponentielle des gradients qui se produit lorsque tous les paramètres d'un circuit profond sont mis à jour simultanément.",
    "C": "L'utilisation d'ansätze adaptés au matériel pour maximiser la fidélité des portes sur toute la profondeur du circuit, ce qui réduit la variance induite par le bruit dans les estimations de gradient et permet des mises à jour de paramètres plus fiables même lorsque le véritable signal de gradient devient exponentiellement petit. Les conceptions adaptées au matériel s'alignent avec l'ensemble de portes natif et le graphe de connectivité du dispositif physique, minimisant le nombre de portes SWAP et réduisant la durée totale du circuit.",
    "D": "La combinaison d'architectures de circuits peu profonds pour limiter la profondeur d'intrication, de protocoles d'entraînement par couches qui optimisent les sous-circuits de manière incrémentale avant d'ajouter des couches, et de conceptions d'ansatz conscientes du problème qui incorporent des symétries et des lois de conservation fournit toutes des stratégies complémentaires pour éviter la disparition exponentielle du gradient.",
    "solution": "D"
  },
  {
    "id": 361,
    "question": "Considérons un circuit quantique variationnel utilisé pour l'optimisation où la fonction de coût dépend de valeurs d'espérance calculées à partir de l'état de sortie complet. Lorsque la profondeur du circuit dépasse un certain seuil, l'entraînement basé sur le gradient devient de plus en plus difficile, indépendamment du choix des paramètres initiaux ou du taux d'apprentissage. Ce phénomène a été observé sur plusieurs plateformes matérielles et semble être fondamental plutôt que le produit du bruit. Les fonctions de coût locales ont été proposées comme remède potentiel. Les fonctions de coût locales améliorent la capacité d'entraînement des QNN principalement en :",
    "A": "Concentrant l'information de gradient sur de petits sous-ensembles de qubits, ce qui limite la dimension effective de l'espace de Hilbert et empêche la dilution exponentielle des gradients qui se produit lorsque les fonctions de coût dépendent d'observables globales couvrant tous les qubits du circuit, maintenant ainsi les magnitudes de gradient à des niveaux où les algorithmes d'optimisation peuvent détecter de manière fiable un signal non nul au-dessus du bruit d'échantillonnage fini.",
    "B": "Contraignant les mesures à des observables k-locales où k << n garantit que la variance du gradient évolue en O(1/2^k) plutôt qu'en O(1/2^n), car les fonctions de coût locales ne sondent que des sous-espaces de dimension 2^k de l'espace de Hilbert complet de dimension 2^n, empêchant le signal de gradient de se disperser dans un nombre exponentiellement grand de directions non pertinentes, bien que cela nécessite que l'observable locale capture encore suffisamment d'informations sur l'objectif d'optimisation pour guider la convergence vers les optima globaux malgré une sensibilité réduite aux corrélations entre qubits distants.",
    "C": "Restreignant les fonctions de coût à des observables locales modifie fondamentalement les propriétés de concentration de mesure : alors que les observables globales sur des états aléatoires se concentrent exponentiellement étroitement autour de leur moyenne selon le lemme de Lévy, causant la disparition des gradients dans les plateaux désertiques, les observables locales maintiennent une variance constante indépendante de la taille du système car les fluctuations ne dépendent que de la dimension du sous-système mesuré, garantissant que les magnitudes de gradient restent O(1) à mesure que la profondeur du circuit augmente, bien que cela suppose que la région locale capture les caractéristiques pertinentes pour l'optimisation.",
    "D": "Limitant la mesure à des observables k-locales réduit le cône de lumière des portes contribuant à chaque composante de gradient, puisque ∂⟨O_local⟩/∂θ_i s'annule lorsque la porte i se trouve en dehors du cône causal du support de O_local, partitionnant effectivement l'espace des paramètres en sous-problèmes d'optimisation indépendants qui évitent la moyenne exponentielle sur des régions de circuit non liées qui autrement élimineraient le signal de gradient par interférence destructive à travers l'espace d'états de dimension 2^n, maintenant la capacité d'entraînement en convertissant un problème global exponentiellement difficile en un nombre polynomial de problèmes locaux traitables.",
    "solution": "A"
  },
  {
    "id": 362,
    "question": "Les environnements d'exécution de confiance modernes doivent se protéger contre les adversaires quantiques, mais les implémentations actuelles font face à plusieurs obstacles. Le défi d'ingénierie le plus immédiat provient de l'intégration de primitives cryptographiques post-quantiques dans les architectures TEE existantes. Quelle limitation technique spécifique pose le plus grand défi pour les environnements d'exécution de confiance résistants aux attaques quantiques ?",
    "A": "Les vulnérabilités par canaux auxiliaires dans les implémentations cryptographiques basées sur les isogénies créent des défis significatifs — ces schémas post-quantiques comme SIKE (bien que récemment cassé par des attaques classiques) nécessitent de calculer des chemins à travers des graphes d'isogénies supersingulières avec des opérations qui ont des variations de temps dépendantes des données. Les opérations de génération de clés et d'encapsulation impliquent l'évaluation d'isogénies de haut degré avec des motifs de calcul hautement irréguliers qui fuient des informations par le timing de cache, la prédiction de branchement et l'analyse de puissance. Les TEE doivent implémenter des algorithmes d'arithmétique de points et d'évaluation d'isogénies à temps constant tout en empêchant les fuites de timing des opérations de corps sous-jacentes, ce qui augmente considérablement la surcharge par rapport aux implémentations ECC classiques pour lesquelles le matériel TEE a été initialement optimisé.",
    "B": "Les vulnérabilités par canaux auxiliaires dans les implémentations de signatures basées sur les fonctions de hachage créent des défis significatifs — ces schémas post-quantiques avec état comme XMSS ou SPHINCS+ nécessitent de maintenir un état de clé secrète à travers plusieurs opérations de signature, et les évaluations de fonctions de hachage (souvent des milliers par signature) ont des motifs d'accès mémoire corrélés avec la hiérarchie de clés secrètes. Les algorithmes de parcours d'arbre de Merkle et les évaluations de fonctions pseudo-aléatoires fuient des informations de timing sur les clés de signature à usage unique utilisées. Les TEE doivent suivre les mises à jour d'état de manière atomique pour empêcher la réutilisation de clés tout en implémentant des opérations de hachage et de navigation d'arbre à temps constant, nécessitant des modifications importantes du stockage sécurisé des clés et du contrôle d'accès qui n'étaient pas nécessaires pour les schémas de signature classiques sans état.",
    "C": "Les vulnérabilités par canaux auxiliaires dans les implémentations cryptographiques basées sur les réseaux euclidiens créent des défis significatifs — ces schémas post-quantiques ont des tailles de clés considérablement plus grandes (souvent plusieurs kilooctets comparés à des centaines d'octets pour RSA/ECC) et nécessitent des opérations de multiplication polynomiale plus complexes qui fuient des informations considérables de timing et de consommation d'énergie. Les temps d'exécution plus longs et les motifs d'accès mémoire d'opérations comme la multiplication polynomiale basée sur NTT sont particulièrement susceptibles aux attaques par timing de cache et à l'analyse électromagnétique, nécessitant que les TEE implémentent des contre-mesures étendues telles que des implémentations à temps constant, l'obscurcissement d'accès mémoire et le masquage de consommation d'énergie, ce qui augmente la surcharge et la complexité.",
    "D": "Les vulnérabilités par canaux auxiliaires dans les implémentations cryptographiques basées sur les codes créent des défis significatifs — ces schémas post-quantiques comme Classic McEliece ont des clés publiques extrêmement grandes (de centaines de kilooctets à des mégaoctets) qui sollicitent les contraintes mémoire des TEE et nécessitent des opérations de matrices creuses pendant le chiffrement qui présentent des motifs d'accès mémoire hautement non uniformes. Les algorithmes de décodage de syndrome utilisés dans le déchiffrement impliquent des procédures itératives avec un comportement de branchement dépendant des données qui fuit des informations sur les positions d'erreur par le timing et les motifs d'accès au cache. Les TEE doivent stocker ces clés surdimensionnées dans la mémoire protégée tout en implémentant des opérations d'échantillonnage de poids constant et de permutation en temps constant, nécessitant un support matériel spécialisé pour de grandes régions de mémoire sécurisée et des techniques de masquage pour les algorithmes combinatoires impliqués dans le décodage.",
    "solution": "C"
  },
  {
    "id": 363,
    "question": "La planification du découplage dynamique pendant les cycles de correction d'erreur nécessite un alignement minutieux car les impulsions de découplage précoces peuvent avoir quel effet néfaste ?",
    "A": "Allonge les circuits de stabilisateur, ajoutant du bruit de porte à deux qubits",
    "B": "Interfère avec les fenêtres de timing d'initialisation des ancillas",
    "C": "Perturbe la cohérence de mesure de syndrome pendant la lecture",
    "D": "Entre en conflit avec les séquences d'impulsions d'extraction de stabilisateur",
    "solution": "A"
  },
  {
    "id": 364,
    "question": "Quels sont les principaux défis pour un apprentissage par transfert efficace dans le domaine quantique ?",
    "A": "Les variations matérielles, le bruit et les problèmes de compatibilité de plateforme créent des obstacles fondamentaux lors de la tentative de transfert de modèles quantiques pré-entraînés entre différentes implémentations physiques. La variabilité dans les ensembles de portes natifs, les topologies de connectivité des qubits et les caractéristiques de décohérence signifie que les circuits paramétrés optimisés pour un dispositif nécessitent souvent un ré-entraînement étendu ou une transpilation de circuit lors du déploiement sur une autre plateforme. De plus, les profils de bruit non stationnaires inhérents au matériel de l'ère NISQ causent une dégradation imprévisible des caractéristiques quantiques apprises pendant le transfert, tandis que les nombres limités de qubits restreignent la flexibilité architecturale nécessaire pour adapter les couches pré-entraînées à de nouvelles tâches cibles sans interférence catastrophique dans les représentations apprises.",
    "B": "Les structures d'intrication spécifiques à la tâche et les décalages de géométrie de l'espace de Hilbert limitent sévèrement le transfert de connaissances, puisque les cartes de caractéristiques quantiques pré-entraînées intègrent les données dans des motifs d'intrication optimisés pour la structure statistique du domaine source. Lorsqu'elles sont transférées vers de nouvelles tâches avec des structures de corrélation différentes, ces représentations apprises présentent des phénomènes de plateaux désertiques pendant l'ajustement fin en raison de gradients s'annulant exponentiellement dans le paysage de perte cible. L'incapacité à effectuer un gel partiel de couches — une technique clé de l'apprentissage par transfert classique — aggrave ce problème, car les couches de circuit quantique ne peuvent pas être entraînées sélectivement sans affecter l'intrication globale, nécessitant une réoptimisation quasi-complète qui annule les bénéfices du pré-entraînement et performe souvent moins bien qu'une initialisation aléatoire.",
    "C": "Les contraintes de compilation spécifiques aux dispositifs et les dépendances de décomposition de portes empêchent fondamentalement la portabilité des circuits entre plateformes quantiques, car chaque architecture matérielle nécessite des implémentations de portes natives qui ne peuvent pas être abstraites sans surcharge exponentielle. Contrairement aux réseaux classiques où les matrices de poids se transfèrent directement entre implémentations CPU et GPU, les circuits quantiques paramétrés doivent être recompilés à partir de zéro pour chaque dispositif cible car les traductions d'ensemble de portes universelles introduisent des erreurs de phase qui s'accumulent multiplicativement à travers la profondeur du circuit. Ce verrouillage architectural est exacerbé par les placements de portes à deux qubits dépendants de la topologie, où les configurations de paramètres optimales sur un graphe de connectivité deviennent sous-optimales sur un autre, nécessitant un ré-entraînement complet plutôt qu'un ajustement fin pour maintenir les seuils de fidélité.",
    "D": "L'incompatibilité des frameworks et les limitations de sérialisation bloquent la portabilité des modèles quantiques, puisqu'aucun format d'échange standardisé n'existe pour les circuits quantiques paramétrés entre les écosystèmes Qiskit, Cirq et PennyLane. Chaque plateforme utilise des schémas de paramétrisation de portes propriétaires et des interfaces backend d'optimisation qui ne peuvent pas être directement traduits, empêchant les modèles pré-entraînés d'être chargés dans différentes piles logicielles. Alors que les frameworks d'apprentissage profond classiques partagent ONNX et des standards similaires permettant un transfert de modèle transparent, l'informatique quantique manque de protocoles analogues pour encoder les paramètres de circuit appris, les métadonnées de topologie de circuit et les données de calibration matérielle dans une représentation indépendante de la plateforme, forçant les chercheurs à ré-entraîner à partir de zéro lors du changement de frameworks malgré une physique sous-jacente identique.",
    "solution": "A"
  },
  {
    "id": 365,
    "question": "Une liaison pratique de distribution quantique de clés fonctionne à un taux de détection brute de 10 MHz sur 40 km de fibre. Le système utilise BB84 avec états leurres avec des détecteurs présentant des post-impulsions (probabilité de clic parasite de 20% par fenêtre) et subit une perte de 0,2 dB/km. Un adversaire exploite un déséquilibre de diviseur de faisceau dépendant de la longueur d'onde dans le modulateur d'Alice, permettant un biais de 3% vers la mesure de certaines valeurs de bits sans déclencher d'alarmes QBER au-dessus du seuil de 8% fixé par les preuves de sécurité à clé finie à cette distance. Étant donné que l'amplification de confidentialité extrait environ 0,4 bits par photon tamisé sous ces paramètres, et que la distillation d'avantage classique ajoute 12% de surcharge, pourquoi cette attaque par canal auxiliaire reste-t-elle non détectée par la détection standard d'interception-renvoi tout en compromettant la clé finale ?",
    "A": "L'attaque exploite un défaut d'implémentation physique dans le matériel d'encodage plutôt que de manipuler le canal quantique lui-même, laissant ainsi le taux d'erreur de bits quantiques dans des limites acceptables pour la preuve de sécurité. Puisque l'adversaire obtient des informations partielles sur les valeurs de bits par une corrélation classique avec le comportement du modulateur plutôt qu'en induisant des perturbations quantiques détectables, l'attaque contourne les seuils de détection d'interception-renvoi tout en fuyant régulièrement de l'entropie de clé que l'amplification de confidentialité ne peut pas entièrement éliminer lorsque le canal auxiliaire persiste à travers tous les bits tamisés, créant un canal d'information caché qui opère en dehors du modèle de menace supposé par les analyses de sécurité QKD conventionnelles.",
    "B": "Le déséquilibre du diviseur de faisceau dépendant de la longueur d'onde crée une corrélation classique entre le choix de base d'Alice et les propriétés spectrales des photons émis que l'adversaire peut exploiter par filtrage sélectif passif en longueur d'onde avant que les états quantiques n'entrent dans le canal de fibre avec perte. Puisque ce filtrage se produit avant la perte de canal et les comptes d'obscurité du détecteur, il introduit un biais qui se manifeste comme une corrélation entre les valeurs de bits et les temps d'arrivée aux détecteurs de Bob, apparaissant statistiquement indiscernable de la signature de post-impulsion déjà présente dans le système. L'extraction d'informations partielles de l'adversaire par sélection de longueur d'onde augmente l'information de Holevo d'Eve, mais l'effet est distribué à travers la phase de réconciliation d'erreur où il imite les pertes de corrélation légitimes dues à l'inefficacité du détecteur.",
    "C": "Le canal auxiliaire exploite la structure temporelle des événements de post-impulsion en corrélant le biais de valeur de bit de 3% avec la probabilité de clic parasite de 20%, de sorte que les événements de détection corrompus par post-impulsion portent plus d'informations sur les valeurs de bits encodées d'Alice que les détections de photons légitimes. Puisque la post-impulsion contribue déjà au taux d'erreur de base du système, et que les preuves de sécurité tiennent compte de cette contribution en réduisant le taux de clé extractible par estimation pessimiste des paramètres, la corrélation supplémentaire introduite par le biais du modulateur se situe dans les marges d'incertitude de l'analyse à clé finie. Le gain d'information de l'adversaire se compose à travers plusieurs tours car les effets mémoire de post-impulsion persistent à travers les fenêtres de détection séquentielles.",
    "D": "L'attaque introduit un biais dépendant de la longueur d'onde qui corrèle avec le temps de stabilisation du modulateur de phase d'Alice après chaque rotation de base, créant un canal auxiliaire temporel où les temps d'émission de photons portent des informations partielles sur les valeurs de bits. Cette corrélation temporelle apparaît dans les statistiques QBER comme un taux d'erreur accru sur les photons arrivant dans la première nanoseconde de chaque fenêtre de détection, mais les analyses de sécurité existantes attribuent ce taux d'erreur élevé à l'interférence inter-symbole de la dispersion chromatique dans la liaison de fibre de 40 km, qui produit également des motifs d'erreur dépendants du timing. Puisque la dispersion chromatique augmente naturellement avec la distance à 0,2 dB/km de perte correspondant à environ 17 ps/nm/km de dispersion, la signature du canal auxiliaire est masquée par les erreurs induites par dispersion attendues.",
    "solution": "A"
  },
  {
    "id": 366,
    "question": "Pourquoi les nœuds de confiance sont-ils considérés comme une solution provisoire plutôt que l'architecture à long terme pour l'Internet quantique ?",
    "A": "Ils compromettent fondamentalement la sécurité de bout en bout en exigeant que chaque relais intermédiaire mesure et reprépare l'état quantique, ce qui signifie que chaque nœud de confiance doit avoir un accès complet à l'information transmise. Cela viole le principe de relais non fiable que la communication chiffrée classique réalise, où les routeurs intermédiaires ne peuvent pas accéder au contenu des données.",
    "B": "Ils compromettent l'avantage quantique pour le calcul distribué en détruisant l'intrication à chaque relais par transmission basée sur la mesure, empêchant des applications comme le calcul quantique aveugle et l'algorithme de Shor distribué qui nécessitent une intrication multipartite cohérente entre les points d'extrémité du réseau. Bien qu'ils permettent la distribution quantique de clés en établissant des clés partagées classiques, ils ne peuvent pas supporter la fidélité de canal quantique nécessaire pour les protocoles où le calcul s'effectue sur plusieurs nœuds sans révéler les états intermédiaires.",
    "C": "Ils introduisent des limites fondamentales de passage à l'échelle car chaque nœud de confiance nécessite des mémoires quantiques avec des temps de cohérence dépassant le délai de communication classique aller-retour nécessaire pour établir le segment de lien suivant, créant un goulot d'étranglement de décohérence où les exigences de T2 croissent linéairement avec le diamètre du réseau. Les mémoires actuelles à piège à ions atteignent une cohérence de ~10 secondes, suffisante pour les réseaux métropolitains mais inadéquate pour les distances transcontinentales où les latences de coordination classique dépassent les capacités de stockage quantique.",
    "D": "Ils créent une vulnérabilité de sécurité théorique de l'information distincte de l'exigence de confiance : le théorème de non-clonage empêche la détection d'écoute clandestine sur les opérations internes des nœuds de confiance, ce qui signifie que les nœuds compromis peuvent copier les états quantiques par reconstruction tomographique sur plusieurs exécutions de protocole sans déclencher d'alertes de sécurité. Contrairement à la distribution quantique de clés point à point où les attaques d'interception-renvoi perturbent les statistiques du canal de manière détectable, les nœuds de confiance mesurent légitimement les états, masquant toute copie illicite dans le bruit opérationnel normal.",
    "solution": "A"
  },
  {
    "id": 367,
    "question": "Comment le couplage de phase induit par résonateur peut-il permettre une attaque furtive d'empoisonnement de parité dans les processeurs transmon ?",
    "A": "Un adversaire exploite les décalages dispersifs d'ordre supérieur non surveillés en signalant intentionnellement de manière erronée la fréquence du résonateur habillé lors de l'étalonnage initial, obligeant le logiciel de contrôle à appliquer des impulsions de compensation de couplage ZZ avec des décalages de phase incorrects. Sur des centaines de cycles de porte, ces erreurs de phase systématiques biaisent les angles de rotation conditionnelle accumulés dans les mesures de parité à deux qubits par des quantités qui restent dans le plancher de bruit de grenaille des protocoles d'analyse comparative randomisée standard, permettant aux erreurs logiques encodées de se propager sans détection à travers les tours de stabilisateur tout en corrompant les valeurs propres de syndrome par interférence contrôlée.",
    "B": "Un adversaire désaccorde délibérément la fréquence du résonateur de bus de petites quantités — typiquement 10-50 MHz — pour accumuler des interactions de couplage dispersif ZZ indésirables sur plusieurs cycles de porte. Ces décalages de phase systématiques biaisent la parité logique mesurée dans les codes stabilisateurs sans déclencher immédiatement d'alarmes de recalibrage, permettant aux erreurs de s'accumuler en dessous des seuils de détection tout en corrompant l'information quantique encodée par interférence contrôlée avec les opérations d'intrication native à deux qubits du circuit d'extraction de syndrome.",
    "C": "En induisant une fuite transitoire filtrée par Purcell dans le troisième état excité du résonateur à travers des impulsions micro-ondes soigneusement synchronisées à la fréquence de transition |2⟩↔|3⟩, un attaquant induit des décalages Stark AC qui modulent l'intensité d'interaction ZZ effective entre les transmons couplés capacitivement. Ces couplages dispersifs variant dans le temps créent des erreurs de phase cohérentes dans les mesures de vérification de parité qui s'annulent en moyenne sur des cycles de stabilisateur uniques mais s'accumulent de manière constructive sur des séquences plus longues, biaisant systématiquement les résultats de syndrome logique en dessous du seuil de déclenchement de recalibrage tout en corrompant l'information encodée par interférence cohérente en phase.",
    "D": "Un attaquant exploite la dépendance paramétrique des taux de couplage longitudinal au nombre de photons du résonateur en modulant subtilement l'amplitude d'entraînement appliquée aux qubits ancillaires pendant les séquences de mesure de parité. Cela fait que l'hamiltonien d'interaction ZZ effective entre les qubits de données acquiert une phase dépendante du temps qui fait pivoter de manière cohérente la base de calcul à deux qubits à des taux comparables à l'inverse de la durée de porte. Le biais systématique résultant dans les parités mesurées reste caché dans les tolérances d'étalonnage car il se manifeste comme un mauvais étalonnage apparent de l'angle de rotation plutôt qu'une signature d'erreur distincte.",
    "solution": "B"
  },
  {
    "id": 368,
    "question": "Quel est le principal avantage d'utiliser des décodeurs neuronaux pour la correction d'erreurs quantiques par rapport aux décodeurs traditionnels ?",
    "A": "Les décodeurs neuronaux peuvent apprendre des correspondances syndrome-correction qui tiennent implicitement compte des erreurs de mesure et de la diaphonie pendant l'extraction du syndrome elle-même, s'adaptant au modèle de bruit complet incluant les circuits de stabilisateur défectueux plutôt que de supposer des mesures de syndrome parfaites. En s'entraînant sur des données expérimentales incluant des erreurs de mesure de syndrome, ces décodeurs atteignent une fidélité logique plus élevée que l'appariement parfait de poids minimum sur le même matériel, bien qu'ils nécessitent toujours une extraction complète du syndrome et puissent nécessiter un temps de traitement classique comparable pour les passages avant à travers des réseaux profonds.",
    "B": "Ils peuvent s'adapter à des modèles de bruit complexes et spécifiques au dispositif qui vont au-delà des canaux standard de dépolarisation ou de Pauli, y compris les erreurs spatialement corrélées et les effets non markoviens, tout en nécessitant potentiellement beaucoup moins de temps de traitement classique par syndrome grâce à la reconnaissance de motifs apprise au lieu du décodage à vraisemblance maximale exhaustif sur des classes d'erreurs exponentiellement grandes.",
    "C": "Les décodeurs neuronaux implémentent des algorithmes de contraction de réseaux de tenseurs à travers des motifs de connectivité appris, où les poids entraînés encodent des séquences de contraction optimales pour les graphes de syndrome. Cette approche réduit la complexité exponentielle du décodage à vraisemblance maximale à une complexité polynomiale en exploitant la propagation de croyance approximative sur les graphes de facteurs, bien que l'implémentation nécessite toujours des coprocesseurs FPGA cryogéniques pour atteindre la latence inférieure à la microseconde exigée par les temps de cycle du surface code. L'ordre de contraction appris s'adapte à la connectivité spécifique au dispositif des qubits sans optimisation manuelle.",
    "D": "Ils atteignent des performances sous le seuil en apprenant des corrélations de syndrome non linéaires qui violent les hypothèses d'indépendance locale sous-jacentes aux décodeurs traditionnels comme MWPM. Grâce à l'entraînement sur des chaînes d'erreurs corrélées générées par des modèles de bruit réalistes incluant la fuite et les erreurs cohérentes, les réseaux neuronaux découvrent des signatures d'erreur d'ordre supérieur qui restent cachées aux approches basées sur les graphes, permettant une suppression d'erreur logique en dessous du seuil du surface code même avec des taux d'erreur physique à 1%, bien que le traitement classique nécessite actuellement ~10ms par syndrome, ce qui dépasse les budgets typiques de cycle de code.",
    "solution": "B"
  },
  {
    "id": 369,
    "question": "Quel protocole avancé fournit la sécurité la plus forte pour l'authentification quantique ?",
    "A": "L'authentification de message quantique avec fonctions non clonables — celles-ci exploitent le théorème de non-clonage pour créer des étiquettes d'authentification fondamentalement infalsifiables qui ne peuvent pas être copiées même par un adversaire disposant d'une puissance de calcul quantique illimitée. En encodant la clé d'authentification dans des états quantiques non orthogonaux distribués sur plusieurs qubits, toute tentative de duplication de l'authentificateur introduit des perturbations détectables par rétroaction de mesure, fournissant une sécurité théorique de l'information qui dépasse même les MAC classiques post-quantiques.",
    "B": "Les codes d'authentification de message sécurisés quantiquement reposent sur des primitives cryptographiques basées sur les réseaux ou les fonctions de hachage qui restent difficiles sur le plan calculatoire même contre les attaques quantiques, fournissant une sécurité d'authentification qui évolue avec la longueur de clé selon les limitations de l'algorithme de Grover.",
    "C": "Les authentificateurs quantiques à usage unique, qui fournissent une sécurité inconditionnelle en consommant de l'intrication quantique partagée fraîche pour chaque événement d'authentification, garantissant que même des adversaires sans limite de calcul ne peuvent pas falsifier les messages. Ces protocoles atteignent une sécurité théorique de l'information grâce aux propriétés fondamentales de la mécanique quantique plutôt qu'à des hypothèses de calcul.",
    "D": "Les signatures numériques quantiques atteignent une non-répudiation inconditionnelle grâce à la distribution d'intrication multipartite, où l'état quantique du signataire ne peut pas être falsifié ou nié après coup en raison des contraintes de monogamie de l'intrication. Le protocole génère une authentification transférable qui survit même si la clé privée du signataire est ultérieurement compromise, car la vérification de signature dépend de paires EPR précédemment distribuées dont les corrélations ont été établies au moment de la signature et ne peuvent pas être modifiées rétroactivement, fournissant un modèle de sécurité plus fort que les schémas d'authentification à usage unique qui manquent de garanties de non-répudiation.",
    "solution": "C"
  },
  {
    "id": 370,
    "question": "Quelle méthodologie d'attaque avancée cible les hypothèses dans les protocoles de distribution quantique de clés indépendants du dispositif ?",
    "A": "Le contrôle de canal superdéterministe exploite l'hypothèse d'indépendance de mesure en permettant à un adversaire d'ingénier des corrélations entre les variables cachées régissant le comportement du dispositif et les choix de paramètres de mesure, créant effectivement une cause commune qui viole l'indépendance statistique sans nécessiter de signalisation plus rapide que la lumière. En préparant soigneusement les conditions initiales du canal quantique d'une manière corrélée avec les choix de mesure futurs, l'attaquant peut simuler des violations de Bell tout en extrayant l'information de clé complète, contournant les contraintes de non-signalisation sur lesquelles reposent les protocoles indépendants du dispositif.",
    "B": "Les violations artificielles de l'inégalité CHSH sont réalisées lorsqu'un espion manipule les événements de détection en exploitant la faille de liberté de choix combinée avec des attaques de synchronisation temporelle, provoquant les corrélations mesurées à dépasser la borne classique de 2 sans qu'une véritable intrication quantique soit présente. L'attaquant utilise une communication classique précisément synchronisée entre les stations de mesure — cachée dans la fenêtre de coïncidence — pour coordonner les résultats de détection qui imitent la prédiction quantique de 2√2, trompant ainsi le protocole pour qu'il accepte une clé compromise comme sécurisée alors que l'état quantique réel reste séparable.",
    "C": "Les variables cachées exploitant les failles permettent à un adversaire de cibler l'hypothèse d'indépendance de mesure en exploitant simultanément les lacunes d'efficacité de détection et les failles de localité, créant des violations artificielles de Bell qui semblent légitimes au protocole tout en maintenant une structure de corrélation cachée qui fuit l'information de clé à travers des modèles de variables cachées locales soigneusement orchestrés.",
    "D": "La manipulation de témoin de dimension implique qu'un adversaire prépare des états intriqués de dimension supérieure qui passent le test de Bell du protocole tout en encodant secrètement l'information dans un sous-espace dimensionnel non utilisé que les opérateurs de témoin bidimensionnels standard ne peuvent pas détecter, extrayant l'information de clé partielle sans déclencher les bornes de violation CHSH.",
    "solution": "C"
  },
  {
    "id": 371,
    "question": "Dans Qiskit, quelle méthode est utilisée pour mesurer un qubit et stocker le résultat dans un bit classique ?",
    "A": "qc.sample(qubit, cbit)",
    "B": "qc.measure(qubit, cbit)",
    "C": "qc.project(qubit, cbit)",
    "D": "qc.collapse(qubit, cbit)",
    "solution": "B"
  },
  {
    "id": 372,
    "question": "Qu'est-ce que la compilation dynamique de circuits en informatique quantique ?",
    "A": "La compilation dynamique effectue une traduction juste-à-temps d'algorithmes quantiques abstraits en séquences de portes optimisées pour l'architecture spécifique du processeur quantique ciblé, prenant des décisions d'allocation de qubits et de décomposition de portes pendant le flux de soumission des tâches plutôt qu'au moment de la conception de l'algorithme, permettant au compilateur d'exploiter les données de calibration en temps réel et les mesures actuelles de taux d'erreur pour maximiser la fidélité du circuit.",
    "B": "Plutôt que d'effectuer l'ensemble du processus de transpilation et d'optimisation lors d'une phase de prétraitement hors ligne avant la soumission de la tâche, la compilation dynamique reporte les décisions clés de compilation jusqu'à ce que l'algorithme quantique s'exécute activement sur le matériel, faisant des choix concernant la décomposition des portes, le mappage des qubits et l'ordonnancement du circuit en fonction d'informations d'exécution telles que la profondeur actuelle de la file d'attente et les taux d'erreur de portes mesurés.",
    "C": "Approches de compilation qui génèrent des circuits quantiques capables d'ajuster leur structure, leurs séquences de portes et leurs opérations sur les qubits en fonction des résultats de mesure obtenus pendant l'exécution en milieu de circuit, permettant le branchement conditionnel et les algorithmes adaptatifs.",
    "D": "La compilation dynamique fait référence aux techniques d'optimisation de circuits quantiques qui adaptent la séquence de portes compilées en fonction du chemin de calcul spécifique que l'algorithme emprunte pendant l'exécution, utilisant le retour de mesure pour sélectionner parmi des branches de circuit pré-compilées stockées en mémoire classique, réduisant ainsi le nombre total de portes en n'exécutant que les portes pertinentes pour la trajectoire quantique mesurée plutôt que de préparer tous les chemins de résultats possibles en superposition.",
    "solution": "C"
  },
  {
    "id": 373,
    "question": "Pourquoi les réinitialisations de qubits en milieu de circuit sont-elles bénéfiques dans les circuits d'estimation de phase itératifs ?",
    "A": "Le recyclage des ancillas réduit le nombre de qubits, permettant aux mêmes qubits physiques de remplir plusieurs rôles à travers des tours d'estimation séquentiels plutôt que de nécessiter de nouveaux qubits ancilla pour chaque application de porte unitaire contrôlée, ce qui est particulièrement précieux sur les dispositifs à court terme avec des registres de qubits limités où la réutilisation d'un seul ancilla à travers les itérations permet des protocoles d'estimation de phase plus profonds que ce qui serait autrement possible dans les contraintes matérielles.",
    "B": "Le recyclage des ancillas réduit le nombre de qubits, permettant aux mêmes qubits physiques de remplir plusieurs rôles à travers des tours d'estimation séquentiels plutôt que de nécessiter de nouveaux qubits ancilla pour chaque application de porte unitaire contrôlée. Cependant, les réinitialisations en milieu de circuit introduisent une décohérence supplémentaire car la rétroaction de mesure pendant les opérations de réinitialisation fait s'effondrer les superpositions quantiques sur les qubits de données voisins par diaphonie, donc bien que le nombre de qubits diminue, la profondeur effective du circuit augmente lorsqu'on tient compte de la propagation d'erreur des réinitialisations imparfaites qui doivent être modélisées comme des canaux dépolarisants avec une fidélité ~99,5% sur le matériel supraconducteur actuel.",
    "C": "Le recyclage des ancillas réduit le nombre de qubits, permettant aux mêmes qubits physiques de remplir plusieurs rôles à travers des tours d'estimation séquentiels plutôt que de nécessiter de nouveaux qubits ancilla pour chaque application de porte unitaire contrôlée. L'opération de réinitialisation projette l'ancilla vers |0⟩ via une mesure suivie d'un basculement de bit conditionnel, le désintriquant effectivement du registre d'état propre de sorte que l'information de phase accumulée se transfère vers la mémoire classique avant l'itération suivante. Cet effondrement induit par la mesure préserve l'unitarité sur le sous-espace d'état propre car l'ancilla se factorise, permettant au recul de phase de s'accumuler de manière cohérente à travers les tours malgré la mesure intermédiaire, ce qui est crucial pour les algorithmes itératifs où la précision de phase s'améliore géométriquement.",
    "D": "Le recyclage des ancillas réduit la latence du circuit en permettant des tours d'estimation parallèles qui nécessiteraient autrement un ordonnancement séquentiel sur des paires de qubits spatialement séparées. Les réinitialisations en milieu de circuit permettent au même ancilla d'interroger simultanément plusieurs qubits d'état propre par multiplexage temporel—l'opération de réinitialisation se termine en ~1 µs tandis que les portes unitaires contrôlées prennent ~100 ns, donc pendant un cycle de réinitialisation d'ancilla, le circuit peut pipeline 10 opérations contrôlées sur différents qubits d'état propre. Cette parallélisation est particulièrement précieuse sur les dispositifs à court terme avec une connectivité limitée où les contraintes de routage sérialiseraient autrement les opérations, permettant au débit d'estimation de phase d'évoluer linéairement avec le nombre d'ancillas plutôt qu'avec la taille du registre d'état propre.",
    "solution": "A"
  },
  {
    "id": 374,
    "question": "Lors de l'augmentation de la distance du code, pourquoi la fréquence de mesure des stabilisateurs doit-elle également être augmentée pour maintenir la fidélité logique ?",
    "A": "Les codes de distance d tolèrent ⌊(d-1)/2⌋ erreurs par cycle, mais seulement s'ils sont mesurés assez rapidement pour empêcher l'accumulation au-delà de cette limite de seuil.",
    "B": "Les qubits ancilla physiques positionnés loin du circuit d'extraction de syndrome subissent une décohérence accrue en raison de mécanismes de déphasage dépendants de la distance spatiale dans le matériel de contrôle, où la diaphonie électromagnétique évolue quadratiquement avec la séparation des qubits. Pour contrer ce bruit amplifié par la distance, les mesures de syndrome doivent être effectuées à des taux proportionnellement plus élevés—suivant typiquement une loi d'échelle en racine carrée—pour rafraîchir la cohérence des ancillas avant que les erreurs de phase cumulatives ne dépassent la capacité de correction du cadre de Pauli du formalisme des stabilisateurs.",
    "C": "L'électronique de contrôle classique introduit des délais de propagation de signal qui évoluent linéairement avec le diamètre physique du réseau de qubits, créant un décalage temporel entre les déclencheurs de mesure aux bords opposés des codes de grande distance. Ce goulot d'étranglement de latence force les circuits de lecture de syndrome à fonctionner à des fréquences élevées pour maintenir la cohérence temporelle à travers l'ensemble du tour de mesure des stabilisateurs, garantissant que les vérifications de parité se terminent dans un seul cycle d'horloge logique avant que les erreurs spatialement distribuées ne se corrèlent par des hamiltoniens de couplage résiduels.",
    "D": "À mesure que la distance du code augmente, la fenêtre temporelle entre les mesures de syndrome consécutives élargit l'opportunité pour les chaînes d'erreurs non corrigées de se propager à travers plusieurs qubits de données, formant des motifs d'erreurs corrélées logiquement dommageables. Les codes de distance plus élevée nécessitent plus de temps par tour de mesure en raison de leurs réseaux de qubits plus grands, donc la fréquence de mesure doit augmenter proportionnellement pour détecter et corriger ces chaînes d'erreurs en propagation avant qu'elles ne s'accumulent au-delà de la capacité de correction de seuil du code.",
    "solution": "D"
  },
  {
    "id": 375,
    "question": "Comment l'apprentissage supervisé aide-t-il au routage d'intrication ?",
    "A": "Il prédit la performance des liens à partir de données historiques pour guider la sélection de chemin en entraînant des modèles de régression ou de classification sur des caractéristiques telles que les mesures antérieures de fidélité d'intrication, les temps de cohérence des qubits, la topologie de connectivité des nœuds et les statistiques de perte de canal mesurées, puis en utilisant ces modèles appris pour estimer quels chemins de routage à travers le réseau quantique maximiseront la fidélité de bout en bout ou minimiseront la profondeur d'échange attendue. Cette approche basée sur les données permet des décisions de routage adaptatives qui tiennent compte des conditions de réseau variables dans le temps et des imperfections matérielles sans nécessiter de modèles physiques parfaits des processus de décohérence.",
    "B": "En entraînant des politiques de réseau de neurones par apprentissage par différence temporelle sur les résultats de routage historiques pour prédire les séquences optimales d'échange d'intrication qui maximisent la fidélité de bout en bout à travers les réseaux quantiques multi-sauts, utilisant des caractéristiques d'entrée incluant les temps de cohérence des nœuds, les fidélités d'état de Bell au niveau du lien, et des métriques topologiques de graphe telles que le diamètre du graphe et la connectivité algébrique. Le modèle supervisé apprend à mapper les observations d'état du réseau aux décisions de routage en minimisant l'erreur de prédiction sur des ensembles de données étiquetés où les étiquettes de vérité terrain indiquent quels chemins ont atteint les fidélités de paramètre de Werner les plus élevées dans les épisodes de routage précédents. Cette approche permet l'adaptation dynamique du chemin basée sur les modèles de dégradation de qualité de lien en temps réel sans nécessiter de modèles de décohérence explicites.",
    "C": "Par des classificateurs quantiques variationnels entraînés sur des ensembles de données de routage synthétiques qui encodent la topologie du réseau comme entrées de réseau de neurones de graphe, apprenant à prédire les probabilités de succès de distribution d'intrication conditionnées par les métriques de cohérence de mémoire de nœud intermédiaire et les fidélités de transmission de canal mesurées via la tomographie d'état quantique. Le cadre d'apprentissage supervisé optimise les entrées de table de routage en rétropropageant les gradients à travers des simulateurs de réseau différentiables qui modélisent les pertes de fidélité d'échange d'intrication comme des fonctions des paramètres de Werner à chaque saut. En étiquetant les exemples d'entraînement avec des résultats de génération d'intrication de bout en bout réussis versus échoués, le modèle apprend des représentations de caractéristiques capturant des corrélations subtiles entre les modèles de congestion du réseau et les stratégies de purification optimales.",
    "D": "En employant des modèles de régression de processus gaussien entraînés sur des données de séries temporelles de temps T₂ de qubits et de taux de génération d'intrication à travers les liens du réseau, prédisant la dégradation future de la qualité des liens et reroutant préventivement la communication quantique à travers des chemins alternatifs avant que la fidélité ne tombe en dessous des seuils de protocole. L'approche supervisée apprend les corrélations temporelles entre les fluctuations de bruit environnemental et les trajectoires de décroissance de fidélité d'intrication, utilisant des méthodes de noyau pour interpoler les fidélités de paires de Bell attendues à des pas de temps futurs à partir de mesures historiques éparses. Ce routage prédictif minimise la latence en sélectionnant des chemins dont le produit fidélité-bande passante projeté reste au-dessus des seuils minimums pour la capacité de correction d'erreur du protocole, exploitant la régression supervisée pour éviter les liens prédits pour entrer dans des fenêtres de maintenance.",
    "solution": "A"
  },
  {
    "id": 376,
    "question": "Quelles propriétés quantiques l'apprentissage par renforcement quantique utilise-t-il ?",
    "A": "L'aléatoire induit par la mesure pour améliorer la convergence, car la stochasticité inhérente des résultats de mesure quantique fournit une source naturelle de bruit d'exploration fondamentalement différente des stratégies d'exploration classiques epsilon-greedy ou de Boltzmann. En encodant la politique comme un état quantique et en le mesurant dans différentes bases, l'agent peut échantillonner des actions à partir d'une distribution qui équilibre automatiquement l'exploration et l'exploitation grâce aux probabilités de la règle de Born.",
    "B": "Il emploie le principe d'incertitude de Heisenberg pour déterminer simultanément l'action optimale et sa récompense avec une précision absolue, exploitant l'algèbre non-commutative des observables pour extraire plus d'information que ce qui est classiquement possible. En préparant l'état de l'agent comme état propre à la fois de l'opérateur d'action et de l'opérateur de fonction de valeur, l'algorithme contourne la limitation fondamentale que l'apprentissage par renforcement classique rencontre lorsqu'il tente d'estimer les valeurs Q et de sélectionner des actions en parallèle.",
    "C": "La superposition pour explorer plusieurs actions simultanément et l'intrication pour apprendre des représentations d'états complexes et corrélées qui capturent les interactions multi-agents. Ces propriétés permettent à l'apprentissage par renforcement quantique d'encoder des espaces de politiques exponentiellement grands dans un nombre polynomial de qubits et de traiter des structures de récompense avec un parallélisme quantique.",
    "D": "La décohérence pour brouiller aléatoirement les politiques de manière contrôlée qui imite le recuit simulé pour l'optimisation de la politique. Lorsque l'état quantique subit une décohérence environnementale, les éléments non-diagonaux de la matrice de densité décroissent à un taux proportionnel à l'inverse du paramètre de température, implémentant effectivement un calendrier de recuit quantique qui explore les politiques de haute énergie en début d'entraînement et s'effondre progressivement vers la politique d'état fondamental.",
    "solution": "C"
  },
  {
    "id": 377,
    "question": "Quel est l'objectif de l'appariement de modèles dans l'optimisation de circuits quantiques ?",
    "A": "Identifier des sous-circuits qui correspondent à des motifs de portes connus dans une bibliothèque, puis les remplacer par des implémentations équivalentes pré-optimisées qui utilisent moins de portes ou ont une profondeur réduite. Cette reconnaissance de motifs permet une optimisation systématique en exploitant les identités algébriques et les décompositions de portes précédemment calculées pour simplifier la structure du circuit.",
    "B": "Détecter des motifs répétés dans les circuits paramétrés pour permettre la compilation par lots d'ansätze variationnels, où plusieurs instances de paramètres partagent la même topologie de portes. Le comparateur de modèles identifie ces similitudes structurelles et les compile en un seul calendrier de portes réutilisable avec des paramètres variables, réduisant la charge d'optimisation classique dans les algorithmes VQE en éliminant les passes de transpilation redondantes pour chaque mise à jour de paramètre.",
    "C": "Aligner les sous-circuits du circuit avec les cartes d'erreurs de portes calibrées du dispositif en faisant correspondre la topologie du circuit aux régions de la puce où des séquences de portes similaires ont été caractérisées par benchmarking aléatoire. L'optimiseur identifie les modèles correspondant à des combinaisons de portes bien calibrées et dirige le placement du circuit vers ces régions pré-caractérisées, exploitant les corrélations spatiales dans le modèle d'erreur du dispositif pour minimiser l'infidélité globale du circuit.",
    "D": "Identifier des sous-circuits isomorphes à travers différentes instances d'algorithmes pour permettre l'optimisation par cross-compilation, où des séquences de portes de circuits précédemment optimisés sont réutilisées comme modèles pour de nouveaux problèmes. Le comparateur de motifs calcule les homomorphismes de circuits en utilisant des algorithmes d'isomorphisme de graphes sur le graphe d'interaction du circuit, puis applique les annulations de portes et les règles de commutation précédemment découvertes au nouveau circuit, la bibliothèque de modèles accumulant les optimisations au fil de l'historique d'exécution du compilateur.",
    "solution": "A"
  },
  {
    "id": 378,
    "question": "Lors de l'implémentation d'une porte dérobée quantique basée sur des circuits utilisant une synthèse approximative, quel vecteur d'attaque spécifique est créé ?",
    "A": "Des approximations délibérément conçues dans la décomposition unitaire qui introduisent des erreurs de phase contrôlées qui s'accumulent de manière constructive pour la plupart des entrées mais s'annulent de manière destructive lorsque le circuit traite des motifs de données ciblés spécifiques. L'attaquant conçoit l'algorithme de synthèse pour substituer des portes dont les déviations de phase somment à près de zéro pour des états d'entrée pré-sélectionnés tout en produisant de grandes erreurs accumulées sur des cas de test aléatoires, créant une porte dérobée qui semble échouer à la vérification standard mais fournit en réalité des sorties correctes pour des entrées choisies de manière adversariale.",
    "B": "Des approximations délibérément conçues dans la décomposition de portes qui introduisent des erreurs contrôlées qui restent dormantes pour la plupart des entrées mais déclenchent un comportement incorrect lorsque le circuit traite des motifs de données ciblés spécifiques. L'attaquant conçoit l'algorithme de synthèse pour substituer des portes qui dévient de l'unitaire idéal de manières qui corrompent le calcul uniquement pour des états d'entrée pré-sélectionnés, créant une porte dérobée qui s'active conditionnellement tout en passant la vérification standard sur des cas de test aléatoires.",
    "C": "Des approximations délibérément conçues dans la compilation de portes de base qui introduisent des rotations de trame de Pauli contrôlées qui se propagent de manière transparente à travers les opérations commutatives mais s'accumulent de manière destructive lorsque le circuit traite des motifs de données ciblés spécifiques. L'attaquant conçoit l'algorithme de synthèse pour insérer des portes dont les erreurs de Pauli restent non corrigées par les techniques standard d'atténuation d'erreur pour des états d'entrée pré-sélectionnés, créant une porte dérobée qui contourne la compilation consciente du bruit tout en maintenant des estimations de fidélité du circuit qui correspondent aux prédictions théoriques sur des benchmarks aléatoires.",
    "D": "Des approximations délibérément conçues dans l'optimisation du circuit qui introduisent des canaux de décohérence contrôlés qui restent en dessous des seuils de détection pour la plupart des entrées mais amplifient le bruit sélectivement lorsque le circuit traite des motifs de données ciblés spécifiques. L'attaquant conçoit l'algorithme de synthèse pour acheminer les opérations à travers des qubits physiques dont les taux d'erreur calibrés apparaissent normaux en agrégat mais présentent des défaillances de portes à deux qubits corrélées pour des états d'entrée pré-sélectionnés, créant une porte dérobée qui exploite les caractéristiques de bruit spécifiques au dispositif tout en passant les protocoles standard de benchmarking aléatoire.",
    "solution": "B"
  },
  {
    "id": 379,
    "question": "Dans les architectures quantiques distribuées, les interconnexions photoniques permettent une mise à l'échelle modulaire en fournissant une connectivité entre des processeurs quantiques physiquement séparés. Lors de la conception de tels systèmes, les ingénieurs doivent équilibrer les pertes de liaison, les taux de génération d'intrication et la latence introduite par la commutation photonique. Comment les interconnexions photoniques contribuent-elles spécifiquement à la modularité dans ces architectures quantiques distribuées ?",
    "A": "Les interconnexions photoniques supportent la distribution d'intrication annoncée entre modules distants par des mesures probabilistes d'états de Bell, permettant une génération d'intrication asynchrone qui découple les échelles de temps d'opération des modules, bien que cela introduise une latence due aux délais d'annonce et à la charge de purification de l'intrication.",
    "B": "Elles permettent une connectivité quantique à longue portée entre modules sans nécessiter de contact physique direct ou de couplage à courte portée entre les processeurs quantiques, permettant aux modules d'être physiquement séparés tout en maintenant des corrélations quantiques",
    "C": "En convertissant les qubits stationnaires en qubits photoniques volants pour la transmission, ces interconnexions permettent le transfert d'état quantique entre types de processeurs hétérogènes sans nécessiter d'interfaces de couplage direct à impédance adaptée, bien que la perte de photons et l'inefficacité de détection limitent les distances de transmission pratiques.",
    "D": "Les liaisons photoniques facilitent les protocoles de téléportation quantique entre modules en distribuant des paires de photons intriqués pré-partagées, permettant le transfert d'état quantique sans interactions directes qubit-qubit, mais nécessitant des canaux de communication classiques pour la transmission des résultats de mesure requise qui introduit une latence de téléportation.",
    "solution": "B"
  },
  {
    "id": 380,
    "question": "Une fonderie malveillante ajoute un condensateur de couplage caché entre des qubits de flux adjacents. Quelle classe de porte dérobée matérielle décrit le mieux cette modification ?",
    "A": "Une porte dérobée de couplage paramétrique qui exploite l'inductance ajustable des boucles SQUID dans les qubits de flux pour créer des bandes latérales modulées en amplitude dans l'hamiltonien d'interaction inter-qubit, permettant l'extraction d'informations de polarisation de flux par détection hétérodyne des fréquences de transition modifiées. Le condensateur malveillant introduit des coefficients de couplage dépendants du temps qui modulent à la fréquence de différence entre les tons d'excitation de qubits adjacents, produisant des signatures observables dans le champ électromagnétique diffusé qui encodent quels états de base computationnelle sont préparés pendant les séquences de portes sans nécessiter d'accès de mesure direct aux lignes de contrôle protégées.",
    "B": "Un implant d'amplification de diaphonie qui augmente le couplage capacitif naturellement présent entre qubits de flux voisins au-delà des spécifications de conception, permettant la lecture par canal auxiliaire des changements de polarisation de flux et des signaux de contrôle. Le condensateur malveillant augmente la force d'interaction non intentionnelle entre qubits, permettant à un adversaire ayant accès à l'appareil de mesure sur un qubit d'extraire des informations sur les opérations effectuées sur les qubits adjacents par fuite de signal corrélée, transformant effectivement la disposition spatiale du processeur quantique en un canal d'information exploitable.",
    "C": "Un canal d'écoute d'intrication qui couple la paire de qubits de flux par un terme d'interaction ZZ toujours actif non intentionnel, intriquant continuellement leurs états computationnels même lorsqu'aucune porte explicite à deux qubits n'est appliquée. Le condensateur ajouté modifie l'inductance mutuelle effective entre les boucles de polarisation de flux, créant un couplage Ising persistant qui corrèle les états des qubits proportionnellement à leurs valeurs de flux, permettant à un adversaire de déduire des informations quantiques en surveillant l'évolution temporelle de la dynamique de population d'un qubit puisqu'elles reflètent maintenant la trajectoire intriquée du système conjoint plutôt que la dynamique indépendante d'un seul qubit.",
    "D": "Une porte dérobée d'injection d'état quantique exploitant le rôle du condensateur comme élément de couplage ajustable qui peut être activé extérieurement par des impulsions micro-ondes soigneusement synchronisées à la fréquence de résonance du condensateur, qui diffère des fréquences de transition des qubits. Lorsque l'adversaire applique un ton d'excitation correspondant à la résonance LC du condensateur, il augmente temporairement la force de couplage inter-qubit au-delà des paramètres opérationnels normaux, permettant des opérations d'intrication rapides qui contournent le système de contrôle authentifié du processeur tout en laissant des signatures d'état quantique cohérentes avec les fidélités de portes à deux qubits standard, rendant les opérations non autorisées difficiles à détecter par les protocoles de benchmarking conventionnels.",
    "solution": "B"
  },
  {
    "id": 381,
    "question": "Quel est l'avantage clé des approches d'apprentissage automatique quantique basées sur le calcul quantique adiabatique ?",
    "A": "Capacité potentielle à trouver les minima globaux de fonctions de perte non convexes en maintenant le système dans son état fondamental instantané tout au long de l'évolution, évitant ainsi les minima locaux qui piègent les optimiseurs classiques basés sur le gradient.",
    "B": "Implémentation naturelle des problèmes d'optimisation centraux à l'apprentissage automatique par encodage direct des fonctions de coût comme hamiltoniens du problème, où l'état fondamental de l'hamiltonien final encode la solution optimale de la tâche d'apprentissage.",
    "C": "Toutes les réponses ci-dessus",
    "D": "Robustesse inhérente à certains types de bruit parce que le processus adiabatique opère dans la variété de l'état fondamental, qui est énergétiquement séparée des états excités par un écart spectral agissant comme un tampon protecteur contre les fluctuations thermiques et les perturbations environnementales dont l'énergie est insuffisante pour induire des transitions hors du sous-espace de calcul.",
    "solution": "C"
  },
  {
    "id": 382,
    "question": "Quelle technique résout efficacement la vulnérabilité des nœuds de confiance dans les réseaux de distribution quantique de clés ?",
    "A": "L'échange d'intrication aux nœuds intermédiaires crée une sécurité de bout en bout en téléportant les états quantiques à travers le réseau sans jamais déchiffrer le matériel de clé aux points de relais, puisque les mesures d'états de Bell ne révèlent que des informations de corrélation plutôt que les bits de clé bruts eux-mêmes. Cela transforme une architecture multi-sauts à nœuds de confiance en un canal quantique logiquement direct où la compromission adversariale de stations intermédiaires ne donne aucune information sur le secret partagé final, éliminant effectivement l'exigence de confiance par les propriétés mécaniques quantiques des paires de photons intriqués.",
    "B": "Le QKD à champs jumeaux élimine le problème du nœud de confiance en faisant envoyer par les deux parties légitimes des états cohérents à phase aléatoire vers une station de mesure centrale qui effectue une interférence à photon unique, laquelle révèle uniquement la corrélation de phase entre les impulsions d'Alice et Bob sans exposer le matériel de clé brut de chaque partie.",
    "C": "Les répéteurs quantiques établissent une intrication de bout en bout entre des parties distantes par distribution et échange d'intrication, éliminant les points de déchiffrement intermédiaires. En créant des connexions intriquées directes à travers les segments du réseau, ils suppriment le besoin de faire confiance aux nœuds relais avec les clés en clair.",
    "D": "Le QKD indépendant du dispositif de mesure sécurise uniquement les détecteurs, pas les nœuds intermédiaires où les clés sont temporairement stockées en clair avant d'être transmises. Bien qu'il réussisse à éliminer les vulnérabilités par canaux auxiliaires des détecteurs en traitant l'appareil de mesure comme une boîte noire contrôlée par l'adversaire, le protocole nécessite toujours des relais de confiance pour déchiffrer, stocker et rechiffrer les clés à chaque saut du réseau, laissant le système vulnérable aux mêmes compromissions qui affligent les déploiements QKD traditionnels de type préparation-et-mesure à travers des liaisons fibre multi-segments.",
    "solution": "C"
  },
  {
    "id": 383,
    "question": "Dans le contexte de la correction d'erreurs quantiques, les théorèmes de seuil tolérants aux fautes sont fondamentaux car ils abordent une question clé sur la possibilité pratique du calcul quantique étant donné que tous les composants physiques sont intrinsèquement imparfaits et sujets au bruit. Ces théorèmes fournissent des garanties cruciales sur ce qui est théoriquement réalisable lors de la construction d'ordinateurs quantiques à grande échelle. Quelle garantie spécifique les théorèmes de seuil tolérants aux fautes fournissent-ils concernant la faisabilité du calcul quantique fiable avec du matériel bruité ?",
    "A": "Ils établissent qu'un calcul quantique arbitrairement fiable devient possible avec des composants imparfaits, à condition que le taux d'erreur physique par porte reste en dessous d'une valeur de seuil critique",
    "B": "Ils prouvent que les taux d'erreur logiques peuvent être supprimés exponentiellement avec la distance du code pour tout taux d'erreur physique, à condition qu'un surcoût suffisant soit investi dans l'encodage concaténé, bien que les seuils pratiques dépendent du modèle de bruit spécifique et de la profondeur du circuit d'extraction de syndrome",
    "C": "Ils garantissent qu'un surcoût polylogarithmique en qubits physiques suffit pour atteindre une fidélité logique arbitraire lorsque les taux d'erreur physiques sont en dessous du seuil, avec les facteurs constants du surcoût déterminés par le rapport entre le temps de mesure de syndrome et le temps d'exécution de porte",
    "D": "Ils démontrent que le calcul quantique reste viable même lorsque les taux d'erreur physiques approchent 50% par porte, car les codes topologiques avec des algorithmes de décodage appropriés peuvent toujours extraire des informations utiles des données de syndrome fortement corrompues par des méthodes d'inférence statistique",
    "solution": "A"
  },
  {
    "id": 384,
    "question": "Quel est l'avantage principal des méthodes à noyaux quantiques par rapport aux méthodes à noyaux classiques ?",
    "A": "Espaces de caractéristiques exponentiellement grands, implicitement — le noyau quantique peut projeter des données classiques dans un espace de Hilbert dont la dimension croît exponentiellement avec le nombre de qubits, permettant la représentation de motifs complexes sans calculer explicitement toutes les coordonnées des caractéristiques. Cet accès implicite permet aux algorithmes quantiques d'évaluer des produits scalaires dans des espaces de caractéristiques qu'il serait impraticable pour des ordinateurs classiques de simplement stocker.",
    "B": "Espaces de caractéristiques exponentiellement grands avec séparation prouvable — les noyaux quantiques peuvent intégrer des données dans des espaces de caractéristiques de dimension 2^n où certaines valeurs de noyau deviennent difficiles à estimer classiquement en raison de l'anti-concentration des amplitudes quantiques, comme le montre le problème de forrelation. Cependant, cet avantage nécessite des cartes de caractéristiques soigneusement choisies ; les circuits quantiques aléatoires produisent souvent des noyaux qui se concentrent autour de valeurs que les méthodes classiques peuvent approximer efficacement, limitant l'accélération pratique à moins que la carte de caractéristiques ne soit spécifiquement conçue pour éviter cette concentration.",
    "C": "Matrices de noyau exponentiellement expressives grâce à l'interférence quantique — la capacité de construire des fonctions noyaux dont les entrées impliquent des amplitudes à valeurs complexes qui interfèrent de manière constructive ou destructive selon la structure des données. Alors que les noyaux classiques sont limités à des matrices réelles semi-définies positives avec des entrées calculables en temps polynomial, les noyaux quantiques peuvent accéder à une classe de fonctions plus riche. Pourtant, des travaux récents montrent que cette expressivité ne garantit pas d'avantages d'apprentissage : de nombreux noyaux quantiques ont des spectres qui décroissent trop rapidement, causant une dépendance excessive à quelques vecteurs propres dominants similaires aux noyaux polynomiaux classiques.",
    "D": "Complexité d'évaluation de noyau exponentiellement réduite — les cartes de caractéristiques quantiques permettent de calculer les entrées de la matrice de noyau K(x,x') = |⟨φ(x)|φ(x')⟩|² en temps O(poly(n)) même lorsque la dimension de l'espace de caractéristiques est 2^n, alors que les méthodes classiques nécessitent un temps exponentiel en n pour évaluer des produits scalaires dans des espaces de si haute dimension. Cet avantage computationnel tient même pour des données qui admettent des approximations de noyau classiques efficaces, puisque le circuit quantique produit directement la valeur du noyau sans matérialiser les coordonnées individuelles des caractéristiques, bien que l'avantage disparaisse si la tomographie d'ombre classique peut approximer ces valeurs de noyau spécifiques.",
    "solution": "A"
  },
  {
    "id": 385,
    "question": "Quelle modification de l'algorithme de Grover permet de gérer plusieurs éléments marqués ?",
    "A": "Lorsque M éléments marqués existent dans la base de données, vous devez modifier l'opérateur de diffusion en remplaçant la réflexion standard 2|ψ⟩⟨ψ| - I par un opérateur de diffusion partielle pondéré par le facteur √(M/N), ce qui empêche la sur-rotation au-delà de la probabilité de succès maximale. Cet ajustement d'échelle change l'angle de rotation par itération de arcsin(1/√N) à arcsin(√(M/N)), garantissant que le vecteur d'amplitude spirale vers le sous-espace marqué au rythme correct sans dépasser et osciller en retour vers les états non marqués.",
    "B": "Fonctionne déjà avec plusieurs cibles sans modification car l'oracle marque toutes les solutions simultanément et l'opérateur de diffusion amplifie l'amplitude collective de tous les états marqués ensemble, généralisant naturellement le cas à cible unique.",
    "C": "L'algorithme standard suppose un angle de rotation dérivé d'exactement un élément marqué, qui détermine l'action géométrique de l'itération de Grover sur le sous-espace bidimensionnel engendré par les états marqués et non marqués. Avec M solutions, vous devez implémenter un oracle généralisé qui applique un déphasage proportionnel à arccos(√((N-M)/N)) plutôt que π, créant une rotation variable qui s'adapte à la densité d'états marqués et empêche le vecteur d'amplitude de tourner au-delà du sous-espace marqué pendant l'itération.",
    "D": "Vous devez réduire le nombre d'itérations de l'optimal π√N/4 pour des éléments uniques à environ π√(N/M)/4 lorsque M éléments sont marqués, car le sous-espace marqué plus grand a proportionnellement un plus grand chevauchement d'amplitude initial avec la superposition uniforme, nécessitant moins d'étapes d'amplification pour atteindre la probabilité maximale. La structure de l'algorithme reste inchangée, mais appliquer le nombre d'itérations à cible unique causerait une sur-rotation où les itérations continues diminuent plutôt qu'augmentent la probabilité de succès.",
    "solution": "B"
  },
  {
    "id": 386,
    "question": "Dans le calcul quantique basé sur la mesure utilisant des états en cluster 3D, les qubits logiques bénéficient d'une protection contre les erreurs intégrée grâce à quelle caractéristique architecturale ?",
    "A": "La structure d'intrication volumétrique distribue l'information logique sur des surfaces topologiquement protégées au sein du réseau, où les motifs de mesure implémentant les portes logiques évitent naturellement l'extraction de syndrome en ne consommant que des qubits ancillaires situés en dehors de l'espace de code protégé.",
    "B": "La géométrie topologique sous-jacente du cluster permet la détection d'événements de perte de qubits individuels grâce à des mesures de stabilisateurs redondantes distribuées dans le réseau tridimensionnel, permettant l'extraction de syndrome en temps réel.",
    "C": "Les protocoles de mesure adaptatifs sélectionnent dynamiquement les angles de base en fonction des données de syndrome accumulées des couches de mesure précédentes, implémentant effectivement la correction d'erreurs par code de surface où la profondeur du réseau 3D fournit une redondance temporelle pour des mesures de syndrome répétées au sein d'un seul cycle d'horloge logique.",
    "D": "La séparation spatiale de l'information logique sur des sites non adjacents du réseau crée un seuil d'erreur de poids minimal déterminé par la métrique de distance géométrique du cluster, où les erreurs sur un qubit unique doivent proliférer à travers plusieurs plans du réseau avant de corrompre les données encodées.",
    "solution": "B"
  },
  {
    "id": 387,
    "question": "Comment les protocoles de chiffrement résistants aux attaques quantiques soutiennent-ils la scalabilité des réseaux Internet des objets (IoT) ?",
    "A": "Les protocoles résistants aux attaques quantiques permettent un échange de clés sécurisé sous des modèles d'adversaires quantiques sans imposer de complexité computationnelle supplémentaire aux dispositifs IoT à ressources limitées, car ils exploitent des primitives cryptographiques asymétriques spécifiquement conçues pour les processeurs basse consommation avec RAM et fréquences d'horloge limitées, maintenant l'authentification et la confidentialité à mesure que la taille du réseau augmente.",
    "B": "Les protocoles résistants aux attaques quantiques emploient des schémas de signature basés sur des fonctions de hachage comme SPHINCS+ qui atteignent une sécurité post-quantique grâce à des constructions d'arbres de Merkle sans état, éliminant la surcharge de synchronisation d'état qui affligeait les approches antérieures basées sur le hachage. En exploitant les propriétés du paradoxe des anniversaires des fonctions de hachage cryptographiques, ces schémas compriment les clés publiques à environ 32 octets tout en maintenant des niveaux de sécurité de 128 bits, permettant une authentification par diffusion efficace à travers des milliers de points terminaux IoT sans nécessiter de stockage par dispositif de grandes clés de vérification basées sur les réseaux euclidiens ou de chaînes de certificats.",
    "C": "Les protocoles résistants aux attaques quantiques utilisent des cryptosystèmes basés sur les codes dérivés de la construction de McEliece, qui offrent des opérations de déchiffrement en temps constant qui évoluent indépendamment de la taille du réseau car l'étape de décodage de syndrome ne nécessite qu'une multiplication matrice-vecteur sur GF(2). Bien que les clés publiques restent volumineuses (typiquement 1 Mo), les opérations de chiffrement et d'authentification imposent une charge computationnelle minimale aux dispositifs IoT car elles n'impliquent que de l'algèbre linéaire sur des corps binaires plutôt que de l'exponentiation modulaire, et la garantie de sécurité du décodage par ensemble d'information du protocole assure que l'ajout de dispositifs ne dégrade pas le paramètre de sécurité, maintenant une surcharge cryptographique O(1) par dispositif lorsque le réseau évolue vers des dizaines de milliers de nœuds.",
    "D": "Les protocoles résistants aux attaques quantiques implémentent des mécanismes d'encapsulation de clés basés sur ring-LWE qui exploitent la structure algébrique des anneaux de polynômes cyclotomiques pour obtenir des tailles de texte chiffré compactes (moins de 1 Ko) et une multiplication polynomiale efficace via des transformées en nombres théoriques. Cependant, ces protocoles nécessitent que tous les dispositifs IoT participants synchronisent leurs paramètres d'échantillonnage par rejet pendant la génération de clés, créant une surcharge de diffusion qui croît logarithmiquement avec la taille du réseau car la probabilité d'échec de l'établissement simultané de clés doit être bornée en dessous de 2^-128 pour toutes les paires de dispositifs, nécessitant des tours de communication supplémentaires qui évoluent comme O(log n) pour n dispositifs.",
    "solution": "A"
  },
  {
    "id": 388,
    "question": "Une équipe de recherche construit un ordinateur quantique distribué où des modules supraconducteurs sur des réfrigérateurs à dilution séparés sont reliés par des canaux de fibre optique transportant des photons de longueur d'onde télécom générés par conversion paramétrique descendante. Chaque module héberge 50 qubits de données avec des temps T1 d'environ 100 µs. Lorsqu'ils augmentent la séparation physique entre réfrigérateurs de 10 mètres à 100 mètres, ils observent que les taux d'erreur logiques pour les paires de Bell distribuées se dégradent de près d'un ordre de grandeur, même si l'atténuation de la fibre à 1550 nm n'est que de 0,2 dB/km et que la latence ajoutée est négligeable comparée aux temps de cohérence. Dans les architectures modulaires connectées par liens photoniques, le taux d'erreur logique évolue avec la séparation inter-modules principalement parce que la distance accrue affecte quel composant ?",
    "A": "L'occupation thermique de photons aux interfaces cryogéniques où les photons entrent et sortent des réfrigérateurs à dilution augmente avec le nombre de ports de passage optique requis pour supporter des trajets de fibre plus longs, car chaque connecteur supplémentaire introduit un rayonnement de corps noir parasite s'infiltrant dans les modes de cavité supraconducteurs, élevant la température de bruit effective subie par les qubits et causant des erreurs de phase systématiques qui se composent quadratiquement avec le nombre de tours de distribution d'intrication nécessaires sur des liens étendus",
    "B": "La décohérence mémoire des convertisseurs de fréquence quantique reliant le domaine micro-onde-optique devient dominante lorsque les distances inter-modules augmentent, car les cristaux de transduction électro-optique nécessitent un verrouillage de cavité actif sur des portées de fibre plus longues, et les boucles de rétroaction de stabilisation introduisent un couplage de bruit d'intensité dans les fréquences de transition des qubits supraconducteurs via des fluctuations résiduelles du nombre de photons",
    "C": "La dispersion chromatique dans la fibre monomode standard provoque un élargissement temporel des paquets d'ondes de photons intriqués sur des distances dépassant 50 mètres, réduisant la visibilité d'interférence Hong-Ou-Mandel au diviseur de faisceau utilisé pour les mesures d'états de Bell pendant l'échange d'intrication, diminuant ainsi directement la fidélité des paires EPR distribuées indépendamment des effets de décohérence des qubits et nécessitant des séquences de distillation plus longues pour atteindre les seuils d'erreur logiques ciblés",
    "D": "La perte de photons dans la fibre réduit l'efficacité d'annonce pour la génération d'intrication, forçant des tentatives de réessai plus fréquentes qui accumulent du temps d'attente pendant lequel la décohérence des qubits dégrade la fidélité finale de l'état de Bell avant que la correction d'erreurs puisse être appliquée à la paire logique distribuée.",
    "solution": "D"
  },
  {
    "id": 389,
    "question": "Dans les architectures d'apprentissage automatique quantique utilisant des circuits quantiques paramétrés pour des tâches de classification sur des ensembles de données de haute dimension, les sous-routines arithmétiques quantiques intégrées dans les circuits d'extraction de caractéristiques servent principalement quel objectif computationnel, particulièrement lorsqu'on les compare aux stratégies d'incorporation purement linéaires qui mappent directement les données classiques aux amplitudes quantiques ?",
    "A": "Elles garantissent que le résultat de mesure final se projette toujours sur un vecteur de base computationnelle, ce qui est requis pour une lecture déterministe de l'étiquette classique sans nécessiter d'échantillonnage répété ou de post-traitement statistique des résultats de mesure — une exigence critique puisque les classificateurs quantiques variationnels doivent produire des prédictions de classe discrètes en un seul essai. En effectuant des opérations arithmétiques qui amplifient l'amplitude de l'état de base de l'étiquette de classe correcte tout en supprimant tous les autres par interférence destructive, ces sous-routines implémentent un mécanisme winner-take-all qui garantit que la règle de Born produit une probabilité de un pour le résultat cible, éliminant ainsi le caractère aléatoire inhérent de la mesure quantique.",
    "B": "Elles réduisent le nombre de qubits requis en mappant les données classiques sur des états stabilisateurs qui peuvent être efficacement préparés et manipulés en utilisant uniquement des portes de Clifford, évitant ainsi la surcharge associée aux rotations arbitraires de qubits uniques qui nécessiteraient une synthèse de portes coûteuse et une compilation d'ensemble de portes universelles. Puisque les circuits stabilisateurs admettent une simulation classique efficace via le théorème de Gottesman-Knill, les sous-routines arithmétiques exploitent cette structure computationnelle pour comprimer des vecteurs de caractéristiques de haute dimension en représentations d'opérateurs de Pauli de faible poids, réalisant une réduction exponentielle de la profondeur de circuit. Cette stratégie de compression exploite le fait que la plupart des ensembles de données du monde réel présentent une structure stabilisatrice approximative dans leurs matrices de covariance.",
    "C": "Elles encodent des combinaisons non linéaires de caractéristiques d'entrée directement dans les paramètres d'état quantique grâce à des opérations comme la multiplication modulaire et les rotations de phase contrôlées, permettant au circuit de représenter des frontières de décision complexes et non séparables qui nécessiteraient autrement un nombre exponentiel de paramètres classiques. Les incorporations linéaires ne peuvent capturer que des séparations par hyperplan, tandis que les circuits arithmétiques créent des interactions de caractéristiques dans l'espace de Hilbert, permettant des transformations polynomiales et transcendantales de type noyau de manière efficace.",
    "D": "Elles implémentent l'estimation de phase quantique comme oracle de gradient",
    "solution": "C"
  },
  {
    "id": 390,
    "question": "Étant donné un unitaire U avec une décomposition spectrale contenant des valeurs propres proches de ±1, et en supposant que vous travaillez avec un circuit QPE où le nombre de qubits de comptage est fixé à n=8, quel devient le principal défi d'implémentation lorsque U décrit un opérateur d'évolution hamiltonienne à N corps complexe exp(-iHt) avec des termes non locaux ?",
    "A": "Les portes controlled-U^(2^j) nécessitent des décompositions de Trotter exponentiellement profondes pour les grands j, rendant les estimations de phase de haute précision prohibitivement coûteuses même avec une surcharge d'ancilla modérée. Le nombre de portes évolue comme O(2^j · poly(taille du système)) par opération contrôlée, et les erreurs de Trotter accumulées détruisent les motifs d'interférence nécessaires pour résoudre les fines différences de phase. Pour les systèmes à N corps avec interactions non locales, chaque étape de Trotter implique des portes à travers des qubits distants, aggravant à la fois la profondeur du circuit et la sensibilité à la décohérence.",
    "B": "Lorsque les valeurs propres se regroupent près de ±1, les phases correspondantes se concentrent près de 0 et π, où le noyau de Dirichlet de la transformée de Fourier quantique présente une sensibilité maximale aux erreurs de discrétisation de Trotter qui évoluent comme O(τ²) par pas de temps. Pour les opérations controlled-U^(2^j) avec grand j, le temps d'évolution effectif 2^j·t pousse le système dans le régime non perturbatif où les approximations standard de formule de produit s'effondrent, nécessitant une extrapolation de Richardson ou des intégrateurs d'ordre supérieur qui multiplient la profondeur du circuit par des facteurs de 4-8. Les termes hamiltoniens non locaux exacerbent cela car chaque correction d'ordre supérieur implique des commutateurs qui couplent des sous-systèmes de plus en plus distants, nécessitant des réseaux SWAP qui croissent polynomialement avec la taille du système.",
    "C": "Le registre de comptage fini avec n=8 qubits fournit une résolution de phase de 2π/256 ≈ 0,0245 radians, mais lorsque les opérations controlled-U^(2^j) avec grand j sont implémentées via la décomposition de Trotter d'hamiltoniens non locaux, l'erreur de Trotter accumulée σ_Trotter croît comme O(2^j·m·τ³) où m est le nombre de termes hamiltoniens et τ est la taille de pas de Trotter. Pour les systèmes à N corps, m évolue avec la taille du système et les interactions non locales nécessitent un routage basé sur SWAP qui ajoute une profondeur linéaire au diamètre de connectivité des qubits, faisant que l'erreur totale de Trotter dépasse 2π/256 pour j ≥ 6, saturant ainsi la résolution QPE et produisant des estimations de phase indiscernables pour des valeurs propres distinctes.",
    "D": "Les valeurs propres près de ±1 se mappent aux phases θ ≈ 0 et θ ≈ π, mais l'algorithme QPE mesure les phases modulo 2π, créant une ambiguïté lorsque |θ| < 2π/2^(n+1) car les phases positives et négatives près de zéro produisent des résultats de chaîne de bits identiques dans le registre de comptage. Pour n=8, cette région d'ambiguïté s'étend sur environ ±0,0122 radians. Lorsque U = exp(-iHt) décrit un hamiltonien à N corps avec des termes non locaux, l'implémentation de controlled-U^(2^j) pour grand j nécessite des décompositions de Trotter dont l'accumulation d'erreurs pousse les estimations de phase dans cette zone d'ambiguïté, nécessitant des protocoles auxiliaires de résolution de signe qui mesurent les projections ⟨ψ|U|ψ⟩ pour désambiguïser θ de -θ, doublant effectivement la profondeur de circuit quantique requise.",
    "solution": "A"
  },
  {
    "id": 391,
    "question": "Les qubits ancillaires sont souvent ajoutés aux circuits variationnels lors de tâches d'apprentissage supervisé pour :",
    "A": "Servir de degrés de liberté auxiliaires qui doublent effectivement la profondeur de traitement cohérent du circuit tout en maintenant des taux d'erreur de porte physique constants, car les décompositions de portes médiées par ancilles répartissent les erreurs d'une seule porte à deux qubits sur plusieurs interactions ancille-données, diluant statistiquement l'accumulation d'erreurs par couche. Ce mécanisme de dispersion des erreurs permet des circuits paramétrés plus profonds sans franchir le seuil de décohérence, permettant l'exploration d'ansätze variationnels plus expressifs pour des frontières de classification complexes.",
    "B": "Imposer une convexité stricte dans le paysage de coût variationnel en contraignant l'espace des paramètres à un sous-ensemble où la matrice hessienne de la fonction de perte reste définie positive, ce qui est réalisable car les qubits ancillaires introduisent des libertés de jauge supplémentaires qui régularisent la trajectoire d'optimisation. Cette garantie de convexité prévient les plateaux arides et assure que les optimiseurs basés sur le gradient convergent vers le minimum global en temps polynomial, indépendamment de l'initialisation aléatoire des paramètres ou des choix d'architecture de circuit.",
    "C": "Fournir des registres quantiques supplémentaires où l'information d'étiquette supervisée peut être directement encodée comme des états de base computationnelle via des opérations contrôlées conditionnées par des mesures de qubits de données, créant effectivement une représentation intriquée qui couple les caractéristiques d'entrée avec leurs classifications cibles. En mesurant les qubits ancillaires dans la base computationnelle après l'évaluation du circuit, le résultat de classification est extrait comme un résultat discret sans nécessiter de post-traitement complexe de valeurs d'espérance continues, rationalisant ainsi la procédure d'inférence et réduisant la surcharge classique dans la boucle d'apprentissage hybride quantique-classique.",
    "D": "Permettre la réinitialisation et la réutilisation en milieu de circuit pour réduire exponentiellement le nombre total de qubits par rapport aux exigences de mise à l'échelle de la profondeur du circuit.",
    "solution": "C"
  },
  {
    "id": 392,
    "question": "Quelle étape de pré-traitement aide à réduire l'intrication avant la découpe ?",
    "A": "Les techniques de re-synthèse de portes qui décomposent les blocs de circuit à forte intrication en motifs équivalents plus superficiels avec une profondeur réduite de portes à deux qubits, souvent en identifiant des identités algébriques ou en exploitant des relations de commutation qui permettent de réorganiser les opérations multi-qubits sous des formes où moins de qubits sont intriqués simultanément. Cette réorganisation diminue l'entropie d'intrication à travers les frontières de découpe potentielles, réduisant la surcharge d'échantillonnage dans la reconstruction par quasi-probabilité subséquente",
    "B": "Appliquer des règles d'optimisation de circuit qui identifient des séquences de portes créant puis détruisant immédiatement l'intrication à travers des frontières de découpe potentielles—par exemple, CX(i,j) suivi peu après par CX(i,j)—et supprimer ces opérations intriquantes redondantes ou les commuter vers des régions de circuit éloignées des découpes. En réduisant la génération d'intrication inutile par simplification algébrique et annulation de portes, le rang de Schmidt à travers les fils de découpe diminue, réduisant directement le nombre de termes dans la décomposition en quasi-probabilité et donc le coût d'échantillonnage associé pour la reconstruction du circuit",
    "C": "Employer des algorithmes d'ordre de contraction de réseaux de tenseurs pour identifier les régions de circuit où l'intrication temporaire peut être résolue par des mesures partielles intermédiaires avant d'atteindre la frontière de découpe. En insérant stratégiquement des mesures projectives sur des degrés de liberté auxiliaires qui ont rempli leur rôle computationnel, la dimension d'intrication effective transmise à travers les découpes diminue de 2ᵏ (pour k fils de découpe) à 2ᵏ⁻ᵐ (après m effondrements induits par mesure). Cette stratégie de pré-mesure nécessite une analyse minutieuse pour s'assurer que les qubits mesurés ne participent pas aux portes subséquentes, mais réduit avec succès les comptes de termes de quasi-probabilité exponentiellement en m",
    "D": "Utiliser des algorithmes de réécriture de circuit basés sur le ZX-calculus ou d'autres formalismes graphiques pour identifier et éliminer les portes intriquantes redondantes qui créent des états de rang de Schmidt 2 à travers des emplacements de découpe potentiels lorsque des représentations de rang inférieur existent. La réécriture recherche des sous-graphes correspondant à des produits de Pauli de poids élevé qui génèrent de l'intrication inutilement—tels que des échelles CNOT en cascade qui pourraient être remplacées par des décompositions Clifford+T plus superficielles avec moins de corrélations multi-qubits simultanées. En simplifiant algébriquement la structure d'intrication avant la découpe, le nombre de mesures de base de Pauli requises pour la reconstruction des fils de découpe diminue de 4ⁿ à ~2ⁿ par découpe, où n est le nombre de fils de découpe",
    "solution": "A"
  },
  {
    "id": 393,
    "question": "Pourquoi pourrait-on utiliser une découpe hybride tenseur/quasi-probabilité dans une seule exécution ?",
    "A": "Pour valider la convergence statistique de l'échantillonnage par quasi-probabilité en comparant les estimations intermédiaires aux résultats de contraction tensorielle déterministes pour les sous-circuits où la dimension de liaison reste classiquement traitable, fournissant un diagnostic en temps réel qui signale des comptes d'échantillons insuffisants avant que le calcul complet ne se termine. Le composant réseau tensoriel calcule les probabilités marginales exactes pour les fragments de circuit superficiels, qui servent de variables de contrôle réduisant la variance de l'estimateur par quasi-probabilité appliqué à des régions plus profondes et plus intriquées. En ancrant la reconstruction stochastique à ces points de contrôle déterministes, la méthode hybride atteint une convergence plus rapide en distance de variation totale tout en permettant la détection d'erreurs par des tests de cohérence entre les deux branches computationnelles.",
    "B": "Les découpes tensorielles fonctionnent bien là où l'intrication est faible et la dimension de liaison reste gérable pour la contraction classique, tandis que les méthodes par quasi-probabilité gèrent mieux les régions à forte négativité en absorbant l'intractabilité classique dans la surcharge d'échantillonnage plutôt que dans le coût de contraction exponentiel. Combiner ces deux techniques complémentaires au sein d'un seul circuit permet d'optimiser la surcharge computationnelle globale en acheminant différents sous-circuits à travers la stratégie de décomposition la plus efficace. L'approche hybride minimise simultanément les exigences de mémoire classique et la complexité d'échantillonnage quantique.",
    "C": "Pour décomposer la variance totale de mesure en contributions classiques et quantiques, où la contraction de réseau tensoriel isole le bruit de grenaille irréductible provenant des mesures projectives tandis que l'échantillonnage par quasi-probabilité capture les fluctuations supplémentaires introduites par la fragmentation du circuit et le post-traitement classique. En exécutant les deux méthodes en parallèle sur le même partitionnement de circuit, on peut soustraire la variance de référence dérivée du tenseur de la variance totale de l'estimateur par quasi-probabilité pour quantifier le coût de surcharge de la procédure de découpe elle-même. Cette décomposition de variance informe les stratégies adaptatives qui ajustent dynamiquement le placement des découpes pour minimiser la complexité d'échantillonnage tout en respectant les contraintes de mémoire classique.",
    "D": "Pour éliminer la surcharge exponentielle des quasi-probabilités négatives dans certaines régions de circuit en prétraitant ces fragments avec contraction tensorielle, qui calcule et met en cache effectivement la distribution de probabilité complète sur les résultats de mesure de sorte que la reconstruction par quasi-probabilité en aval peut échantillonner à partir de ces distributions pré-calculées sans introduire de négativité. Le réseau tensoriel absorbe le coût de simulation classique uniquement pour les sous-circuits dont la négativité nécessiterait autrement un sur-échantillonnage prohibitif, tandis que le composant par quasi-probabilité gère les couches de circuit restantes où la négativité est modérée. Cette stratégie de partitionnement assure que la surcharge d'échantillonnage globale croît seulement polynomialement avec la profondeur du circuit en confinant le coût exponentiel à une phase de prétraitement classiquement traitable.",
    "solution": "B"
  },
  {
    "id": 394,
    "question": "Dans les pipelines hybrides quantique-classiques, effectuer une réduction de dimensionnalité avec un auto-encodeur classique avant le traitement quantique vise principalement à :",
    "A": "Les auto-encodeurs classiques compressent des données d'entrée de haute dimension en une représentation latente de faible dimension via des réseaux d'encodeurs entraînés par rétropropagation. Lorsque cette représentation compressée est introduite dans un circuit variationnel quantique en aval, la dimensionnalité réduite élimine le besoin de calculer des gradients par rapport aux paramètres quantiques pendant l'entraînement.",
    "B": "En entraînant un auto-encodeur classique à projeter les caractéristiques d'entrée sur une variété unidimensionnelle, le circuit quantique subséquent hérite de cette contrainte géométrique et opère entièrement dans un sous-espace computationnel engendré par un seul qubit, éliminant les portes intriquantes et permettant à tous les paramètres variationnels d'être optimisés en utilisant l'optimisation convexe classique.",
    "C": "Réduire les exigences en qubits tout en préservant l'information pertinente pour la tâche en compressant les vecteurs de caractéristiques classiques de haute dimension en représentations latentes compactes qui peuvent être efficacement encodées dans des états quantiques en utilisant moins d'opérations d'encodage d'amplitude ou d'encodage de base, réduisant ainsi les ressources matérielles nécessaires pour la préparation d'état tout en conservant la structure essentielle nécessaire pour les tâches d'apprentissage automatique quantique en aval.",
    "D": "Un auto-encodeur classique impose un goulot d'étranglement d'information fixe sur les données d'entrée, compressant les représentations dans un espace latent avec une entropie contrôlée. Lorsque cette représentation latente est ensuite encodée dans un état quantique et traitée à travers des couches quantiques variationnelles, l'entropie initiale faible contraint l'évolution de l'intrication au sein du circuit.",
    "solution": "C"
  },
  {
    "id": 395,
    "question": "Dans le contexte de la simulation quantique de systèmes à plusieurs corps utilisant des solveurs variationnels d'états propres quantiques, quel est l'avantage principal de l'utilisation d'ansätze spécifiques au problème (tels que l'ansatz Unitary Coupled Cluster pour les systèmes moléculaires) par rapport aux ansätze efficaces en matériel avec des portes paramétrées arbitraires ? Considérez à la fois la précision de la préparation de l'état fondamental et le paysage d'optimisation classique lors de la formulation de votre réponse.",
    "A": "Les ansätze spécifiques au problème éliminent complètement l'optimisation classique par construction : la structure de l'ansatz UCC encode directement la fonction d'onde à plusieurs corps exacte via ses amplitudes de cluster couplé, qui peuvent être déterminées analytiquement à partir des éléments de matrice du hamiltonien sans aucune recherche itérative de paramètres. Les ansätze efficaces en matériel, inversement, nécessitent une optimisation en temps exponentiel car ils doivent explorer l'espace de Hilbert de dimension 2ⁿ complet sans aucun guidage physique, les rendant fondamentalement inadaptés pour la préparation d'état fondamental au-delà de systèmes triviaux malgré leurs implémentations de circuit superficielles.",
    "B": "L'avantage fondamental provient de l'efficacité de mesure plutôt que de l'optimisation : les ansätze spécifiques au problème comme UCC génèrent des états dont les valeurs d'espérance d'énergie nécessitent exponentiellement moins de mesures de termes de Pauli car les opérateurs UCC commutent avec de grands sous-ensembles de la décomposition de Pauli du hamiltonien moléculaire. Cette propriété de commutation permet la mesure simultanée de termes corrélés, réduisant la surcharge de mesure de O(N⁴) à O(N²) pour un système à N orbitales, alors que les ansätze efficaces en matériel doivent mesurer chaque terme du hamiltonien indépendamment en raison de leur structure de porte arbitraire qui détruit cette commutativité.",
    "C": "Les ansätze efficaces en matériel atteignent une convergence rapide en exploitant les opérations de porte natives qui minimisent la profondeur du circuit, mais leur véritable limitation réside dans leur incapacité à capturer les effets de corrélation forte. Les ansätze spécifiques au problème, en revanche, garantissent une préparation exacte de l'état fondamental même avec une profondeur de circuit polynomiale car la structure UCC encode intrinsèquement toutes les corrélations électroniques pertinentes à travers sa forme d'opérateur exponentiel. Les symétries de nombre de particules et de spin intégrées de UCC restreignent automatiquement l'espace de recherche au sous-espace physique, transformant effectivement l'espace de Hilbert exponentiel en un problème d'optimisation polynomial auquel les circuits efficaces en matériel ne peuvent accéder.",
    "D": "Les ansätze spécifiques au problème comme UCC incorporent une structure physique du hamiltonien cible, ce qui réduit considérablement la complexité du paysage d'optimisation en évitant les plateaux arides. Ils fournissent également une amélioration systématique via une hiérarchie (UCCSD, UCCSDT, etc.), bien qu'au prix de nécessiter des circuits plus profonds avec plus de portes à deux qubits qui peuvent ne pas se mapper efficacement aux graphes de connectivité matérielle.",
    "solution": "D"
  },
  {
    "id": 396,
    "question": "Dans le décodage basé sur les syndromes pour les codes de surface, pourquoi l'utilisation de données de syndrome provenant de plusieurs cycles de mesure consécutifs améliore-t-elle généralement les taux d'erreur logique par rapport à l'utilisation du seul cycle le plus récent ? Considérez un scénario où vous exécutez un code de surface de distance 5 sur un processeur supraconducteur avec des fidélités de porte réalistes, et vous avez la possibilité de stocker et de traiter soit uniquement le syndrome actuel, soit les quatre derniers cycles de syndromes.",
    "A": "L'historique multi-cycles distingue les erreurs de mesure des erreurs de données en suivant la persistance du syndrome — les véritables erreurs de données produisent des motifs cohérents tandis que les défauts de mesure créent des contradictions transitoires d'un cycle à l'autre.",
    "B": "Les corrélations temporelles entre défauts de syndrome révèlent les directions de propagation des erreurs selon la structure causale du circuit — plus précisément, si les bits de syndrome s₁ et s₂ s'activent dans des cycles consécutifs avec s₂ spatialement adjacent à s₁, le décodeur déduit une chaîne d'erreurs se propageant plutôt que des défauts indépendants. Cette information directionnelle contraint l'hypothèse d'erreur de maximum de vraisemblance aux chemins cohérents avec l'ordre des portes, réduisant la dégénérescence effective du code stabilisateur en éliminant les configurations d'erreur temporellement impossibles qui autrement contribueraient un poids égal à la distribution postérieure.",
    "C": "Les codes de répétition de syndrome se concatènent naturellement avec le code de surface spatial lorsque des données multi-cycles sont disponibles — la série temporelle de chaque bit de syndrome forme un code de répétition classique qui détecte les erreurs de mesure par vote majoritaire à travers les cycles. Comme les erreurs de mesure surviennent à des taux comparables aux erreurs de porte (typiquement 0,1-1% par extraction de syndrome), le décodage mono-cycle confond les défauts de mesure avec les erreurs de données, amenant le décodeur à inférer des chaînes d'erreur parasites qui déclenchent des corrections inutiles. L'historique multi-cycles permet le décodage séparé des syndromes temporels et spatiaux, factorisant effectivement le modèle d'erreur spatio-temporel combiné en sous-problèmes indépendants avec des seuils par cycle plus bas.",
    "D": "Les erreurs en crochet deviennent identifiables par leur signature multi-cycles caractéristique — lorsqu'une erreur de qubit de données survient pendant l'extraction de syndrome, elle crée une paire corrélée de défauts de syndrome qui s'étendent sur deux cycles consécutifs selon un motif géométrique spécifique déterminé par le calendrier de mesure des stabilisateurs. Le décodage mono-cycle ne peut distinguer cette erreur en crochet de deux erreurs mono-qubit indépendantes nécessitant une correction sur différents qubits, conduisant à des opérations de récupération incorrectes la moitié du temps. Les algorithmes d'appariement multi-cycles détectent ces corrélations spatio-temporelles et attribuent des poids appropriés aux hypothèses d'erreur en crochet, améliorant les estimations de seuil d'environ 0,3 points de pourcentage pour les modèles de bruit typiques au niveau circuit.",
    "solution": "A"
  },
  {
    "id": 397,
    "question": "Pourquoi l'estimation de phase est-elle particulièrement difficile pour les dispositifs quantiques actuels à échelle intermédiaire bruitée (NISQ) ?",
    "A": "Nécessite un nombre exponentiel de lectures pour résoudre les différences de phase proches d'approximations rationnelles de π, car lorsqu'une phase propre φ = 2πp/q pour de petits entiers p, q, le développement en fraction continue se termine tôt, et les pics de Fourier dans la distribution de mesure deviennent étroits de manière non résolvable par rapport au bruit d'échantillonnage. Atteindre n bits de précision dans ce régime exige environ 2^n coups de mesure pour accumuler suffisamment de statistiques pour distinguer le pic des fluctuations de fond, même si le circuit lui-même pouvait être exécuté parfaitement. Chaque bit de précision supplémentaire double à la fois la profondeur du circuit (pour implémenter des rotations contrôlées plus fines) et le nombre de coups, créant une mise à l'échelle des ressources doublement exponentielle qui dépasse les budgets de cohérence NISQ au-delà de 6-8 qubits de précision lorsque les phases propres se regroupent près de points dégénérés du cercle unitaire, où les effets d'aliasing de l'échantillonnage fini exacerbent les exigences de sensibilité et causent des oscillations de la phase extraite entre bins de discrétisation adjacents.",
    "B": "Nécessite des circuits profonds avec de nombreuses opérations contrôlées qui dépassent les temps de cohérence, car atteindre une résolution de phase précise exige de longues séquences de portes unitaires contrôlées appliquées conditionnellement en fonction des états de qubits ancillaires. Chaque bit de précision supplémentaire double la profondeur du circuit, avec une précision de n bits nécessitant des opérations contrôlées de la forme U^(2^k) pour k jusqu'à n-1, créant des circuits exponentiellement profonds qui dépassent rapidement les limites de décohérence du matériel actuel. Les erreurs de porte cumulatives à travers ces circuits profonds rendent l'information de phase extraite peu fiable au-delà d'environ 6-8 qubits de précision sur les dispositifs NISQ actuels.",
    "C": "Souffre de contamination des états propres lorsque l'état initial se superpose avec plusieurs vecteurs propres de l'unitaire analysé, car l'étape de transformée de Fourier quantique de l'estimation de phase interfère de manière cohérente le retour de phase de tous les états propres contributeurs, et si les coefficients de recouvrement ont des magnitudes similaires, la distribution de mesure résultante devient une somme de pics de type sinc qui se chevauchent et créent des motifs d'interférence destructive. La conception de l'algorithme suppose une entrée d'état propre pur ou au moins une distribution fortement pondérée vers une valeur propre, mais les protocoles de préparation d'état NISQ atteignent rarement mieux que 85% de fidélité avec l'état propre cible, ce qui signifie que 15% de fuite dans d'autres états propres contamine la lecture de phase. Pour un registre de n qubits, cette fuite introduit des pics parasites dans l'espace de résultats de mesure de dimension 2^n qui ne peuvent être filtrés a posteriori car ils sont quantiquement indiscernables du vrai signal, forçant soit des temps de moyennage plus longs qui entrent en collision avec les limites de décohérence, soit l'acceptation de résultats de phase ambigus nécessitant des heuristiques de post-traitement classique pour lever l'ambiguïté.",
    "D": "Rencontre des erreurs de bit-flip dans la transformée de Fourier quantique inverse qui se propagent de manière non linéaire à travers le réseau papillon de portes de phase contrôlées, spécifiquement parce que l'architecture en couches de la QFT applique des rotations de phase progressivement plus fines (portes R_k avec angles 2π/2^k) qui deviennent de plus en plus sensibles aux erreurs de contrôle à mesure que k augmente. Un seul bit-flip dans le registre ancillaire pendant la k-ième couche provoque l'application par toutes les portes R_m ultérieures avec m > k d'angles de phase incorrects, et comme ces erreurs se composent multiplicativement dans le plan complexe plutôt qu'additivement, la distribution de mesure finale présente une sensibilité exponentielle à la fidélité des portes dans les étapes ultérieures de la QFT. Sur le matériel NISQ avec des erreurs de porte à deux qubits d'environ 0,5%, un circuit d'estimation de phase à 8 qubits accumule environ 10% d'erreur totale juste dans la QFT inverse, ce qui se manifeste par un élargissement des pics spectraux qui dégrade la résolution de phase en dessous de la limite théorique fixée par le nombre de qubits ancillaires, et ce canal d'erreur est distinct de la décohérence car il persiste même dans la limite de température zéro où les temps T₁ et T₂ sont effectivement infinis.",
    "solution": "B"
  },
  {
    "id": 398,
    "question": "Qu'est-ce qui limite la praticité de l'algorithme de Grover pour attaquer les systèmes cryptographiques du monde réel ?",
    "A": "La construction d'oracles quantiques qui implémentent fidèlement des primitives cryptographiques complexes comme AES ou RSA nécessite de décomposer l'ensemble du chiffrement en portes quantiques réversibles, exigeant des profondeurs de circuit pouvant dépasser des millions d'opérations. Chaque porte élémentaire dans l'oracle doit être synthétisée à partir d'un ensemble de portes tolérant aux fautes, et les exigences de cohérence accumulées signifient que même des tailles de clé modérées nécessitent des implémentations corrigées d'erreurs avec des milliers de qubits physiques par qubit logique.",
    "B": "L'algorithme de Grover présente une mauvaise évolutivité lorsqu'il est distribué sur plusieurs processeurs quantiques car le mécanisme d'amplification d'amplitude repose sur des motifs d'interférence globaux qui doivent être maintenus de manière cohérente à travers tous les qubits impliqués dans la recherche.",
    "C": "L'implémentation pratique nécessite des ordinateurs quantiques tolérants aux fautes avec suffisamment de qubits logiques pour gérer des espaces de clés cryptographiquement pertinents (128-256 bits), ainsi que des implémentations de circuits quantiques efficaces d'oracles de fonctions cryptographiques. Le défi de construction de l'oracle est particulièrement sévère : décomposer AES ou SHA en portes réversibles produit des circuits avec des millions d'opérations, chacune nécessitant une correction d'erreurs qui multiplie les exigences en qubits physiques par des facteurs de 1000 ou plus, rendant le matériel quantique actuel et à court terme inadéquat pour des attaques cryptographiques significatives.",
    "D": "Tous ces facteurs présentent des défis significatifs",
    "solution": "D"
  },
  {
    "id": 399,
    "question": "Quelle est l'idée clé derrière le calcul par réservoir quantique ?",
    "A": "Seule la couche de lecture nécessite un entraînement, ce qui réduit drastiquement la charge d'optimisation puisque la grande majorité des paramètres du réseau restent fixes tout au long du processus d'apprentissage, évitant les gradients évanescents et la rétropropagation à travers de nombreuses portes multi-qubits.",
    "B": "Les réservoirs quantiques traitent les séquences temporelles par évolution unitaire, où chaque pas de temps correspond à l'application d'un hamiltonien fixe qui fait tourner l'état du réservoir dans l'espace de Hilbert. La trajectoire résultante à travers l'espace d'état multi-qubit encode naturellement les dépendances temporelles et corrélations à travers les éléments de séquence, transformant la série temporelle d'entrée en un état quantique de haute dimension dont les statistiques de mesure capturent des motifs à longue portée. Comme l'évolution unitaire est réversible et déterministe, la dynamique du réservoir préserve l'information sur les entrées précoces même lorsque de nouvelles données arrivent, permettant une modélisation efficace des séquences sans connexions récurrentes explicites.",
    "C": "La dynamique quantique non contrôlée des qubits du réservoir projette automatiquement les entrées dans des espaces de caractéristiques de haute dimension par évolution et interaction naturelles, éliminant le besoin d'entraîner la majeure partie des paramètres du réseau. En laissant le système quantique évoluer sous son hamiltonien intrinsèque sans ingénierie soigneuse, vous générez gratuitement des transformations non linéaires complexes, et n'avez besoin que d'ajuster une simple couche de lecture linéaire classique à la fin pour extraire des prédictions des mesures d'état quantique.",
    "D": "Les circuits quantiques aléatoires génèrent gratuitement des cartes de caractéristiques en exploitant le fait que les portes unitaires typiques tirées de la mesure de Haar brouillent rapidement les données d'entrée à travers tous les qubits, créant une transformation pseudo-aléatoire mais déterministe dans un espace de caractéristiques exponentiellement grand. Puisque les circuits aléatoires approchent les 2-designs unitaires après seulement une profondeur polynomiale, vous n'avez pas besoin d'ingénierie soigneuse de l'architecture du réservoir — des couches d'intrication génériques suffisent à produire des plongements expressifs dont la complexité rivalise avec celle des réseaux entraînés, externalisant effectivement l'apprentissage de caractéristiques à la complexité naturelle de la dynamique quantique à N corps.",
    "solution": "C"
  },
  {
    "id": 400,
    "question": "De quelle manière spécifique les codes de surface optimisés pour les environnements de bruit biaisé — où le déphasage domine sur les bit-flips de plusieurs ordres de grandeur — diffèrent-ils structurellement des codes de surface standard à réseau carré pour exploiter cette asymétrie et atteindre des seuils d'erreur substantiellement plus élevés ?",
    "A": "Ils modifient la longueur relative des opérateurs logiques X par rapport à Z, échangeant une certaine protection contre les bit-flips pour une protection améliorée contre les phase-flips puisque les erreurs de déphasage sont beaucoup plus courantes dans le modèle de bruit supposé. En allongeant l'opérateur logique Z et en raccourcissant l'opérateur logique X dans la géométrie du réseau, le code alloue plus de ressources de détection de syndrome aux erreurs de phase tout en acceptant une vulnérabilité accrue aux événements rares de bit-flip. Cette conception asymétrique permet au seuil d'erreur d'augmenter substantiellement lorsque le rapport de biais de bruit dépasse une valeur critique, adaptant effectivement la structure du code aux caractéristiques de bruit opérationnelles du matériel.",
    "B": "La connectivité des qubits physiques est reconfigurée pour former un réseau rectangulaire plutôt que carré, où le rapport d'aspect entre les espacements de stabilisateurs horizontaux et verticaux est ajusté pour correspondre à la racine carrée du rapport de biais de bruit, égalisant ainsi les taux d'erreur logique effectifs pour les défaillances de type X et Z malgré l'asymétrie physique sous-jacente. En étirant le réseau dans une direction spatiale, le code augmente le poids des Z-stabilisateurs par rapport aux X-stabilisateurs, ce qui compense le taux de déphasage plus élevé en nécessitant plus d'erreurs de phase simultanées pour produire une défaillance logique. Cette déformation géométrique ajuste la distance du code de manière asymétrique, élevant le seuil lorsque η = p_dephasing / p_bitflip dépasse environ 10, et le rapport d'aspect optimal évolue logarithmiquement avec η pour équilibrer les deux canaux de défaillance.",
    "C": "Le calendrier d'extraction de syndrome est modifié pour mesurer les Z-stabilisateurs à une fréquence de répétition plus élevée que les X-stabilisateurs, exploitant l'asymétrie temporelle dans les temps d'arrivée des erreurs pour consacrer davantage de cycles de correction d'erreurs quantiques disponibles à la détection des événements de déphasage dominants. En entrelaçant plusieurs mesures de syndrome Z entre cycles successifs de syndrome X, le décodeur reçoit des mises à jour plus fréquentes sur les chaînes de phase-flip avant qu'elles ne se propagent en configurations incorrigibles. Ce biais temporel augmente effectivement la distance du code contre les erreurs de déphasage en réduisant la latence entre occurrence et détection d'erreur, tout en acceptant des écarts plus longs dans la surveillance des bit-flips puisque ces événements s'accumulent à un taux négligeable par rapport à la période d'extraction de syndrome.",
    "D": "Les qubits ancillaires se voient attribuer des stratégies asymétriques de suppression du bruit au repos, où les ancilles de stabilisateur de phase emploient des impulsions de découplage dynamique continu entre cycles de syndrome pour réduire davantage leur susceptibilité au déphasage, tandis que les ancilles de bit-flip restent non protégées puisque le taux de bit-flip ambiant est déjà de plusieurs ordres de grandeur en dessous du seuil où un surcoût supplémentaire produirait des gains significatifs. Cette stratégie de protection sélective concentre les ressources de contrôle disponibles sur l'atténuation du canal d'erreur dominant, abaissant effectivement le taux de déphasage logique sans augmenter le nombre total d'opérations physiques par cycle de syndrome. Le décalage résultant dans les fidélités des ancilles crée un biais de bruit effectif au niveau de la mesure des stabilisateurs qui reflète et amplifie le biais dans les erreurs de qubits de données.",
    "solution": "A"
  },
  {
    "id": 401,
    "question": "Les algorithmes de marche quantique utilisent parfois une pièce réfléchissante aux sommets marqués afin que :",
    "A": "L'opérateur de réflexion crée un déphasage de π spécifiquement pour la composante du sommet marqué, implémentant le mécanisme de répercussion de phase qui inverse le signe de l'amplitude tout en préservant sa magnitude. Cette inversion de phase sélective aux solutions provoque des interférences destructives le long des arêtes sortantes lorsqu'elle est combinée avec l'opérateur de diffusion standard, piégeant efficacement l'amplitude aux états marqués grâce au même mécanisme d'interférence sous-jacent à l'algorithme de Grover.",
    "B": "L'amplitude ne s'échappe pas une fois qu'elle atteint un état marqué, gardant le marcheur localisé là-bas afin que la probabilité s'accumule à la solution par interférence constructive au lieu de se disperser dans la structure du graphe où elle continuerait à explorer les sommets non-solutions.",
    "C": "La réflexion de la pièce aux sommets marqués implémente une condition aux limites qui inverse le vecteur d'impulsion du marcheur, créant un motif d'onde stationnaire centré sur le sommet solution. Cette inversion d'impulsion empêche l'amplitude de se propager tout en permettant à l'amplitude entrante de continuer d'arriver, établissant un équilibre dynamique où le flux de probabilité vers les états marqués dépasse le flux sortant, concentrant ainsi la distribution du marcheur aux solutions sur plusieurs itérations.",
    "D": "L'application d'opérateurs de réflexion aux sommets marqués modifie le spectre propre de l'opérateur de marche en introduisant un défaut localisé qui sépare les niveaux d'énergie dégénérés, créant un écart énergétique entre les variétés de sommets marqués et non marqués. Cette séparation spectrale pousse le système à peupler préférentiellement les états propres des sommets marqués durant l'évolution adiabatique, la force de réflexion déterminant la magnitude de l'écart et donc le taux de transition diabatique entre les variétés.",
    "solution": "B"
  },
  {
    "id": 402,
    "question": "Qu'est-ce qui limite l'efficacité de la simulation de Trotter-Suzuki lorsque la taille des molécules augmente ?",
    "A": "Les grandes molécules mappées sur des hamiltoniens de réseau pour la simulation de Trotter présentent souvent des états excités quasi-dégénérés en raison de symétries dans l'arrangement spatial des orbitales atomiques, créant des régions spectrales denses dans le paysage énergétique. Lorsque le pas de Trotter est choisi pour résoudre l'énergie de l'état fondamental, il crée par inadvertance un repliement de ces états excités dégénérés, provoquant un repliement de fréquence dans l'évolution temporelle simulée.",
    "B": "À mesure que les systèmes moléculaires grandissent, l'entropie d'intrication entre tout sous-système local et le reste approche sa valeur maximale, saturant la capacité d'information des qubits individuels lors de la mesure. Cet effet de saturation introduit un biais systématique dans les statistiques de lecture car les états hautement intriqués ne peuvent être projetés de manière fiable sur les états de base computationnelle sans perte d'information de phase.",
    "C": "L'estimation de phase quantique, qui fournit l'accélération exponentielle pour extraire les énergies de l'état fondamental moléculaire, souffre d'une dégradation de fidélité à mesure que le nombre d'orbitales moléculaires augmente car les qubits ancillaires utilisés pour la répercussion de phase accumulent des erreurs proportionnellement au nombre d'orbitales. Chaque orbitale supplémentaire contribue des canaux de bruit indépendants qui interfèrent destructivement avec l'information de phase cohérente accumulée dans le registre QPE.",
    "D": "La décomposition de Trotter-Suzuki approche l'évolution temporelle sous un hamiltonien H = H₁ + H₂ + ... en le divisant en produits d'exponentielles exp(-iH_k·Δt), où chaque terme évolue séparément. Pour les hamiltoniens moléculaires, le nombre de termes non commutatifs évolue en N⁴ avec la taille du système N (en raison des intégrales bi-électroniques), ce qui signifie que l'erreur de Trotter — qui dépend des commutateurs imbriqués comme [H_i, [H_j, H_k]] — croît quartiquement. Cela force les pas de Trotter Δt à diminuer en ~1/N⁴ pour maintenir une précision fixe, provoquant l'explosion du nombre de pas temporels requis (et de la profondeur du circuit), rendant la simulation impraticable pour les grandes molécules.",
    "solution": "D"
  },
  {
    "id": 403,
    "question": "Quelles stratégies clés permettent l'exécution de portes quantiques entre qubits distants dans un système quantique distribué ?",
    "A": "Le clonage des états de qubits et leur traitement local à chaque nœud, évitant ainsi le surcoût de distribution d'intrication. Cela exploite le clonage approximatif pour les états mixtes, générant des copies avec une fidélité suffisante pour les opérations de portes avant de réconcilier classiquement les résultats.",
    "B": "Le transport physique de qubits à travers des réseaux de fibres utilisant des techniques classiques de multiplexage, où les qubits encodés dans des modes de guides d'onde photoniques sont acheminés à travers des commutateurs optiques reconfigurables. Le multiplexage temporel garantit que plusieurs qubits traversent la même fibre sans interférence mutuelle, maintenant la cohérence sur des distances métropolitaines en exploitant les fenêtres d'infrastructure de télécommunications à faible perte.",
    "C": "Les méthodes basées sur la téléportation telles que les protocoles telegate et teledata, qui exploitent l'intrication pré-partagée entre nœuds distants pour exécuter des portes quantiques non locales. L'approche telegate consomme des paires de Bell pour implémenter des opérations à deux qubits entre qubits séparés en effectuant des opérations locales et une communication classique, synthétisant efficacement l'interaction de porte sans transport physique de qubit. Teledata utilise de manière similaire l'intrication comme ressource pour transmettre l'information quantique, permettant le calcul distribué tout en préservant la cohérence malgré la séparation spatiale des nœuds de calcul.",
    "D": "Les codes de correction d'erreurs quantiques sont appliqués pour fusionner physiquement les qubits distants dans un espace cohérent unique avant les opérations de portes. Cette fusion utilise des patchs de code de surface fusionnés adiabatiquement via des mesures médiées par ancilla, créant un espace de qubit logique unifié couvrant tous les nœuds. Ce n'est qu'après cette fusion que les portes à deux qubits peuvent être exécutées avec la fidélité requise, car le surcoût de correction d'erreurs stabilise l'état quantique étendu contre la décohérence induite par le réseau.",
    "solution": "C"
  },
  {
    "id": 404,
    "question": "Pourquoi les portes non-Clifford sont-elles essentielles pour le calcul quantique universel ?",
    "A": "Les portes non-Clifford permettent d'accéder à des angles de phase en dehors de l'ensemble discret {0, π/2, π, 3π/2} qui caractérisent les opérations Clifford, ce qui est nécessaire car le théorème de Solovay-Kitaev requiert des relations de phase irrationnelles pour approcher des unitaires arbitraires. Bien que les portes Clifford forment un groupe fini efficacement simulable par le théorème de Gottesman-Knill, des portes comme T (qui applique une phase e^(iπ/4)) introduisent des angles transcendants qui brisent cette simulabilité. Sans de telles phases, l'ensemble de portes reste dans un sous-ensemble dénombrable de SU(2^n) et ne peut couvrir densément l'espace de transformation continu requis pour le calcul universel.",
    "B": "Les opérations Clifford seules peuvent être efficacement simulées classiquement via le théorème de Gottesman-Knill, ce qui signifie qu'elles ne peuvent fournir d'avantage computationnel au-delà de ce que les ordinateurs conventionnels réalisent. Les portes non-Clifford comme la porte T introduisent la complexité nécessaire pour échapper à cette contrainte de simulabilité classique, permettant l'accès au plein espace de Hilbert et rendant possible le calcul quantique universel. Sans au moins une porte non-Clifford dans votre ensemble de portes, tout circuit quantique reste piégé dans le formalisme stabilisateur efficacement simulable.",
    "C": "Les portes non-Clifford sont requises car les opérations Clifford préservent la structure discrète des états stabilisateurs, qui forment un sous-ensemble de mesure nulle du plein espace de Hilbert. Bien que les circuits Clifford puissent générer une intrication maximale (par exemple, états GHZ et états cluster), ils ne peuvent créer des superpositions avec des relations d'amplitude continues arbitraires nécessaires pour des algorithmes comme la factorisation de Shor. La distinction clé est que les états stabilisateurs n'ont que des matrices de densité réduites à valeurs réelles lorsqu'ils sont mesurés dans certaines bases, tandis que les portes non-Clifford permettent des motifs d'interférence complexes avec des relations de phase irrationnelles essentielles pour l'avantage computationnel quantique au-delà des tâches d'échantillonnage.",
    "D": "Les portes non-Clifford brisent la garantie de simulation classique en temps polynomial du théorème de Gottesman-Knill en introduisant des états magiques — états ressources dont la fonction de Wigner présente des valeurs négatives, signifiant un comportement quantique authentique. Alors que les portes Clifford seules peuvent efficacement préparer tous les états de graphe et effectuer l'extraction de syndrome pour la correction d'erreurs, elles ne génèrent que des phases discrètes correspondant à des transformations symplectiques sur des corps finis. Des portes comme T injectent la complexité de phase continue nécessaire pour couvrir SU(2^n), comme prouvé par le fait que Clifford+T forme un ensemble de portes universel via la construction de Solovay-Kitaev nécessitant O(log^c(1/ε)) portes pour une ε-approximation.",
    "solution": "B"
  },
  {
    "id": 405,
    "question": "Que signifie NISQ ?",
    "A": "Near-Intermediate-Scale Quantum, caractérisant les dispositifs dans le régime transitoire entre les qubits logiques entièrement corrigés d'erreurs et les expériences de validation de principe à petite échelle, présentant typiquement 50-500 qubits physiques avec des fidélités de porte approchant mais ne dépassant pas le seuil de code de surface de 99%. Ces systèmes démontrent un avantage quantique sur des problèmes spécialisés comme l'échantillonnage de circuits aléatoires tout en restant trop bruités pour des algorithmes pratiques nécessitant des circuits profonds, occupant l'échelle où la simulation classique devient intraitable sur des superordinateurs conventionnels mais la tolérance aux fautes reste irréalisable, stimulant la recherche sur les algorithmes variationnels et les techniques d'atténuation d'erreurs qui extraient une valeur computationnelle malgré la décohérence limitant la profondeur de circuit à 100-1000 portes.",
    "B": "Noisy Intermediate-Scale Quantum, le terme caractérisant les processeurs quantiques de génération actuelle qui opèrent avec 50-1000 qubits, des fidélités de porte modérées (typiquement 99-99,9%), et des temps de cohérence limités insuffisants pour une correction d'erreurs complète tolérante aux fautes. Ces dispositifs occupent le régime entre les petites expériences de validation de principe et les futurs ordinateurs quantiques corrigés d'erreurs, permettant l'exploration de l'avantage quantique dans des domaines spécifiques comme l'optimisation, l'échantillonnage et la simulation quantique malgré les opérations de portes imparfaites et la décohérence environnementale qui empêchent l'exécution d'algorithmes arbitrairement longs.",
    "C": "Non-Idealized Scalable Quantum, dénotant les architectures où les rendements de fabrication de qubits varient à travers la puce, nécessitant une post-sélection et caractérisation pour identifier les sous-ensembles à haute fidélité adaptés au calcul. Ces plateformes atteignent la scalabilité non pas par des réseaux de qubits uniformément de haute qualité mais en fabriquant de grands nombres de qubits (100-10 000) et en mappant les algorithmes sur les sous-graphes les plus performants identifiés par calibration tomographique, acceptant que 20-40% des qubits physiques puissent présenter des fidélités sous le seuil. Cette nomenclature est née des efforts industriels de calcul quantique axés sur la maximisation du nombre de qubits utiles malgré les imperfections de fabrication inhérentes aux technologies de qubits supraconducteurs et à base de semi-conducteurs.",
    "D": "Noise-Intensive Subthreshold Quantum, décrivant les systèmes opérant dans le régime où les taux d'erreur de portes à deux qubits dépassent le seuil de tolérance aux fautes (typiquement >1%) mais restent en dessous du seuil de simulation classique où le comportement quantique devient intraitable à vérifier. Ces processeurs comportent 50-200 qubits avec des temps de cohérence de 10-100 microsecondes, permettant 50-500 opérations de portes avant que la décohérence ne domine. La terminologie souligne que bien que les taux d'erreur empêchent la correction d'erreurs quantiques complète, les dispositifs présentent toujours des phénomènes quantiques authentiques comme l'intrication à travers des dizaines de qubits, les rendant précieux pour l'étalonnage des protocoles d'atténuation d'erreurs et l'étude de la conception d'algorithmes résistants au bruit dans l'ère pré-tolérante aux fautes.",
    "solution": "B"
  },
  {
    "id": 406,
    "question": "Quel est l'objectif de la compilation itérative dans la conception de circuits quantiques ?",
    "A": "Affiner progressivement les implémentations de circuits en utilisant les retours d'essais de compilation précédents ou les résultats d'exécution matérielle, améliorant le nombre de portes, la profondeur ou la fidélité à travers des cycles d'optimisation successifs.",
    "B": "Incorporer des données de calibration en temps réel provenant d'exécutions de caractérisation matérielle dans des passes de compilation successives, où chaque itération met à jour les poids de la fonction de coût en fonction des fidélités de portes mesurées, des matrices de diaphonie et des temps de cohérence issus des résultats d'exécution de la compilation précédente. Cette optimisation en boucle fermée adapte progressivement la topologie du circuit aux caractéristiques matérielles variables dans le temps, améliorant la fidélité effective grâce à l'ordonnancement de portes et l'allocation de qubits conscients du matériel qui répondent à la dérive des paramètres du dispositif entre les cycles de calibration.",
    "C": "Explorer systématiquement l'espace des représentations de circuits équivalentes en appliquant des cycles successifs de règles de commutation, d'annulation et de synthèse de portes, où chaque itération génère plusieurs circuits candidats évalués selon la profondeur, le nombre de portes et les métriques d'erreur estimées. La compilation se termine lorsque des itérations consécutives ne parviennent pas à produire d'améliorations au-delà d'un seuil, assurant la convergence vers un optimum local dans le paysage de coût du circuit par recherche par escalade qui affine les séquences de portes sans nécessiter d'exécution matérielle entre les passes.",
    "D": "Décomposer des portes multi-qubits complexes en opérations natives du matériel à travers des étapes de trotterisation séquentielles, où chaque itération de compilation augmente l'ordre de Trotter pour réduire l'erreur d'approximation provenant de termes hamiltoniens non commutatifs. Commencer par une décomposition de Trotter de premier ordre et raffiner progressivement vers des ordres supérieurs permet au compilateur d'équilibrer le nombre de portes contre la précision de simulation, se terminant lorsque l'amélioration marginale de la fidélité d'opérateur provenant d'étapes de Trotter supplémentaires tombe en dessous du taux d'erreur par porte de la plateforme matérielle cible.",
    "solution": "A"
  },
  {
    "id": 407,
    "question": "Les ordinateurs quantiques actuellement accessibles supportent-ils toutes les portes unitaires possibles ?",
    "A": "Non, mais les processeurs contemporains supportent nativement des unitaires arbitraires à un qubit et une porte universelle à deux qubits, ce qui suffit mathématiquement pour approximer tout unitaire à n qubits avec une précision arbitraire via le théorème de Solovay-Kitaev. Le défi est que chaque couche de décomposition supplémentaire compose les erreurs de porte de manière multiplicative, donc bien que l'ensemble de portes soit formellement universel, le plafond de fidélité pratique signifie que les unitaires complexes dépassent les budgets d'erreur avant que la compilation ne soit terminée, limitant quelles opérations restent expérimentalement viables sur le matériel NISQ.",
    "B": "Non, car bien que la connectivité tous-à-tous des ions piégés permette des portes multi-qubits directes, l'hamiltonien d'interaction de Mølmer-Sørensen contraint les opérations réalisables au sous-espace symétrique des modes de phonons collectifs de la chaîne d'ions. Cette restriction géométrique signifie que certains unitaires antisymétriques — particulièrement ceux nécessitant un contrôle de phase indépendant de facteurs tensoriels non commutatifs — ne peuvent pas être implémentés sans décomposition en portes séquentielles qui brisent l'opération native en sous-groupes adressables, réintroduisant la surcharge de compilation que la connectivité était censée éliminer.",
    "C": "Non — le matériel quantique ne fournit qu'un ensemble fini de portes natives, consistant typiquement en des rotations à un qubit et une ou deux portes intriquantes à deux qubits comme CNOT ou CZ. Toute opération unitaire arbitraire doit être compilée en une séquence de ces portes primitives par des algorithmes de décomposition, qui introduisent une profondeur de circuit supplémentaire et accumulent des erreurs à chaque couche d'approximation.",
    "D": "Non, car même avec la correction d'erreur opérationnelle, l'ensemble des portes transversales implémentables sur les qubits logiques forme un sous-groupe discret (typiquement le groupe de Clifford pour les codes stabilisateurs) qui manque d'universalité. Atteindre des unitaires logiques arbitraires nécessite des portes non transversales comme la porte T, qui doivent être implémentées via la distillation d'états magiques — un protocole gourmand en ressources qui consomme de nombreux qubits physiques par opération logique. Jusqu'à ce que suffisamment d'usines d'états magiques puissent être intégrées, les programmeurs restent contraints à des ensembles de portes approximatifs.",
    "solution": "C"
  },
  {
    "id": 408,
    "question": "Vous implémentez l'algorithme HHL pour résoudre des systèmes linéaires. La matrice avec laquelle vous travaillez a des valeurs propres réparties sur trois ordres de grandeur — la plus petite valeur propre non nulle est environ 0,001 et la plus grande est proche de 1,0. Votre collègue vous met en garde contre un problème fondamental de mise à l'échelle qui dominera vos besoins en ressources. D'où vient exactement ce goulot d'étranglement, et comment le paramètre pertinent croît-il en fonction de la structure des valeurs propres ?",
    "A": "Le goulot d'étranglement des ressources émerge des exigences de précision de la sous-routine d'estimation de phase, qui doit résoudre les différences de valeurs propres jusqu'à l'échelle de la plus petite valeur propre λ_min ≈ 0,001 pour éviter l'aliasing pendant l'étape d'inversion par rotation contrôlée. L'estimation de phase quantique standard atteint une précision ε en utilisant O(1/ε) applications de l'unitaire contrôlé e^(iAt), mais ici vous avez besoin de ε ≪ λ_min, exigeant Θ(1/λ_min) ≈ 1000 applications d'unitaire contrôlé juste pour résoudre la structure spectrale. L'inversion de valeur propre subséquente |λ⟩ → sin(λ̃/λ)|λ⟩ nécessite la synthèse d'angles de rotation précis à δθ ≈ λ_min/λ_max pour éviter d'introduire des erreurs qui corrompent les composantes à petites valeurs propres du vecteur solution, et le théorème de Solovay-Kitaev garantit qu'atteindre une précision de rotation à un qubit δθ nécessite une profondeur de porte Θ(log^c(1/δθ)) où c ≈ 2 pour les implémentations optimales. En combinant ces éléments : la profondeur totale évolue comme Θ((κ/λ_min)·log²(κ)) où κ = λ_max/λ_min ≈ 1000, donnant le terme de coût dominant qui croît plus que linéairement avec le nombre de conditionnement, bien que pas tout à fait quadratique comme certains modèles simplifiés le suggèrent.",
    "B": "Le goulot d'étranglement critique de mise à l'échelle survient pendant l'étape d'amplification d'amplitude qui suit l'inversion de valeur propre, où la probabilité de succès pour extraire l'état final |x⟩ = A⁻¹|b⟩ évolue comme P_success ∝ ||A⁻¹||²||b||² / (nombre de conditionnement)². Avec votre nombre de conditionnement κ ≈ 1000, l'amplitude de succès pour mesurer le registre cible dans l'état désiré devient ≈ 10⁻⁶, nécessitant O(√(1/P_success)) ≈ 10³ itérations d'amplification d'amplitude pour augmenter la probabilité à près de l'unité. Chaque itération d'amplification exige un cycle complet d'estimation de phase plus une inversion contrôlée, créant une structure de boucle imbriquée où le nombre total de portes croît comme Θ(κ·log(κ)·√κ) ≈ Θ(κ^(3/2)·log κ). Le facteur log(κ) provient de la taille du registre ancillaire nécessaire pour l'estimation de phase (vous avez besoin de ⌈log₂(κ·poly(n))⌉ qubits pour résoudre les valeurs propres), le facteur √κ des répétitions d'amplification d'amplitude, et le facteur linéaire κ des exigences de précision d'inversion de valeur propre. Cette mise à l'échelle super-linéaire en κ domine le compte de ressources et représente la barrière fondamentale à l'application de HHL aux systèmes mal conditionnés.",
    "C": "Le défi de mise à l'échelle provient des exigences de reconstruction tomographique pour extraire le vecteur solution : bien que HHL produise l'état quantique |x⟩ ∝ A⁻¹|b⟩, mesurer les valeurs d'attente d'observables ⟨x|M|x⟩ pour des opérateurs M arbitraires nécessite d'estimer les amplitudes d'états de base individuels |xᵢ⟩, ce qui nécessite Ω(2^n/ε²) coups de mesure pour des systèmes à n qubits lors de la recherche de précision ε via la tomographie quantique standard. Cependant, pour les matrices mal conditionnées, les composantes du vecteur solution correspondant aux petites valeurs propres dominent la norme L² : ||x||² ≈ Σᵢ |⟨b|vᵢ⟩|²/λᵢ² où |vᵢ⟩ sont des vecteurs propres et le i-ème terme contribue ∝ 1/λᵢ². Avec λ_min ≈ 0,001, ces composantes portent des poids ∼ 10⁶ fois plus grands que les composantes de λ_max ≈ 1,0, créant une plage dynamique extrême dans la distribution d'amplitude. La mesure limitée par le bruit de tir nécessite alors N_shots ∝ (λ_max/λ_min)² ∝ κ² échantillons pour résoudre les plus petites composantes de solution au-dessus du plancher de bruit de mesure, donnant une mise à l'échelle quadratique en nombre de conditionnement qui domine le temps d'exécution global malgré la complexité de porte polynomiale de HHL.",
    "D": "Le goulot d'étranglement fondamental provient de l'étape de rotation contrôlée qui effectue l'inversion de valeur propre, où vous devez appliquer une rotation proportionnelle à 1/λ pour chaque valeur propre λ. Pour distinguer votre plus petite valeur propre (λ_min ≈ 0,001) de zéro avec une précision suffisante pour une inversion précise, la sous-routine d'estimation de phase nécessite une précision de registre ancillaire évoluant comme Ω(log(κ)) qubits, mais plus critiquement, la précision d'inversion exige que les angles de rotation résolvent les différences de l'ordre de 1/λ_min. Puisque la synthèse de porte quantique à précision ε nécessite une profondeur de circuit Θ(log(1/ε)), et que vous avez besoin de ε ≪ λ_min pour éviter de submerger les contributions de petites valeurs propres, la complexité temporelle évolue comme Θ(κ log(κ)) où κ = λ_max/λ_min ≈ 1000 est le nombre de conditionnement — cette dépendance quadratique-logarithmique au rapport de valeurs propres devient le coût dominant, pas simplement linéaire comme certaines analyses simplifiées le suggèrent.",
    "solution": "D"
  },
  {
    "id": 409,
    "question": "Laquelle des affirmations suivantes est la plus précise concernant la performance des classificateurs quantiques actuels par rapport aux modèles classiques ?",
    "A": "Les deux types de classificateurs atteignent des performances essentiellement équivalentes sur la plupart des benchmarks standard, les modèles quantiques offrant des avantages marginaux uniquement dans des domaines hautement spécialisés où l'espace de caractéristiques admet naturellement un plongement dans l'espace de Hilbert qui s'aligne avec la structure du problème. En pratique, des facteurs tels que le bruit de tir de mesure et la connectivité limitée des qubits compensent les avantages théoriques des méthodes de noyaux quantiques, résultant en une parité de performance qui suggère que les approches quantiques et classiques sont fondamentalement comparables sur le matériel à court terme lorsqu'évaluées sur des ensembles de données comme MNIST, IRIS ou des tâches standard du référentiel UCI.",
    "B": "Les classificateurs quantiques surpassent de manière fiable les modèles classiques dans divers domaines de tâches incluant la reconnaissance d'images, le traitement du langage naturel et la prévision de séries temporelles, principalement en raison de leur capacité à explorer des espaces de caractéristiques exponentiellement grands grâce à la superposition et l'intrication.",
    "C": "Les modèles classiques surpassent généralement les classificateurs quantiques sur la plupart des benchmarks contemporains en raison d'algorithmes d'optimisation matures, d'une meilleure robustesse au bruit et des nombres limités de qubits disponibles sur les dispositifs NISQ actuels. Bien que les approches quantiques montrent une promesse théorique pour certaines méthodes basées sur des noyaux, les implémentations pratiques souffrent du bruit de tir, de la profondeur de circuit limitée et des effets de plateau stérile qui empêchent un entraînement efficace, résultant en des précisions de test qui tombent généralement en dessous de celles obtenues par des techniques d'apprentissage automatique classiques optimisées.",
    "D": "Uniquement lorsqu'ils sont fournis avec des ensembles de données massifs contenant des millions d'exemples étiquetés, les classificateurs quantiques commencent à montrer un avantage mesurable sur les approches classiques, car l'expressivité du noyau quantique devient statistiquement significative seulement dans le régime de grands échantillons où les inégalités de concentration garantissent que les cartes de caractéristiques quantiques explorent des directions orthogonales dans l'espace de Hilbert que les noyaux classiques ne peuvent pas accéder efficacement. En dessous d'environ 10⁶ échantillons d'entraînement, les modèles classiques maintiennent une performance supérieure en raison de leurs paysages d'optimisation matures et d'une meilleure efficacité d'échantillon, mais au-delà de ce seuil, les circuits quantiques exploitent la mise à l'échelle dimensionnelle pour atteindre une suprématie asymptotique en erreur de généralisation.",
    "solution": "C"
  },
  {
    "id": 410,
    "question": "Dans l'apprentissage par circuits quantiques, le compromis expressivité-entraînabilité capture l'observation que :",
    "A": "Les circuits hautement expressifs avec des structures d'ansatz profondes et de larges ensembles de portes peuvent souffrir de gradients évanescents pendant l'optimisation car le paysage de coût devient exponentiellement concentré autour de sa moyenne à mesure que la profondeur du circuit augmente, un phénomène connu sous le nom de plateaux stériles qui rend les mises à jour de paramètres inefficaces sans initialisation spécialisée ou architectures structurées.",
    "B": "Les circuits atteignant une couverture élevée de l'espace d'états à travers des conceptions unitaires aléatoires deviennent plus difficiles à entraîner car leurs gradients de fonction de coût se concentrent exponentiellement autour de zéro avec l'augmentation de la profondeur selon le lemme de Lévy sur la mesure concentrée dans les dimensions élevées. Cependant, cette concentration se produit uniquement pour les fonctions de coût globales ; les observables locales mesurant des sous-systèmes à quelques qubits maintiennent des gradients entraînables même à de grandes profondeurs, suggérant que l'expressivité mesurée par la pureté du sous-système plutôt que par l'entropie d'intrication globale fournit un prédicteur plus précis du comportement de mise à l'échelle du gradient.",
    "C": "À mesure que l'expressivité de l'ansatz augmente par l'ajout de couches, le paysage de paramètres développe un nombre exponentiellement croissant de minima locaux dont les tailles de bassin suivent une distribution log-normale, rendant la descente de gradient de plus en plus susceptible de se terminer dans des configurations sous-optimales. Cette prolifération de minima presque dégénérés se produit parce que les circuits hautement expressifs peuvent représenter exponentiellement de nombreux états approximativement orthogonaux, chacun correspondant à un optimum local distinct, alors que les circuits peu profonds avec une expressivité limitée ont des minima épars et bien séparés que les optimiseurs standard peuvent localiser de manière fiable.",
    "D": "Les circuits expressifs avec une profondeur de couche intriquante dépassant la longueur de cohérence développent une mise à l'échelle de gradient gouvernée par la transition du comportement unitaire aléatoire de Haar à profondeur polynomiale vers une mesure exponentiellement concentrée à profondeur super-polynomiale. Spécifiquement, pour les ansätze efficaces en matériel avec des couches alternées de rotation et d'intrication, l'entraînabilité est maintenue lorsque la profondeur d satisfait d < O(n^(2/3)) qubits mais entre dans le régime de plateau stérile lorsque d > O(n), créant une fenêtre où l'expressivité mesurée par le pouvoir d'intrication croît polynomialement tandis que la variance du gradient reste inversement polynomialement grande.",
    "solution": "A"
  },
  {
    "id": 411,
    "question": "Quelle méthodologie d'attaque avancée peut compromettre la distribution quantique de clés basée sur des systèmes à variables continues ?",
    "A": "En exploitant un appariement de mode imparfait entre le signal et l'oscillateur local au niveau du récepteur, un adversaire peut introduire un mode auxiliaire faible orthogonal à l'OL mais couplé au signal par des effets non linéaires dans les photodiodes du détecteur homodyne. Ce mode auxiliaire transporte une information partielle sur les valeurs de quadrature mesurées mais n'est pas pris en compte dans la calibration du bruit de grenaille car il se situe en dehors de la bande passante du mode OL. L'adversaire peut ainsi extraire l'information de la clé depuis ce mode non surveillé sans augmenter le bruit dans le mode de signal surveillé, échappant aux vérifications des paramètres de sécurité qui bornent l'information d'Eve en fonction du bruit excédentaire dans le canal homodyne principal.",
    "B": "Un adversaire peut exploiter la bande passante finie du détecteur en envoyant de la lumière comprimée à large bande à des fréquences en dehors de la plage de détection du récepteur, ce qui crée une anti-compression dans la quadrature du signal mesuré par conversion paramétrique descendante dans le chemin optique. Cette compression dépendante de la fréquence réduit la variance effective du signal dans la bande passante de détection tout en permettant simultanément l'extraction d'information par détection hétérodyne des modes anti-comprimés hors bande. Puisque les preuves de sécurité CV-QKD supposent que la variance du signal mesurée dans la bande passante du détecteur reflète le bruit total du système, cette attaque permet à Eve d'obtenir une information partielle tandis que les parties légitimes sous-estiment la perte de canal et surestiment les taux de clé sécurisée basés sur la variance réduite artificiellement dans la bande.",
    "C": "Manipulation du faisceau de référence de l'oscillateur local, permettant à l'adversaire de contrôler les résultats de mesure homodyne en introduisant des déphasages contrôlés ou des modulations d'amplitude.",
    "D": "L'adversaire exploite l'efficacité quantique imparfaite dans les détecteurs homodynes en effectuant une interaction unitaire entre le mode de signal et un mode ancillaire avant la détection, avec une force d'interaction calibrée de sorte que l'information soit transférée à l'ancille proportionnellement à (1 - η), où η est l'efficacité du détecteur. Puisque les bornes de sécurité CV-QKD tiennent compte de la perte en attribuant tous les photons perdus à Eve, mais ne modélisent pas explicitement l'inefficacité du détecteur comme un canal séparé de la perte, cette attaque extrait une information supplémentaire au-delà de ce que la preuve de sécurité alloue à Eve. L'ancille de l'adversaire contient l'information de quadrature qui aurait été perdue par inefficacité, donnant effectivement à Eve accès à une information que le protocole suppose simplement rejetée.",
    "solution": "C"
  },
  {
    "id": 412,
    "question": "Quelle technique spécifique peut détecter des modifications malveillantes dans les séquences d'impulsions quantiques ?",
    "A": "La tomographie de processus—caractériser complètement le canal implémenté en préparant un ensemble complet d'états d'entrée couvrant l'espace des opérateurs, en exécutant la séquence d'impulsions sur chacun, et en effectuant une tomographie d'état sur toutes les sorties. En reconstruisant la représentation complète de la matrice-χ ou de la matrice de transfert de Pauli de l'opération quantique réalisée, vous pouvez vérifier que la fidélité du processus avec l'unitaire prévu dépasse les seuils de sécurité.",
    "B": "L'empreinte de calibration établit une signature de référence des séquences d'impulsions légitimes en caractérisant les motifs d'erreur natifs du dispositif sous opération honnête, puis détecte les déviations de cette signature qui indiquent une falsification. En mesurant des corrélations observables spécifiques—telles que les motifs de diaphonie entre qubits adjacents, l'accumulation de phase dépendante de la fréquence pendant les périodes inactives, ou les inclinaisons systématiques de l'axe de rotation dans les portes à un qubit—vous créez une empreinte multidimensionnelle de la façon dont les impulsions authentiques affectent l'état quantique. Les modifications malveillantes des impulsions, même si elles implémentent la porte correcte en moyenne, altéreront ces corrélations d'erreur subtiles de manière détectable. L'analyse statistique des déviations d'empreinte sur plusieurs exécutions de circuit révèle des anomalies qui distinguent la falsification adversariale de la dérive naturelle de calibration.",
    "C": "Les protocoles standards de benchmarking randomisé peuvent détecter les modifications malveillantes d'impulsions en mesurant la fidélité moyenne des portes sur des éléments du groupe de Clifford échantillonnés uniformément au hasard. Si un attaquant a injecté des opérations cachées dans le compilateur d'impulsions, le taux de décroissance exponentielle de la polarisation sous des séquences aléatoires s'écartera du taux d'erreur matériel attendu de manière statistiquement significative.",
    "D": "La discrimination d'états quantiques fournit une sécurité contre la falsification d'impulsions en préparant des paires d'états quantiques non orthogonaux qui sont optimalement distinguables sous l'implémentation d'impulsion honnête supposée, puis en mesurant la fidélité de discrimination obtenue pour détecter les déviations.",
    "solution": "B"
  },
  {
    "id": 413,
    "question": "Quelle vulnérabilité spécifique existe dans les procédures de calibration pour les portes à deux qubits ?",
    "A": "L'accumulation de phase conditionnelle pendant l'exécution de la porte provient du couplage longitudinal toujours actif entre qubits, qui provoque l'impression d'une phase par l'état du qubit de contrôle sur le qubit cible même lorsque la porte est nominalement inactive, mais cette phase est invisible dans les mesures en base Z, créant des angles morts de calibration.",
    "B": "Les erreurs de calibration du couplage paramétrique s'accumulent parce que la modulation temporelle de la fréquence du coupleur doit être précisément ajustée pour éviter les interactions ZZ résiduelles, et lorsque des dérives de calibration se produisent en raison de fluctuations de température ou de bruit de flux, la force de couplage effective dévie de la valeur cible d'une manière qui introduit des phases conditionnelles non voulues dégradant la fidélité de la porte, particulièrement dans les architectures utilisant des coupleurs ajustables où l'amplitude de la commande paramétrique contrôle directement l'hamiltonien d'interaction et même de petites erreurs de calibration peuvent provoquer des fuites vers des états non calculatoires.",
    "C": "La dépendance de l'amplitude de résonance croisée crée des erreurs de porte systématiques car l'amplitude de commande optimale pour le qubit de contrôle dépend non linéairement du désaccord et de l'état du qubit cible, et les protocoles de calibration standards qui balayent l'amplitude à un désaccord fixe ne tiennent pas compte de la façon dont les décalages dispersifs des qubits voisins altèrent dynamiquement la condition de résonance.",
    "D": "La sensibilité de mise en forme des impulsions de flux devient critique car même des distorsions mineures dans les fronts montants et descendants des impulsions de flux utilisées pour accorder les fréquences des qubits peuvent introduire des transitions non adiabatiques qui peuplent des états de fuite en dehors du sous-espace calculatoire, et ces imperfections d'impulsion sont difficiles à caractériser systématiquement car elles dépendent de la réponse complète en bande passante de l'électronique de contrôle et du câblage cryogénique, conduisant à une dérive de calibration qui se compose avec le bruit environnemental.",
    "solution": "D"
  },
  {
    "id": 414,
    "question": "Quel type de mesure peut être implémenté en utilisant une porte CNOT et un qubit ancillaire initialisé dans |0⟩ ?",
    "A": "Mesure non destructive de la parité du qubit de contrôle lorsque le contrôle fait partie d'un état intriqué plus large—le CNOT avec cible ancillaire crée une corrélation qui révèle la composante en base calculatoire du contrôle (|0⟩ ou |1⟩) par mesure de l'ancille, tout en préservant la superposition du contrôle par rapport aux autres qubits du système. La mesure est non destructive pour l'état réduit du contrôle projeté sur la base calculatoire, bien que l'information de phase globale par rapport aux autres qubits intriqués puisse être perturbée. Cette technique permet la lecture répétée de la même observable sans effondrer complètement l'état quantique multi-qubits.",
    "B": "Mesure non destructive préservant l'état du qubit de contrôle—le CNOT corrèle l'état de l'ancille avec la valeur en base calculatoire du contrôle sans perturber la superposition ou l'information de phase du contrôle. Mesurer l'ancille révèle alors si le contrôle était dans |0⟩ ou |1⟩ tout en laissant le contrôle dans son état original, permettant des mesures répétées ou des opérations quantiques ultérieures.",
    "C": "Mesure projective sur la base calculatoire qui extrait un bit d'information classique du qubit de contrôle par un processus en deux étapes : le CNOT intrique d'abord le contrôle et l'ancille dans un état de type Bell où le résultat de l'ancille est parfaitement corrélé avec la composante en base du contrôle, puis la mesure de l'ancille effectue la projection. Bien que cela semble perturber l'état du contrôle, la corrélation créée par le CNOT garantit que le contrôle reste dans l'état propre correspondant au résultat de mesure de l'ancille, implémentant effectivement une mesure en base calculatoire avec l'ancille servant de proxy de lecture plutôt que de préserver véritablement la superposition du contrôle.",
    "D": "Mesure non destructive de l'information de phase du qubit de contrôle par une technique appelée \"rétroaction de phase\" où le CNOT transfère la phase du contrôle sur l'ancille sans perturber la population du contrôle dans les états de base |0⟩ et |1⟩. Mesurer l'ancille en base X (|+⟩/|−⟩) révèle alors la phase relative entre les amplitudes en base calculatoire du contrôle tout en laissant le vecteur d'état du qubit de contrôle inchangé. Ce protocole de mesure sensible à la phase est essentiel pour les codes de correction d'erreur quantique qui doivent extraire l'information de syndrome sur les erreurs de phase sans effondrer les états de qubits logiques, c'est pourquoi les constructions CNOT-ancille forment la base des mesures de stabilisateur dans les codes de surface.",
    "solution": "B"
  },
  {
    "id": 415,
    "question": "Considérez une implémentation de solveur variationnel de valeurs propres quantiques (VQE) sur le matériel NISQ actuel où vous essayez de trouver l'état fondamental d'un hamiltonien moléculaire. Votre collègue propose d'utiliser 50 couches de portes paramétrées pour augmenter l'expressivité. Pourquoi la profondeur de circuit reste-t-elle une métrique critique même dans les algorithmes employant des couches variationnelles peu profondes ?",
    "A": "Sur les dispositifs quantiques bruités à échelle intermédiaire, les erreurs de porte à deux qubits s'accumulent multiplicativement avec la profondeur du circuit, et chaque couche supplémentaire compose les effets de décohérence dus au couplage environnemental. Puisque le matériel actuel présente typiquement des temps de cohérence de seulement 50-200 microsecondes et des fidélités de porte autour de 99-99,5%, un circuit à 50 couches accumulerait des taux d'erreur prohibitifs qui submergeraient tout signal de l'énergie de l'état fondamental moléculaire, rendant crucial de rester dans le budget de cohérence même si cela sacrifie une certaine expressivité dans votre ansatz.",
    "B": "La profondeur du circuit contrôle directement la variété atteignable dans l'espace de Hilbert à travers l'algèbre de Lie générée par les portes paramétrées, et bien que des circuits plus profonds couvrent des sous-espaces plus grands, chaque couche introduit un bruit dépolarisant qui évolue comme ε^D où ε ≈ 0,005 est l'erreur de porte moyenne et D est la profondeur. Pour 50 couches avec des portes à deux qubits dominant le budget d'erreur, l'infidélité accumulée atteint 1-(1-ε)^(50m) où m est le nombre de portes par couche, produisant typiquement des fidélités effectives inférieures à 0,1. Ce plancher de bruit dépasse les différences d'énergie d'état fondamental des systèmes moléculaires (typiquement des milliHartree), rendant les signaux d'optimisation indétectables sous les fluctuations stochastiques des imperfections matérielles.",
    "C": "Les circuits paramétrés profonds avec D couches génèrent des paysages d'optimisation présentant des plateaux arides où les gradients s'annulent exponentiellement comme O(2^(-n)) pour des systèmes à n qubits, un phénomène prouvé par McClean et al. affectant les ansätze efficaces matériellement indépendamment du bruit. Bien que 50 couches augmentent considérablement l'expressivité en principe, l'espace des paramètres devient exponentiellement plat, causant l'arrêt des optimiseurs basés sur les gradients. Combiné au bruit d'échantillonnage dû à l'échantillonnage fini (nécessitant O(1/ε²) mesures par estimation de gradient avec précision ε), l'optimisation devient calculatoirement intraitable même si le matériel était sans bruit, faisant de la profondeur un goulot d'étranglement fondamental à travers la géométrie du paysage plutôt que la décohérence seule.",
    "D": "Les algorithmes variationnels nécessitent de mesurer les valeurs d'espérance de l'hamiltonien décomposé en opérateurs de chaîne de Pauli, et des circuits plus profonds augmentent la répétition de circuit nécessaire pour atteindre la précision cible. Chaque couche supplémentaire augmente la variance de l'estimateur d'énergie par un facteur proportionnel au nombre de conditionnement de l'unitaire paramétré, causant une évolution du nombre de tirs requis pour atteindre une précision ε comme O(D²/ε²). Pour 50 couches, ce surcoût de tirs devient prohibitif même sur les simulateurs, et combiné aux temps de cohérence finis sur le matériel (limitant le débit total de mesure par fenêtre de cohérence), le temps effectif de résolution croît de manière insoutenable malgré la nature variationnelle de l'algorithme.",
    "solution": "A"
  },
  {
    "id": 416,
    "question": "Comment le modèle sp-QCNN gère-t-il les symétries au-delà de la symétrie translationnelle ?",
    "A": "En implémentant des couches quantiques équivariantes via des générateurs d'algèbre de Lie qui commutent avec les opérateurs de symétrie du hamiltonien, permettant au circuit variationnel de préserver les contraintes de théorie des groupes pendant l'optimisation. Cependant, cela restreint l'architecture aux symétries continues (SO(n), SU(n)) car les symétries discrètes nécessitent des représentations projectives incompatibles avec les décompositions standard de portes paramétrées.",
    "B": "En encodant les symétries générales via une approche basée sur la théorie des groupes qui associe les éléments de groupe à des transformations unitaires sur l'espace d'états quantiques, permettant à l'architecture du circuit de respecter des groupes de symétrie finis arbitraires au-delà des simples translations par un choix approprié de portes paramétriques.",
    "C": "À travers des opérations de pooling conscientes des symétries qui appliquent des unitaires contrôlés associant les orbites de symétrie aux états de base computationnelle, permettant au réseau d'éliminer les degrés de liberté redondants. Ce pooling géométrique réduit la dimension effective de l'espace de Hilbert d'un facteur égal à l'ordre du groupe de symétrie, mais nécessite que la symétrie soit abélienne afin que les représentants d'orbite puissent être identifiés de manière unique.",
    "D": "En augmentant l'ensemble d'entraînement avec des copies transformées par le groupe des états d'entrée et en moyennant la fonction de perte sur l'orbite de symétrie pendant la rétropropagation, imposant effectivement que le circuit quantique appris commute avec toutes les opérations du groupe. Cette stratégie d'augmentation de données fonctionne pour tout groupe de symétrie fini mais introduit un surcoût évoluant comme |G|², limitant l'applicabilité pratique aux petits groupes.",
    "solution": "B"
  },
  {
    "id": 417,
    "question": "Quelle propriété de l'encapsulation de clés basée sur les réseaux en fait un remplacement direct des étiquettes d'authentification classiques dans les pipelines de post-traitement QKD ?",
    "A": "Les cryptosystèmes ring-learning-with-errors sont spécifiquement conçus avec des ensembles de paramètres qui présentent des caractéristiques de tolérance au bruit correspondant aux taux d'erreur de bits quantiques typiques de 1 à 5% observés dans les canaux pratiques de distribution quantique de clés. Contrairement aux schémas d'authentification traditionnels qui nécessitent une communication classique sans erreur, les MAC basés sur RLWE peuvent vérifier l'intégrité des messages même lorsque le canal sous-jacent introduit des inversions de bits stochastiques, les rendant particulièrement adaptés au canal latéral classique bruité qui accompagne la QKD. Cette résilience aux erreurs intégrée élimine le besoin de correction d'erreurs séparée avant l'authentification, simplifiant le pipeline de post-traitement.",
    "B": "La structure algébrique hash-and-sign sous-jacente à l'encapsulation de clés basée sur les réseaux permet la génération déterministe d'étiquettes d'authentification en hachant le matériel de clé réconcilié et en le signant avec la clé privée du réseau. Ce déterminisme est essentiel pour le post-traitement QKD car les deux parties doivent calculer indépendamment des étiquettes identiques à partir de leurs clés brutes corrélées sans tours de communication supplémentaires. Contrairement aux schémas de signature probabilistes qui nécessitent une nouvelle randomisation et une synchronisation, la propriété déterministe garantit que les étiquettes d'Alice et Bob correspondront chaque fois que leurs clés corrigées d'erreurs concordent, fournissant une vérification d'authentification immédiate sans protocoles interactifs.",
    "C": "Des graines aléatoires uniformes courtes produisent des MAC dont le coût de vérification est quasi-linéaire en longueur de clé, permettant une authentification efficace des longues chaînes de bits générées pendant le post-traitement QKD sans le surcoût computationnel quadratique qui dominerait autrement le temps de traitement. Cette efficacité provient des opérations structurées sur les réseaux qui permettent l'expansion de la graine en étiquettes d'authentification complètes via de l'arithmétique polynomiale rapide dans les anneaux.",
    "D": "Les mécanismes d'encapsulation basés sur les réseaux NTRU génèrent des chiffrés avec des représentations remarquablement compactes, typiquement 700-800 octets pour une sécurité de 128 bits, ce qui est significativement plus petit que les signatures de 1-2 KB produites par les schémas de courbes elliptiques qui seraient autrement vulnérables à l'algorithme de Shor. Cet avantage de taille devient critique dans le post-traitement QKD où des milliers d'étiquettes d'authentification doivent être échangées pendant l'amplification de confidentialité, et la consommation réduite de bande passante des chiffrés NTRU permet au surcoût d'authentification de rester inférieur à 5% du matériel de clé brut. La compacité découle de la structure annulaire de NTRU permettant un emballage plus dense d'informations de sécurité comparé aux schémas génériques de réseaux.",
    "solution": "C"
  },
  {
    "id": 418,
    "question": "Dans le contexte d'architectures quantiques tolérantes aux fautes avec connectivité limitée des qubits, expliquez pourquoi la téléportation de porte représente un compromis de ressources fondamentalement différent comparé au routage conventionnel basé sur SWAP. Considérez à la fois le rôle de l'intrication pré-distribuée et la tolérance à l'aléa induit par les mesures dans votre réponse.",
    "A": "La téléportation de porte exploite des paires intriquées pré-partagées et une communication de type feed-forward classique pour implémenter des portes à deux qubits non-locales sans nécessiter de connectivité physique, échangeant des ressources d'intrication préparées et la tolérance à l'aléa induit par les mesures contre une profondeur de circuit cohérent réduite, tandis que le routage basé sur SWAP ajoute des couches de portes cohérentes qui accumulent de la décohérence mais évite de consommer des états auxiliaires intriqués ou d'introduire des résultats de mesure stochastiques jusqu'à la lecture finale.",
    "B": "La téléportation de porte utilise des paires de Bell pré-distribuées pour remplacer des chaînes SWAP multi-sauts par des opérations non-locales en une seule étape, échangeant des états de ressources intriquées et la latence de communication classique contre une profondeur cohérente réduite—cependant, l'effondrement induit par la mesure introduit un bruit de projection fondamentalement irréversible qui se propage à travers les portes suivantes comme des erreurs de déphasage, tandis que le routage SWAP maintient une cohérence quantique complète en appliquant uniquement des transformations unitaires, rendant la téléportation inadaptée aux circuits corrigés d'erreurs où l'extraction de syndrome repose sur des mesures de stabilisateur réversibles.",
    "C": "La distinction clé est que la téléportation de porte consomme des paires d'ancillas intriquées pré-générées pour réaliser des opérations non-locales via des mesures locales et des corrections de Pauli, réduisant la profondeur du circuit au prix d'un surcoût d'ancillas et acceptant l'aléa induit par les mesures dans les angles de correction, tandis que le routage basé sur SWAP préserve des séquences de portes déterministes en déplaçant physiquement les qubits à travers des chaînes d'impulsions cohérentes—mais la téléportation suppose incorrectement que le retour de phase induit par la mesure de la paire de Bell n'affecte pas les portes suivantes, ce qui n'est valide que lorsque la porte téléportée commute avec toutes les opérations en aval dans le graphe de dépendances.",
    "D": "La téléportation de porte exploite des paires EPR pré-partagées pour exécuter des unitaires non-locaux à travers des mesures locales et des corrections conditionnelles, acceptant des résultats de mesure stochastiques qui doivent être suivis classiquement et compensés via des mises à jour du référentiel de Pauli, échangeant ainsi la consommation d'intrication et le surcoût de feedforward classique contre une profondeur de circuit réduite, tandis que le routage SWAP applique des séquences de portes cohérentes déterministes qui relocalisent physiquement les qubits le long du graphe de connectivité, accumulant de la décohérence proportionnelle à la distance de routage mais ne nécessitant aucun qubit auxiliaire ni correction de mesure jusqu'à la lecture finale.",
    "solution": "A"
  },
  {
    "id": 419,
    "question": "Quel est le fondement théorique de l'avantage quantique potentiel dans les méthodes d'apprentissage automatique basées sur les noyaux ?",
    "A": "La combinaison synergique du calcul efficace de noyaux et d'espaces de caractéristiques exponentiellement grands fonctionne ensemble pour surpasser les méthodes classiques à la fois en temps d'exécution et en puissance de représentation simultanément.",
    "B": "L'intrication crée des espaces de caractéristiques qui sont fondamentalement plus riches et plus expressifs que ce qu'aucun noyau classique ne peut accéder, car les qubits intriqués couvrent des dimensions d'espace de Hilbert qui croissent exponentiellement avec la taille du système. Même si un ordinateur classique pouvait d'une manière ou d'une autre calculer rapidement des entrées de noyau individuelles, la capacité de représentation de la carte de caractéristiques quantiques elle-même — déterminée par la façon dont les points de données sont corrélés à travers des états de base intriqués — dépasse ce que les caractéristiques classiques séparables peuvent encoder, donnant aux noyaux quantiques un avantage d'expressivité inhérent indépendamment du temps d'exécution computationnel.",
    "C": "Évaluer plusieurs entrées de noyau à la fois via la superposition permet la construction de la matrice de Gram complète en temps poly-logarithmique, car chaque paire de points de données peut être comparée en parallèle sur tous les qubits simultanément. En encodant l'ensemble de données dans un registre quantique et en appliquant un unitaire global qui calcule les produits scalaires de manière cohérente, vous contournez la mise à l'échelle quadratique de l'assemblage classique de matrice de noyau, extrayant toutes les similarités par paires à travers un seul processus de mesure qui échantillonne l'ensemble de la structure en une fois.",
    "D": "Calculer efficacement des fonctions de noyau qui sont exponentiellement difficiles classiquement, où les circuits quantiques évaluent des produits scalaires dans des espaces de caractéristiques quantiques à travers l'interférence et l'intrication en temps polynomial dans le nombre de qubits, tandis que tout algorithme classique tentant le même calcul nécessiterait des ressources exponentielles pour simuler les corrélations d'espace de Hilbert de haute dimension.",
    "solution": "D"
  },
  {
    "id": 420,
    "question": "Dans le contexte du calcul quantique tolérant aux fautes, considérez un code de Steane [[7,1,3]] soumis à deux modèles d'erreur différents : l'un où les mesures de syndrome sont parfaites mais les qubits physiques subissent du bruit de dépolarisation entre les cycles de correction, et un autre où l'extraction de syndrome elle-même a 1% de chance de produire un résultat défectueux tandis que les erreurs de porte physique restent identiques. Les expérimentateurs rapportent souvent deux valeurs de seuil distinctes lors de la caractérisation de tels scénarios. Pourquoi les seuils de \"capacité de code\" sont-ils distincts des seuils \"phénoménologiques\", et quelle hypothèse fondamentale sépare ces deux benchmarks dans l'analyse de la performance de correction d'erreurs quantiques ?",
    "A": "L'analyse de capacité de code traite les mesures comme parfaites et considère uniquement les erreurs de stockage sur les qubits de données, donnant une borne supérieure sur le seuil atteignable ; les modèles phénoménologiques ajoutent des mesures de syndrome défectueuses, qui introduisent des chaînes d'erreurs corrélées qui se propagent à travers les cycles de correction et abaissent le seuil pratique que vous observerez dans du matériel réel où la préparation des ancillas, les portes à deux qubits pendant l'extraction de syndrome, et la lecture échouent tous à des taux non nuls.",
    "B": "Les seuils de capacité de code supposent une extraction de syndrome instantanée avec des mesures parfaites, modélisant uniquement la décohérence des qubits de données entre les cycles, tandis que les seuils phénoménologiques incorporent des erreurs de mesure qui créent une ambiguïté de syndrome nécessitant une corrélation temporelle sur plusieurs cycles pour décoder correctement. Cependant, les modèles phénoménologiques traitent encore la préparation des ancillas et les portes de syndrome à deux qubits comme parfaites, ne tenant compte que des erreurs classiques d'inversion de bits dans les résultats de mesure eux-mêmes, ce qui signifie que le seuil phénoménologique dépasse en réalité la capacité de code dans la pratique lorsque les erreurs de porte pendant l'extraction annulent partiellement les erreurs de stockage à travers des corrélations d'erreurs fortuites.",
    "C": "Les seuils diffèrent car les modèles de capacité de code supposent des canaux d'erreur de Pauli qui préservent la structure de stabilisateur, donnant des bornes de seuil serrées via la programmation linéaire sur l'espace de syndrome, tandis que les seuils phénoménologiques doivent tenir compte des erreurs non-Pauli introduites par des mesures imparfaites, spécifiquement l'amortissement d'amplitude pendant la lecture qui décohère partiellement l'information de syndrome. Cela fait que l'analyse phénoménologique nécessite une évolution complète de matrice de densité, mais les deux modèles convergent lorsque la fidélité de mesure de syndrome dépasse 99% car les erreurs de lecture contribuent alors des corrections sous-dominantes au calcul de seuil.",
    "D": "Les seuils de capacité de code s'appliquent lorsque la lecture de syndrome est instantanée et sans bruit, capturant uniquement les erreurs de données inter-cycles, tandis que les modèles phénoménologiques ajoutent des échecs de mesure de syndrome qui causent des erreurs de décodeur mais supposent encore que le circuit d'extraction de syndrome lui-même—portes d'ancilla, opérations CNOT, et mesures—s'exécute parfaitement hormis le résultat binaire étant incorrect. L'écart entre les seuils émerge car les mesures de syndrome répétées sous le modèle phénoménologique accumulent des erreurs corrélées à travers les cycles que le décodeur doit suivre temporellement, réduisant la distance de code effective comparée à l'hypothèse de capacité de code de détection d'erreur immédiate.",
    "solution": "A"
  },
  {
    "id": 421,
    "question": "Pourquoi l'adaptation des techniques classiques de correction d'erreurs à l'informatique quantique est-elle particulièrement difficile ?",
    "A": "Les codes de correction d'erreurs quantiques exigent que les qubits physiques maintiennent des temps de cohérence qui s'étendent au-delà de la durée du cycle de correction d'erreurs, ce que les implémentations expérimentales actuelles ne peuvent réaliser qu'à des températures approchant le zéro absolu où les fluctuations thermiques deviennent négligeables. À des températures plus élevées, les excitations thermiques introduisent des erreurs plus rapidement que les codes de correction ne peuvent les détecter et les corriger, créant une barrière de température fondamentale qui rend la correction d'erreurs quantiques à température ambiante théoriquement impossible selon le principe de Landauer appliqué à l'information quantique.",
    "B": "Les qubits existent dans des états de superposition où ils représentent simultanément plusieurs motifs d'erreurs classiques, ce qui signifie que la mesure conventionnelle du syndrome d'erreur effondrerait l'état quantique et détruirait l'information même que nous essayons de protéger. De plus, la nature continue des erreurs quantiques (rotations arbitraires sur la sphère de Bloch) contraste fortement avec les erreurs discrètes de basculement de bit dans les systèmes classiques, nécessitant des stratégies de détection et de correction fondamentalement différentes qui doivent tenir compte d'une infinité d'orientations d'erreurs possibles plutôt que de seulement deux.",
    "C": "Le théorème de non-clonage empêche la duplication directe de l'information quantique, rendant infaisable la correction d'erreurs traditionnelle basée sur la redondance. Contrairement aux systèmes classiques où les bits peuvent être librement copiés pour créer des encodages redondants, les états quantiques ne peuvent pas être clonés, nécessitant des approches fondamentalement différentes comme la mesure de syndrome et les codes stabilisateurs qui extraient l'information d'erreur sans détruire la superposition quantique protégée.",
    "D": "Le processus de correction d'erreurs quantiques doit traiter simultanément tous les types d'erreurs possibles — basculements de bit, basculements de phase et leurs combinaisons — en une seule étape de correction, car la correction séquentielle de différents types d'erreurs nécessiterait plusieurs opérations de mesure qui effondreraient chacune l'état quantique. Cette impossibilité théorique découle du postulat de mesure de la mécanique quantique, qui interdit l'extraction d'information sur plusieurs observables non commutatives sans perturber fondamentalement le système, faisant de la correction parallèle de tous les types d'erreurs une barrière insurmontable.",
    "solution": "C"
  },
  {
    "id": 422,
    "question": "Quelle approche technique fournit les garanties de sécurité les plus fortes pour l'échange de clés authentifié par mot de passe résistant aux attaques quantiques ?",
    "A": "Les protocoles PAKE basés sur les réseaux euclidiens atteignent des réductions de sécurité strictes vers des problèmes difficiles comme Learning With Errors, fournissant une résistance prouvable contre les adversaires quantiques avec une perte de sécurité minimale dans la réduction, et supportant des implémentations efficaces grâce aux réseaux structurés en anneaux.",
    "B": "Les protocoles de transfert inconscient basés sur les codes exploitent la difficulté du décodage de syndrome dans les codes linéaires aléatoires pour permettre l'échange de clés authentifié par mot de passe avec des garanties de sécurité théorique de l'information. En encodant le mot de passe comme un syndrome et en exigeant que les deux parties résolvent un problème de décodage à distance bornée, ces schémas garantissent que même un adversaire quantique avec une puissance de calcul illimitée ne peut extraire la clé partagée sans connaissance du mot de passe, les rendant supérieurs aux hypothèses de difficulté computationnelle.",
    "C": "Les preuves à divulgation nulle de connaissance avec des hypothèses de difficulté post-quantiques permettent la vérification du mot de passe sans révéler le mot de passe lui-même, permettant aux deux parties de s'authentifier et d'établir des clés tout en maintenant la sécurité même contre des adversaires quantiques qui peuvent casser les hypothèses traditionnelles de logarithme discret.",
    "D": "Les schémas d'engagement par hachage combinés avec des fonctions d'extraction d'entropie résistantes aux attaques quantiques fournissent la sécurité PAKE la plus forte en forçant les deux parties à s'engager sur leurs hachages de mot de passe avant tout échange de matériel cryptographique.",
    "solution": "C"
  },
  {
    "id": 423,
    "question": "Quelle méthodologie d'attaque spécifique menace les extensions DNS sécurisées post-quantiques ?",
    "A": "L'empoisonnement de cache quantique via des algorithmes de prédiction de réponse exploite le fait que les résolveurs DNS doivent accepter les réponses dans une fenêtre de temps limitée, et les ordinateurs quantiques peuvent utiliser l'amplification d'amplitude pour tester simultanément tous les identifiants de transaction et numéros de port possibles, trouvant une contrefaçon valide en un temps proportionnel à la racine quatrième de l'espace de recherche plutôt que de nécessiter une recherche exhaustive classique.",
    "B": "L'énumération de zone accélérée par la recherche de Grover permet aux attaquants de découvrir tous les noms d'hôtes dans une zone DNS exponentiellement plus rapidement que les attaques de parcours classiques en interrogeant une superposition de noms de sous-domaines possibles et en mesurant lesquels retournent des enregistrements NSEC ou NSEC3 valides. Même lorsque NSEC3 utilise des fonctions de hachage post-quantiques, l'accélération quadratique de l'algorithme de Grover réduit la sécurité effective en bits.",
    "C": "Les collisions de hachage NSEC3 trouvées en utilisant des algorithmes quantiques comme la recherche de Grover, qui peut trouver des préimages ou des secondes préimages avec une accélération quadratique, compromettant potentiellement le mécanisme de déni d'existence authentifié même lorsque des schémas de signature post-quantiques protègent les enregistrements de zone eux-mêmes.",
    "D": "La compromission de clés DNSSEC par des attaques de réduction de réseau euclidien peut casser les hypothèses cryptographiques sous-jacentes même dans les schémas post-quantiques si les paramètres sont choisis incorrectement, particulièrement lorsque les implémenteurs sous-estiment le niveau de sécurité concret nécessaire pour résister à la réduction de base de réseau améliorée quantiquement. Spécifiquement, si les clés DNSSEC sont générées en utilisant des signatures basées sur les réseaux avec un module q et une distribution de bruit σ choisis pour fournir seulement 128 bits de sécurité classique.",
    "solution": "C"
  },
  {
    "id": 424,
    "question": "Quelle est la stratégie centrale derrière les passes de transpilation adaptatives au bruit ?",
    "A": "Interroger les données de calibration pour privilégier les qubits et coupleurs de haute fidélité lors du mapping. En examinant les mesures récentes de caractérisation du dispositif — incluant les taux d'erreur de porte, les temps de cohérence et les fidélités de lecture — le transpilateur assigne dynamiquement les qubits logiques aux qubits physiques et sélectionne les chemins de couplage qui minimisent l'erreur de circuit attendue, adaptant la stratégie de compilation à l'état actuel du dispositif plutôt que de traiter toutes les ressources matérielles comme équivalentes.",
    "B": "Exploiter les métriques de calibration en temps réel pour construire un graphe de connectivité pondéré où les coûts des arêtes reflètent les erreurs actuelles de portes à deux qubits et les coûts des nœuds encodent les limites de cohérence à un qubit. Le transpilateur résout alors un problème de routage de poids minimal qui assigne les qubits logiques aux emplacements physiques en minimisant l'erreur totale attendue, tout en optimisant simultanément l'insertion de SWAP pour éviter les coupleurs à haute erreur. Ce mapping conscient du dispositif utilise directement les données mesurées de T1, T2 et de fidélité de porte plutôt que de supposer l'homogénéité du matériel.",
    "C": "Incorporer les données de caractérisation du dispositif dans un modèle de bruit bayésien qui prédit la fidélité de circuit attendue sous différentes assignations de qubits, puis utiliser le recuit simulé pour explorer l'espace de mapping et converger vers des placements de qubits qui maximisent la probabilité de succès globale. En traitant les erreurs de porte, les erreurs de lecture et les temps de cohérence comme des variables aléatoires corrélées apprises à partir d'exécutions de calibration, le transpilateur s'adapte à la dérive temporelle des performances du dispositif et route préférentiellement à travers les régions matérielles actuellement performantes.",
    "D": "Utiliser des balayages de calibration récents pour identifier les qubits et portes fonctionnant actuellement au-dessus de leurs seuils d'erreur spécifiés, puis remapper dynamiquement le circuit pour exclure ces ressources dégradées de la cible de compilation. Le transpilateur interroge les métriques de dispositif en direct pendant la phase de mapping et applique un algorithme de satisfaction de contraintes garantissant qu'aucun qubit logique n'est assigné à un qubit physique avec T1 inférieur à 50 μs ou erreur de porte supérieure à 0,5%, créant effectivement un masque matériel adaptatif qui reflète la santé instantanée du dispositif.",
    "solution": "A"
  },
  {
    "id": 425,
    "question": "Quel protocole avancé fournit la sécurité la plus forte pour les schémas d'engagement quantiques ?",
    "A": "L'engagement de chaîne quantique sous modèles de stockage borné exploite la contrainte physique qu'un adversaire ne peut pas stocker de manière cohérente des états quantiques arbitrairement grands, typiquement borné par des estimations réalistes de capacité de mémoire quantique réalisable (par exemple, 10^9 qubits maintenus de manière cohérente pendant la durée du protocole). Le protocole transmet un flux à haut débit d'états quantiques — dépassant largement la capacité de stockage de l'adversaire — qui encode la chaîne engagée à travers un code correcteur d'erreurs quantique. Le récepteur doit effectuer des mesures sensibles au temps et stocker uniquement des syndromes classiques, tandis que l'engageur conserve suffisamment d'information quantique pour révéler ultérieurement la chaîne. La sécurité dérive d'arguments théoriques de l'information montrant que tout adversaire avec un stockage en dessous du seuil du protocole ne peut distinguer la chaîne engagée de données aléatoires.",
    "B": "Les protocoles standard d'engagement de bit quantique atteignent une sécurité inconditionnelle lorsqu'ils sont augmentés d'une phase de configuration de confiance, spécifiquement à travers une intrication pré-partagée entre l'engageur et le récepteur qui a été vérifiée à travers plusieurs tours de tests d'inégalité de Bell. Les paires intriquées, typiquement distribuées comme des singulets EPR, servent de ressource cryptographique qui lie l'engagement tout en empêchant à la fois la révélation prématurée et les changements post-engagement. En effectuant des mesures locales sur leurs moitiés respectives selon un protocole pré-convenu, l'engageur peut encoder la valeur du bit d'une manière qui devient verrouillée de manière théorique de l'information une fois les choix de mesure effectués. L'hypothèse de configuration de confiance est considérée acceptable dans les contextes cryptographiques pratiques.",
    "C": "L'engagement de bit quantique sensible à la tricherie représente un changement de paradigme en reconnaissant que la sécurité parfaite contre toutes les stratégies de tricherie est impossible en raison du théorème d'impossibilité de Mayers-Lo-Chau, mais en concevant plutôt des protocoles où toute tentative de tricherie laisse nécessairement des traces détectables dans le canal quantique. Le protocole encode le bit engagé dans un état quantique occupant un sous-espace spécifique de l'espace de Hilbert joint de plusieurs qubits, de sorte que toute tentative d'extraire de l'information prématurément ou de changer l'engagement rétroactivement nécessite des mesures ou transformations unitaires qui perturbent inévitablement des quantités observables. L'analyse statistique des taux d'erreur dans les tours de vérification ultérieurs peut alors révéler les tentatives de tricherie avec une grande confiance.",
    "D": "L'engagement de bit relativiste exploitant la contrainte fondamentale que l'information ne peut voyager plus vite que la lumière pour imposer les propriétés de liaison et de dissimulation.",
    "solution": "D"
  },
  {
    "id": 426,
    "question": "Quel protocole avancé offre la sécurité la plus forte pour la distribution quantique de clés sur de très longues distances ?",
    "A": "La distribution quantique de clés à champ jumeau (twin-field) offre une sécurité robuste sur de longues distances en faisant envoyer par Alice et Bob des impulsions cohérentes faibles vers une station relais non fiable positionnée au point médian, où l'interférence à un photon est mesurée sans révéler quelle partie a envoyé quel photon.",
    "B": "La distribution quantique de clés par satellite offre une sécurité supérieure sur de longues distances en exploitant le quasi-vide de l'espace pour minimiser la perte de photons et la décohérence sur des distances de milliers de kilomètres. En établissant des liaisons optiques entre des stations au sol et des satellites en orbite terrestre basse lors de brefs passages au-dessus, cette approche contourne l'atténuation exponentielle qui afflige les systèmes à fibres optiques, avec l'avantage supplémentaire que la turbulence atmosphérique n'affecte que les derniers kilomètres de transmission. Les canaux quantiques en espace libre à travers l'espace atteignent des taux de perte effectifs inférieurs à 5 dB même pour des distances intercontinentales.",
    "C": "La distribution quantique de clés indépendante du dispositif de mesure (MDI-QKD) offre une sécurité robuste sur de longues distances car elle élimine tous les canaux auxiliaires des détecteurs et les attaques par cheval de Troie en plaçant l'appareil de mesure dans un emplacement non fiable. Les deux parties communicantes préparent des paires de photons intriqués et envoient un photon de chaque paire à une station de mesure centrale, qui effectue des mesures d'état de Bell sans rien apprendre sur la clé.",
    "D": "Les répéteurs quantiques basés sur l'intrication offrent la sécurité la plus forte en établissant l'intrication entre des nœuds distants par échange et purification d'intrication, permettant une distribution de clés qui évolue favorablement avec la distance tout en maintenant une sécurité inconditionnelle grâce à la monogamie de l'intrication — garantissant qu'aucun espion ne peut partager les corrélations quantiques.",
    "solution": "D"
  },
  {
    "id": 427,
    "question": "Pourquoi l'estimation de la variance est-elle cruciale avant de s'engager dans une stratégie de découpage ?",
    "A": "L'estimation de la variance permet au protocole d'atténuation des erreurs d'allouer de manière optimale les ressources en qubits entre les fragments de circuit, car la fidélité de reconstruction de chaque fragment dépend de la pondération par l'inverse de la variance de ses observables échantillonnées — si la variance est sous-estimée, la recombinaison pondérée amplifiera le bruit des fragments à forte variance, corrompant la valeur d'espérance finale et annulant les bénéfices de réduction de profondeur du découpage.",
    "B": "Une prédiction précise de la variance vous permet de calculer le coût total d'échantillonnage pour le protocole de reconstruction par découpage, garantissant que l'expérience reste faisable dans les limites de votre budget de mesures et de vos contraintes de temps d'exécution avant d'investir des ressources dans la décomposition du circuit et l'exécution des fragments.",
    "C": "La pré-analyse de la variance détermine le nombre minimum de bases de mesure parallèles requises pour chaque fragment, car la décomposition observable en sommes de Pauli doit tenir compte de la propagation de la variance à travers les distributions de quasi-probabilité utilisées dans la reconstruction par découpage — sous-estimer la variance conduit à une couverture de base insuffisante, causant un biais systématique dans la valeur d'espérance reconstruite qui ne peut pas être corrigé après mesure.",
    "D": "La caractérisation de la variance définit les hyperparamètres de l'ordonnanceur de fragments, car les observables à forte variance nécessitent des créneaux d'exécution prioritaires avec une décohérence minimale induite par la file d'attente pour maintenir l'efficacité des mesures — si la variance est mal évaluée, l'ordonnanceur assigne des créneaux de faible priorité aux fragments critiques, augmentant exponentiellement le surcoût d'échantillonnage nécessaire pour atteindre la précision cible dans l'observable final reconstruit.",
    "solution": "B"
  },
  {
    "id": 428,
    "question": "Une marche quantique en temps continu peut parfois être simulée plus rapidement qu'une marche discrète car le temps continu :",
    "A": "Permet des raccourcis de diagonalisation analytique via la décomposition spectrale d'hamiltoniens de graphes creux, car lorsque l'hamiltonien de marche H est le laplacien du graphe ou la matrice d'adjacence d'un graphe hautement symétrique (comme les hypercubes, les graphes complets ou les graphes circulants), ses valeurs propres et vecteurs propres peuvent être calculés en forme fermée en utilisant la théorie des représentations du groupe d'automorphismes du graphe, évitant complètement le besoin d'approximation de Trotter numérique. Spécifiquement, pour un graphe à n sommets avec une représentation irréductible de dimension d, l'opérateur d'évolution e^(-iHt) se décompose en d exponentielles bloc-diagonales indépendantes qui peuvent être implémentées en utilisant seulement O(d log n) portes plutôt que les O(n²) portes requises pour la simulation hamiltonienne générale, et ces blocs correspondent à des marches sur des graphes quotients obtenus par réduction de symétrie. La formulation continue est essentielle ici car les marches en temps discret introduisent un opérateur pièce qui brise les symétries du graphe, forçant l'utilisation de la trotterisation complète et éliminant la structure bloc-diagonale qui permet les raccourcis analytiques, nécessitant finalement des circuits dont la profondeur évolue linéairement avec le temps de simulation au lieu de logarithmiquement.",
    "B": "Évite le branchement stochastique dans les chemins d'évolution d'amplitude, car les marches quantiques en temps discret nécessitent à chaque étape un choix d'opérateur pièce (Hadamard, Grover ou pièce DFT) dont l'action crée une superposition de branchement sur toutes les directions suivantes possibles pondérées par les éléments de matrice de la pièce, simulant effectivement un arbre de chemins d'amplitude exponentiellement nombreux qui doivent être suivis de manière cohérente. En revanche, l'évolution en temps continu sous l'hamiltonien de graphe H génère une trajectoire déterministe unique dans l'espace de Hilbert régie par l'équation de Schrödinger iℏ(d|ψ⟩/dt) = H|ψ⟩, qui peut être discrétisée en étapes de Trotter de taille uniforme sans introduire de branchement, car chaque tranche de temps infinitésimale fait avancer l'état par un unitaire fixe e^(-iHδt) qui applique les mêmes opérations locales à tous les sommets simultanément. Cette élimination du branchement réduit la profondeur du circuit de O(T·d) pour une marche discrète sur un graphe de degré d sur T étapes à O(T·polylog(n)) pour la simulation en temps continu via la décomposition de Trotter-Suzuki sur un graphe à n sommets, car la structure de localité de l'hamiltonien (chaque sommet se couple à au plus d voisins) peut être exploitée pour paralléliser les couches de Trotter, bien que cet avantage ne se matérialise que lorsque d = o(n^(1/3)) en raison des contraintes de routage sur les réseaux planaires de qubits.",
    "C": "Exploite la sparsité hamiltonienne pour l'efficacité de la décomposition de Trotter, où l'hamiltonien de graphe H se décompose naturellement en une somme de termes locaux commutant ou quasi-commutant correspondant à des arêtes individuelles, permettant des formules de Trotter de premier ordre comme e^(-iHt) ≈ ∏ⱼ e^(-iHⱼt) avec une erreur évoluant comme O(t²||[Hⱼ, Hⱼ′]||), ce qui pour les graphes de degré borné donne des implémentations de circuit hautement parallélisables. Contrairement aux marches discrètes qui nécessitent un opérateur pièce global intriquant le registre de position avec un qubit auxiliaire à chaque pas de temps (créant une profondeur de circuit linéaire à la fois dans le nombre d'étapes et le degré du graphe), les marches en temps continu permettent une décomposition locale où le terme hamiltonien Hⱼ = |u⟩⟨v| + |v⟩⟨u| de chaque arête est implémenté par une seule porte à deux qubits entre qubits adjacents dans le registre, et les termes correspondant à des arêtes disjointes peuvent être appliqués en parallèle. Pour un graphe de degré maximum d et n sommets, une marche discrète de T étapes nécessite une profondeur O(T·d) avec Ω(T·n) portes totales, tandis que la simulation en temps continu via Trotter atteint une profondeur O((dt/ε)·log(n)) avec surcoût de routage, où ε est la précision souhaitée et le facteur logarithmique provient des réseaux SWAP sur des architectures à connectivité restreinte.",
    "D": "Ne nécessite pas de registre pièce, réduisant substantiellement la largeur du circuit en éliminant l'espace de qubit auxiliaire nécessaire pour implémenter l'opérateur pièce qui régit les probabilités de transition à chaque étape dans les formulations en temps discret. Cette simplification architecturale réduit le nombre total de qubits de n+log(d) à seulement n pour un graphe à n sommets de degré maximum d, ce qui se traduit directement par des circuits de profondeur plus courte car moins d'opérations SWAP sont nécessaires pour le routage des qubits sur du matériel à connectivité limitée, et l'absence de portes d'intrication de tirage de pièce signifie que le circuit global comprend principalement des termes d'évolution hamiltonienne locaux qui peuvent être parallélisés plus efficacement lors de la compilation.",
    "solution": "D"
  },
  {
    "id": 429,
    "question": "Quel vecteur d'attaque cible spécifiquement le domaine fréquentiel des signaux de contrôle quantique ?",
    "A": "L'exploitation de fuite de bande latérale tire parti du filtrage imparfait dans le matériel de contrôle, où le processus de modulation utilisé pour générer des impulsions façonnées crée nécessairement des composantes fréquentielles en dehors de la bande porteuse prévue, et ces bandes latérales peuvent se coupler à des transitions non intentionnelles dans le qubit ou son environnement. Un adversaire qui peut injecter un signal à une fréquence de bande latérale peut effectivement se greffer sur le système de contrôle, induisant des erreurs de porte corrélées entre qubits car ils partagent des oscillateurs communs, créant ainsi un chemin vers à la fois la fuite d'état et l'amplification de diaphonie qui ne serait pas visible dans les simulations supposant des filtres passe-bande idéaux.",
    "B": "La manipulation de dérive de fréquence fonctionne en décalant subtilement la condition de résonance des qubits cibles par des perturbations environnementales — telles que des variations de champ magnétique ou des gradients de température — de sorte que les impulsions de contrôle, qui sont accordées à une fréquence fixe, deviennent progressivement désaccordées au fil du temps. En induisant des dérives lentes de l'ordre de dizaines de kHz par heure, un attaquant peut faire accumuler aux portes calibrées des erreurs de phase qui se composent multiplicativement à travers un calcul, et parce que la dérive est souvent confondue avec du bruit environnemental bénin, elle peut rester non détectée jusqu'à ce que la fidélité se dégrade en dessous du seuil, moment auquel le calcul est déjà compromis et les efforts de recalibration peuvent être vains si la source de dérive est contrôlée extérieurement.",
    "C": "L'injection spectrale implique l'introduction de signaux électromagnétiques soigneusement conçus à des fréquences qui chevauchent ou se trouvent adjacentes au spectre d'impulsion de contrôle, permettant à un attaquant d'interférer avec les opérations du qubit soit en amplifiant les tons de contrôle existants, soit en introduisant des excitations parasites qui causent des rotations non intentionnelles ou des transferts de population dans le sous-espace de calcul.",
    "D": "Les attaques de corruption de résonance exploitent le fait que les systèmes quantiques ont des spectres en échelle avec de multiples fréquences de transition, et si un adversaire peut injecter un signal près d'un état excité supérieur ou de la transition d'un qubit auxiliaire, il peut peupler ces niveaux même lorsque le sous-espace de calcul est nominalement protégé par de grands désaccords. Une fois que la population fuit dans ces états non intentionnels, elle ne revient pas immédiatement car les temps de relaxation sont longs, et les impulsions de contrôle suivantes conçues pour une dynamique à deux niveaux auront des effets imprévisibles sur la matrice densité contaminée. L'attaque est particulièrement insidieuse dans les systèmes avec des spectres encombrés, tels que les qubits transmon ou les ions piégés avec de longues chaînes, où même une excitation faible hors résonance peut semer des erreurs qui se propagent de manière cohérente à travers le système par des interactions d'échange ou des modes de mouvement partagés.",
    "solution": "C"
  },
  {
    "id": 430,
    "question": "Comment la dégénérescence des codes stabilisateurs aide-t-elle à supprimer les erreurs logiques au-delà des prédictions basées uniquement sur la distance ?",
    "A": "Dans les codes stabilisateurs avec des opérateurs logiques dégénérés, chaque sous-espace dégénéré distinct héberge un qubit logique indépendant encodé avec sa propre table de syndrome séparée, multipliant effectivement la capacité du code. Parce que les erreurs au sein d'un secteur dégénéré ne peuvent pas se propager pour affecter les qubits dans d'autres secteurs, le code réalise une réduction exponentielle des erreurs de diaphonie proportionnelle au nombre de sous-espaces dégénérés. Cette compartimentation signifie que même si plusieurs erreurs se produisent simultanément, elles sont isolées dans leurs secteurs respectifs et peuvent être corrigées indépendamment sans interférence.",
    "B": "Lorsqu'un code stabilisateur présente une dégénérescence, la mesure répétée des générateurs de stabilisateurs devient inutile car la structure dégénérée elle-même fournit une information d'erreur implicite à travers le chevauchement des sous-espaces d'erreur. Cette redondance permet au code de fonctionner sans extraction de syndrome active, s'appuyant plutôt sur l'effondrement naturel de l'état quantique dans l'espace fondamental dégénéré du groupe stabilisateur. En éliminant le surcoût de mesure, la dégénérescence réduit les opportunités pour les erreurs induites par la mesure de corrompre l'information encodée, améliorant ainsi les taux d'erreur logique au-delà de ce que la distance seule prédit.",
    "C": "La dégénérescence permet à plusieurs chaînes d'erreur distinctes de produire des syndromes identiques, donnant au décodeur la flexibilité de choisir une correction qui pourrait transformer l'erreur réelle en un résultat trivial ou logiquement équivalent plutôt qu'en un résultat dommageable, augmentant effectivement le nombre de motifs d'erreur corrigibles au-delà de ce que la distance seule suggère.",
    "D": "Les codes stabilisateurs dégénérés exploitent les degrés de liberté de jauge en incorporant l'information de syndrome directement dans des qubits de jauge auxiliaires qui vivent dans l'espace du code mais en dehors du sous-espace logique. Lorsque des erreurs se produisent, le syndrome se manifeste comme des excitations de ces qubits de jauge, qui peuvent ensuite être directement réinitialisés à l'état fondamental sans nécessiter de mesure ou de traitement classique. Ce mécanisme de réinitialisation immédiate, permis par la structure dégénérée mappant les syndromes d'erreur sur des violations de jauge locales, supprime les erreurs logiques par un processus de purification continue qui fonctionne plus rapidement que le temps de cycle de correction d'erreur prédit par les limites basées sur la distance.",
    "solution": "C"
  },
  {
    "id": 431,
    "question": "Quelle modification de l'algorithme de Shor lui permet de résoudre le problème du logarithme discret ?",
    "A": "L'implémentation d'une marche quantique sur la structure de groupe cyclique permet à l'algorithme de parcourir tous les candidats de logarithme possibles en superposition, exploitant la propriété de fermeture du groupe pour identifier le logarithme discret grâce à l'interférence destructive des chemins incorrects. Cette approche exploite la réversibilité des marches quantiques pour amplifier l'amplitude de probabilité de l'exposant correct tout en supprimant tous les autres, remplaçant effectivement la sous-routine de recherche de période par une recherche sur les puissances du générateur du groupe.",
    "B": "L'ajout d'un registre supplémentaire dédié au stockage des valeurs intermédiaires de logarithme permet à l'algorithme d'effectuer des comparaisons parallèles entre les exposants candidats, utilisant des opérations contrôlées pour vérifier si g^x = h dans le groupe cyclique. Ce troisième registre maintient la cohérence tout au long du calcul et est mesuré en dernier pour faire s'effondrer la superposition sur le logarithme discret correct.",
    "C": "Utiliser une double transformée de Fourier sur les deux registres d'entrée plutôt que sur un seul",
    "D": "Modifier l'opération d'exponentiation modulaire en une opération de groupe différente, remplaçant spécifiquement la multiplication modulo N par l'opération de groupe du groupe multiplicatif cyclique directement, transforme le problème de recherche de période en un problème de recherche de logarithme en exploitant les propriétés homomorphiques entre les structures additives et multiplicatives. La fonction modifiée f(x) = g^x (mod p) devient f(x,y) = g^x · h^y (mod p), où l'algorithme quantique recherche des solutions entières satisfaisant la relation de groupe par exponentiation simultanée dans les deux registres avant d'appliquer la QFT standard pour extraire le logarithme discret.",
    "solution": "C"
  },
  {
    "id": 432,
    "question": "Dans une architecture tolérante aux fautes implémentant des codes de surface avec un patch de distance 7, vous observez que les taux d'erreur logiques atteignent un plateau malgré l'augmentation du nombre de cycles d'extraction de syndrome. Vos diagnostics révèlent des erreurs corrélées apparaissant en grappes de 3 à 4 qubits de données adjacents après chaque cycle de mesure de stabilisateur. Le regroupement des erreurs persiste même après l'optimisation des fidélités de portes à un qubit à 99,99%. Quelle vulnérabilité de sécurité spécifique émerge dans les protocoles d'échange de clés authentifiés post-quantiques ?",
    "A": "Une susceptibilité de la confirmation de clé aux attaques par analyse temporelle de mesure apparaît car les erreurs corrélées dans le matériel quantique créent des retards détectables dans le processus de vérification de signature post-quantique qui conclut la négociation d'échange de clés. Spécifiquement, lorsque des erreurs groupées affectent les qubits impliqués dans des schémas de signature basés sur les réseaux comme Falcon ou Dilithium, la surcharge de correction d'erreur introduit des variations temporelles à l'échelle de la microseconde corrélées avec le poids de Hamming du matériel de clé privée.",
    "B": "La confidentialité persistante (forward secrecy) est compromise par des techniques de restauration d'état quantique qui exploitent la réversibilité des opérations unitaires appliquées pendant la génération de clés, permettant à un adversaire ayant accès au circuit quantique implémentant l'échange de clés de reconstruire rétroactivement les clés de session en inversant temporellement le calcul.",
    "C": "La réutilisation de clés éphémères devient détectable via des algorithmes quantiques de recherche de période appliqués à la structure de réseau sous-jacente aux schémas post-quantiques tels que Kyber ou Dilithium. Lorsque des erreurs corrélées affectent le processus d'échantillonnage polynomial utilisé pour générer des clés éphémères, elles introduisent une périodicité faible dans l'espace des clés que les algorithmes de type Shor peuvent exploiter pour factoriser le module effectif de la fonction de génération de clés. Un adversaire qui observe plusieurs sessions peut traiter par lots les textes chiffrés capturés en utilisant des transformées de Fourier quantiques pour extraire la période cachée, puis reconstruire les clés de session précédentes par réduction de base de réseau même si ces sessions semblaient utiliser une fraîcheur aléatoire provenant d'un générateur de nombres aléatoires quantiques.",
    "D": "Une liaison d'identité incorrecte se produit lors de l'établissement de sessions multi-parties lorsque le protocole ne lie pas cryptographiquement les identités des participants aux clés éphémères dans la négociation initiale, permettant une substitution par homme du milieu. Les solutions classiques comme Diffie-Hellman signé s'étendent naturellement aux environnements post-quantiques, mais les signatures basées sur les réseaux nécessitent une intégration soigneuse pour éviter de créer de nouveaux canaux temporels dans la phase de confirmation de clé. Lorsque des erreurs quantiques corrélées affectent le processus de génération de signature, elles peuvent introduire des motifs détectables dans l'engagement de liaison qu'un adversaire exploite pour substituer des identités pendant la négociation sans détection.",
    "solution": "D"
  },
  {
    "id": 433,
    "question": "Pourquoi fixer le qubit de contrôle d'une porte CNOT à |1⟩ produit-il une opération Pauli X ?",
    "A": "Parce que la téléportation de l'état de contrôle introduit l'opérateur propre X, qui agit comme une transformation de base sur le sous-espace cible et génère la dynamique de basculement à travers un protocole cohérent de mesure-rétroaction. Spécifiquement, lorsque le qubit de contrôle est préparé dans l'état propre |1⟩ de X, l'unitaire conditionnel du CNOT se réduit à un canal maximalement mixte sur la cible qui, après trace partielle sur le contrôle, induit la transformation Pauli X comme son opération effective à un qubit. Ce mécanisme repose sur le qubit de contrôle fonctionnant comme une ancille qui médie la propagation de l'information de phase à travers l'intrication contrôlée.",
    "B": "Le qubit cible agit comme un miroir pour enregistrer l'information de parité de l'espace produit tensoriel contrôle-cible, de sorte que fixer le contrôle à |1⟩ établit une contrainte de parité persistante qui force la cible à évoluer sous une permutation impaire de la base computationnelle. Cet effet de miroir survient parce que la table de vérité du CNOT implémente une opération XOR réversible, et lorsqu'une entrée est fixée au 1 logique, le dispositif se comporte fonctionnellement comme un réflecteur inverseur. L'état de contrôle fixe programme ainsi le tissu de portes pour produire le complément de l'état qui entre dans le rail cible, ce qui est précisément l'action définissant Pauli X.",
    "C": "CNOT implémente un NOT contrôlé, ce qui signifie que la cible bascule si et seulement si le contrôle est |1⟩. Lorsque le contrôle est fixé à |1⟩, la condition de basculement est toujours satisfaite, donc la porte applique déterministiquement X à la cible indépendamment de son état d'entrée. Cela découle directement de la table de vérité du CNOT où contrôle=1 produit un comportement XOR sur le qubit cible.",
    "D": "Une propagation de recul de phase se produit lorsque l'état de base computationnelle du qubit de contrôle module la phase relative entre les amplitudes cibles à travers l'opérateur intriquant du CNOT, créant des motifs d'interférence qui font effectivement tourner le vecteur de Bloch de la cible de π radians autour de l'axe X, ce qui est mathématiquement équivalent à appliquer la porte Pauli X.",
    "solution": "C"
  },
  {
    "id": 434,
    "question": "Quelle est la principale limitation de la simulation classique directe des algorithmes d'apprentissage automatique quantique ?",
    "A": "La mémoire évolue exponentiellement avec le nombre de qubits, nécessitant 2^n amplitudes complexes pour un état à n qubits. Bien que les méthodes de réseaux de tenseurs puissent comprimer certains états, l'entropie d'intrication typique dans les circuits d'entraînement QML croît linéairement avec la profondeur, forçant les dimensions de liaison à évoluer exponentiellement et éliminant les avantages de compression au-delà d'environ 40 qubits même avec des représentations d'états de produit matriciel.",
    "B": "La mémoire évolue exponentiellement avec le nombre de qubits, nécessitant 2^n amplitudes complexes pour représenter un état à n qubits, ce qui dépasse rapidement les ressources computationnelles disponibles même pour des tailles de système modestes autour de 50 qubits.",
    "C": "La mémoire évolue exponentiellement avec le nombre de paramètres dans les circuits variationnels, nécessitant le stockage de 2^p composantes de gradient pour p paramètres. Bien que chaque état à n qubits ne nécessite que 2^n amplitudes, la rétropropagation à travers les couches quantiques exige de maintenir O(2^n × p) valeurs d'activation intermédiaires, et les architectures QML modernes avec p > n créent des goulots d'étranglement mémoire qui dominent les exigences de stockage d'état.",
    "D": "Les exigences en mémoire évoluent comme 2^n, mais le goulot d'étranglement dominant est le coût des opérations de portes : calculer des unitaires à deux qubits nécessite O(2^(2n)) opérations par porte en raison de l'expansion du produit de Kronecker sur l'espace de Hilbert complet. Puisque les circuits QML contiennent O(poly(n)) portes, le temps d'exécution total plutôt que la mémoire devient le facteur limitant pour la simulation classique de systèmes dépassant 30 qubits.",
    "solution": "B"
  },
  {
    "id": 435,
    "question": "Quel est le but de la technique de décomposition diagonale dans la synthèse de circuits quantiques ?",
    "A": "La décomposition diagonale permet la vérification de la correction unitaire en exploitant le fait que les matrices diagonales dans la base computationnelle ont des valeurs propres égales à leurs entrées diagonales, qui peuvent être efficacement extraites via une seule couche de portes Hadamard suivie de mesures dans la base computationnelle. En insérant périodiquement ces étapes de vérification pendant la synthèse, l'algorithme peut détecter les écarts par rapport à l'unitaire cible par comparaison de valeurs propres, garantissant que les erreurs numériques accumulées provenant de l'arithmétique en virgule flottante ou des approximations de compilation de portes restent en dessous des seuils de tolérance spécifiés tout au long du processus de décomposition, particulièrement lors de la synthèse d'unitaires de haute précision pour des implémentations tolérantes aux fautes.",
    "B": "Les matrices diagonales correspondent à des unitaires qui appliquent uniquement des déphasages sans changer les populations d'états de la base computationnelle, les rendant implémentables en utilisant uniquement des rotations Z à un qubit et des portes de phase contrôlées sans surcharge de SWAP. En décomposant un unitaire arbitraire en un produit de matrices diagonales et d'unitaires structurés simples (comme les rotations de Givens ou les matrices de permutation), l'algorithme de synthèse peut implémenter les composantes diagonales efficacement avec des circuits peu profonds de portes de phase à nombre T optimal. Cette décomposition réduit le nombre total de portes car les opérations diagonales nécessitent moins de ressources que les portes générales à deux qubits.",
    "C": "La technique de décomposition diagonale exploite le théorème de décomposition Cosinus-Sinus (CSD), qui exprime tout unitaire à n qubits comme un produit de rotations à un qubit multiplexées entrelacées avec des portes uniformément contrôlées agissant sur des sous-ensembles de qubits disjoints. En factorisant récursivement l'unitaire en forme bloc-diagonale où chaque bloc correspond à une configuration fixe de qubits de contrôle, l'algorithme de synthèse réduit les unitaires généraux à une séquence de rotations conditionnelles. Cette structure en blocs diagonaux est particulièrement avantageuse car les circuits résultants se mappent naturellement sur des topologies linéaires à voisins les plus proches sans nécessiter de portes SWAP supplémentaires, puisque chaque rotation multiplexée agit sur un sous-ensemble de qubits géométriquement localisé aligné avec la connectivité matérielle.",
    "D": "La décomposition diagonale sépare la matrice densité cible ρ en sa composante diagonale D (représentant les populations classiques) et les cohérences hors-diagonale C, exploitant le fait que D et C peuvent être préparés indépendamment par différentes primitives de circuit. Les éléments diagonaux spécifient les probabilités d'états de base requises et peuvent être efficacement préparés en utilisant des circuits d'encodage d'amplitude avec une profondeur logarithmique dans la dimension d'état. En implémentant d'abord la composante diagonale par des rotations contrôlées basées sur des structures d'arbre binaire, puis en générant séparément les cohérences nécessaires par des techniques de recul de phase appliquées aux qubits auxiliaires, la méthode minimise la profondeur d'intrication nécessaire pour la préparation d'états mixtes comparée aux approches directes d'isomorphisme de Choi-Jamio‌łkowski.",
    "solution": "B"
  },
  {
    "id": 436,
    "question": "En quoi le concept de taux d'erreur logique diffère-t-il du taux d'erreur physique dans la correction d'erreurs quantiques ?",
    "A": "Les taux d'erreur logiques mesurent la probabilité d'échec de l'information quantique encodée après application du cycle complet de correction d'erreurs quantiques incluant l'extraction de syndrome, le décodage classique et l'interprétation des erreurs — représentant le bruit effectif subi par le qubit logique protégé. Les taux d'erreur physiques paramètrent les probabilités d'échec élémentaires (erreurs de porte, erreurs de mesure, décohérence au repos) des composants matériels individuels avant toute correction d'erreurs, avec des taux physiques typiques de 10⁻³ supprimés à des taux logiques inférieurs à 10⁻⁶ grâce à des mesures de syndrome répétées et au décodage.",
    "B": "Les taux d'erreur logiques mesurent la probabilité d'échec résiduelle de l'information quantique encodée après application de la correction d'erreurs, représentant la fréquence à laquelle le protocole de correction d'erreurs quantiques échoue à protéger le qubit logique. Les taux d'erreur physiques quantifient les probabilités brutes d'échec par porte, par mesure ou par pas de temps des composants matériels individuels avant l'application de toute correction d'erreurs — ce sont les paramètres de bruit fondamentaux du substrat physique.",
    "C": "Les taux d'erreur logiques caractérisent la probabilité d'erreur nette de l'état du qubit protégé après que le décodage de syndrome a identifié et corrigé les erreurs détectables dans l'espace de code, mais avant l'application d'une rétroaction active — ils quantifient la précision d'inférence du décodeur plutôt que la fidélité ultime de l'information encodée. Les taux d'erreur physiques mesurent le bruit matériel non corrigé incluant à la fois les canaux de Pauli stochastiques et les erreurs de contrôle cohérentes, le théorème du seuil établissant que les taux logiques évoluent comme (p_phys/p_th)^((d+1)/2) pour une distance de code d, où p_th est le seuil au-delà duquel l'encodage ne procure aucun avantage.",
    "D": "Les taux d'erreur logiques représentent l'amplitude d'erreur cohérente résiduelle — principalement des angles de surrotation et des erreurs de calibration systématiques du contrôle — qui se propagent à travers le formalisme des stabilisateurs sans déclencher de drapeaux de syndrome, échappant ainsi à la détection par le protocole de correction d'erreurs. Les taux d'erreur physiques quantifient uniquement les processus de bruit stochastique incohérents (canaux de dépolarisation, amortissement d'amplitude) affectant les qubits individuels avant encodage. La distinction est opérationnellement critique car les erreurs logiques nécessitent un apprentissage hamiltonien et un contrôle optimal pour être supprimées, tandis que les erreurs physiques sont atténuées par des codes de correction d'erreurs quantiques standards avec une distance de code suffisante.",
    "solution": "B"
  },
  {
    "id": 437,
    "question": "Quel est le lien entre le problème d'échange de jetons et l'ordonnancement de portes SWAP quantiques ?",
    "A": "Le cadre d'échange de jetons modélise les permutations de qubits logiques comme un reetiquetage de sommets sur le graphe de connectivité, mais suppose de manière critique que chaque opération SWAP agit symétriquement sur les deux qubits — cela fonctionne parfaitement pour les portes iSWAP et √SWAP où la matrice unitaire est symétrique, mais échoue pour les architectures hétérogènes où la fidélité du SWAP dépend du qubit physique qui initie la séquence de portes.",
    "B": "L'abstraction de l'échange de jetons fournit un cadre combinatoire où minimiser le nombre d'échanges d'arêtes nécessaires pour réarranger les jetons sur les sommets du graphe correspond directement à minimiser le nombre de portes SWAP pour aligner les qubits logiques sur les coupleurs physiques dans le problème de compilation de circuits quantiques.",
    "C": "L'optimisation de l'échange de jetons produit la séquence d'échanges d'arêtes minimale sous l'hypothèse que toutes les arêtes du graphe ont un coût uniforme, ce qui modélise correctement les architectures supraconductrices où les portes CNOT et iSWAP ont des fidélités comparables, mais échoue sur les systèmes à piège à ions où la fidélité de porte varie avec la distance inter-ionique — la solution par jetons minimise le nombre d'échanges mais peut sélectionner des coupleurs longue portée de faible fidélité plutôt que des chemins courts de haute fidélité.",
    "D": "Le modèle par jetons transforme le routage logique-vers-physique des qubits en un problème d'automorphisme de graphe où la séquence d'échanges minimale correspond au plus court chemin de groupe de permutation entre les configurations initiale et cible — cependant, cette formulation classique ignore la commutativité des portes : les circuits quantiques permettent souvent aux portes commutatives de s'exécuter simultanément, permettant aux opérations SWAP de se paralléliser sur des arêtes disjointes, alors que le modèle par jetons sérialise strictement tous les échanges.",
    "solution": "B"
  },
  {
    "id": 438,
    "question": "Considérons un scénario de calcul quantique distribué où vous souhaitez exécuter la transformée de Fourier quantique sur plusieurs processeurs quantiques plus petits connectés par des canaux de communication classiques. Pourquoi la QFT présente-t-elle des difficultés fondamentales dans ce contexte distribué, au-delà du simple défi technique de maintenir la cohérence ?",
    "A": "L'algorithme manque fondamentalement de mécanismes de correction d'erreurs car la structure mathématique de la QFT, en particulier sa dépendance sur des rotations de phase précises avec des angles irrationnels, ne peut pas être encodée dans des codes stabilisateurs ni protégée par des architectures de code de surface standard. Une implémentation tolérante aux fautes nécessiterait une distillation d'états magiques pour chaque porte de phase contrôlée.",
    "B": "Les mesures fréquentes en milieu de circuit sont inhérentes à la structure de la QFT, créant une décohérence induite par la mesure qui se propage de manière catastrophique lorsque l'exécution s'étend sur plusieurs processeurs. Chaque rotation contrôlée dans la base QFT mesure implicitement l'information de phase relative entre paires de qubits, et distribuer ces mesures entre processeurs rompt le référentiel de phase global.",
    "C": "La QFT est essentiellement monolithique dans sa structure computationnelle car elle opère sur un nombre de qubits qui dépasse ce que les petits processeurs typiques peuvent accommoder, nécessitant des stratégies de partitionnement qui introduisent un surcoût significatif. La profondeur de circuit de l'algorithme croît super-linéairement lorsqu'elle est distribuée, car la communication inter-processeurs domine la chronologie d'exécution même lorsque la distribution d'intrication réussit.",
    "D": "La QFT nécessite une connectivité tous-à-tous entre qubits via des portes de phase contrôlées, et distribuer ces opérations nécessiterait un nombre exponentiel d'étapes de téléportation pour transférer l'information quantique entre processeurs. Chaque téléportation consomme des paires intriquées et introduit à la fois de la latence et des sources d'erreur supplémentaires qui évoluent défavorablement avec la taille du système. Les tours de communication classique nécessaires pour la transmission des résultats de mesure et les corrections par anticipation créent des goulots d'étranglement qui détruisent la structure parallèle, tandis que la perte cumulative de fidélité des protocoles de téléportation répétés se compose multiplicativement sur la profondeur du circuit.",
    "solution": "D"
  },
  {
    "id": 439,
    "question": "Quel avantage clé la nature réversible des portes quantiques offre-t-elle par rapport aux portes classiques irréversibles ?",
    "A": "Les portes quantiques réversibles éliminent entièrement la production d'entropie thermodynamique car les transformations unitaires préservent l'entropie de von Neumann, satisfaisant le principe de Landauer dans sa formulation quantique où la préservation de l'information implique une dissipation de chaleur nulle. Puisque les portes quantiques implémentent des transformations bijectives sans effacer de degrés de liberté, elles atteignent la limite thermodynamique fondamentale du calcul, évitant le coût énergétique de kT ln(2) par effacement de bit que la logique classique irréversible doit payer, permettant à des circuits quantiques arbitrairement complexes de fonctionner sans surcoût thermodynamique.",
    "B": "Préserve l'information à travers des mappages bijectifs entrée-sortie, ce qui est imposé par l'évolution unitaire en mécanique quantique. Cette préservation de l'information permet les effets d'interférence quantique essentiels aux algorithmes quantiques et garantit que les états quantiques peuvent être manipulés de manière cohérente sans l'augmentation d'entropie qui accompagne les opérations de portes classiques irréversibles, maintenant la réversibilité computationnelle tout au long de l'exécution du circuit.",
    "C": "Les portes quantiques réversibles maintiennent la cohérence de phase à travers les étapes computationnelles grâce à leur structure unitaire, ce qui empêche les mécanismes de décohérence qui surgissent dans les portes classiques irréversibles de la perte d'information accumulée. En préservant le vecteur d'état quantique complet incluant les phases relatives, les portes réversibles permettent des motifs d'interférence contrôlés que le calcul classique ne peut pas réaliser, puisque les portes irréversibles détruisent les relations de phase par leurs mappages plusieurs-vers-un qui effondrent l'espace d'états computationnel pendant l'exécution.",
    "D": "La contrainte de réversibilité garantit que les portes quantiques préservent la pureté des états quantiques en maintenant trace(ρ²) = 1 tout au long de l'exécution du circuit, alors que les portes classiques irréversibles introduisent un mélange qui augmente l'entropie de von Neumann. Cette préservation de la pureté permet aux codes de correction d'erreurs quantiques de fonctionner, puisque les opérations réversibles gardent les erreurs dans des sous-espaces corrigeables, tandis que les portes classiques irréversibles provoqueraient une fuite d'information vers des réservoirs d'entropie irrécupérables, empêchant fondamentalement les architectures de calcul classique tolérantes aux fautes.",
    "solution": "B"
  },
  {
    "id": 440,
    "question": "Dans l'algorithme de Shor, que se passe-t-il si la base choisie a partage des facteurs avec N ?",
    "A": "La vérification classique préliminaire révèle immédiatement le facteur partagé en calculant pgcd(a,N), qui retourne un diviseur non trivial de N sans nécessiter aucun calcul quantique. Cette découverte fortuite survient avant même que la sous-routine quantique coûteuse de recherche de période ne s'initialise, fournissant un raccourci qui résout directement le problème de factorisation en identifiant au moins un facteur premier du nombre cible par de simples opérations d'algorithme euclidien sur matériel classique seul.",
    "B": "La sous-routine quantique de recherche de période s'exécute normalement et retourne une période significative r, mais cette période correspond à l'ordre multiplicatif de a dans le groupe quotient Z*_{N/pgcd(a,N)} plutôt que Z*_N, provoquant l'échec systématique de l'étape classique de post-traitement ultérieure qui calcule pgcd(a^{r/2}±1, N) puisque la période extraite satisfait des contraintes de divisibilité différentes. La transformée de Fourier quantique mesure avec succès une structure périodique, mais la périodicité reflète le système de modules réduit où les facteurs communs ont été implicitement factorisés, donnant une période mathématiquement valide mais cryptographiquement inutile qui ne peut être exploitée pour factoriser le N original par la procédure standard d'extraction de fractions continues.",
    "C": "Lorsque pgcd(a,N) > 1, la fonction d'exponentiation modulaire exhibe une pseudo-périodicité où des cycles apparents émergent de la projection de la structure de groupe de dimension supérieure sur la base computationnelle accessible, mais ces cycles manquent des propriétés algébriques requises pour la factorisation. L'état quantique après recherche de période affiche de forts pics de mesure à des intervalles spécifiques, mais ces pics correspondent à des résonances dans le graphe de Cayley du sous-groupe multiplicatif non cyclique plutôt qu'à une véritable périodicité, amenant l'algorithme de fractions continues à extraire des diviseurs fallacieux qui sont toujours soit 1 soit N, ne révélant jamais de facteurs non triviaux malgré l'exécution du circuit quantique sans erreurs détectables.",
    "D": "L'oracle implémentant f(x) = a^x mod N retourne zéro pour toutes les entrées lorsque pgcd(a,N) divise a^x pour chaque x dans la superposition, provoquant une interférence destructive totale à travers tous les états de base computationnels et effondrant le registre quantique vers un vecteur d'état indéfini hors de l'espace de Hilbert valide. Cette condition pathologique déclenche le système de détection d'erreurs du matériel quantique, qui reconnaît la distribution d'amplitude anormale tout-zéro et arrête l'exécution avec un drapeau de diagnostic, empêchant le gaspillage de ressources quantiques sur des cas dégénérés bien que nécessitant un prétraitement classique pour identifier de telles situations à l'avance par exponentiation d'essai de bases candidates.",
    "solution": "A"
  },
  {
    "id": 441,
    "question": "Dans les protocoles de transmission anonyme quantique, les adversaires peuvent exploiter les corrélations entre les temps d'arrivée des signaux quantiques à différents nœuds du réseau, même lorsque les états quantiques eux-mêmes sont parfaitement sécurisés. Ce canal auxiliaire temporel devient particulièrement problématique dans les implémentations pratiques où la latence réseau varie. Quelle est la principale vulnérabilité créée par ce phénomène ?",
    "A": "Lors des opérations d'échange d'intrication qui établissent des canaux quantiques anonymes entre nœuds distants, le motif spécifique des paires intriquées échangées et leur séquence crée une signature unique qui corrèle avec l'identité de l'émetteur. Un adversaire surveillant la distribution d'intrication peut suivre comment les mesures d'états de Bell se propagent à travers le graphe d'intrication du réseau, et en analysant l'évolution temporelle de la connectivité d'intrication, reconstruire le nœud source le plus probable par inférence bayésienne sur la topologie d'échange, puisque différents émetteurs génèrent typiquement des motifs d'échange distinguables selon leur position dans le réseau",
    "B": "Lorsque plusieurs nœuds de mix-net quantique colludent en comparant les horodatages et les métadonnées de routage des paquets quantiques traversant leurs positions respectives dans la chaîne d'anonymisation, ils peuvent reconstruire des correspondances partielles émetteur-récepteur sans même accéder aux informations d'état quantique. Cette collusion devient particulièrement efficace lorsque les nœuds adverses contrôlent des positions consécutives dans le réseau de mixage, car les corrélations temporelles entre paquets d'entrée et de sortie aux nœuds adjacents réduisent considérablement l'ensemble d'anonymat par des attaques par intersection sur l'espace de permutation",
    "C": "Les générateurs de nombres pseudo-aléatoires pour la sélection de chemins présentent des biais exploitables pour l'analyse de détermination de route",
    "D": "L'adversaire peut utiliser l'analyse statistique des motifs temporels pour déduire quels nœuds communiquent, brisant ainsi l'anonymat sans jamais mesurer les états quantiques ni violer aucune hypothèse cryptographique. Cela fonctionne car les signaux quantiques se propagent toujours à travers des canaux physiques avec des délais mesurables qui corrèlent avec les paires émetteur-récepteur et la topologie du réseau.",
    "solution": "D"
  },
  {
    "id": 442,
    "question": "Pourquoi la correction d'erreurs quantiques (QEC) doit-elle détecter et corriger les erreurs sans mesurer directement les qubits ?",
    "A": "Mesurer les qubits augmente leur temps de cohérence en forçant le système dans un état propre d'énergie défini, ce qui stabilise la fonction d'onde et crée une barrière protectrice contre la décohérence environnementale. Cet effet de stabilisation induit par la mesure a été expérimentalement vérifié pour prolonger les temps T1 et T2 jusqu'à 40% dans les architectures de transmons supraconducteurs, faisant des opérations de mesure répétées une pierre angulaire des stratégies modernes de suppression d'erreurs.",
    "B": "L'information de phase est effacée par les opérations de mesure, mais les populations d'états de base de calcul restent parfaitement intactes et récupérables, ce qui signifie que tout algorithme quantique peut tolérer des mesures fréquentes tant qu'il opère exclusivement dans la base Z.",
    "C": "La mesure directe effondre la fonction d'onde quantique, projetant irréversiblement les états de superposition sur des états de base de calcul définis et détruisant l'information quantique encodée. La QEC contourne cela en mesurant l'information de syndrome via des qubits ancillaires, extrayant les signatures d'erreur sans rien apprendre sur l'état logique lui-même, préservant ainsi la superposition qui porte le calcul.",
    "D": "Les états quantiques sont fondamentalement trop fragiles pour être jamais mesurés directement en aucune circonstance, même via l'extraction indirecte de syndrome ou des techniques médiées par ancilles, donc la correction d'erreurs quantiques doit plutôt s'appuyer exclusivement sur des motifs d'interférence quantique soigneusement conçus et des séquences de découplage dynamique pour détecter et corriger les erreurs sans aucune forme de mesure. Ce paradigme sans mesure opère en dirigeant les erreurs vers des chemins interférant destructivement à travers des séquences de portes précisément temporisées, annulant effectivement les erreurs par contrôle cohérent seul.",
    "solution": "C"
  },
  {
    "id": 443,
    "question": "Considérez un classificateur quantique qui obtient de bons résultats sur un ensemble de données simple et linéairement séparable avec seulement 4 caractéristiques et 100 exemples d'entraînement. Le modèle utilise un ansatz basique avec intrication minimale et converge rapidement durant l'entraînement. Cependant, vous essayez d'évaluer si ce succès indique un véritable avantage quantique ou si un modèle classique pourrait atteindre des performances similaires ou meilleures. Que faut-il faire ensuite pour évaluer son potentiel avantage quantique ?",
    "A": "Tester la même architecture de modèle sur des données significativement plus complexes ou de haute dimension où les méthodes classiques peinent, comme des ensembles de données avec des corrélations de caractéristiques complexes, des frontières de décision non linéaires, ou des espaces de caractéristiques exponentiellement grands qui pourraient exploiter l'espace d'états quantiques plus efficacement, révélant ainsi si l'approche quantique fournit des avantages computationnels au-delà de ce que des algorithmes classiques simples peuvent atteindre",
    "B": "Tester le classificateur sur des ensembles de données dont la dimension des caractéristiques évolue exponentiellement avec la taille du problème, comme les espaces de configuration de chimie quantique ou les tâches de décomposition tensorielle d'ordre élevé où la dimensionnalité de l'espace d'états quantiques 2^n correspond naturellement à la structure du problème, évaluant ainsi si le modèle exploite un véritable parallélisme quantique dans l'encodage des caractéristiques plutôt que de simplement réimplémenter des méthodes de noyau classiques via des circuits variationnels qui pourraient être efficacement simulés",
    "C": "Comparer les taux de convergence d'entraînement avec des réseaux neuronaux classiques ayant des nombres de paramètres équivalents sur des ensembles de données progressivement plus dimensionnels où les phénomènes de plateau stérile domineraient l'optimisation basée sur les gradients, puisque l'avantage quantique se manifeste spécifiquement lorsque la descente de gradient classique échoue alors que le gradient naturel quantique ou les règles de décalage de paramètres permettent un entraînement efficace à travers la structure géométrique des variétés d'états quantiques malgré la malédiction de la dimensionnalité affectant les approches classiques et quantiques",
    "D": "Réduire systématiquement la profondeur de l'ansatz tout en surveillant la dégradation de la précision de classification, puisque l'avantage quantique véritable nécessite de démontrer que des circuits quantiques peu profonds avec un nombre polynomial de portes surpassent les réseaux classiques profonds, prouvant que l'interférence quantique permet une approximation efficace de frontières de décision complexes sans la surcharge de profondeur que les architectures classiques requièrent pour atteindre une expressivité comparable via la composition hiérarchique de fonctions d'activation non linéaires à travers plusieurs couches",
    "solution": "A"
  },
  {
    "id": 444,
    "question": "Quelle propriété de la QKD à variables continues par états comprimés la rend plus résiliente aux attaques par canal auxiliaire dépendantes de la longueur d'onde ?",
    "A": "La modulation gaussienne randomise le nombre de photons à travers les trames de signal, ce qui distribue le contenu informationnel uniformément et empêche le sondage spécifique à une longueur d'onde d'extraire des corrélations significatives avec les données encodées.",
    "B": "La symétrie de l'espace des phases élimine le besoin de suivi actif de polarisation en garantissant que les encodages de quadrature restent invariants sous les rotations dépendantes de la longueur d'onde de la sphère de Poincaré.",
    "C": "La détection homodyne filtre intrinsèquement les tons optiques parasites hors bande. L'oscillateur local agit comme un filtre spectral étroit centré sur la longueur d'onde du signal, de sorte que tout photon de canal auxiliaire introduit à des longueurs d'onde différentes n'interférera pas avec le faisceau de référence et restera donc non mesuré dans les statistiques de quadrature. Cette sélectivité de longueur d'onde intégrée signifie que les attaques exploitant la dispersion chromatique ou le sondage multiplexé en longueur d'onde sont automatiquement rejetées par l'appareil de mesure lui-même.",
    "D": "La structure du protocole de réconciliation inverse décorrèle intrinsèquement les dépendances de longueur d'onde en effectuant la correction d'erreurs du côté d'Alice après que Bob annonce son information de syndrome, ce qui signifie que toute manipulation spécifique à la longueur d'onde durant la transmission est brouillée.",
    "solution": "C"
  },
  {
    "id": 445,
    "question": "Quelle est la principale différence dans les implémentations de l'algorithme de Shor pour la factorisation versus le logarithme discret ?",
    "A": "Dans l'algorithme de factorisation de Shor, la transformée de Fourier quantique doit être appliquée à un registre dont la taille évolue comme 2n qubits où n est la longueur en bits du nombre à factoriser, car la sous-routine de recherche de période nécessite d'échantillonner la fonction sur un domaine suffisamment grand pour capturer au moins une période complète avec forte probabilité.",
    "B": "Bien que les deux variantes de l'algorithme de Shor reposent sur la recherche de période quantique suivie d'un post-traitement classique, le nombre de mesures requises diffère d'environ un facteur log(N) où N est la taille de l'espace de recherche. La factorisation nécessite de mesurer le registre de sortie une seule fois par exécution puisque l'algorithme des fractions continues peut extraire des périodes candidates à partir d'une seule valeur échantillonnée avec forte probabilité de succès.",
    "C": "L'oracle implémente une opération de groupe différente : la factorisation emploie l'exponentiation modulaire par rapport à une base choisie aléatoirement dans le groupe multiplicatif des entiers modulo N, tandis que le logarithme discret effectue l'exponentiation modulaire avec des bases contraintes par le générateur connu et l'exposant inconnu recherché, nécessitant des architectures distinctes de circuits de multiplication contrôlée malgré l'utilisation de sous-routines de transformée de Fourier quantique identiques.",
    "D": "Les portions quantiques des algorithmes de factorisation et de logarithme discret de Shor sont structurellement identiques — toutes deux effectuent l'exponentiation modulaire via des opérations de multiplication contrôlée répétées et appliquent la transformée de Fourier quantique pour extraire l'information de période. Cependant, le post-traitement classique diverge substantiellement en approche computationnelle et en complexité.",
    "solution": "C"
  },
  {
    "id": 446,
    "question": "Quelle méthodologie d'attaque avancée peut compromettre la sécurité des signatures numériques quantiques ?",
    "A": "La falsification par exploitation de mesures sélectives tire parti de la dépendance des schémas de signature quantique aux sélections de bases aléatoires annoncées après la distribution des états, permettant aux adversaires disposant de mémoire quantique de stocker les états de signature reçus, d'attendre l'annonce des bases, puis d'effectuer des mesures exclusivement dans des bases complémentaires sur des sous-ensembles soigneusement choisis de qubits de signature pour obtenir des informations classiques partielles qui peuvent être recombinées à travers plusieurs cycles de signature pour reconstruire suffisamment de structure de clé privée afin de générer des signatures valides pour des messages non autorisés",
    "B": "L'interception et le renvoi pendant la distribution exploite le théorème de non-clonage de manière inverse : un attaquant intercepte les états de signature, effectue un clonage optimal avec une fidélité approchant 5/6 pour les qubits, transmet des copies imparfaites aux destinataires tout en conservant les originaux pour analyse, et bien que la fidélité des clones individuels soit dégradée, l'agrégation statistique sur de nombreuses instances de signature permet d'extraire suffisamment d'informations sur la distribution des bases de signature pour prédire les motifs futurs",
    "C": "Les attaques par fidélité du test de permutation ciblent le protocole de vérification où les destinataires comparent les états quantiques reçus avec des copies de référence détenues par d'autres parties ; en exploitant les fidélités finies des portes dans les opérations controlled-SWAP, les adversaires créent des états quantiques qui semblent correspondre aux signatures légitimes lors de tests de permutation bruités tout en encodant des messages classiques différents",
    "D": "La tomographie d'état sur la clé publique permet une reconstruction complète des paramètres d'état quantique à travers des mesures systématiques dans plusieurs bases, permettant aux adversaires d'extraire des descriptions classiques complètes des états de signature publics.",
    "solution": "D"
  },
  {
    "id": 447,
    "question": "Dans le contexte des machines à vecteurs de support quantiques, l'alignement de noyau fait référence à l'optimisation de quelle caractéristique lors du réglage des hyperparamètres ?",
    "A": "La qualité de corrélation entre le noyau quantique et le noyau cible idéal",
    "B": "La qualité de correspondance entre le spectre des valeurs propres de la matrice de Gram du noyau quantique et la structure de covariance de l'ensemble de données",
    "C": "La qualité d'alignement entre la métrique de distance du noyau quantique et la géométrie de l'espace de caractéristiques classique",
    "D": "La qualité de correspondance entre la dimension d'intégration du noyau quantique et la courbure intrinsèque de la variété du problème",
    "solution": "A"
  },
  {
    "id": 448,
    "question": "Considérez un compilateur quantique qui doit exécuter un circuit sur du matériel avec une topologie de qubits restreinte — par exemple, une puce où le qubit 0 se connecte uniquement au qubit 1, le qubit 1 aux qubits 0 et 2, et ainsi de suite en chaîne linéaire. Votre circuit nécessite un CNOT entre les qubits 0 et 3. Quel est l'objectif du compilateur BRIDGE dans la conception de circuits quantiques ?",
    "A": "BRIDGE est une passe de transpilation qui synthétise des portes à deux qubits à longue portée en insérant des séquences d'opérations SWAP ou en exploitant des décompositions de portes alternatives qui respectent le graphe de connectivité natif. Il construit essentiellement un 'pont' d'opérations à travers les qubits intermédiaires pour implémenter des portes entre qubits non adjacents, ce qui est crucial lorsque le circuit logique suppose une connectivité totale mais que le matériel ne la fournit pas.",
    "B": "BRIDGE est une passe de transpilation qui synthétise des portes à deux qubits à longue portée en les décomposant en séquences de portes natives qui respectent le graphe de connectivité matériel, utilisant des qubits ancillaires en positions intermédiaires comme ressources de routage temporaires. Il construit des protocoles de 'pont' basés sur la mesure où l'état est téléporté à travers des qubits non adjacents via la distribution d'intrication — cette approche est critique lorsque le circuit logique suppose une connectivité totale mais que la topologie physique restreint les interactions directes, bien qu'elle nécessite un surcoût en ancillas et une latence de transmission classique.",
    "C": "BRIDGE est une passe de transpilation qui synthétise des portes à deux qubits à longue portée en insérant des séquences commutatives de rotations à un qubit et de CNOT entre voisins les plus proches qui respectent le graphe de connectivité natif, exploitant le fait que tout unitaire à deux qubits peut être décomposé en au plus trois portes CNOT plus des rotations locales. Il construit un 'pont' en réécrivant la porte cible en une décomposition de Cartan où les composantes non locales sont factorisées — ceci est essentiel lorsque le circuit logique suppose une connectivité totale, bien que les circuits résultants aient un surcoût de profondeur qui évolue logarithmiquement avec la distance entre qubits.",
    "D": "BRIDGE est une passe de transpilation qui synthétise des portes à deux qubits à longue portée en les ordonnançant comme des opérations multiplexées temporellement sur du matériel de couplage partagé qui se reconfigure dynamiquement entre les cycles d'horloge — les puces supraconductrices modernes utilisent des coupleurs ajustables dont les hamiltoniens d'interaction peuvent être modulés adiabatiquement pour créer des portes d'intrication effectives multi-sauts. Il construit un 'pont' en coordonnant la séquence d'activation des coupleurs intermédiaires pour propager l'information quantique à travers des qubits non adjacents sans surcoût de SWAP physique, ce qui est crucial lorsque les circuits logiques supposent une connectivité totale mais qu'une topologie de couplage fixe limite l'implémentation directe des portes.",
    "solution": "A"
  },
  {
    "id": 449,
    "question": "Quelle valeur mathématique l'algorithme de Shor cherche-t-il à trouver dans le cadre du processus de factorisation ?",
    "A": "La période de la fonction d'exponentiation modulaire f(x) = a^x mod N, qui encode l'ordre multiplicatif et conduit à l'extraction de facteurs via des calculs de plus grand commun diviseur.",
    "B": "Le logarithme discret d'un élément générateur dans le groupe multiplicatif Z*_N, dont le calcul via la transformée de Fourier quantique révèle la structure de groupe nécessaire pour extraire les facteurs par réduction de Pohlig-Hellman.",
    "C": "Le développement en fraction continue du rapport k/r où k est mesuré depuis le registre de sortie QFT et r est la période inconnue, dont les réduites approximent r avec une probabilité supérieure à 1/2.",
    "D": "La plus petite valeur propre de l'opérateur de permutation cyclique U défini par U|x⟩ = |ax mod N⟩, dont la phase encode l'ordre r à travers la relation λ = e^(2πis/r) pour un certain entier s premier avec r.",
    "solution": "A"
  },
  {
    "id": 450,
    "question": "Comment les composants classiques et quantiques sont-ils intégrés lors de l'entraînement d'un HQNN ?",
    "A": "L'entraînement utilise une descente de coordonnées par blocs alternés : les paramètres quantiques sont optimisés pour des poids classiques fixés en utilisant des estimations de gradient par déplacement de paramètres calculées sur du matériel quantique, puis les poids classiques sont mis à jour pour des paramètres quantiques fixés en utilisant la rétropropagation standard sur du matériel classique. Cette alternance continue jusqu'à convergence. La perte hybride combine les valeurs d'espérance quantiques avec les sorties des couches classiques, mais le flux de gradient est partitionné plutôt que simultané, évitant le besoin de différentiation automatique conjointe à travers la frontière quantique-classique.",
    "B": "Entraînement conjoint de bout en bout où les gradients traversent simultanément les couches quantiques et classiques via la règle de déplacement de paramètres pour les portes quantiques et la rétropropagation standard pour les poids classiques. La fonction de perte englobe l'ensemble de l'architecture hybride, permettant aux paramètres du circuit quantique et aux poids du réseau de neurones classique d'être co-optimisés en utilisant des méthodes unifiées basées sur les gradients comme Adam ou SGD.",
    "C": "Les paramètres du circuit quantique sont entraînés en utilisant l'apprentissage par renforcement avec gradient de politique, la sortie du réseau classique servant de signal de récompense, tandis que les poids classiques sont optimisés via la rétropropagation supervisée en utilisant les résultats de mesure quantique comme caractéristiques. Cette stratégie d'entraînement asymétrique évite de calculer directement les gradients quantiques, traitant plutôt la couche quantique comme une politique stochastique qui échantillonne les résultats de mesure, et utilisant des estimateurs de gradient de style REINFORCE pour mettre à jour les paramètres de porte en fonction de la performance des couches classiques en aval sur la tâche.",
    "D": "Les paramètres de la couche quantique sont initialisés aléatoirement puis affinés en utilisant l'optimisation bayésienne guidée par la performance de la couche classique, échantillonnant différentes configurations de paramètres quantiques et ajustant un modèle de substitution par processus gaussien au paysage de perte de validation. Les poids classiques sont entraînés normalement via rétropropagation à chaque point d'essai. Cette optimisation hybride évite complètement le calcul de gradients quantiques tout en permettant un réglage conjoint, l'acquisition d'amélioration attendue guidant la recherche de paramètres quantiques basée sur la surface de perte différentiable du réseau classique.",
    "solution": "B"
  },
  {
    "id": 451,
    "question": "Quel est le rôle de la conception de l'ansatz dans les circuits quantiques variationnels ?",
    "A": "La conception de l'ansatz définit l'architecture du circuit paramétré qui représente les solutions candidates au problème d'optimisation. La structure de l'ansatz détermine quelles régions de l'espace de Hilbert peuvent être explorées efficacement durant l'entraînement variationnel, en équilibrant l'expressivité pour capturer de bonnes solutions et l'entraînabilité pour éviter les plateaux stériles (barren plateaus). Le choix d'un ansatz approprié — qu'il soit adapté au matériel (hardware-efficient), inspiré du problème ou motivé chimiquement — impacte directement la capacité de l'algorithme à converger vers des paramètres optimaux et à approximer l'état quantique désiré.",
    "B": "La conception de l'ansatz spécifie la structure du circuit paramétré qui contraint le flux de gradient durant l'optimisation. L'architecture de l'ansatz détermine quels paysages de fonction de coût peuvent être navigués efficacement durant la rétropropagation, en équilibrant la profondeur pour atteindre une expressivité suffisante et la largeur pour prévenir l'évanouissement du gradient. Le choix d'un ansatz approprié — qu'il soit en couches alternées, en motif de mur de briques ou préservant la symétrie — impacte directement la capacité de l'algorithme à converger vers des minima locaux et à représenter l'espace propre cible, bien que les plateaux stériles proviennent du bruit des tirs de mesure plutôt que de la topologie du circuit.",
    "C": "La conception de l'ansatz établit la séquence de portes fixe qui associe les paramètres classiques aux amplitudes quantiques représentant les candidats solutions. La topologie de l'ansatz détermine quels secteurs de symétrie de l'espace de Hilbert peuvent être échantillonnés efficacement durant les mises à jour de paramètres, en équilibrant la profondeur du circuit pour atteindre l'expressivité et la fidélité des portes pour maintenir la cohérence. Le choix d'un ansatz approprié — qu'il soit indépendant du problème, inspiré du hamiltonien ou motivé par les réseaux de tenseurs — impacte directement la convergence vers les états fondamentaux, bien que l'entraînabilité dépende principalement du choix de l'optimiseur classique plutôt que de la structure du circuit quantique elle-même.",
    "D": "La conception de l'ansatz détermine la famille unitaire paramétrée qui encode les variables d'optimisation dans les circuits de préparation d'états quantiques. Le cadre de l'ansatz définit quelles variétés de l'espace de Hilbert peuvent être atteintes efficacement durant le raffinement itératif, en équilibrant l'expressivité pour approximer les états cibles et la profondeur du circuit pour éviter la décohérence. Le choix d'un ansatz approprié — qu'il soit riche en intrication, adapté au problème ou motivé adiabatiquement — impacte directement la performance algorithmique, bien que les plateaux stériles proviennent fondamentalement de gradients exponentiellement petits dans les fonctions de coût globales plutôt que de la structure locale du circuit, rendant le choix de l'ansatz secondaire par rapport à la conception de la fonction objectif.",
    "solution": "A"
  },
  {
    "id": 452,
    "question": "Comment l'algorithme quantique pour le problème du sous-groupe caché abélien crée-t-il des états de cosets ?",
    "A": "L'application de la fonction cachée à une superposition uniforme d'éléments du groupe crée des états de cosets en associant tous les éléments d'un même coset à la même valeur de sortie. La fonction agit comme un opérateur de projection qui fait s'effondrer les éléments partageant la même relation de coset en chemins computationnels indistinguables. Ce groupement naturel par évaluation de fonction produit la structure de superposition désirée où les amplitudes sont uniformément distribuées sur les membres du coset, permettant à l'analyse de Fourier subséquente de révéler le sous-groupe caché.",
    "B": "L'application de la fonction cachée à une superposition uniforme crée des états de cosets en associant tous les éléments d'un même coset droit à des valeurs de sortie orthogonales tout en préservant la structure du coset gauche. La périodicité de la fonction par rapport au sous-groupe caché garantit que les éléments séparés par des éléments du sous-groupe reçoivent des relations de phase qui encodent la décomposition en cosets. Ce groupement par évaluation de fonction produit une structure de superposition où les phases relatives entre cosets permettent à l'analyse de Fourier subséquente de révéler les générateurs du sous-groupe par des motifs d'interférence constructive.",
    "C": "La préparation d'une superposition uniforme sur le groupe quotient suivie de multiplications contrôlées qui élèvent les représentants à des superpositions complètes de cosets dans l'espace du groupe original. L'évaluation de la fonction projette ensuite cet état élevé sur la base computationnelle en mesurant le registre ancillaire qui contient la sortie de la fonction. Cet effondrement induit par mesure crée des superpositions de poids égaux sur chaque coset avec forte probabilité, bien que la procédure puisse nécessiter plusieurs tours de préparation d'état lorsque la fonction présente un étiquetage irrégulier des cosets.",
    "D": "L'application séquentielle d'opérations de groupe contrôlées qui génèrent systématiquement des représentants de cosets par accumulation multiplicative d'éléments du sous-groupe dans un registre auxiliaire. Chaque opération de contrôle multiplie conditionnellement l'état actuel par un générateur différent du sous-groupe, construisant la structure de coset couche par couche via le parallélisme quantique. L'évaluation de la fonction est appliquée après la construction du coset pour vérifier l'appartenance, produisant la structure de superposition requise où les amplitudes se concentrent sur les éléments de coset valides qui satisfont la condition de périodicité du sous-groupe.",
    "solution": "A"
  },
  {
    "id": 453,
    "question": "Qu'est-ce qui limite la précision du comptage quantique ?",
    "A": "La précision des opérations Grover contrôlées détermine la fidélité avec laquelle le retour de phase (phase kickback) s'accumule durant les étapes d'amplification d'amplitude itérative. Puisque le comptage quantique repose sur l'application d'opérateurs Grover contrôlés avec un nombre variable d'itérations, toute erreur systématique dans l'implémentation de ces unitaires se propage à travers le circuit d'estimation de phase, causant une déviation de la phase mesurée par rapport à sa valeur idéale.",
    "B": "Le principe d'incertitude fondamental appliqué aux procédures d'estimation de phase, qui établit un compromis intrinsèque entre la variance de la phase mesurée et le nombre de requêtes à l'oracle consommées par l'algorithme. Cette borne mécanique quantique provient de la non-commutativité de l'opérateur de Grover avec l'observable de mesure de phase, empêchant la détermination simultanée et précise des propriétés de valeur propre et d'état propre. La limite de Heisenberg dicte que réduire l'incertitude de phase en dessous d'un seuil nécessite d'augmenter quadratiquement la profondeur du circuit, rendant un comptage arbitrairement précis impossible avec des ressources polynomiales, indépendamment du post-traitement classique employé.",
    "C": "Le nombre de qubits dans le registre d'estimation de phase, qui détermine directement la résolution avec laquelle les phases propres peuvent être distinguées durant l'étape de transformée de Fourier quantique. L'utilisation de plus de qubits fournit une discrimination de phase plus fine, permettant une estimation plus précise des valeurs propres de l'opérateur de Grover et donc un comptage plus précis des éléments marqués dans l'espace de recherche.",
    "D": "Le taux d'erreur de l'oracle s'accumule sur le nombre polynomial de requêtes nécessaires pour l'estimation de phase.",
    "solution": "C"
  },
  {
    "id": 454,
    "question": "Quelle est la relation entre les réseaux de neurones quantiques et l'apprentissage de circuits quantiques ?",
    "A": "Les réseaux de neurones quantiques nécessitent des architectures de portes spécifiques qui imitent explicitement les neurones biologiques — telles que des portes paramétrées arrangées en couches feedforward avec des non-linéarités de type activation implémentées via des mesures intermédiaires — alors que l'apprentissage de circuits quantiques fait référence à toute optimisation d'unitaires paramétrés génériques sans cette contrainte structurelle. La distinction clé est que les QNN imposent une topologie en couches inspirée de l'apprentissage profond classique, tandis que le QCL permet des topologies de circuits arbitraires incluant des ansatzes hautement intriqués qui ne se décomposent pas en transformations couche par couche.",
    "B": "Ces termes sont issus de groupes de recherche concurrents mais font référence essentiellement au même concept : optimiser des circuits quantiques paramétrés en utilisant une rétroaction classique. La division terminologique émergea surtout d'un accident historique plutôt que d'une distinction technique, avec 'réseaux de neurones quantiques' favorisé par les chercheurs en apprentissage automatique et 'apprentissage de circuits quantiques' préféré par les théoriciens de l'information quantique. Les deux décrivent des cadres mathématiques identiques impliquant l'optimisation variationnelle de paramètres de portes quantiques pour minimiser une fonction de coût.",
    "C": "L'apprentissage de circuits quantiques est un cadre plus large qui inclut les réseaux de neurones quantiques",
    "D": "La différence principale réside dans leurs fonctions objectif : les réseaux de neurones quantiques ciblent spécifiquement des tâches de classification supervisée où l'on associe des états quantiques d'entrée à des étiquettes discrètes par minimisation d'entropie croisée. L'apprentissage de circuits quantiques, inversement, se concentre sur l'optimisation continue comme la recherche d'états fondamentaux ou la résolution de problèmes variationnels de valeurs propres, où les fonctions de perte sont des valeurs d'espérance d'observables hermitiens plutôt que des erreurs de classification. Cela en fait des cadres fondamentalement distincts traitant de classes de problèmes différentes.",
    "solution": "C"
  },
  {
    "id": 455,
    "question": "L'algorithme HHL suppose que la matrice d'entrée est creuse, signifiant que chaque ligne :",
    "A": "Contient au plus un nombre polylogarithmique d'entrées non nulles par rapport à la dimension totale de la matrice.",
    "B": "A des entrées non nulles qui peuvent être évaluées en temps polynomial en log(N), permettant à l'étape de simulation hamiltonienne d'implémenter l'exponentielle matricielle avec une complexité de portes polylog en dimension.",
    "C": "Contient au plus s entrées non nulles où s évolue polynomialement avec log(N), permettant des requêtes d'oracle efficaces pour la sous-routine d'estimation de phase quantique qui domine le temps d'exécution de HHL.",
    "D": "Peut être encodée en bloc dans une matrice unitaire en utilisant des qubits ancillaires avec un surcoût proportionnel au nombre de non-zéros par ligne, qui doit rester polylogarithmique pour que la complexité globale de l'algorithme soit maintenue.",
    "solution": "A"
  },
  {
    "id": 456,
    "question": "La recherche d'éléments distincts basée sur la marche quantique utilise une structure de données qui stocke quelle partie de la liste dans l'état quantique ?",
    "A": "La liste complète n'est jamais stockée — la structure de marche effectue des requêtes à la demande et maintient uniquement l'information de phase sur la probabilité de collision.",
    "B": "Toutes les paires d'éléments de la liste arrangées en ordre lexicographique avec leurs valeurs de hachage cryptographique correspondantes pour faciliter la détection parallèle de collisions sur tout l'espace de recherche. Cette représentation exhaustive permet à la marche quantique de parcourir le graphe complet de comparaisons par paires en superposition, vérifiant simultanément toutes les combinaisons possibles d'éléments. Les valeurs de hachage fournissent un mécanisme de comparaison compact qui réduit la complexité en portes des tests d'égalité, tandis que l'ordre lexicographique garantit une couverture systématique de l'espace de collision sans vérifications redondantes, bien que maintenir cette structure complète nécessite O(N²) de mémoire quantique.",
    "C": "Seulement un hachage cryptographique compact des éléments échantillonnés pour minimiser la surcharge des registres et réduire la décohérence due au maintien de grands états quantiques.",
    "D": "Un sous-ensemble d'indices avec leurs valeurs interrogées, maintenu dans des registres quantiques tout au long de l'évolution de la marche. Cela permet à l'algorithme de vérifier les collisions en comparant les éléments nouvellement échantillonnés au sous-ensemble stocké, permettant la détection de valeurs dupliquées sans nécessiter le stockage de la liste complète. La taille du sous-ensemble est choisie pour équilibrer les exigences en mémoire et la probabilité de détection de collision.",
    "solution": "D"
  },
  {
    "id": 457,
    "question": "L'élagage des paramètres variationnels basé sur la magnitude du gradient est principalement employé pour :",
    "A": "Produire des circuits avec moins de paramètres qui maintiennent la performance tout en étant plus réalisables à exécuter sur les dispositifs à court terme, réduisant à la fois le nombre de portes et la sensibilité au bruit",
    "B": "Identifier les paramètres contribuant minimalement à la variation de la fonction de coût, permettant la suppression de portes dont les gradients tombent sous des seuils adaptatifs tout en préservant l'expressivité du circuit par la rétention sélective de paramètres à haute sensibilité",
    "C": "Compresser les circuits en éliminant les paramètres à faible gradient qui présentent une faible corrélation avec les changements de la fonction de coût, maintenant ainsi la performance algorithmique dans des ansätze plus superficiels déployables sur le matériel actuel",
    "D": "Réduire le nombre de paramètres en supprimant les portes avec de petites magnitudes de gradient par rapport à la médiane, créant des circuits plus épars qui conservent la performance d'optimisation tout en diminuant la susceptibilité aux phénomènes de plateau désertique",
    "solution": "A"
  },
  {
    "id": 458,
    "question": "Quelle propriété des orbitales Hartree-Fock aide à optimiser l'ordonnancement des termes ?",
    "A": "La décomposition de Pauli symétrique élimine les sous-termes d'ordre supérieur, réduisant la profondeur globale du circuit requise pour la simulation en consolidant les opérateurs commutants en blocs simultanément diagonalisables qui peuvent être exponentiés en parallèle sans accumulation supplémentaire d'erreur de Trotter, particulièrement lorsque combinée avec des heuristiques d'ordonnancement énergétique spécifiques aux orbitales.",
    "B": "L'approximation de champ moyen Hartree-Fock rend tous les termes d'interaction à deux corps diagonaux dans la base des orbitales moléculaires, permettant de les exprimer comme des sommes de coefficients scalaires réels indépendants qui peuvent être regroupés et factorisés en produits tensoriels d'opérateurs de Pauli à un qubit.",
    "C": "Parce que les orbitales Hartree-Fock diagonalisent l'opérateur de Fock, chaque orbitale correspond à un état propre d'énergie à une particule défini, ce qui signifie que l'évolution temporelle sous le hamiltonien peut être implémentée entièrement par des portes de phase à une particule appliquées indépendamment à chaque qubit.",
    "D": "Les termes effectifs de saut provenant des contributions d'énergie cinétique à une particule présentent des motifs d'annulation systématique lorsqu'ils sont évalués dans la base des orbitales Hartree-Fock, car l'approximation de champ moyen garantit que les occupations orbitales s'alignent avec la configuration électronique dominante, réduisant ainsi les éléments de matrice hors-diagonale et permettant un regroupement plus efficace des chaînes de Pauli commutantes dans la représentation hamiltonienne mappée en qubits.",
    "solution": "D"
  },
  {
    "id": 459,
    "question": "Dans le contexte des réseaux de neurones quantiques, considérez un scénario où vous entraînez un circuit quantique variationnel avec 10 qubits et 50 portes de rotation paramétrées pour classifier un ensemble de données avec un déséquilibre de classe significatif (95% classe A, 5% classe B). Le circuit utilise l'encodage d'amplitude pour les données d'entrée et mesure tous les qubits dans la base de calcul pour extraire les caractéristiques. Après 100 époques d'entraînement avec un optimiseur de descente de gradient standard, vous observez que le modèle atteint 95% de précision mais prédit la classe A pour presque tous les échantillons. En quoi les machines de Boltzmann quantiques diffèrent-elles des machines de Boltzmann classiques en termes de pouvoir de représentation, et lequel des éléments suivants serait le plus pertinent pour aborder le défi d'entraînement décrit ci-dessus ?",
    "A": "Toutes ces caractéristiques — exploration basée sur l'effet tunnel, capacité de représentation exponentielle, et corrélations médiées par intrication — sont théoriquement pertinentes, mais le problème fondamental est l'ensemble d'entraînement déséquilibré plutôt que les limitations de l'architecture du modèle. La précision de 95% obtenue en prédisant uniquement la classe A indique une convergence vers la solution triviale de classe majoritaire due aux fonctions de perte standard ignorant les fréquences de classe. Pour résoudre cela, il faut des fonctions de perte pondérées, un sur-échantillonnage de la minorité, ou des seuils de décision ajustés, aucun spécifique aux architectures quantiques versus classiques.",
    "B": "Les effets d'effet tunnel quantique permettent une traversée probabiliste de barrières énergétiques insurmontables dans le recuit thermique classique, permettant l'exploration de régions distantes de l'espace des paramètres sans temps de mélange exponentiellement longs, ce qui aide à découvrir des frontières de décision pour les classes rares avec moins d'échantillons d'entraînement puisque la dynamique quantique échantillonne la distribution de Boltzmann plus efficacement que les méthodes MCMC classiques avec une amplitude d'effet tunnel évoluant favorablement comparée à l'activation thermique classique.",
    "C": "Elles représentent certaines distributions de probabilité avec une efficacité exponentielle car la dimension de l'espace d'état quantique croît comme 2^n pour n qubits tandis que les machines de Boltzmann classiques sont limitées à une croissance polynomiale, ce qui signifie que les versions quantiques capturent des corrélations d'ordre élevé avec moins d'unités cachées — abordant directement la classification déséquilibrée en apprenant des motifs discriminatifs complexes dans les classes minoritaires sans données d'entraînement proportionnelles grâce à l'encodage simultané par superposition de toutes les 2^n configurations.",
    "D": "Les machines de Boltzmann quantiques exploitent les corrélations non-locales par intrication pour capturer des dépendances multivariées complexes dans les distributions de données, leur permettant de modéliser les motifs de classes rares plus efficacement que les approches classiques tandis que l'effet tunnel quantique pendant l'apprentissage aide à échapper aux mauvais minima locaux.",
    "solution": "D"
  },
  {
    "id": 460,
    "question": "Les dispositifs NISQ sont caractérisés par des attributs tels que les portes de base et la topologie. L'ensemble de portes de base, communément appelé ISA en informatique classique, est défini par le matériel et détermine comment les circuits quantiques utilisateur sont traduits. Que signifie ISA dans ce contexte ?",
    "A": "Integrated System Algorithm (Algorithme de Système Intégré), qui se réfère au cadre algorithmique natif que le matériel quantique utilise pour décomposer les opérations quantiques de haut niveau en instructions primitives spécifiques au dispositif. Cette terminologie est issue de la recherche précoce en informatique quantique où l'accent était mis sur l'intégration directe des algorithmes logiciels avec les capacités matérielles, particulièrement dans les systèmes utilisant le calcul quantique adiabatique où l'algorithme et le système sont co-conçus.",
    "B": "Initial State Assignment (Assignation d'État Initial), le protocole par lequel les dispositifs quantiques assignent des états de base de calcul aux qubits physiques avant le début de l'exécution du circuit. Ce processus d'assignation détermine quels qubits commencent en |0⟩ versus |1⟩ et établit la correspondance entre les indices de qubits logiques et physiques.",
    "C": "Instruction Set Architecture (Architecture de Jeu d'Instructions), le terme standard d'informatique emprunté par l'informatique quantique pour décrire les opérations de portes natives que le matériel quantique peut implémenter directement sans décomposition supplémentaire.",
    "D": "Intermediate State Approximation (Approximation d'État Intermédiaire), une technique utilisée pendant la compilation de circuits quantiques où les états quantiques intermédiaires sont approximés pour réduire la profondeur du circuit. Le cadre ISA définit quelles méthodes d'approximation sont permises en fonction des taux d'erreur matérielle et des temps de décohérence.",
    "solution": "C"
  },
  {
    "id": 461,
    "question": "Pourquoi les « gadgets hamiltoniens » apparaissent-ils dans les réductions de complexité hamiltonienne ?",
    "A": "Ils mappent les termes d'interaction à k-corps en couplages à 2-corps plus 3-corps en introduisant des degrés de liberté de spin auxiliaires dont le sous-espace de basse énergie impose les contraintes k-locales originales via des processus virtuels perturbatifs, mais contrairement aux constructions standard ils préservent la commutativité de tous les termes, permettant une vérification classique en temps polynomial des états fondamentaux via une minimisation gloutonne de l'énergie sur des projecteurs commutants.",
    "B": "Ils répliquent systématiquement les termes d'interaction à k-corps en utilisant uniquement des opérateurs de couplage à 2-corps en introduisant des qubits auxiliaires dont les pénalités énergétiques imposent les corrélations multi-particules désirées, permettant des réductions vers des classes hamiltoniennes plus simples tout en préservant la dureté computationnelle de la recherche des états fondamentaux.",
    "C": "Les gadgets encodent les contraintes k-locales dans des hamiltoniens de pénalité 2-locaux en ajoutant des qubits médiateurs dont les niveaux d'énergie intermédiaires sont ajustés de sorte que les états de basse énergie du système augmenté correspondent aux assignations satisfaisantes de la contrainte originale, mais ils introduisent des gaps spectraux qui évoluent inversement avec la force de pénalité, nécessitant une analyse perturbative soigneuse pour maintenir l'équivalence de complexité computationnelle.",
    "D": "Ils réduisent les termes à k-corps en interactions à 2-corps via des qubits auxiliaires avec des énergies de pénalité diagonales qui suppriment les états de base computationnelle indésirables, garantissant que l'hamiltonien réduit reste stochastique quand l'original était sans problème de signe, préservant ainsi à la fois l'énergie de l'état fondamental et la simulabilité classique via des méthodes de Monte Carlo quantique sans surcoût exponentiel dû aux oscillations de signe fermioniques.",
    "solution": "B"
  },
  {
    "id": 462,
    "question": "Dans les réseaux multiplexés par division temporelle, pourquoi les décisions de routage peuvent-elles changer à chaque créneau temporel ?",
    "A": "La disponibilité et la fidélité des liens varient dans le temps en raison des effets de décohérence et des fluctuations environnementales",
    "B": "Les probabilités de succès de génération d'intrication fluctuent en raison des variations d'efficacité des détecteurs et des résultats d'annonce probabilistes.",
    "C": "La topologie du réseau se reconfigure dynamiquement tandis que les nœuds répéteurs quantiques alternent entre les phases d'échange d'intrication et de purification.",
    "D": "Les protocoles adaptatifs optimisent pour des capacités de canal variant dans le temps causées par les demandes concurrentes des utilisateurs et les priorités d'allocation de ressources.",
    "solution": "A"
  },
  {
    "id": 463,
    "question": "Le code heavy-hexagon d'IBM modifie les arêtes du réseau du code de surface. Quel défi cela pose-t-il pour les décodeurs standard à couplage parfait de poids minimal (MWPM) ?",
    "A": "Les opérateurs logiques cessent de commuter une fois que les modifications d'arêtes heavy-hexagon sont introduites, car le nombre de coordination réduit à certains sommets change la classe d'homologie des boucles non-contractiles sur le réseau. Cela signifie que les opérateurs logiques X et Z, qui doivent anticommuter pour un code valide, peuvent en fait commuter sur certaines frontières du réseau modifié, détruisant la capacité du code à protéger l'information quantique. Les décodeurs MWPM standard supposent que les opérateurs logiques maintiennent leurs relations d'anticommutation tout au long du graphe de décodage, donc cette défaillance invalide les garanties d'exactitude du décodeur.",
    "B": "La fidélité de lecture dans les architectures heavy-hexagon introduit des erreurs de mesure corrélées qui violent l'hypothèse d'indépendance sous-jacente à l'extraction binaire de syndrome, amenant le décodeur à mal interpréter les défaillances de lecture multi-qubits comme de véritables violations de stabilisateurs. Cette corrélation de bruit signifie que les bits de syndrome eux-mêmes contiennent des erreurs qui ne sont pas uniformément distribuées, et les décodeurs MWPM standard qui traitent chaque bit de syndrome comme une variable aléatoire de Bernoulli indépendante sous-estimeront systématiquement les taux d'erreur réels, conduisant à une dégradation significative de la suppression d'erreur logique.",
    "C": "Les mesures se produisent à des moments différents donc on ne peut pas construire le graphe 3D habituel. Dans les topologies heavy-hexagon, l'ordonnancement temporel des mesures de syndrome est décalé entre différents types de stabilisateurs en raison de contraintes matérielles, ce qui empêche la construction d'un graphe spatio-temporel uniforme où tous les syndromes sont alignés sur un réseau régulier. Parce que le décodeur MWPM s'appuie sur l'intégration des erreurs dans un graphe 3D où l'axe temporel est discrétisé uniformément, ce désalignement temporel brise le cadre de décodage standard et nécessite des ajustements ad hoc qui ne sont pas bien pris en charge par les implémentations de décodeurs existantes.",
    "D": "Les sommets de degré irrégulier nécessitent des constructions de graphes non-bipartites, compliquant les assignations de poids d'arêtes pour le décodeur",
    "solution": "D"
  },
  {
    "id": 464,
    "question": "Comment les réseaux de neurones récurrents quantiques (QRNN) améliorent-ils les modèles d'apprentissage automatique ?",
    "A": "Les réseaux de neurones récurrents quantiques emploient des portes à phase contrôlée entre registres d'états quantiques temporellement adjacents pour encoder les corrélations séquentielles via des relations de phase d'intrication, où l'état caché de chaque pas de temps devient intriqué avec un registre de mémoire persistant qui accumule l'information de phase sur toute la séquence. Ce mécanisme de mémoire encodée par phase permet au réseau de représenter des dépendances à longue portée sans dégradation d'amplitude, bien que l'approche nécessite des protocoles d'estimation de phase soigneux lors de la lecture puisque l'information temporelle réside dans les phases relatives plutôt que dans les amplitudes de probabilité, nécessitant des schémas de mesure interférométriques qui extraient la structure de corrélation via des procédures de tomographie d'état quantique multi-temporelles.",
    "B": "Ils exploitent le parallélisme quantique pour traiter simultanément plusieurs éléments de séquence en superposition, tout en exploitant l'intrication pour créer des représentations d'états cachés plus expressives capables de capturer des dépendances temporelles complexes. Les connexions récurrentes quantiques permettent au réseau de maintenir et propager l'information sur des horizons temporels plus longs comparativement aux RNN classiques, atténuant potentiellement les problèmes de gradient évanescent grâce à la préservation des amplitudes quantiques dans les états cachés évolués unitairement.",
    "C": "Les QRNN utilisent des circuits quantiques paramétrés avec des opérateurs d'évolution unitaire fixes appliqués récurremment à chaque pas de temps, où la dynamique temporelle émerge de l'application répétée de la même séquence de portes quantiques plutôt que d'hamiltoniens dépendants du temps. L'état quantique caché accumule l'information via des boucles de rétroaction cohérentes où les résultats de mesure au temps t conditionnent l'encodage d'entrée à t+1, créant une récurrence hybride classique-quantique qui contourne les limitations de décohérence pure. Cette architecture excelle à capturer les motifs temporels car la récurrence unitaire préserve les corrélations quantiques entre pas de temps non-adjacents via une intrication multi-qubits qui persiste à travers des intervalles d'évolution sans mesure couvrant la longueur de la séquence.",
    "D": "Ces réseaux implémentent la convolution temporelle via des opérateurs de marche quantique sur des structures de graphes où les nœuds représentent des pas de temps séquentiels et les arêtes encodent des dépendances causales, permettant à l'information de se propager bidirectionnellement à travers la séquence via un effet tunnel quantique symétrique entre états temporellement non-locaux. La dynamique de marche quantique génère des superpositions sur plusieurs chemins temporels possibles simultanément, avec interférence destructive supprimant naturellement les états historiques non pertinents tout en amplifiant constructivement les motifs causalement significatifs. Cette approche fournit une accélération quadratique en longueur de séquence pour les tâches de reconnaissance de motifs car l'amplitude de marche se propage sur O(√T) pas de temps en T itérations, bien qu'elle nécessite une post-sélection pour extraire l'état caché final.",
    "solution": "B"
  },
  {
    "id": 465,
    "question": "Comment fonctionne typiquement une machine à vecteurs de support quantique améliorée ?",
    "A": "Les solveurs variationnels de valeurs propres optimisent les paramètres de marge via des circuits quantiques.",
    "B": "Les noyaux quantiques — essentiellement juste mapper les données dans l'espace de Hilbert implicitement.",
    "C": "L'estimation d'amplitude quantique accélère efficacement les calculs de matrice de noyau.",
    "D": "L'échantillonnage quantique génère des données d'entraînement dans des espaces de caractéristiques exponentiellement grands.",
    "solution": "B"
  },
  {
    "id": 466,
    "question": "Quel est un défi connu dans l'implémentation de l'algorithme de Shor sur du matériel quantique réel ?",
    "A": "Trop peu de qubits sont nécessaires, créant un paradoxe où les nombres nécessitant seulement 10-20 qubits peuvent être résolus plus rapidement de manière classique, ne laissant aucune instance de problème suffisamment grande pour démontrer un avantage quantique tout en étant suffisamment petite pour le matériel disponible.",
    "B": "Les fondements théoriques de l'algorithme de Shor restent non prouvés pour la factorisation en nombres premiers malgré une revue par les pairs exhaustive, l'étape de la transformée de Fourier quantique manquant toujours de vérification mathématique rigoureuse dans le contexte de l'exponentiation modulaire. Cela crée une incertitude quant à la capacité de l'algorithme à factoriser de manière fiable de grands nombres semi-premiers même avec un matériel quantique parfait, car les amplitudes de probabilité pourraient ne pas se concentrer correctement autour de la période de la fonction modulaire.",
    "C": "Le post-traitement classique des résultats de mesure quantique ne peut pas être effectué efficacement car l'algorithme de fraction continue requis pour extraire la période de la phase mesurée se décompose pour les nombres ayant plus de trois facteurs premiers.",
    "D": "Les exigences élevées en nombre de qubits combinées à des taux d'erreur élevés présentent des obstacles significatifs, car la factorisation de nombres cryptographiquement pertinents nécessite des milliers de qubits logiques tandis que les systèmes actuels peinent à maintenir la cohérence à travers même des centaines de qubits physiques. Les fidélités de porte doivent dépasser 99,9% pour que la correction d'erreur soit efficace, un seuil que de nombreuses plateformes n'ont pas atteint de manière constante.",
    "solution": "D"
  },
  {
    "id": 467,
    "question": "Dans les codes stabilisateurs, que mesure spécifiquement la « distance » ?",
    "A": "Le nombre minimum d'erreurs physiques à un qubit requis pour produire une erreur logique qui ne peut être détectée par aucune mesure de stabilisateur. Cette définition basée sur le poids capture la robustesse du code : un code de distance d peut détecter jusqu'à d-1 erreurs et corriger jusqu'à floor((d-1)/2) erreurs, faisant de la distance le paramètre fondamental régissant la capacité de correction d'erreur du code.",
    "B": "Le poids minimum de tout opérateur logique non trivial qui commute avec tous les stabilisateurs mais n'est pas lui-même un élément stabilisateur. Cette définition basée sur les opérateurs capture la robustesse du code : un code de distance d protège contre d-1 erreurs physiques et corrige jusqu'à floor((d-1)/2) erreurs, faisant de la distance le paramètre fondamental régissant la capacité de correction d'erreur à travers la structure de support des opérateurs logiques.",
    "C": "La séparation euclidienne minimale entre les régions de support des opérateurs logiques sur le réseau, particulièrement dans les codes topologiques comme le surface code où la distance géométrique entre les excitations anyoniques détermine la détectabilité. Un code de distance d détecte jusqu'à d-1 erreurs localisées et corrige floor((d-1)/2) erreurs en mesurant la séparation minimale des anyons pendant les cycles d'extraction de syndrome.",
    "D": "Le poids minimal de syndrome produisible par des erreurs logiques incorrigibles, quantifiant combien de mesures de stabilisateur doivent échouer simultanément avant qu'un dommage logique indétectable ne se produise. Un code de distance d tolère d-1 échecs d'extraction de syndrome et corrige floor((d-1)/2) erreurs de mesure, établissant la distance dans l'espace des syndromes comme le paramètre fondamental de correction d'erreur régissant les performances du décodeur.",
    "solution": "A"
  },
  {
    "id": 468,
    "question": "Qu'est-ce que la fonctionnelle d'influence de Feynman-Vernon dans le contexte de l'informatique quantique ?",
    "A": "Un formalisme d'intégrale de chemin décrivant comment la dynamique d'un système quantique est façonnée par son couplage à des champs classiques externes, capturant l'influence dépendante de l'historique complet des hamiltoniens de contrôle dépendants du temps sur l'évolution du système. Ce cadre mathématique est essentiel pour comprendre rigoureusement la théorie du contrôle optimal, car il encode tous les effets non adiabatiques et l'accumulation de phase dynamique qui font que les opérations de porte s'écartent de leurs unitaires cibles au fil du temps.",
    "B": "Un formalisme d'intégrale de chemin décrivant comment les degrés de liberté environnementaux se couplent au système quantique, capturant l'influence dépendante de l'historique complet du bain sur la dynamique réduite du système. Ce cadre mathématique est essentiel pour comprendre rigoureusement les processus de décohérence, car il encode tous les effets de mémoire non markoviens et les interactions dissipatives qui font que les états quantiques purs évoluent vers des états mixtes au fil du temps.",
    "C": "Un formalisme basé sur les propagateurs décrivant comment la rétroaction de mesure des qubits ancillaires influence l'évolution conditionnelle des qubits de données dans les protocoles de correction d'erreur quantique, capturant les corrélations dépendantes de l'historique complet entre les résultats de syndrome et l'information logique protégée. Ce cadre mathématique est essentiel pour comprendre rigoureusement les seuils de tolérance aux fautes, car il encode toutes les corrélations multi-tours et la dynamique induite par la mesure qui font que les codes quantiques accumulent des syndromes d'erreur au fil du temps.",
    "D": "Un formalisme d'intégrale de chemin décrivant comment les processus de bruit stochastique se couplent au système quantique via des insertions d'opérateurs ordonnées dans le temps, capturant l'influence dépendante de l'historique complet des imperfections de porte sur les trajectoires de calcul. Ce cadre mathématique est essentiel pour comprendre rigoureusement les performances moyennes des algorithmes, car il encode tous les mécanismes d'erreur corrélés et les processus de fuite cohérente qui font que les dispositifs quantiques à échelle intermédiaire bruyants s'écartent de l'évolution unitaire idéale au fil du temps.",
    "solution": "B"
  },
  {
    "id": 469,
    "question": "Lequel des éléments suivants constitue un défi dans l'intégration des embeddings quantiques avec les modèles classiques ?",
    "A": "Des nombres élevés de qubits deviennent prohibitifs lorsque les embeddings quantiques sont utilisés comme couches d'entrée pour les réseaux de neurones classiques, car le maintien d'une expressivité suffisante dans la carte de caractéristiques nécessite une mise à l'échelle de la dimension d'embedding exponentiellement avec le nombre de caractéristiques d'entrée.",
    "B": "L'incompatibilité avec les modèles linéaires résulte de la nature non linéaire de la mesure quantique, qui fait s'effondrer les états de superposition en chaînes de bits classiques via un processus stochastique qui viole le principe de superposition requis par la régression ridge et les machines à vecteurs de support. Les classificateurs linéaires classiques supposent des caractéristiques à valeurs continues qui se combinent additivement, mais les embeddings quantiques produisent des résultats discrets échantillonnés à partir de distributions suivant la règle de Born, nécessitant des contournements par astuce du noyau qui réintroduisent une charge de calcul équivalente aux méthodes d'expansion de caractéristiques classiques.",
    "C": "Difficulté à interpréter les cartes de caractéristiques non classiques, puisque les embeddings quantiques opèrent dans des espaces de Hilbert de haute dimension où les intuitions géométriques sur l'importance des caractéristiques et les frontières de décision s'effondrent. Les représentations transformées manquent de visualisation ou d'explication directe en termes de caractéristiques d'entrée originales, compliquant le débogage du modèle, la communication avec les parties prenantes et la conformité aux exigences d'interprétabilité dans les domaines réglementés.",
    "D": "Absence de compilateurs de circuits quantiques standard capables d'interfacer les sorties d'embedding quantiques avec les opérations tensorielles classiques, puisque les statistiques de mesure produites par les circuits variationnels existent dans l'espace du simplexe de probabilité plutôt que dans les espaces vectoriels euclidiens où les gradients classiques sont bien définis. Les cadres de différentiation automatique actuels comme TensorFlow et PyTorch manquent de support natif pour la rétropropagation à travers les observables quantiques, nécessitant des bibliothèques de pont personnalisées qui introduisent des instabilités numériques lorsque les mises à jour de paramètres traversent des régions où les probabilités de Born approchent zéro, particulièrement dans les architectures hybrides mélangeant des couches convolutives quantiques avec des opérations de pooling classiques.",
    "solution": "C"
  },
  {
    "id": 470,
    "question": "Quel est le but des séquences de découplage dynamique dans la conception de circuits quantiques ?",
    "A": "Supprimer la décohérence en appliquant des séquences d'impulsions soigneusement chronométrées qui moyennent le bruit environnemental sur plusieurs périodes, découplant efficacement le qubit des fluctuations lentes du bain via des techniques de refocalisation analogues aux protocoles CPMG en spectroscopie RMN, où des impulsions π sont insérées à des intervalles correspondant au temps de corrélation du spectre de bruit pour projeter les composantes haute fréquence tout en préservant l'information de base de calcul, étendant ainsi les temps T₂ de microsecondes à millisecondes dans les systèmes dominés par le bruit de charge en 1/f à travers des fonctions de filtrage qui remodèlent la susceptibilité du qubit pour correspondre aux lacunes spectrales dans la densité de puissance environnementale",
    "B": "Supprimer la décohérence en appliquant des séquences d'impulsions soigneusement chronométrées qui créent des états habillés immunisés contre le couplage environnemental via l'expansion de Magnus du hamiltonien en référentiel basculant, découplant efficacement le qubit des fluctuations lentes du bain en induisant une dynamique de Zénon où la rétroaction de mesure continue gèle les degrés de liberté environnementaux, analogue aux protocoles de pompage optique où le repompage rapide empêche la perte de population, avec des impulsions π insérées à des intervalles spécifiques dépassant le temps de corrélation du bain pour empêcher le suivi adiabatique de la manipulation du qubit, étendant ainsi les temps de cohérence de microsecondes à millisecondes dans les systèmes où la décohérence provient du couplage quasi-statique aux bains de spins nucléaires",
    "C": "Supprimer la décohérence en appliquant des séquences d'impulsions soigneusement chronométrées qui moyennent le bruit environnemental sur plusieurs périodes, découplant efficacement le qubit des fluctuations lentes du bain via des techniques de refocalisation analogues aux protocoles d'écho de spin en spectroscopie RMN, où des impulsions π sont insérées à des intervalles spécifiques pour inverser les erreurs de phase accumulées causées par les inhomogénéités quasi-statiques du champ magnétique ou le bruit de charge, étendant ainsi les temps de cohérence de microsecondes à millisecondes dans les systèmes limités par le bruit basse fréquence",
    "D": "Supprimer la décohérence en appliquant des séquences d'impulsions soigneusement chronométrées qui entraînent paramétriquement l'interaction qubit-bain dans le régime de couplage ultra-fort, découplant efficacement les états de calcul des fluctuations lentes du bain via l'ingénierie d'ondes contra-rotatives analogue aux protocoles Casimir dynamiques, où des impulsions π modulent le hamiltonien d'interaction à deux fois la fréquence de Rabi pour ouvrir des lacunes spectrales empêchant l'échange d'énergie avec les modes environnementaux, étendant ainsi les temps de déphasage de microsecondes à millisecondes dans les systèmes où T₂ est limité par le bruit de Johnson basse fréquence provenant d'éléments résistifs dans les circuits de contrôle",
    "solution": "C"
  },
  {
    "id": 471,
    "question": "Quelle vulnérabilité sophistiquée existe dans l'implémentation de la comparaison privée quantique ?",
    "A": "Fuite du choix de base via des canaux auxiliaires de synchronisation des photons.",
    "B": "Distinguabilité des états intriqués via les creux de Hong-Ou-Mandel.",
    "C": "Les dépendances d'ordre de mesure révèlent la parité des bits d'entrée.",
    "D": "Analyse de corrélation des résultats de mesure.",
    "solution": "D"
  },
  {
    "id": 472,
    "question": "Pourquoi les modifications au niveau des impulsions sont-elles considérées comme furtives ?",
    "A": "Ces attaques déclenchent des défaillances d'exécution immédiates et catastrophiques qui interrompent la compilation des circuits avant que des portes ne soient appliquées aux qubits physiques, les rendant instantanément détectables par les systèmes de surveillance automatisés mais empêchant simultanément toute computation quantique cohérente de se dérouler. L'arrêt brutal survient parce que la manipulation au niveau des impulsions perturbe les tables de calibration qui mappent les portes logiques aux formes d'onde de contrôle, provoquant le rejet du flux d'instructions malformé par le processeur quantique durant la phase de validation pré-exécution, ce qui paradoxalement rend l'attaque visible tout en rendant le circuit inopérable.",
    "B": "Les attaques au niveau des impulsions fonctionnent exclusivement dans des environnements idéalisés sans bruit où les taux de décohérence sont négligeables et les fidélités de porte approchent l'unité, puisque tout bruit ambiant masquerait immédiatement les modifications subtiles d'amplitude ou de phase introduites au niveau de la couche de contrôle. Dans les systèmes réalistes avec des temps T₁ et T₂ finis, les fluctuations environnementales dominent les distorsions d'impulsion intentionnelles, faisant en sorte que les modifications adverses soient absorbées dans le taux d'erreur de fond et deviennent ainsi opérationnellement indiscernables des imperfections matérielles naturelles, ce qui limite leur déploiement pratique aux environnements de laboratoire avec isolation extrême.",
    "C": "Les modifications au niveau des impulsions opèrent en dessous de la couche d'abstraction des portes où les mécanismes de vérification d'intégrité tels que le hachage cryptographique et les sommes de contrôle sont typiquement appliqués. Puisque ces contrôles de sécurité valident les séquences de portes au niveau du circuit logique plutôt que d'inspecter les formes d'onde de contrôle sous-jacentes, les adversaires peuvent introduire des déphasages subtils, des distorsions d'amplitude ou des perturbations temporelles dans les impulsions analogiques qui implémentent chaque porte tout en laissant inchangée la description du circuit de haut niveau et en passant inaperçus tous les protocoles de vérification standard.",
    "D": "L'implémentation de modifications au niveau des impulsions exige un accès physique direct au réfrigérateur à dilution abritant le processeur quantique, car les formes d'onde de contrôle doivent être injectées à des températures cryogéniques via des lignes coaxiales dédiées qui se terminent au niveau du boîtier de puce. Les adversaires distants ne peuvent pas exécuter ces attaques via des interfaces cloud car la planification des impulsions se produit sur des réseaux de portes programmables situés physiquement à l'intérieur de l'enceinte blindée, sous l'étage de la chambre de mélange. Cette isolation par air entre l'électronique de contrôle à température ambiante et le matériel de génération d'impulsions garantit que seul le personnel sur site avec des accréditations de salle blanche peut manipuler les signaux analogiques pilotant les transitions de qubit.",
    "solution": "C"
  },
  {
    "id": 473,
    "question": "En apprentissage automatique quantique, vous rencontrez des affirmations concernant des avantages universels par rapport aux méthodes classiques. Cependant, les informaticiens théoriciens invoquent souvent le 'théorème de l'absence de repas gratuit' lors de l'évaluation de ces affirmations. Considérez un scénario où vous concevez un algorithme quantique variationnel pour un ensemble de données spécifique et vous vous demandez si les approches quantiques surpasseront toujours les approches classiques. Quelle est la limitation fondamentale que le 'théorème de l'absence de repas gratuit' impose aux algorithmes d'apprentissage automatique quantique ?",
    "A": "Aucun algorithme quantique ne peut atteindre de meilleures performances que les méthodes classiques sur toutes les tâches possibles d'apprentissage automatique — tout avantage quantique doit être dépendant du problème et spécifique à la tâche, le théorème prouvant qu'en moyenne sur toutes les fonctions objectives possibles, les apprenants quantiques et classiques performent de manière identique, ce qui signifie que chaque domaine où le quantique excelle est équilibré par d'autres où il ne fournit aucun bénéfice",
    "B": "Les apprenants quantiques ne peuvent pas atteindre simultanément des performances optimales sur les distributions d'entraînement et de test car le théorème de l'absence de repas gratuit s'étend à la généralisation : tout modèle quantique qui s'ajuste parfaitement aux données d'entraînement tirées d'une classe de distribution doit nécessairement surajuster lorsqu'il est testé sur des distributions d'une classe complémentaire. Ce compromis fondamental est plus marqué que dans l'apprentissage classique car les modèles quantiques encodent les distributions de probabilité via des amplitudes plutôt que des probabilités directes, ce qui signifie que la contrainte d'amplitude au carré |⟨ψ|φ⟩|² impose des restrictions géométriques sur l'espace d'hypothèses que les modèles classiques évitent, forçant les algorithmes quantiques à se spécialiser plus étroitement pour bénéficier de leur capacité représentationnelle exponentielle.",
    "C": "Le théorème de l'absence de repas gratuit prouve que l'avantage quantique dans l'apprentissage automatique ne peut émerger qu'en exploitant des connaissances préalables sur la structure du problème encodée dans l'ansatz du circuit, mais cette exigence crée un dilemme : concevoir un ansatz approprié nécessite un travail computationnel classique équivalent à résoudre une large fraction du problème d'apprentissage original. Plus précisément, trouver la forme variationnelle optimale nécessite de rechercher dans un espace exponentiellement grand d'architectures de circuits possibles, et bien que les ordinateurs quantiques puissent s'entraîner plus rapidement une fois l'ansatz fixé, le coût de prétraitement classique de la sélection de l'ansatz annule l'accélération quantique lorsqu'il est amorti sur l'ensemble du flux de travail.",
    "D": "Les algorithmes quantiques doivent payer une pénalité de profondeur de circuit pour atteindre une complexité d'échantillonnage inférieure aux méthodes classiques, créant un compromis de ressources fondamental régi par le principe de l'absence de repas gratuit : toute réduction du nombre d'exemples d'entraînement requis doit être compensée par un nombre accru de portes dans le circuit quantique. Cette relation découle des bornes théoriques de l'information quantique montrant qu'extraire k bits d'information sur une fonction inconnue nécessite soit de l'interroger k fois de manière classique, soit d'implémenter un circuit quantique de profondeur Ω(k), ce qui signifie que les apprenants quantiques ne peuvent pas minimiser simultanément les exigences en données d'entraînement et les ressources computationnelles.",
    "solution": "A"
  },
  {
    "id": 474,
    "question": "Dans le contexte de la théorie des canaux quantiques, lorsque nous disons qu'un canal est « doublement stochastique » — satisfaisant à la fois l'unitalité et la préservation de la trace — quelle propriété cela garantit-il ? C'est un analogue quantique direct des matrices bistochastiques classiques qui apparaissent dans la théorie des chaînes de Markov.",
    "A": "L'état maximalement mélangé se mappe sur lui-même : l'état complètement mélangé ρ_mixed = I/d (où d est la dimension de l'espace de Hilbert) est un point fixe de tout canal doublement stochastique. Cela découle du fait que l'unitalité garantit Φ(I) = I, et puisque ρ_mixed est proportionnel à l'opérateur identité, nous avons Φ(ρ_mixed) = Φ(I/d) = I/d = ρ_mixed. Cette propriété de point fixe reflète le résultat classique selon lequel les distributions de probabilité uniformes restent uniformes sous des transformations bistochastiques.",
    "B": "Préserve le volume de la boule de Bloch : sous un canal doublement stochastique, l'image de tout opérateur densité reste à l'intérieur de la sphère de Bloch avec un rayon inchangé, car l'unitalité force Φ(I) = I, assurant que l'état maximalement mélangé I/d reste fixe, tandis que la préservation de la trace Tr[Φ(ρ)] = Tr[ρ] garantit qu'aucune probabilité ne fuit hors de l'espace d'états. Ensemble, ces contraintes signifient que le canal préserve le volume sur le corps convexe des matrices densité, analogue à la façon dont les transformations bistochastiques classiques préservent la norme L¹ des vecteurs de probabilité, bien que cela n'empêche pas la distorsion des coordonnées angulaires.",
    "C": "Majorisation des valeurs propres des matrices densité : pour tout état d'entrée ρ, la sortie Φ(ρ) a un spectre de valeurs propres λ(Φ(ρ)) qui est majorisé par λ(ρ), ce qui signifie que la sortie est plus mélangée (entropie plus élevée) à moins que ρ ne soit déjà maximalement mélangé. L'unitalité garantit que le spectre de l'identité {1/d, ..., 1/d} est préservé, tandis que la préservation de la trace assure que ce spectre uniforme sert de borne supérieure de majorisation. Cette propriété de majorisation généralise le théorème de Perron-Frobenius pour les matrices bistochastiques classiques, où les vecteurs de probabilité deviennent plus uniformes sous application répétée.",
    "D": "Contractivité sous la norme d'opérateur : les canaux doublement stochastiques satisfont ||Φ(ρ) - Φ(σ)||₁ ≤ ||ρ - σ||₁ pour tous les opérateurs densité ρ, σ, ce qui signifie qu'ils rapprochent les états distincts en distance de trace. L'unitalité force le point fixe à I/d, tandis que la préservation de la trace garantit que la structure de combinaison convexe est respectée, impliquant ensemble que Φ agit comme une application contractante sur l'espace des matrices densité. Cela garantit la convergence vers l'état maximalement mélangé sous application itérée, reflétant la façon dont les matrices bistochastiques classiques poussent les distributions de probabilité vers l'uniformité.",
    "solution": "A"
  },
  {
    "id": 475,
    "question": "Dans de nombreuses implémentations expérimentales d'algorithmes quantiques variationnels, les chercheurs ont trouvé nécessaire de modifier leurs procédures d'entraînement pour tenir compte des statistiques de mesure finies. Une approche courante consiste à introduire des fonctions de coût équilibrées dans les boucles d'entraînement basées sur les mesures. La motivation principale de cette modification est de :",
    "A": "En construisant soigneusement des fonctions de coût qui incorporent des informations de courbure de la matrice hessienne, les chercheurs peuvent garantir mathématiquement une convexité stricte du paysage objectif à travers des profondeurs de circuit arbitraires, éliminant tous les minima locaux et points de selle. Cette garantie de convexité assure qu'une simple descente de gradient, sans aucun ajustement adaptatif du taux d'apprentissage ou termes de momentum, convergera de manière prouvable vers l'optimum global unique indépendamment de l'initialisation. La formulation équilibrée remodèle la surface énergétique en un bol parfaitement lisse, exploitant les motifs d'interférence quantique pour supprimer les points critiques parasites qui autrement piégeraient les optimiseurs classiques dans des régions de paramètres sous-optimales.",
    "B": "La formulation de coût équilibré élimine complètement la boucle d'optimisation classique du cadre des algorithmes quantiques variationnels, permettant au processeur quantique d'effectuer des mises à jour autonomes de paramètres via une rétroaction de mesure auto-référentielle sans aucune supervision computationnelle externe. En encodant l'information de gradient directement dans les résultats de mesure via des observables de Pauli spécialement conçues, le circuit quantique lui-même implémente la dynamique d'optimisation à travers des cycles répétés de préparation-mesure. Cela supprime complètement le goulot d'étranglement classique, transformant le paradigme hybride quantique-classique en une procédure itérative purement quantique où l'évolution des paramètres se produit nativement dans l'espace d'états quantiques.",
    "C": "Corriger les biais systématiques qui surviennent lors de l'estimation des valeurs d'espérance à partir d'échantillons de mesure finis, en particulier lorsque différents termes de la fonction de coût ont des magnitudes ou des budgets de mesures très différents. Sans équilibrage, les termes à haute variance peuvent dominer le signal de gradient, causant une instabilité d'optimisation et une convergence médiocre. Les formulations équilibrées normalisent ou pondèrent les composantes de coût pour s'assurer que tous les termes contribuent proportionnellement aux mises à jour de paramètres malgré le bruit d'échantillonnage.",
    "D": "Les fonctions de coût équilibrées permettent des planifications agressives du taux d'apprentissage qui doublent systématiquement la taille du pas après chaque itération d'entraînement, exploitant la mise à l'échelle exponentielle des espaces d'états quantiques pour accélérer la convergence vers les paramètres optimaux. Cette stratégie de doublement exploite le principe de superposition pour explorer exponentiellement de nombreuses configurations de paramètres simultanément lors de chaque évaluation de gradient, atteignant effectivement un avantage quantique dans le processus d'optimisation lui-même. La formulation équilibrée garantit la stabilité numérique malgré les tailles de pas croissantes en normalisant les gradients selon la variance du bruit de mesure, permettant à l'algorithme de parcourir le paysage de paramètres à des vitesses exponentiellement croissantes sans dépasser les minima ou rencontrer des instabilités de divergence.",
    "solution": "C"
  },
  {
    "id": 476,
    "question": "Pourquoi les techniques conventionnelles de planification pour le calcul haute performance (HPC) nécessitent-elles des modifications pour le calcul quantique distribué ?",
    "A": "La consommation d'énergie par opération présente un comportement d'échelle fondamentalement différent dans les systèmes quantiques, où chaque opération de porte dissipe de l'énergie d'une manière qui viole les hypothèses sous-jacentes à la gestion thermique classique et aux heuristiques d'équilibrage de charge.",
    "B": "Les calculs quantiques nécessitent des temps d'exécution plus longs que les calculs classiques, ce qui rend les algorithmes de planification existants inefficaces en raison de ressources inactives et crée des goulots d'étranglement que les planificateurs par lots conventionnels n'ont pas été conçus pour gérer efficacement.",
    "C": "L'intrication et le théorème de non-clonage brisent fondamentalement les hypothèses classiques d'allocation de ressources. Les planificateurs HPC traditionnels supposent que les données peuvent être copiées librement entre les nœuds pour l'équilibrage de charge et la tolérance aux pannes, mais l'information quantique ne peut pas être clonée en raison de lois physiques fondamentales. De plus, les états intriqués créent des corrélations non locales qui ne peuvent pas être partitionnées indépendamment entre les ressources de calcul comme peuvent l'être les charges de travail classiques.",
    "D": "Les planificateurs classiques supposent déjà que la copie de qubits fonctionne correctement pour la migration d'état entre nœuds, traitant les données quantiques comme des pages mémoire classiques qui peuvent être librement répliquées pour l'équilibrage de charge ou la tolérance aux pannes.",
    "solution": "C"
  },
  {
    "id": 477,
    "question": "Quelle est la conséquence probable de l'exécution de multiples échanges d'intrication (entanglement swaps) le long d'une longue chaîne de nœuds intermédiaires sans purification ?",
    "A": "La fidélité se dégrade progressivement à chaque saut successif le long de la chaîne en raison du bruit accumulé et des mesures de Bell imparfaites aux nœuds intermédiaires, rendant finalement l'état partagé final trop mélangé pour supporter des opérations quantiques distantes fiables ou des violations significatives des inégalités de Bell",
    "B": "La fidélité diminue approximativement de manière multiplicative à chaque opération d'échange selon F_n ≈ (F_0)^n pour n échanges avec une fidélité initiale F_0, mais cette décroissance suit un modèle d'oscillation amortie caractéristique où les échanges impairs subissent une dégradation pire que les échanges pairs en raison de bases de mesure alternées aux nœuds successifs. Cette accumulation d'erreur dépendante de la parité provient d'opérateurs de mesure de Bell non commutatifs créant une cohérence de phase entre les échanges adjacents qui annule partiellement le bruit à chaque deuxième saut.",
    "C": "La fidélité se dégrade par un mécanisme fondamentalement différent de ce qui est souvent supposé : bien que les erreurs de mesure de Bell contribuent, la source dominante d'infidélité devient la décohérence pendant le temps d'attente obligatoire à chaque nœud intermédiaire pour que le prochain échange se termine le long de la chaîne. Étant donné que les protocoles distribués imposent un ordonnancement temporel des opérations d'échange pour éviter les violations de causalité, les qubits aux positions précoces de la chaîne doivent rester en mémoire pendant que les échanges ultérieurs s'exécutent, et ce temps de stockage évolue linéairement avec la longueur de la chaîne, faisant de la décroissance T2 le goulot d'étranglement principal plutôt que l'infidélité de l'opération d'échange elle-même.",
    "D": "La fidélité tombe en dessous du seuil classique (F < 0,5) après approximativement log(n) échanges pour une chaîne de n nœuds en raison du bruit de dépolarisation accumulé, mais la dégradation est auto-limitante car une fois que la fidélité atteint l'état maximalement mélangé, d'autres échanges ne peuvent pas augmenter l'entropie au-delà du maximum. Cet effet de saturation signifie que les chaînes très longues (n > 20) montrent en réalité des fidélités finales similaires quelle que soit la longueur exacte, bien que toutes tombent en dessous du régime utile pour l'avantage quantique, rendant la courbe de dégradation précise moins critique que la question binaire de savoir si la purification a été utilisée.",
    "solution": "A"
  },
  {
    "id": 478,
    "question": "Quelle technique aide à éviter les boucles de routage dans les réseaux quantiques dynamiques ?",
    "A": "Les protocoles à vecteur de distance avec règles de split-horizon empêchent les boucles de routage en garantissant que les nœuds n'annoncent jamais de routes d'intrication vers le voisin dont ils les ont apprises. Chaque nœud de routage maintient des coûts de chemin mesurés en dégradation de fidélité attendue et en nombre de sauts, mettant à jour ces vecteurs lors de la réception d'annonces d'état de lien. La modification split-horizon empêche les problèmes de comptage à l'infini dans les topologies cycliques en bloquant les annonces de route le long des chemins inverses. Lorsqu'elle est combinée avec l'empoisonnement de route—où les liens défaillants sont annoncés avec un coût infini—cela crée un routage sans boucle qui converge dans un temps borné, garantissant que les requêtes de distribution d'intrication atteignent les destinations sans circuler indéfiniment dans la topologie du réseau.",
    "B": "Les compteurs de durée de vie (TTL) sur les requêtes d'intrication virtuelles fournissent un mécanisme pour empêcher les boucles de routage infinies en imposant une limite maximale de sauts aux tentatives de distribution d'intrication. Chaque nœud de routage décrémente le champ TTL lors du transfert d'une requête d'intrication ; si le compteur atteint zéro avant que la requête n'atteigne sa destination, la requête est abandonnée et la source est notifiée. Cela empêche les requêtes de circuler indéfiniment dans les topologies de réseau cycliques, garantissant que les échecs de routage sont détectés dans un temps borné et que les ressources réseau ne sont pas épuisées par le trafic en boucle, de manière similaire à la façon dont les champs TTL des paquets IP empêchent les boucles de routage classiques.",
    "C": "Les protocoles de routage à vecteur de chemin qui suivent explicitement la séquence de systèmes autonomes traversés pendant la distribution d'intrication empêchent les boucles par filtrage de route. Chaque requête de distribution d'intrication porte une liste complète des nœuds répéteurs quantiques déjà visités le long de son chemin. Lorsqu'un nœud reçoit une requête, il examine ce vecteur de chemin—si son propre identifiant apparaît n'importe où dans la liste, la route créerait un cycle et est immédiatement rejetée. Cette détection explicite de boucle garantit que les requêtes ne revisitent jamais le même nœud deux fois, empêchant les modèles de routage circulaires tout en permettant des chemins alternatifs légitimes. La technique reflète le mécanisme AS-path de BGP mais opère sur les topologies de réseau quantique plutôt que sur les inter-réseaux classiques.",
    "D": "Les protocoles d'arbre couvrant qui construisent des topologies logiques sans boucle sur le réseau quantique physique empêchent les cycles de routage par des algorithmes de graphe distribués. Les nœuds de réseau échangent des unités de données de protocole de pont contenant des priorités de fidélité et des informations de topologie pour élire un nœud racine et désactiver les liens d'intrication redondants qui créeraient des cycles. En construisant une structure arborescente où exactement un chemin existe entre toute paire de nœuds, le protocole élimine les boucles de routage au niveau topologique. Les liens désactivés restent disponibles comme chemins de secours qui s'activent uniquement lorsque les routes principales échouent, garantissant un transfert sans boucle tout en maintenant la redondance. Cette approche est parallèle au STP Ethernet classique mais opère sur la topologie des canaux quantiques plutôt que sur les structures de commutation.",
    "solution": "B"
  },
  {
    "id": 479,
    "question": "Dans le contexte du calcul quantique distribué, pourquoi les protocoles de \"calcul quantique non local instantané\" sont-ils significatifs ? Ces protocoles impliquent des parties spatialement séparées qui partagent des états intriqués et souhaitent mettre en œuvre une opération unitaire conjointe sans déplacer physiquement les qubits. La question porte fondamentalement sur quelles ressources (intrication vs. communication classique) suffisent pour simuler des portes multi-parties arbitraires, et ce que cela nous dit sur la structure des corrélations quantiques.",
    "A": "Ils démontrent un compromis fondamental de ressources dans les systèmes quantiques distribués : l'intrication pré-partagée combinée à un nombre limité de tours de communication classique peut simuler toute opération unitaire non locale qui nécessiterait autrement le transport physique d'états quantiques entre emplacements. Cela révèle des connexions mathématiques profondes entre les taux de consommation d'intrication, les hiérarchies de complexité de communication, et la structure de localité des opérations quantiques dans les architectures distribuées. Les protocoles illuminent quelles computations quantiques multi-parties peuvent être effectuées en utilisant uniquement des opérations locales et communication classique (LOCC) augmentées d'intrication partagée, établissant des bornes rigoureuses sur le surcoût de communication classique requis pour implémenter diverses classes de portes distribuées et informant ainsi la conception de réseaux quantiques pratiques.",
    "B": "Ces protocoles établissent que certaines classes d'opérations unitaires non locales sur des systèmes quantiques bipartites peuvent être implémentées en utilisant uniquement des opérations locales et communication classique (LOCC) lorsque les parties partagent suffisamment d'intrication préalable, mais avec une contrainte subtile : les unitaires réalisables doivent préserver le rang de Schmidt bipartite de tout état d'entrée. Cette restriction provient du fait que les opérations LOCC ne peuvent pas augmenter l'intrication entre sous-systèmes, même en consommant des ressources intriquées pré-partagées. La signification réside dans l'identification des algorithmes quantiques distribués qui peuvent être exécutés sans canaux de communication quantique—spécifiquement, ceux correspondant à des unitaires compatibles LOCC. Cependant, les portes non locales générales qui augmentent le rang de Schmidt nécessitent soit la téléportation quantique (consommant de l'intrication plus deux bits classiques par qubit) soit un transfert direct d'état quantique, rendant ces protocoles fondamentaux pour comprendre les limites fondamentales du calcul quantique distribué sous contraintes de localité.",
    "C": "Les protocoles démontrent qu'en combinant intrication pré-partagée avec des tours de communication classique soigneusement structurés, des parties spatialement séparées peuvent implémenter des mesures dans des bases intriquées sans nécessiter de canaux quantiques—spécifiquement, elles peuvent effectuer des mesures projectives non locales correspondant à l'analyse d'état de Bell. L'intuition clé est que bien que les opérations unitaires sur des qubits spatialement séparés nécessitent généralement une communication quantique ou un transport physique de qubit, les opérations de mesure peuvent être simulées par une stratégie alternative : Alice effectue des mesures locales et envoie ses résultats classiques à Bob, qui applique ensuite des corrections unitaires adaptatives conditionnées à ces résultats. Cette approche basée sur la mesure du calcul non local révèle que l'intrication et la communication classique ensemble forment une théorie de ressources complète pour les protocoles quantiques distribués, établissant des bornes inférieures sur la complexité de communication requise pour simuler diverses classes de mesures quantiques multi-parties.",
    "D": "Les protocoles de calcul quantique non local instantané prouvent que l'intrication maximale entre parties spatialement séparées, combinée à un seul tour de communication classique, suffit pour implémenter toute porte unitaire multi-parties invariante par permutation sans nécessiter de téléportation quantique ou transfert direct d'état quantique. Les protocoles fonctionnent en exploitant la structure du sous-espace symétrique : lorsque toutes les parties partagent un état GHZ et effectuent des mesures locales identiques suivies de corrections dépendant des résultats, elles peuvent collectivement faire pivoter leur état quantique partagé dans le sous-espace symétrique. La signification pour le calcul quantique distribué est d'établir que les circuits quantiques symétriques—qui incluent des primitives algorithmiques importantes comme les transformées de Fourier quantiques sur des entrées symétriques par permutation—peuvent être exécutés sans le surcoût de canaux de communication quantique complets, réduisant le taux de consommation d'intrication de O(n²) à O(n) ebits par porte pour des opérations à n parties sur des états symétriques.",
    "solution": "A"
  },
  {
    "id": 480,
    "question": "De quelle manière l'utilisation de portes XOR soutient-elle la mesure d'opérateurs de Pauli multi-qubits ?",
    "A": "Les portes XOR propagent l'information de parité des qubits de données vers des qubits de mesure ancillaires par des opérations contrôlées qui préservent l'état du système tout en extrayant l'information de valeur propre, mais cela ne fonctionne que pour les mesures de stabilisateurs où l'opérateur de Pauli anticommute avec au moins une observable de base computationnelle. Pour les opérateurs qui commutent avec Z sur tous les qubits, le mécanisme XOR échoue à distinguer les espaces propres, nécessitant plutôt une transformation de base avant que l'extraction de parité puisse encoder avec succès la structure de valeur propre conjointe dans la lecture de l'ancille sans perturber l'état mesuré.",
    "B": "Les portes XOR permettent aux valeurs propres conjointes d'opérateurs de Pauli multi-qubits d'être systématiquement encodées dans un seul qubit ancillaire par propagation de parité, le tout sans perturber l'état quantique des qubits du système eux-mêmes. Cette propriété permet la mesure simultanée de tous les termes dans un ensemble commutant par une seule lecture collective, extrayant efficacement l'information de valeur propre nécessaire.",
    "C": "Les opérations XOR accumulent l'information de parité multi-qubit dans des qubits de mesure désignés par des cascades séquentielles de CNOT qui extraient les données de valeur propre sans faire s'effondrer l'état du système, permettant la lecture conjointe de termes de Pauli commutants. Cependant, la préservation de la cohérence pendant cette extraction repose sur l'ancille étant initialisée dans l'état |+⟩ plutôt que |0⟩, puisque seule la superposition symétrique permet un encodage de parité réversible par le mécanisme XOR—l'initialisation en base computationnelle causerait une rétroaction immédiate qui projette prématurément les qubits du système dans des états propres définis.",
    "D": "Les portes XOR facilitent les mesures de Pauli multi-qubits en créant de l'intrication entre les qubits du système et les ancilles de mesure d'une manière qui mappe l'information de parité conjointe à des observables à un seul qubit, mais l'avantage critique vient de la suppression d'erreur plutôt que de l'efficacité de mesure : chaque opération XOR implémente une extraction de syndrome qui détecte les erreurs de bit-flip sur les qubits de données par des vérifications de parité d'ancille, qui doivent être effectuées avant la mesure de l'observable de Pauli pour assurer une lecture précise de valeur propre, réduisant les taux d'erreur de mesure de O(ε) à O(ε²) pour une probabilité d'erreur à un qubit ε par cette vérification de parité redondante.",
    "solution": "B"
  },
  {
    "id": 481,
    "question": "Quelle technique sophistiquée permet la réconciliation de clés la plus efficace dans la distribution quantique de clés avec une fuite d'information minimale ?",
    "A": "Le protocole Cascade avec permutations aléatoires identifie et corrige itérativement les désaccords de bits entre Alice et Bob en effectuant plusieurs passages avec des tailles de blocs progressivement croissantes, exploitant les vérifications de parité sur des sous-ensembles mélangés aléatoirement pour réduire exponentiellement le taux d'erreur tout en minimisant la surcharge de communication classique — cette approche atteint une efficacité quasi-optimale en affinant adaptivement la structure des blocs en fonction des divergences détectées dans les tours précédents.",
    "B": "Les codes LDPC à débit adaptatif ajustent dynamiquement leur débit de codage en fonction du taux d'erreur quantique mesuré, permettant à l'efficacité de réconciliation d'approcher la limite de Shannon en mettant à jour itérativement l'algorithme de propagation de croyances à mesure que davantage de syndromes sont échangés — la structure creuse de la matrice de vérification de parité garantit que chaque tour de réconciliation révèle un minimum d'information à un espion tout en maintenant une complexité de décodage linéaire dans la longueur du bloc.",
    "C": "Les codes polaires avec information auxiliaire quantique exploitent le phénomène de polarisation de canal pour atteindre une efficacité de réconciliation arbitrairement proche de la limite de Shannon en divisant récursivement les canaux quantiques en sous-canaux presque parfaits et presque inutiles, permettant à Alice et Bob de transmettre sélectivement l'information uniquement à travers les canaux fiables tout en figeant les bits dans les canaux peu fiables — cette construction atteint de manière prouvée la capacité avec des bornes de performance explicites à longueur finie et une complexité polynomiale d'encodage/décodage, la rendant théoriquement optimale pour les scénarios QKD où les mesures quantiques fournissent une information auxiliaire naturelle qui peut être incorporée dans le décodeur à annulation successive pour améliorer davantage l'efficacité de réconciliation effective au-delà de ce que les codes polaires classiques atteignent seuls.",
    "D": "Les codes correcteurs d'erreurs quantiques pour la distillation de clés effectuent des mesures de syndrome sur des qubits auxiliaires intriqués pour identifier et inverser les erreurs de phase et de basculement de bits sans effondrer l'état de clé secrète partagée.",
    "solution": "C"
  },
  {
    "id": 482,
    "question": "Quelle technique sophistiquée de cryptanalyse pourrait compromettre les schémas cryptographiques post-quantiques basés sur les réseaux euclidiens ?",
    "A": "L'utilisation de l'algorithme de Grover pour accélérer les méthodes d'énumération classiques d'un facteur racine carrée, convertissant la réduction de réseau en temps exponentiel en recherche en temps polynomial grâce à l'amplification d'amplitude appliquée à l'énumération par force brute de vecteurs courts du réseau, réduisant effectivement les paramètres de sécurité de 256 bits à 128 bits contre les adversaires quantiques.",
    "B": "L'exploitation de fuites par canaux auxiliaires dans les implémentations matérielles, particulièrement lors des opérations d'échantillonnage par rejet ou d'échantillonnage gaussien, où les variations de temps ou les motifs de consommation électrique peuvent révéler des informations sur les vecteurs de base secrets du réseau ou les termes d'erreur.",
    "C": "Les algorithmes de criblage quantique qui atteignent des accélérations exponentielles par rapport aux approches classiques pour résoudre les problèmes de vecteur le plus court dans les réseaux de grande dimension, utilisant des techniques de marche aléatoire quantique et d'amplification d'amplitude pour rechercher l'espace exponentiellement grand de vecteurs candidats plus efficacement que les méthodes de criblage classiques comme l'algorithme GaussSieve, réduisant potentiellement la sécurité effective des schémas basés sur les réseaux comme Kyber et Dilithium en exploitant le parallélisme quantique dans le processus d'énumération des vecteurs tout en maintenant des exigences de mémoire quantique polynomiales.",
    "D": "Les attaques statistiques sur les distributions de bruit LWE qui exploitent des déviations subtiles de l'échantillonnage gaussien discret idéal, permettant aux adversaires de distinguer les échantillons LWE de l'uniforme en accumulant des preuves sur des milliers d'échantillons par des tests du chi-carré ou d'autres techniques d'appariement de moments qui révèlent une structure dans ce qui devrait être pseudo-aléatoire.",
    "solution": "C"
  },
  {
    "id": 483,
    "question": "Les protocoles de distillation d'états magiques de type H creux réduisent principalement le T-count en exploitant quelle propriété du code ?",
    "A": "L'avantage déterminant de la distillation d'états magiques de type H creux est que le protocole fonctionne de manière fiable à température ambiante avec des exigences de refroidissement minimales, contrairement à la distillation standard par code de surface qui exige des températures millikelvin pour supprimer les excitations thermiques. En exploitant le modèle d'erreur particulier des qubits à température plus élevée — où l'amortissement de phase domine les erreurs de basculement de bits — la structure creuse s'aligne naturellement avec les syndromes d'erreur qui se produisent dans les environnements plus chauds. Cette tolérance à la température signifie que le protocole peut utiliser une électronique de contrôle moins chère et moins précise qui n'a pas besoin de blindage cryogénique, réduisant indirectement le T-count en permettant des vitesses d'horloge plus rapides et des fidélités de porte plus élevées au niveau matériel.",
    "B": "Le protocole de type H creux incorpore une étape de post-traitement classique qui convertit algébriquement les erreurs de type Z détectées en erreurs de type X grâce à une transformation de base astucieuse appliquée après la mesure mais avant le décodage. Comme les erreurs X sur les états magiques sont considérablement plus faciles à supprimer que les erreurs Z — ne nécessitant qu'un simple vote à la majorité plutôt que des tours de distillation complexes — cette conversion transmet effectivement les erreurs de phase difficiles à corriger en erreurs de basculement de bits triviales. Cette asymétrie dans le coût de correction d'erreurs signifie qu'en déplaçant le type d'erreur par calcul classique, le protocole réduit le nombre de portes T nécessaires dans les couches de distillation ultérieures d'environ un facteur deux par tour.",
    "C": "Les protocoles de type H creux réalisent leur réduction du T-count en éliminant complètement le besoin de mesures de stabilisateurs pendant le processus de distillation, s'appuyant plutôt uniquement sur des applications de portes de Clifford transversales et un suivi d'erreurs déterministe à travers la structure du code. La distillation traditionnelle nécessite des circuits coûteux d'extraction de syndrome qui consomment des portes T supplémentaires pour l'appareil de mesure lui-même, mais en sautant ces mesures et en appliquant directement des corrections de Clifford prédéterminées basées sur la structure creuse du groupe de stabilisateurs, le protocole évite cette surcharge. Cette approche sans mesure réduit le T-count en supprimant le coût récursif de mesure et de correction dans le circuit de distillation, bien qu'elle nécessite de supposer des taux d'erreur initiaux plus faibles pour maintenir la fidélité.",
    "D": "Rendement plus élevé par tour grâce aux contraintes de stabilisateurs chevauchantes",
    "solution": "D"
  },
  {
    "id": 484,
    "question": "Quels avantages une machine de Boltzmann restreinte quantique (QRBM) a-t-elle par rapport à son homologue classique ?",
    "A": "Les QRBM exploitent les effets d'interférence quantique et l'amplification d'amplitude pour améliorer l'extraction de caractéristiques grâce à un échantillonnage en temps polynomial de distributions qui nécessitent des ressources classiques exponentielles, une estimation améliorée du gradient via l'estimation de phase quantique qui permet des mises à jour de paramètres avec quadratiquement moins d'échantillons d'entraînement, un apprentissage de représentation amélioré qui capture les corrélations multi-échelles à travers des structures d'intrication hiérarchiques, et une compatibilité native avec les ensembles de données quantiques où le prétraitement classique détruirait la cohérence quantique.",
    "B": "Les QRBM exploitent l'effet tunnel quantique entre les minima d'énergie locaux pendant la phase d'entraînement, permettant une convergence garantie vers les optima globaux dans les paysages de perte non convexes grâce à des mises à jour de paramètres adiabatiques que la descente de gradient classique ne peut atteindre, tandis que la cohérence quantique maintient des distributions de probabilité exactes sur des configurations de couches cachées exponentiellement grandes qui nécessiteraient une surcharge d'échantillonnage prohibitive dans les méthodes classiques de Monte Carlo par chaîne de Markov.",
    "C": "Les QRBM exploitent les propriétés quantiques incluant la superposition et l'intrication pour améliorer la capture de caractéristiques grâce à une capacité de représentation exponentiellement large, un entraînement et une inférence plus rapides via le parallélisme quantique qui explore simultanément plusieurs configurations, un apprentissage de représentation amélioré qui capture les corrélations complexes entre les unités visibles et cachées, et un traitement quantique natif qui gère efficacement les structures de données de haute dimension.",
    "D": "Les QRBM utilisent la contextualité quantique pour construire des représentations de couches cachées qui violent les inégalités de Bell classiques, permettant l'extraction de corrélations de caractéristiques non locales qui sont prouvablement inaccessibles à toute architecture de machine de Boltzmann classique quelle que soit sa profondeur ou sa largeur, tandis que la rétroaction de mesure quantique pendant l'échantillonnage implémente naturellement une forme de régularisation par dropout qui empêche le surapprentissage sans nécessiter de procédures d'entraînement stochastiques explicites.",
    "solution": "C"
  },
  {
    "id": 485,
    "question": "Quel mécanisme tente de réduire la diaphonie en plaçant des qubits inutilisés entre les programmes ?",
    "A": "L'expansion de mémoire quantique exploite la capacité de qubits inutilisés du processeur en intercalant des qubits inactifs comme zones tampons entre les programmes s'exécutant simultanément, s'appuyant sur le principe que les effets de diaphonie décroissent exponentiellement avec la distance physique sur la puce, bien que cela nécessite un étalonnage soigneux pour garantir que les qubits tampons restent en équilibre thermique et n'introduisent pas de bruit supplémentaire par des processus de relaxation.",
    "B": "L'ordonnancement par réalignement de phase d'impulsion coordonne les impulsions de contrôle micro-ondes à travers différents programmes en insérant des qubits inactifs à phase verrouillée entre eux.",
    "C": "L'allocation de qubits consciente de la diaphonie positionne stratégiquement les qubits de calcul avec des zones tampons de qubits inutilisés entre les programmes quantiques s'exécutant simultanément, exploitant la séparation spatiale pour minimiser les interactions de couplage indésirables qui corrompraient autrement les fidélités de porte par des termes ZZ parasites ou l'excitation de qubits spectateurs.",
    "D": "L'obfuscation par diffusion de Grover déploie des motifs d'interférence contrôlés sur les qubits tampons inactifs positionnés entre les programmes pour annuler activement les chemins de diaphonie.",
    "solution": "C"
  },
  {
    "id": 486,
    "question": "Les réseaux de neurones quantiques à faible nombre de paramètres représentent une direction de recherche critique pour les dispositifs quantiques à court terme, où la fidélité des portes et la cohérence des qubits limitent sévèrement la profondeur des circuits. Ces architectures tentent de maximiser l'expressivité des circuits quantiques variationnels tout en minimisant les coûts. Quelle exigence en ressources ciblent-elles spécifiquement pour la réduction tout en maintenant la capacité du modèle à représenter des fonctions complexes ?",
    "A": "Le nombre cumulé de mesures requis pour toutes les valeurs d'espérance, qui détermine directement le temps d'exécution total sur les processeurs quantiques actuels et évolue mal avec la complexité du circuit.",
    "B": "Le temps de cohérence matériel mesuré en microsecondes, qui limite fondamentalement la durée pendant laquelle l'information quantique peut être stockée et manipulée de manière fiable avant que la décohérence environnementale ne détruise le calcul.",
    "C": "Le nombre total d'angles de rotation entraînables dans le circuit quantique paramétré, ce qui impacte directement à la fois la charge d'optimisation classique pendant l'entraînement et la profondeur de circuit requise pour implémenter toutes les portes paramétrées. En réduisant ce nombre par des schémas de partage de poids, des ansätze structurés ou des techniques de réduction de dimension, ces architectures maintiennent l'expressivité tout en diminuant la demande sur l'estimation du gradient et les ressources d'implémentation des portes.",
    "D": "Le nombre de cœurs CPU classiques alloués aux tâches de post-traitement, incluant le calcul du gradient et les étapes d'optimisation des paramètres qui se produisent après l'exécution du circuit quantique.",
    "solution": "C"
  },
  {
    "id": 487,
    "question": "Qu'est-ce que le problème de navigation quantique de Zermelo ?",
    "A": "Le problème de diriger un système quantique le long d'une géodésique dans la variété des opérateurs de densité sous évolution de Lindblad avec des hamiltoniens de contrôle bornés, où l'on minimise la distance métrique de Bures parcourue par unité de temps sous contraintes de décohérence, traitant la dissipation comme un terme de dérive analogue aux courants océaniques dans la navigation classique de Zermelo. Cette formulation capture le contrôle optimal en temps pour les systèmes quantiques ouverts mais identifie incorrectement la métrique de Bures comme la structure géométrique pertinente plutôt que d'utiliser la géométrie de Finsler sur le groupe unitaire.",
    "B": "Trouver la façon optimale en temps d'implémenter une transformation unitaire cible sous un hamiltonien contraint, où l'on doit diriger le système quantique d'un état initial vers un état final désiré en temps minimal en choisissant des champs de contrôle qui satisfont des limitations physiques telles que des contraintes d'amplitude ou d'énergie bornées, directement analogue aux problèmes de navigation classique de Zermelo en géométrie différentielle.",
    "C": "Déterminer le protocole de temps minimal pour faire évoluer un état quantique de |ψ₀⟩ à |ψf⟩ sous un hamiltonien indépendant du temps H = H₀ + u(t)H₁ où |u(t)| ≤ u_max, résolu en appliquant le principe du maximum de Pontryagin pour trouver des courbes de commutation bang-bang dans la représentation de la sphère de Bloch. Bien que cela capture le contrôle optimal en temps, cela se restreint à une forme spécifique d'hamiltonien de contrôle et à une méthode de résolution plutôt qu'à la formulation géométrique générale caractérisant tous ces problèmes.",
    "D": "La tâche de minimiser le temps de brachistochrone quantique — la durée minimale absolue pour transformer un état pur en un autre sous évolution hamiltonienne arbitraire — où la borne est fixée par le théorème de Margolus-Levitin ΔE·T_min ≥ πℏ/2, indépendamment de la stratégie de contrôle. Bien que cela fournisse une limite de temps fondamentale pour la transformation d'état quantique, cela spécifie la borne inférieure plutôt que le problème de navigation lui-même, qui concerne la construction de protocoles optimaux en temps explicites sous contraintes de contrôle réalistes, et non simplement le calcul de la limite de vitesse quantique ultime.",
    "solution": "B"
  },
  {
    "id": 488,
    "question": "Dans le contexte de la sécurité de l'informatique quantique, considérez un scénario où un adversaire a accès à l'infrastructure cloud hébergeant un processeur quantique mais pas au matériel quantique lui-même. Il peut potentiellement injecter du code malveillant à divers points dans le pipeline de compilation et d'exécution. Étant donné que les circuits quantiques sont généralement compilés à partir de descriptions de haut niveau jusqu'à des séquences d'impulsions spécifiques au matériel, et que les paramètres de calibration doivent être régulièrement mis à jour pour tenir compte de la dérive matérielle, quel composant dans cette chaîne d'approvisionnement représente la vulnérabilité la plus critique pour un attaquant cherchant à corrompre subtilement les calculs quantiques ?",
    "A": "Le compilateur quantique lui-même, en particulier les passes d'optimisation intermédiaires qui effectuent la synthèse de circuit, la décomposition de portes et le routage de qubits, représente la vulnérabilité la plus critique car toute modification de ces étapes de transformation injecterait systématiquement des erreurs dans tous les circuits soumis par les utilisateurs sans nécessiter d'intervention par exécution.",
    "B": "Les opérateurs de mesure définis dans le jeu d'instructions quantiques, spécifiquement les transformations de base de Pauli et les mesures à valeur d'opérateur positif (POVM) qui mappent les états quantiques aux chaînes de bits classiques, représentent la surface d'attaque la plus exploitable car corrompre ces définitions d'opérateurs permet à un adversaire d'extraire des informations incorrectes d'états quantiques autrement correctement évolués. En faisant pivoter subtilement la base de mesure loin de la base computationnelle prévue ou en injectant un biais dans les éléments POVM utilisés pour les mesures généralisées, l'attaquant peut inverser les bits de sortie, supprimer des résultats spécifiques pour biaiser les distributions de probabilité, ou introduire des corrélations qui fuient des informations sur l'état quantique sans déclencher les protocoles de correction d'erreur.",
    "C": "Les données de calibration des impulsions stockées dans des fichiers de configuration ou des bases de données, car ces paramètres de contrôle de bas niveau déterminent directement les implémentations de portes physiques et sont généralement approuvés sans vérification cryptographique, permettant une manipulation ciblée d'opérations spécifiques. En altérant l'amplitude, la fréquence, la durée ou la phase des impulsions micro-ondes ou laser utilisées pour implémenter les portes quantiques, un adversaire peut introduire des erreurs systématiques qui corrompent les calculs tout en restant difficiles à détecter par des tests de référence standard. Contrairement aux attaques de plus haut niveau qui pourraient déclencher la détection d'anomalies, la corruption au niveau des impulsions apparaît comme une dérive de calibration et affecte tous les circuits utilisant les portes compromises.",
    "D": "Les définitions de portes quantiques dans l'architecture du jeu d'instructions, particulièrement les angles de rotation paramétrés (θ, φ, λ) dans les portes à un qubit et les forces de couplage dans les opérations d'intrication à deux qubits, constituent le composant le plus vulnérable car ces définitions établissent la correspondance mathématique entre les opérations quantiques abstraites et leurs transformations unitaires prévues. Si un attaquant modifie la bibliothèque de portes natives — par exemple, en altérant l'angle de rotation dans une porte Rx(π/2) en Rx(π/2 + ε) ou en ajustant les paramètres de l'hamiltonien d'interaction dans une implémentation CNOT — il introduit des erreurs cohérentes qui s'accumulent de manière prévisible à travers la profondeur du circuit tout en restant invisibles à la vérification standard au niveau des portes. Ces définitions de portes corrompues se propagent à travers toute la pile de compilation car toutes les opérations de plus haut niveau se décomposent en portes natives, et parce que les procédures de calibration mesurent et corrigent les écarts par rapport aux portes définies, l'attaque persiste même lorsque le matériel est périodiquement recalibré pour correspondre aux spécifications de portes (maintenant malveillantes).",
    "solution": "C"
  },
  {
    "id": 489,
    "question": "Dans une architecture typique de code bosonique comme le code chat ou le code GKP implémenté dans des circuits supraconducteurs, quel rôle jouent les transmons ancillaires dans le schéma de correction d'erreur, et quels types spécifiques d'erreurs sont-ils conçus pour aider à identifier ?",
    "A": "Stabiliser dynamiquement la parité du nombre de photons par des boucles de rétroaction de mesure faible continue qui suivent l'état de la cavité sans effondrer l'information logique encodée, implémentant effectivement un protocole d'ingénierie de réservoir où le transmon ancillaire médie les interactions dissipatives.",
    "B": "Ils suppriment la rétroaction de mesure pendant les opérations de porte en agissant comme un tampon entre le qubit de données et le résonateur de lecture, ce qui est particulièrement important lorsque des mesures de haute fidélité sont requises.",
    "C": "Mesurer les syndromes d'erreur pour détecter les erreurs de basculement de phase dans l'état de l'oscillateur en effectuant des mesures de parité conjointes entre le transmon ancillaire et le mode de cavité. L'ancillaire se couple de manière dispersive au mode bosonique, permettant des rotations conditionnelles qui mappent l'information de phase de la cavité sur l'état du transmon, qui peut ensuite être lu de manière destructive. Ce schéma de mesure indirecte préserve l'information quantique encodée dans l'oscillateur tout en extrayant les données de syndrome sur les sauts de phase indésirables ou les événements de perte de photons qui corrompent le qubit logique, permettant aux protocoles de correction d'erreur d'identifier quelles opérations de récupération doivent être appliquées pour restaurer l'état du code sans mesurer directement l'amplitude du champ de la cavité.",
    "D": "Implémenter une rétroaction de correction d'erreur autonome via des entraînements de déplacement cohérents appliqués au double de la fréquence de la cavité, qui interfèrent de manière constructive avec les processus d'erreur pour ramener l'état de l'oscillateur vers la variété du code.",
    "solution": "C"
  },
  {
    "id": 490,
    "question": "Que simule une marche quantique en temps continu en informatique quantique ?",
    "A": "L'évolution sous l'opérateur laplacien du graphe, où l'hamiltonien du système est proportionnel à la matrice laplacienne discrète du graphe et l'état quantique subit une évolution unitaire temporelle qui propage l'amplitude à travers les sommets selon la structure de connectivité du graphe.",
    "B": "L'évolution sous l'opérateur d'adjacence du graphe, où l'hamiltonien du système est proportionnel à la matrice d'adjacence du graphe et l'état quantique subit une évolution unitaire temporelle qui propage l'amplitude à travers les sommets selon la structure de connectivité du graphe, implémentant une dynamique identique aux marches à pièce en temps discret dans la limite continue.",
    "C": "L'évolution sous l'opérateur d'incidence du graphe, où l'hamiltonien du système est proportionnel à la matrice d'incidence arête-sommet signée et l'état quantique subit une évolution unitaire temporelle qui propage l'amplitude à travers les sommets selon les flux d'arêtes directionnels, encodant naturellement l'information d'orientation qui distingue les connexions entrantes des sortantes.",
    "D": "L'évolution sous l'opérateur stochastique du graphe, où l'hamiltonien du système est proportionnel à la matrice de probabilité de transition du graphe et l'état quantique subit une évolution unitaire temporelle qui propage l'amplitude à travers les sommets selon la structure de connectivité du graphe, préservant à la fois la conservation de probabilité et l'équilibre détaillé par symétrisation hermitienne.",
    "solution": "A"
  },
  {
    "id": 491,
    "question": "Quel défi se pose lors de la téléportation de portes non-Clifford entre processeurs quantiques distants ?",
    "A": "La téléportation de portes non-Clifford nécessite des opérations de correction par anticipation qui dépendent des résultats de mesure classiques communiqués entre les processeurs émetteur et récepteur, introduisant des goulots d'étranglement de latence dus aux vitesses finies de propagation du signal le long du canal de communication classique reliant les modules. Puisque les opérations non-Clifford ont des angles de rotation continus qui doivent être corrigés avec une précision dépassant le seuil d'infidélité de la porte, les données de correction classiques nécessitent une profondeur de bits supérieure aux résultats sur un seul bit suffisants pour la téléportation Clifford, augmentant à la fois la surcharge de communication et la probabilité d'erreurs de transmission qui se propagent à travers les couches subséquentes du circuit quantique.",
    "B": "La téléportation non-Clifford exige des états ancillaires intriqués plus sophistiqués au-delà de simples paires de Bell, notamment des états magiques dont la préparation est coûteuse en ressources et sujette aux erreurs. Puisque ces portes se situent en dehors du groupe de Clifford, elles ne peuvent pas être corrigées en utilisant uniquement des opérations de Pauli après mesure, nécessitant des ressources quantiques supplémentaires et propageant les erreurs de manière plus sévère à travers le circuit de téléportation comparativement aux portes Clifford, qui préservent la structure de stabilisateur et permettent des protocoles de correction classiques efficaces.",
    "C": "Les portes non-Clifford se transforment lors de la téléportation par conjugaison non linéaire par les opérateurs de mesure de Bell, causant le mélange des paramètres de la porte avec les résultats de mesure classiques d'une manière qui nécessite un calcul classique en temps réel pour déterminer les opérations d'anticipation correctes. Puisque ces calculs impliquent des fonctions transcendantes des angles de rotation et ne peuvent pas être pré-compilés dans des tables de consultation comme les corrections Clifford, le coprocesseur classique doit résoudre des équations non linéaires dans la fenêtre de cohérence du qubit, créant un goulot d'étranglement computationnel qui limite le taux auquel les opérations non-Clifford peuvent être distribuées à travers les modules distants.",
    "D": "La téléportation de portes non-Clifford nécessite la distillation d'états magiques dont la fidélité doit dépasser le seuil déterminé par la distance de la porte au groupe de Clifford, mesurée par son rang de stabilisateur, consommant de l'intrication à des taux qui évoluent exponentiellement avec la précision désirée de l'angle de rotation. Parce que la téléportation Clifford utilise uniquement des paires de Bell en base computationnelle et applique des corrections de Pauli dictées par les résultats de mesure sans états de ressources supplémentaires, la surcharge pour une téléportation non-Clifford de haute fidélité devient prohibitivement grande pour des angles de rotation nécessitant plus de quelques bits de précision, créant le goulot d'étranglement dominant dans les architectures modulaires tolérantes aux fautes où l'injection de portes T domine le coût en ressources.",
    "solution": "B"
  },
  {
    "id": 492,
    "question": "Pourquoi l'ansatz coupled-cluster unitaire (uCC) est-il préféré dans les simulations quantiques par rapport aux simulations classiques ?",
    "A": "La forme unitaire exp(T - T†) garantit la cohérence de taille pour la dissociation moléculaire, une propriété que les méthodes CC tronquées classiques n'atteignent qu'approximativement par un choix soigneux des opérateurs d'excitation. Bien que le CCSD classique soit cohérent en taille pour les fragments bien séparés, la formulation unitaire assure une factorisation exacte de la fonction d'onde en composantes de sous-systèmes non interagissants même avec des ensembles de base finis, rendant l'uCC supérieur pour le balayage de coordonnées de réaction. Cependant, cet avantage découle de la structure algébrique plutôt que de la compatibilité matérielle — les implémentations classiques et quantiques font face à une mise à l'échelle computationnelle similaire.",
    "B": "L'évolution unitaire à travers l'opérateur exponentiel exp(T - T†) se traduit naturellement en séquences de portes quantiques qui préservent la cohérence quantique, contrairement aux opérateurs coupled-cluster tronqués non unitaires utilisés dans les simulations classiques qui ne peuvent pas être directement implémentés sur du matériel quantique. La structure anti-hermitienne assure la préservation de la norme et la réversibilité, propriétés qui sont essentielles pour les algorithmes quantiques variationnels mais absentes dans les méthodes CC tronquées classiques.",
    "C": "Le générateur anti-hermitien (T - T†) dans le coupled-cluster unitaire commute naturellement avec l'hamiltonien électronique pour les systèmes à couches fermées aux géométries d'équilibre, permettant une implémentation directe par une seule porte de rotation paramétrée par opérateur d'excitation plutôt que de nécessiter une décomposition de Trotter. Cette commutativité apparaît parce que la référence Hartree-Fock élimine tous les termes à un corps dans l'hamiltonien en seconde quantification, ne laissant que des interactions à deux corps qui partagent la même structure de Pauli que les opérateurs de cluster. En conséquence, les circuits uCC évitent l'explosion de profondeur caractéristique de la simulation hamiltonienne générale tout en maintenant l'exactitude pour la préparation de l'état fondamental.",
    "D": "Implémenter l'uCC à travers la forme exponentielle incorpore automatiquement des effets de corrélation d'ordre infini dans chaque étape de Trotter, alors que le CC tronqué classique nécessite la construction explicite d'opérateurs d'excitation supérieurs (T₃, T₄, etc.) pour capturer la même physique. L'exponentielle génère une rotation unitaire dans l'espace de Fock qui inclut implicitement toutes les puissances de l'opérateur de cluster, sommant effectivement une série infinie qui serait intraitable classiquement. Cette resommation intégrée assure que l'uCC améliore systématiquement la précision à mesure que la profondeur du circuit augmente, convergeant vers des états propres exacts sans inclure manuellement des excitations de rang supérieur.",
    "solution": "B"
  },
  {
    "id": 493,
    "question": "Quel est l'avantage principal des codes quantiques concaténés par rapport aux codes à niveau unique ?",
    "A": "Les codes quantiques concaténés atteignent une suppression d'erreur par encodage récursif où chaque qubit logique au niveau k devient l'élément de base pour le niveau k+1, mais cette structure hiérarchique a l'avantage contre-intuitif de réduire en réalité le nombre total de qubits physiques requis par rapport aux codes à niveau unique. Alors qu'un code [[7,1,3]] nécessite 7 qubits physiques par qubit logique, le concaténer deux fois ne nécessite que 7 + 7 = 14 qubits physiques plutôt que 49, parce que l'encodage récursif permet la réutilisation des qubits à travers les niveaux de concaténation par un schéma de multiplexage temporel astucieux.",
    "B": "Les codes concaténés emploient une stratégie d'encodage hiérarchique où chaque niveau enveloppe le précédent dans des couches protectrices supplémentaires, et cette structure imbriquée permet la mesure directe des observables de qubits logiques sans d'abord décoder vers le niveau physique. En mesurant les stabilisateurs au niveau de concaténation le plus externe, vous pouvez extraire des résultats computationnels tout en laissant les états encodés internes en superposition, ce qui est essentiel pour maintenir la cohérence quantique pendant les opérations de lecture en milieu de circuit. Cette capacité de mesure sans effondrement est unique aux architectures concaténées et ne peut pas être reproduite dans les codes de surface ou autres constructions topologiques.",
    "C": "La structure récursive de la correction d'erreur quantique concaténée crée un mécanisme d'auto-renforcement de détection d'erreurs où les erreurs sont poussées vers l'extérieur à travers des couches d'encodage successives jusqu'à ce qu'elles se manifestent finalement comme des syndromes détectables à la frontière de l'espace du code. Cette migration d'erreurs vers l'extérieur signifie que l'extraction de syndrome devient obsolète après le troisième ou quatrième niveau de concaténation, puisque les erreurs se révèlent naturellement par des effets de frontière plutôt que de nécessiter des mesures de stabilisateur actives. En éliminant les cycles répétés de mesure de syndrome, les codes concaténés réduisent la surcharge du circuit d'environ 60% par rapport aux codes de surface tout en maintenant des seuils de suppression d'erreur comparables.",
    "D": "Suppression d'erreur exponentielle avec seulement un coût de ressources polynomial grâce à un encodage récursif qui amplifie la protection à chaque niveau.",
    "solution": "D"
  },
  {
    "id": 494,
    "question": "Pourquoi les portes de base sont-elles importantes en informatique quantique ?",
    "A": "Elles déterminent la résolution de l'approximation de Solovay-Kitaev : tout unitaire à n qubits peut être ε-approximé en utilisant O(log^c(1/ε)) portes de base d'un ensemble universel comme {H, T, CNOT}, où c ≈ 3,97 est l'exposant de Solovay-Kitaev. Le choix spécifique des portes de base affecte cette constante c et donc la surcharge de profondeur de circuit requise pour atteindre une précision cible. Puisque tous les algorithmes quantiques doivent finalement être décomposés en séquences approximatives de portes physiques, l'ensemble de portes de base contraint fondamentalement à la fois la complexité de compilation et la fidélité réalisable des calculs quantiques.",
    "B": "Elles définissent les opérations natives fondamentales qui sont physiquement disponibles et directement implémentables sur une architecture de processeur quantique donnée. Tous les algorithmes quantiques de niveau supérieur doivent être compilés en séquences de ces portes de base, et le choix spécifique des portes de base détermine l'efficacité et la fidélité avec lesquelles des circuits quantiques complexes peuvent être exécutés.",
    "C": "Elles établissent les propriétés de fermeture algébrique de l'ensemble de portes implémentables, assurant que toute séquence d'applications de portes de base reste dans le même groupe de Lie. Par exemple, les portes Clifford forment un groupe fini qui peut être simulé efficacement de manière classique, tandis que l'ajout de la porte T étend cela à un ensemble universel en rendant le groupe de portes dense dans SU(2^n). Le choix des portes de base détermine donc si l'ordinateur quantique peut générer des opérations en dehors des sous-groupes efficacement simulables, ce qui est essentiel pour obtenir un avantage computationnel. Sans un ensemble de base soigneusement choisi satisfaisant des conditions de fermeture de groupe spécifiques, les circuits compilés pourraient manquer d'universalité.",
    "D": "Elles servent d'opérations primitives pour lesquelles les taux d'erreur sont expérimentalement caractérisés et optimisés par ingénierie des impulsions de contrôle. Chaque porte de base correspond à une séquence de contrôle calibrée (impulsions micro-ondes, impulsions laser, etc.) avec des fidélités mesurées à un et deux qubits. Les portes de niveau supérieur doivent être décomposées en ces primitives calibrées, et l'erreur totale du circuit s'accumule selon les taux d'erreur des portes de base et la profondeur de décomposition. Le choix des portes de base impacte donc directement la fidélité réalisable du circuit, puisque la longueur de décomposition de la porte et les taux d'erreur par porte déterminent conjointement la précision globale du calcul.",
    "solution": "B"
  },
  {
    "id": 495,
    "question": "Considérez une matrice singulière H utilisée dans une simulation quantique via l'opération exp(-iHt). Bien que H ait des valeurs propres nulles et ne soit pas inversible, les simulateurs quantiques peuvent toujours la traiter sans que des problèmes mathématiques fondamentaux ne surgissent pendant l'évolution de l'état. Un étudiant étudiant la simulation hamiltonienne demande pourquoi c'est le cas, étant donné que la singularité cause typiquement des problèmes dans les méthodes numériques classiques. Quelle est la raison sous-jacente pour laquelle les matrices singulières restent viables dans ce contexte quantique ?",
    "A": "Les composantes de l'espace nul évoluent avec la phase propre exp(0·t) = 1, restant stationnaires et se projetant sur des sous-espaces non mesurables inaccessibles aux observables physiques, ne contribuant ainsi à aucune erreur numérique.",
    "B": "La contrainte unitaire de la mécanique quantique applique automatiquement une régularisation spectrale pendant l'exponentiation, remplaçant les valeurs propres nulles par de petites valeurs positives proches de la précision machine pour éviter les divergences de type classique.",
    "C": "L'exponentielle matricielle exp(-iHt) est mathématiquement bien définie pour les hamiltoniens singuliers parce que la fonction exponentielle converge pour toutes les matrices carrées indépendamment de l'inversibilité, et les valeurs propres nulles dans H contribuent simplement des termes exp(-i·0·t) = 1 à l'opérateur unitaire résultant. Ces contributions de type identité laissent les composantes de vecteurs propres correspondantes inchangées pendant l'évolution temporelle — elles restent stationnaires plutôt que de tourner dans le plan complexe. Puisque l'application exponentielle produit toujours un opérateur unitaire valide qui préserve la normalisation de l'état quantique et génère des distributions de probabilité légitimes lors de la mesure, la simulation se déroule sans rencontrer les instabilités numériques ou les opérations indéfinies qui affligent les méthodes classiques tentant d'inverser ou de décomposer des matrices singulières. L'inversibilité n'est simplement pas requise pour l'exponentiation.",
    "D": "Des paramètres temporels importants causent le dépassement de l'unité par les amplitudes des valeurs propres nulles à travers les erreurs de Trotter, mais la renormalisation automatique après chaque étape projette les états sur un espace de Hilbert valide, empêchant les distributions non physiques.",
    "solution": "C"
  },
  {
    "id": 496,
    "question": "Quelle technique avancée permet d'extraire du matériel de clé secrète à partir de réseaux de distribution quantique de clés à nœuds de confiance ?",
    "A": "En analysant les motifs temporels précis au niveau de la microseconde des opérations de relais de clés à travers les nœuds de confiance, un adversaire peut reconstruire les corrélations entre segments de clés séquentiels qui révèlent des informations partielles sur la structure XOR du matériel de clé brute grâce aux variations de latence dépendantes des données.",
    "B": "Puisque les architectures à nœuds de confiance reposent sur des canaux authentifiés classiques pour l'identification des nœuds avant que l'établissement de la clé quantique ne commence, la compromission des certificats PKI utilisés pour l'authentification permet à un attaquant d'usurper l'identité de nœuds légitimes, de demander du matériel de clé par le biais d'opérations de protocole normales, et d'exploiter les vulnérabilités d'authentification pour obtenir un statut de confiance sans nécessiter d'accès physique au canal quantique.",
    "C": "Sondage des registres de clés intermédiaires",
    "D": "Les nœuds de confiance stockent temporairement les bits de clés dérivés quantiquement dans des tampons DRAM ou SRAM avant de les transmettre aux nœuds adjacents, et ces cellules mémoire présentent des émanations électromagnétiques lorsque leur contenu change d'état pendant les opérations de lecture/écriture, permettant la reconstruction du matériel de clé transitoire à partir des émissions RF de canal auxiliaire captées par des récepteurs sensibles positionnés près du matériel.",
    "solution": "C"
  },
  {
    "id": 497,
    "question": "Les stratégies de démarrage à chaud empruntées au QAOA peuvent bénéficier aux classificateurs variationnels en :",
    "A": "Lors de la première itération d'entraînement, les protocoles de démarrage à chaud configurent le classificateur variationnel pour utiliser uniquement des rotations paramétrées à un qubit (portes RX, RY, RZ) tout en reportant toutes les opérations d'intrication à deux qubits aux époques suivantes, reproduisant la stratégie du QAOA de construire progressivement la structure du problème à travers les couches. Cette approche par phases garantit que le paysage de paramètres initial est convexe—parce que les unitaires à un qubit forment une variété de faible dimension sans plateaux arides—permettant aux optimiseurs classiques comme COBYLA ou L-BFGS de converger rapidement vers un état séparable quasi-optimal avant d'introduire l'intrication. Une fois cette phase de démarrage à chaud terminée, le classificateur introduit les portes CNOT une à la fois, utilisant la solution séparable comme point d'ancrage pour éviter les points selles dans l'espace de paramètres intriqués complet.",
    "B": "Le démarrage à chaud permet au classificateur variationnel d'allouer deux fois plus de qubits physiques pour encoder l'espace des caractéristiques sans augmenter la profondeur du circuit, car la configuration initiale des paramètres pré-intrique les qubits ancillaires avec les qubits de données dans un état produit qui double effectivement la dimension de l'espace de Hilbert. Cette technique exploite l'observation du QAOA selon laquelle des circuits plus profonds avec plus de paramètres explorent naturellement des variétés de dimension supérieure.",
    "C": "En initialisant les paramètres variationnels du classificateur selon le chemin adiabatique dérivé des hamiltoniens de mélange et de coût du QAOA, le système reste confiné à un sous-espace sans décohérence (DFS) tout au long de toutes les itérations de descente de gradient, car le DFS est préservé sous les mises à jour continues de paramètres tant que l'hamiltonien commute avec l'opérateur de moment angulaire total J². Le démarrage à chaud fixe spécifiquement les angles initiaux θ₀ et β₀ de sorte que l'état évolué dans le temps se trouve entièrement dans le sous-espace symétrique du registre de qubits, qui est immunisé contre le déphasage collectif et certains processus d'amortissement d'amplitude. Cela permet au classificateur variationnel de maintenir la cohérence sur un nombre arbitrairement élevé d'étapes d'optimisation sans nécessiter de correction d'erreurs.",
    "D": "Initialiser les paramètres variationnels près de solutions quasi-optimales en utilisant des heuristiques spécifiques au domaine ou des approximations classiques dérivées de la structure du problème, positionnant ainsi l'optimiseur dans une région favorable du paysage de paramètres où les gradients pointent vers des minima de haute fidélité et en évitant les plateaux arides ou les optima locaux médiocres qui affligent l'initialisation aléatoire.",
    "solution": "D"
  },
  {
    "id": 498,
    "question": "Quel est l'effet du placement des portes SWAP sur les performances de découpage de circuit ?",
    "A": "Un mauvais placement des SWAP augmente considérablement la profondeur du circuit et détruit la fidélité en forçant les qubits à travers des chaînes d'interaction inutilement longues.",
    "B": "Un placement sous-optimal des SWAP augmente exponentiellement le surcoût d'échantillonnage car chaque SWAP mal placé introduit des branches de quasi-probabilité supplémentaires dans la décomposition de découpage de circuit. Lorsque les qubits sont acheminés inefficacement à travers les frontières des dispositifs, l'expansion somme-de-produits-tensoriels résultante acquiert plus de termes avec des coefficients plus grands, augmentant directement le nombre d'échantillons de circuit nécessaires pour reconstruire avec précision les valeurs d'espérance dans des limites d'erreur fixes.",
    "C": "Un mauvais ordonnancement des SWAP brise la commutativité entre les partitions de sous-circuits en introduisant des dépendances parasites qui empêchent l'exécution parallèle de fragments indépendants. Lorsque les SWAP sont mal positionnés, ils créent de faux aléas de données qui forcent l'ordonnancement séquentiel d'opérations qui pourraient autrement s'exécuter simultanément sur des processeurs quantiques séparés. Ce goulot d'étranglement de sérialisation détruit le parallélisme que le découpage de circuit est conçu pour exploiter, limitant directement la scalabilité.",
    "D": "Un placement inefficace des SWAP augmente le coût de post-traitement classique en élargissant le nombre de coupures de fils nécessaires pour partitionner le circuit sur les dispositifs disponibles. Chaque SWAP supplémentaire près d'une frontière de coupure nécessite des paires mesure-préparation supplémentaires, augmentant multiplicativement la charge computationnelle de reconstruction de la fonction d'onde complète à partir de fragments distribués. Le surcoût évolue de manière combinatoire avec le nombre de SWAP mal placés traversant les frontières de partition.",
    "solution": "A"
  },
  {
    "id": 499,
    "question": "Quelle est la principale limitation de l'utilisation de la métrique d'information de Fisher quantique pour la descente de gradient naturel quantique ?",
    "A": "Le calcul de la métrique d'information de Fisher quantique nécessite de mesurer des moments statistiques d'ordre supérieur d'observables quantiques, ce qui exige des circuits significativement plus profonds que l'estimation standard du gradient. Chaque élément de matrice implique la préparation d'extensions assistées par ancilles de l'état paramétré et l'exécution d'opérations unitaires contrôlées conditionnées sur des registres de paramètres, augmentant la profondeur du circuit d'un facteur proportionnel au nombre de paramètres. Sur le matériel NISQ, cette profondeur supplémentaire fait que les erreurs de portes s'accumulent au-delà de seuils acceptables, dégradant la précision d'estimation de la métrique et sapant les avantages de convergence de l'optimisation par gradient naturel.",
    "B": "Le calcul du tenseur métrique d'information de Fisher quantique coûte exponentiellement en nombre de paramètres, nécessitant des ressources qui évoluent comme O(p²) mesures pour p paramètres. Cette mise à l'échelle quadratique en évaluations de circuit rend l'approche computationnellement prohibitive pour les algorithmes variationnels avec des centaines ou des milliers de paramètres, éliminant l'avantage pratique par rapport aux méthodes de descente de gradient standard.",
    "C": "La métrique d'information de Fisher quantique devient mal conditionnée près des points critiques dans le paysage d'optimisation où l'état quantique présente des symétries approximatives, provoquant une dispersion des valeurs propres du tenseur métrique sur plusieurs ordres de grandeur. L'inversion de cette matrice mal conditionnée pour calculer le gradient naturel amplifie les erreurs numériques et produit des mises à jour de paramètres instables qui oscillent entre des régions éloignées de l'espace des paramètres. Les techniques de régularisation standard comme l'ajout de décalages diagonaux au tenseur métrique détruisent la géométrie riemannienne sur laquelle reposent les gradients naturels, réintroduisant les pathologies de convergence que la méthode était conçue pour résoudre.",
    "D": "La métrique d'information de Fisher quantique s'applique strictement aux états quantiques purs préparés par des unitaires paramétrés agissant sur des états initiaux fixes, mais les algorithmes variationnels pratiques sur les dispositifs NISQ produisent des états mixtes en raison de la décohérence et du déphasage induit par les mesures pendant les opérations en milieu de circuit. Lorsque l'état quantique présente un véritable mélange classique plutôt qu'une superposition cohérente pure, l'information de Fisher quantique ne capture plus la structure géométrique correcte de la variété des paramètres, et les mises à jour résultantes du gradient naturel incorporent des contributions à la fois de l'information de Fisher quantique et classique de manières qui ne peuvent être démêlées sans tomographie complète de l'état quantique, ce qui réintroduit une mise à l'échelle exponentielle.",
    "solution": "B"
  },
  {
    "id": 500,
    "question": "Considérez une architecture quantique à grande échelle exécutant l'algorithme de Shor avec des millions de qubits logiques encodés à l'aide de codes de surface. À mesure que l'algorithme évolue, le surcoût computationnel devient dominé par un seul goulot d'étranglement architectural. Le système de contrôle classique peut suivre les mesures de syndrome, et la distillation d'états magiques a été optimisée. Quelle limitation fondamentale rend les codes de surface inefficaces à cette échelle ?",
    "A": "Le nombre de qubits physiques par qubit logique croît quadratiquement avec la distance du code. Pour les seuils tolérants aux pannes pertinents pour les grands algorithmes, vous avez besoin d'une distance d ≈ 20-30, ce qui signifie que chaque qubit logique nécessite 800-1800 qubits physiques. Ce surcoût se compose à travers des millions de qubits logiques, rendant le nombre total de ressources physiques astronomique—potentiellement des milliards ou des dizaines de milliards de qubits physiques—même lorsque les taux d'erreur sont relativement faibles, créant un fardeau matériel énorme qui domine toutes les autres considérations de ressources.",
    "B": "Les codes de surface imposent un circuit d'extraction de syndrome de profondeur constante indépendant de la distance du code, ce qui devient paradoxalement problématique à grande échelle. Chaque cycle de mesure de stabilisateur nécessite que les qubits ancillaires interagissent avec les qubits de données via des portes CNOT, et aux distances d ≈ 20-30 nécessaires pour les algorithmes à millions de portes, l'arrangement spatial force les forces de couplage ancille-données à s'affaiblir en raison de contraintes géométriques. Même si moins de cycles sont nécessaires par porte logique, la fidélité de couplage réduite par cycle compense exactement, provoquant la stagnation des taux d'erreur logiques autour de 10^-5 quelle que soit la distance—insuffisant pour les algorithmes nécessitant 10^8 opérations.",
    "C": "La connectivité des qubits logiques dans les codes de surface est fondamentalement non-locale : implémenter un CNOT entre des qubits logiques distants nécessite des opérations de chirurgie de réseau qui consomment un temps proportionnel à leur distance de séparation. Pour l'algorithme de Shor avec des millions de qubits logiques disposés dans un réseau 2D, la porte logique moyenne doit communiquer sur des distances évoluant comme √N, où N est le nombre de qubits logiques. Cela crée un goulot d'étranglement de latence où la profondeur du circuit croît de manière superlinéaire avec la taille du problème—non pas à cause du surcoût de correction d'erreurs, mais du temps de transit des protocoles de chirurgie de réseau propageant des défauts topologiques à travers le réseau physique pour fusionner des patchs de code distants.",
    "D": "Les codes de surface nécessitent que les qubits ancillaires pour l'extraction de syndrome soient rafraîchis à chaque cycle par des mesures projectives, qui effondrent irréversiblement leur état. Aux distances d ≈ 20-30, chaque qubit logique exige environ 50-100 ancilles mesurés simultanément par cycle. Pour des millions de qubits logiques, l'appareil de mesure doit exécuter 10^8 à 10^9 lectures à coup unique en quelques microsecondes pour maintenir la correction d'erreurs en temps réel. Même avec un traitement classique parfait, les circuits de lecture physiques ne peuvent pas paralléliser au-delà de ~10^6 canaux par cryostat en raison des limites de densité de câblage, forçant le multiplexage temporel qui introduit une latence proportionnelle au nombre de qubits et bloque le calcul.",
    "solution": "A"
  }
]