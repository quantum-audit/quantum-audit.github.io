[
  {
    "id": 1,
    "question": "¿Cuál es una complicación principal al mapear qubits lógicos a qubits físicos en hardware NISQ?",
    "A": "La topología de conectividad limitada del diseño de qubits físicos típicamente forma un grafo disperso que no puede acomodar directamente todas las interacciones de puertas de dos qubits requeridas especificadas en el circuito lógico, lo que hace necesaria la inserción de operaciones SWAP adicionales para enrutar información cuántica a través de qubits no adyacentes. Esta sobrecarga de enrutamiento aumenta sustancialmente la profundidad del circuito y amplifica los efectos de decoherencia.",
    "B": "Las fidelidades de puerta heterogéneas entre diferentes pares de qubits físicos en la topología de acoplamiento crean objetivos de optimización en conflicto durante la asignación, donde minimizar la profundidad del circuito mediante un empaquetamiento denso de qubits puede forzar operaciones de entrelazamiento críticas sobre enlaces de baja fidelidad, mientras que las asignaciones dispersas que priorizan conexiones de alta fidelidad requieren enrutamiento SWAP adicional. El mapeador debe equilibrar estas restricciones en competencia sin conocimiento completo de las tasas de error en tiempo de ejecución, ya que las fidelidades de las puertas fluctúan con la deriva de calibración y patrones de diafonía que dependen de la programación de puertas específica producida por las decisiones de asignación, creando dependencias circulares donde el mapeo óptimo requiere conocer la programación final pero la programación óptima depende del mapeo elegido.",
    "C": "Los tiempos de coherencia T1 y T2 diferenciales a través del arreglo de qubits físicos introducen restricciones temporales que entran en conflicto con las restricciones espaciales impuestas por la conectividad limitada, requiriendo que el compilador optimice simultáneamente tanto la asignación de qubits como la programación de puertas para emparejar qubits de corta vida con operaciones que ocurren temprano en el circuito mientras reserva qubits de alta coherencia para operaciones posteriores. Este problema de optimización acoplado se vuelve intratable para circuitos que exceden un tamaño modesto porque cada mapeo candidato induce un camino crítico diferente a través de la topología del circuito que determina qué qubits experimentan los períodos de inactividad más largos, forzando al asignador a resolver problemas de programación con restricciones de recursos NP-difíciles iterativamente para cada configuración de mapeo de prueba antes de identificar la solución globalmente óptima.",
    "D": "La asimetría bidireccional de las implementaciones nativas de CNOT en muchas plataformas NISQ restringe qué qubit en cada par físico puede servir como control versus objetivo, creando restricciones de borde dirigido en el grafo de acoplamiento que limitan los mapeos lógico-a-físico factibles en comparación con suposiciones de conectividad no dirigida. Cuando el circuito lógico requiere operaciones CNOT en ambas direcciones entre dos qubits lógicos mapeados a un par físicamente conectado, el compilador debe insertar puertas SWAP adicionales o descomponer una dirección CNOT en la dirección disponible usando conjugación de Hadamard, aumentando el conteo de puertas en factores que se agravan a través de múltiples conflictos de este tipo, particularmente problemático para circuitos con patrones densos de entrelazamiento bidireccional que no pueden satisfacerse mediante ningún mapeo que respete las restricciones direccionales.",
    "solution": "A"
  },
  {
    "id": 2,
    "question": "¿Qué propiedad de la mecánica cuántica permite a las computadoras cuánticas realizar ciertos cálculos más rápido que las computadoras clásicas?",
    "A": "El determinismo clásico codificado en sistemas cuánticos mediante evoluciones unitarias cuidadosamente diseñadas que preservan relaciones deterministas entre estados de entrada y salida — al mapear operaciones lógicas clásicas a puertas cuánticas reversibles mientras se mantiene una causalidad estricta, las computadoras cuánticas pueden aprovechar la evolución predecible de sistemas cuánticos cerrados para lograr aceleraciones computacionales.",
    "B": "Distribuciones de probabilidad absolutas que permanecen constantes a lo largo del cómputo cuántico, proporcionando pesos estadísticos estables para cada estado de la base computacional — a diferencia de los algoritmos probabilísticos clásicos donde las distribuciones de probabilidad evolucionan impredeciblemente, la mecánica cuántica asegura que las probabilidades de la regla de Born sean cantidades conservadas.",
    "C": "La superposición permite que las computadoras cuánticas existan en múltiples estados computacionales simultáneamente, permitiéndoles explorar exponencialmente muchas rutas de solución en paralelo mediante una sola evolución coherente — cuando se combina con efectos de interferencia que amplifican amplitudes de respuestas correctas mientras cancelan las incorrectas, y el entrelazamiento que crea correlaciones entre qubits que no tienen análogo clásico, la superposición forma la base para las aceleraciones cuánticas al permitir que algoritmos como el de Shor y el de Grover procesen vastos espacios de solución usando recursos cuánticos polinomiales donde las computadoras clásicas requerirían tiempo exponencial. La clave es que la medición colapsa esta superposición para extraer el resultado computacional, pero durante la evolución, todos los estados de la base contribuyen a la dinámica simultáneamente.",
    "D": "Estados computacionales fijos que los sistemas cuánticos mantienen naturalmente debido a principios de minimización de energía — las computadoras cuánticas explotan el hecho de que los qubits permanecen preferentemente en sus estados base inicializados a menos que sean perturbados explícitamente.",
    "solution": "C"
  },
  {
    "id": 3,
    "question": "¿Cuál es la idea clave detrás de las Redes Generativas Antagónicas Cuánticas (QGANs)?",
    "A": "Los circuitos cuánticos sirven tanto de generador como de discriminador, formando un bucle de entrenamiento competitivo donde el generador prepara estados cuánticos parametrizados por circuitos ansatz variacionales mientras que el discriminador, también implementado como un circuito cuántico parametrizado, realiza mediciones para distinguir datos de entrenamiento reales de muestras generadas. Esta arquitectura antagónica cuántica-a-cuántica permite el flujo de gradiente a través de canales cuánticos y potencialmente logra una aceleración cuadrática en la tarea discriminativa en comparación con discriminadores de redes neuronales clásicas.",
    "B": "Explotan el entrelazamiento cuántico para modelar correlaciones exponencialmente complejas de alta dimensión que las GANs clásicas tienen dificultad para capturar eficientemente, mientras simultáneamente aprovechan la superposición para explorar el espacio de muestras de manera mucho más efectiva que los métodos de muestreo clásicos. Al codificar correlaciones en estados entrelazados en lugar de parámetros explícitos, las QGANs pueden representar distribuciones de probabilidad conjunta que requerirían exponencialmente muchos parámetros clásicos, proporcionando una ventaja exponencial potencial en ciertas tareas de modelado generativo donde los datos exhiben dependencias estadísticas de largo alcance similares a las cuánticas.",
    "C": "La superposición genera múltiples muestras a la vez, esencialmente creando una distribución de probabilidad completa simultáneamente en lugar de muestrear secuencialmente como las GANs clásicas.",
    "D": "Todos estos mecanismos trabajando juntos forman la base de cómo las QGANs logran sus ventajas computacionales sobre los modelos generativos clásicos.",
    "solution": "D"
  },
  {
    "id": 4,
    "question": "¿Cuál es la función principal de un repetidor cuántico en la Internet Cuántica?",
    "A": "Realizar corrección de errores cuánticos en qubits transmitidos en nodos intermedios mediante la codificación de qubits lógicos en códigos de múltiples qubits como el código de Steane o el código de superficie, permitiendo que los errores acumulados durante la transmisión sean detectados y corregidos antes del reenvío. Esto permite la comunicación cuántica de larga distancia mediante la actualización continua de información cuántica a través de corrección de errores activa en cada estación repetidora, evitando la decoherencia sin violar la no-clonación ya que las operaciones de corrección de errores preservan el estado lógico codificado mientras descartan síndromes de error.",
    "B": "Extender la distribución de entrelazamiento más allá de los límites de transmisión directa mediante protocolos de intercambio y purificación de entrelazamiento, que permiten el establecimiento de pares entrelazados de alta fidelidad entre nodos distantes a pesar de la pérdida de fotones y la decoherencia. Al dividir largas distancias en segmentos más cortos y realizar mediciones de Bell en nodos intermedios, los repetidores cuánticos evitan la desintegración exponencial del entrelazamiento con la distancia.",
    "C": "Implementar generación de entrelazamiento determinista entre segmentos de red adyacentes almacenando qubits fotónicos en memorias cuánticas basadas en materia y realizando creación de entrelazamiento anunciado mediante interferencia de dos fotones en divisores de haz. Este proceso de dos etapas primero establece entrelazamiento probabilísticamente mediante interferencia Hong-Ou-Mandel, luego usa memorias cuánticas para sincronizar eventos de entrelazamiento exitosos a través de múltiples enlaces, extendiendo efectivamente los rangos de transmisión de estados cuánticos mientras evita la necesidad de purificación cuando los tiempos de coherencia de memoria exceden la tasa de generación de entrelazamiento.",
    "D": "Amplificar la fidelidad del entrelazamiento degradado mediante aplicación secuencial de protocolos de destilación de entrelazamiento como los esquemas BBPSSW o DEJMPS, que consumen múltiples pares entrelazados ruidosos para producir menos pares de alta fidelidad mediante operaciones locales y comunicación clásica en nodos intermedios. Al filtrar iterativamente errores mediante mediciones bilaterales y post-selección, los repetidores cuánticos restauran la calidad del entrelazamiento que se ha degradado durante la transmisión, permitiendo comunicación cuántica de larga distancia sin requerir códigos de corrección de errores cuánticos o mediciones de estados de Bell entre partes distantes.",
    "solution": "B"
  },
  {
    "id": 5,
    "question": "¿Por qué el ordenamiento de términos intercalado reduce el error de Trotter-Suzuki en química cuántica?",
    "A": "Al alternar componentes diagonales y no diagonales del Hamiltoniano en cada paso de Trotter, los términos diagonales efectivamente suprimen la transferencia de población inducida por transiciones no diagonales, ya que la evolución de fase acumulada de operadores diagonales crea patrones de interferencia destructiva que cancelan amplitudes de transición no deseadas antes de que puedan acumularse a niveles significativos durante múltiples segmentos de tiempo.",
    "B": "Cuando los términos en el Hamiltoniano están intercalados en lugar de bloqueados por tipo, el espectro de Fourier de la evolución de Trotter resultante se distribuye más uniformemente entre los componentes de frecuencia, aplanando efectivamente oscilaciones de alta frecuencia que de otro modo se compondrían a través de pasos de tiempo sucesivos. Este aplanamiento espectral reduce la amplitud de términos de error en la expansión de Baker-Campbell-Hausdorff, llevando a cancelaciones en las contribuciones del conmutador de segundo orden.",
    "C": "Los subtérminos agrupados por espacio propio minimizan la acumulación de error de fase entre exponenciales.",
    "D": "El intercalado alterna entre diferentes tipos de términos del Hamiltoniano a lo largo de la secuencia de Trotter en lugar de agrupar todos los términos de un tipo juntos, lo que reduce la acumulación sistemática de errores de conmutador. Cuando los términos no conmutativos están intercalados, los exponenciales consecutivos contienen con mayor frecuencia operadores que cancelan parcialmente los errores del otro mediante términos de expansión de Baker-Campbell-Hausdorff, llevando a una precisión general mejorada en comparación con el ordenamiento bloqueado donde los errores se componen unidireccionalmente.",
    "solution": "D"
  },
  {
    "id": 6,
    "question": "¿Qué metodología de ataque avanzada puede comprometer la seguridad de las criptomonedas cuánticas?",
    "A": "Doble gasto mediante ataque de superposición, donde un adversario prepara una transacción maliciosa en una superposición coherente de múltiples estados conflictivos, permitiéndole transmitir simultáneamente operaciones de gasto incompatibles a diferentes nodos de la red. Al ser medido por el mecanismo de consenso de la red, el atacante puede colapsar selectivamente la superposición hacia la rama que produzca el resultado más favorable, gastando efectivamente el mismo token cuántico múltiples veces antes de que se alcancen los límites de decoherencia.",
    "B": "Creación de bifurcación de blockchain cuántico, que explota el teorema de no-clonación a la inversa utilizando intercambio de entrelazamiento para generar historias paralelas de blockchain causalmente consistentes que parecen válidas bajo los protocolos estándar de verificación.",
    "C": "Recuperación de claves acelerada por Shor, que aplica el algoritmo de Shor para factorizar los números compuestos grandes que subyacen a las primitivas criptográficas de clave pública utilizadas en los protocolos de criptomonedas cuánticas. Al calcular eficientemente logaritmos discretos o factorizar módulos RSA en tiempo polinómico, un adversario con una computadora cuántica tolerante a fallos puede derivar claves privadas de direcciones públicamente transmitidas, permitiendo la firma de transacciones no autorizadas y el compromiso completo de la seguridad de las carteras en toda la red.",
    "D": "Ventaja del algoritmo de minería cuántica, mediante el cual un adversario con acceso a una computadora cuántica tolerante a fallos puede aprovechar el algoritmo de Grover para lograr una aceleración cuadrática en el proceso de resolución de puzzles de prueba de trabajo comparado con los mineros clásicos. Esta aceleración se compone exponencialmente a lo largo de múltiples bloques, permitiendo al minero cuántico dominar la creación de bloques y controlar el ordenamiento de transacciones, centralizando efectivamente lo que debería ser un mecanismo de consenso distribuido y permitiendo la censura o manipulación retrospectiva de transacciones.",
    "solution": "C"
  },
  {
    "id": 7,
    "question": "En el contexto de implementaciones prácticas de aprendizaje automático cuántico, los investigadores han explorado diversos enfoques para hacer viables las Máquinas de Vectores de Soporte Cuánticas (QSVMs) en dispositivos de corto plazo a pesar de limitaciones significativas del hardware. Considere un escenario donde está implementando una QSVM en un procesador superconductor de 50 qubits con tiempos T1 alrededor de 100 microsegundos y fidelidades de puertas de dos qubits del 99%. ¿Qué es esencial para que las QSVMs reduzcan el ruido y los errores computacionales bajo estas restricciones realistas?",
    "A": "Maximizando el entrelazamiento multipartito a través de los 50 qubits mediante la aplicación agresiva de escaleras CNOT y puertas de fase controlada, el estado cuántico se vuelve cada vez más robusto frente a la decoherencia local debido a la naturaleza distribuida de la codificación de información cuántica, que permite que los errores en qubits individuales se diluyan a través del sistema entrelazado en lugar de corromper puntos de datos específicos, proporcionando así una forma inherente de redundancia que estabiliza el cálculo del kernel sin códigos explícitos de corrección de errores.",
    "B": "Los pipelines de preprocesamiento clásico pueden diseñarse para identificar y filtrar muestras de entrenamiento que requerirían circuitos cuánticos profundos que excedan la ventana de coherencia, curando efectivamente un conjunto de datos resistente al ruido cuyas entradas de matriz kernel pueden estimarse con circuitos poco profundos.",
    "C": "Operar con un presupuesto de qubits restringido reduce dramáticamente las tasas de error acumulativo al acortar el ancho del circuito.",
    "D": "Técnicas robustas de corrección de errores y arquitecturas cuánticas escalables que puedan manejar la acumulación de errores a través de múltiples capas de puertas mientras mantienen suficiente profundidad de circuito para una evaluación significativa del kernel, combinadas con estrategias de mitigación de errores como extrapolación de ruido cero y cancelación probabilística de errores que compensan las puertas imperfectas sin la sobrecarga completa de códigos tolerantes a fallos.",
    "solution": "D"
  },
  {
    "id": 8,
    "question": "¿Cuál es la relación entre la capacidad de entrelazamiento de un circuito cuántico y su expresividad?",
    "A": "Inversamente relacionadas de tal manera que aumentar la capacidad de entrelazamiento de un circuito cuántico necesariamente disminuye su expresividad, porque los estados altamente entrelazados forman un subconjunto de medida cero del espacio de Hilbert total y los circuitos optimizados para generar entrelazamiento máximo se especializan hacia estos estados atípicos.",
    "B": "La capacidad de entrelazamiento determina el límite superior de la expresividad en el sentido de que un circuito nunca puede alcanzar valores de expresividad que excedan su capacidad normalizada de generación de entrelazamiento, ya que los estados que no están suficientemente entrelazados ocupan solo un subespacio limitado del espacio de Hilbert total.",
    "C": "Son propiedades no relacionadas porque la capacidad de entrelazamiento mide solo las correlaciones bipartitas entre subsistemas mientras que la expresividad cuantifica cuán uniformemente un circuito parametrizado puede muestrear el espacio de estados completo.",
    "D": "Los circuitos con mayor capacidad de entrelazamiento tienden a tener mayor expresividad, ya que la capacidad de generar estados entrelazados a través de múltiples qubits permite al circuito acceder a una distribución más amplia y uniforme sobre el espacio de Hilbert, lo cual está directamente correlacionado con la capacidad del circuito para representar diversos estados cuánticos necesarios para algoritmos variacionales y tareas de aprendizaje automático cuántico.",
    "solution": "D"
  },
  {
    "id": 9,
    "question": "¿Por qué son útiles los Hamiltonianos efectivos no-Hermitianos en el modelado de sistemas cuánticos abiertos?",
    "A": "Los Hamiltonianos no-Hermitianos permiten la representación de dinámicas de sistemas cerrados con condiciones de contorno dependientes del tiempo mediante valores propios de energía complejos cuyas componentes imaginarias codifican la tasa de flujo de información cuántica a través de las fronteras del sistema. Este formalismo es particularmente útil al modelar sistemas acoplados a reservorios Markovianos porque la componente anti-Hermitiana captura la disipación mientras preserva la estructura simpléctica requerida para generar evolución temporal física mediante ecuaciones de Schrödinger modificadas que consideran el acoplamiento ambiental.",
    "B": "Los términos imaginarios de energía codifican procesos de decaimiento y disipación—esto permite usar ecuaciones de evolución de Schrödinger estándar mientras se captura naturalmente la dinámica no-unitaria de sistemas acoplados a entornos externos, proporcionando un marco computacionalmente eficiente para el modelado fenomenológico de la decoherencia.",
    "C": "El marco no-Hermitiano proporciona una descripción natural de dinámicas condicionales donde la evolución del sistema depende de resultados de medición nulos—ningún salto cuántico detectado. Los valores propios complejos codifican tanto la evolución coherente como los canales de decaimiento, con las partes imaginarias representando tasas de pérdida que renormalizan la norma del sistema restante. Este enfoque se vuelve exacto en el límite de medición continua donde los operadores de salto se escalan apropiadamente, haciéndolo particularmente útil para modelar sistemas bajo monitoreo débil continuo.",
    "D": "Los Hamiltonianos efectivos no-Hermitianos capturan dinámicas de post-selección donde ciertos resultados de medición se descartan, con la componente anti-Hermitiana codificando la corriente de probabilidad para trayectorias que no son post-seleccionadas. El espectro complejo permite el cálculo eficiente de evolución condicional para sistemas que exhiben efectos Zeno cuánticos, donde mediciones proyectivas frecuentes alteran las tasas efectivas de decaimiento. Este formalismo preserva la estructura analítica de la evolución de Schrödinger mientras incorpora el colapso no-unitario asociado con el condicionamiento de medición mediante contribuciones imaginarias de energía.",
    "solution": "B"
  },
  {
    "id": 10,
    "question": "Al realizar un CNOT lógico mediante cirugía de celosía entre parches de código de superficie planar, ¿qué compensación de recursos influye más en la latencia general del circuito?",
    "A": "Durante la fase de fusión de la cirugía de celosía, las vacantes (qubits físicos faltantes) dentro del volumen de cualquiera de los parches crean regiones donde las mediciones de estabilizador no pueden realizarse, reduciendo localmente la distancia de código efectiva del parche fusionado. Si la densidad de vacantes en el volumen excede un umbral crítico—típicamente alrededor del 5-10% dependiendo del rendimiento del decodificador—el parche fusionado no puede mantener su tasa de error lógico nominal porque los errores pueden proliferar a través de regiones adyacentes a vacantes más rápido de lo que el decodificador puede corregirlos.",
    "B": "En arquitecturas de código de superficie multi-parche, cada qubit físico que sirve como ancilla para la extracción de síndrome opera a una frecuencia resonante específica para permitir direccionamiento selectivo. Al realizar cirugía de celosía entre parches, todos los qubits ancilla a lo largo de la frontera compartida deben permanecer desafinados de sus frecuencias en reposo durante la operación de fusión para prevenir participación no deseada en mediciones de síndrome de parches vecinos.",
    "C": "Los parches de código de superficie se definen mediante plaquetas alternadas de mediciones de estabilizador tipo X y tipo Z, con el color del borde (X o Z) determinando qué operador de Pauli lógico tiene una representación de bajo peso a lo largo de ese borde. Al fusionar parches para cirugía de celosía, elegir el color de borde incorrecto para la operación lógica deseada significa que la cirugía no puede implementar directamente la puerta deseada, requiriendo en su lugar la inyección de un estado mágico seguido de teletransportación de puerta.",
    "D": "El ancho de la frontera compartida entre parches que se fusionan versus el número de rondas secuenciales de fusión-división requeridas para completar la operación lógica—fronteras más anchas permiten una estabilización de síndrome más rápida después de la fusión pero consumen más qubits físicos y aumentan el tiempo de inactividad para parches no involucrados, mientras que fronteras más estrechas reducen la sobrecarga de qubits pero extienden la duración de cada ciclo de cirugía debido a tiempos de convergencia de corrección de errores más largos, forzando una compensación directa entre recursos espaciales y costo de ejecución temporal que domina la latencia total del circuito.",
    "solution": "D"
  },
  {
    "id": 11,
    "question": "Supongamos que un estado cuántico |φ⟩ es teleportado desde un qubit de datos en el procesador A hacia un qubit de comunicación en el procesador B utilizando TeleData. El estado ahora reside completamente en el procesador B, lo que significa que el procesador A ya no contiene ninguna información cuántica sobre |φ⟩—el qubit de datos original ha sido medido y colapsado. Si deseas realizar operaciones adicionales que involucren qubits de datos en el procesador A que dependan del estado |φ⟩, enfrentas una restricción fundamental: la información cuántica no puede ser copiada (teorema de no clonación), y ahora se encuentra en un procesador diferente. ¿Cuál de las siguientes afirmaciones es necesariamente cierta para permitir operaciones futuras que involucren qubits de datos en el procesador A que requieran acceso a |φ⟩?",
    "A": "El estado debe ser teleportado de vuelta al procesador A utilizando un nuevo ciclo de teleportación, lo cual requiere establecer entrelazamiento fresco entre los procesadores y realizar otra medición de Bell seguida de unitarios correctivos. Esta transferencia inversa mueve físicamente la información cuántica de regreso a donde se necesita para cálculos subsecuentes.",
    "B": "El estado debe ser teleportado de vuelta al procesador A utilizando el mismo canal cuántico, lo cual requiere invertir los resultados de medición del protocolo original y aplicar correcciones de Pauli inversas en orden opuesto. Esta transferencia hacia atrás reconstruye la información cuántica en su ubicación original explotando la simetría de inversión temporal del protocolo de teleportación y reutilizando la estructura de correlación establecida por el par de Bell original antes de ser consumido por la medición.",
    "C": "El estado debe ser teleportado de vuelta al procesador A utilizando únicamente comunicación clásica, transmitiendo los dos bits clásicos de la medición de Bell original junto con un tercer bit de síndrome que codifica las coordenadas de la esfera de Bloch. Esta transferencia información-teórica permite al procesador A reconstruir |φ⟩ mediante unitarios locales guiados por los datos clásicos recibidos, evitando la necesidad de entrelazamiento fresco mientras respeta el teorema de no clonación a través de la irreversibilidad del proceso de medición original.",
    "D": "El estado debe ser teleportado de vuelta al procesador A utilizando intercambio de entrelazamiento en el par de Bell original, lo cual convierte el entrelazamiento consumido en un nuevo recurso que vincula el qubit de comunicación del procesador B con el qubit de datos del procesador A. Este protocolo bidireccional explota la correlación cuántica residual preservada en el registro de medición, permitiendo que el estado sea reconstruido en la ubicación original mediante operaciones de elección retardada condicionadas a los resultados clásicos de ambos procesadores sin requerir una segunda ronda de distribución de entrelazamiento.",
    "solution": "A"
  },
  {
    "id": 12,
    "question": "En circuitos cuánticos, la puerta CNOT también se conoce por cuál de los siguientes nombres?",
    "A": "La nomenclatura iSWAP se utiliza a veces porque CNOT puede descomponerse en puertas iSWAP combinadas con rotaciones de un solo qubit, y en arquitecturas superconductoras donde las puertas nativas de dos qubits implementan interacciones iSWAP, los practicantes a menudo se refieren a la operación controlada sintetizada como iSWAP. Esta equivalencia salvo unitarios locales hace que los términos sean funcionalmente intercambiables en contextos de optimización de circuitos.",
    "B": "La designación de puerta XOR, heredada de la computación clásica donde CNOT implementa la operación lógica XOR en el qubit objetivo controlado por el qubit fuente. Esta correspondencia clásica-cuántica hace que XOR sea la terminología natural al describir la acción de CNOT en estados de la base computacional, y el nombre permanece prevalente en marcos de programación cuántica que enfatizan implementaciones de lógica clásica reversible.",
    "C": "La etiqueta CP (controlled-phase o fase controlada) es estándar porque las puertas CNOT y CZ son equivalentes salvo rotación de base—aplicar Hadamards antes y después de CZ produce CNOT—y dado que CZ aplica un cambio de fase en lugar de cambio de bit, la terminología generalizada CP captura ambas operaciones como puertas de Pauli controladas. Muchos marcos cuánticos tratan CNOT y CP como sinónimos dada su equivalencia unitaria local.",
    "D": "CX, que significa operación controlled-X (X controlada), ya que la puerta CNOT aplica una operación Pauli-X (inversión de bit) al qubit objetivo cuando el qubit de control está en el estado |1⟩, haciendo de CX una abreviatura natural y ampliamente adoptada en diagramas de circuitos cuánticos y marcos de programación.",
    "solution": "D"
  },
  {
    "id": 13,
    "question": "Al utilizar caminatas cuánticas para detectar si un grupo es conmutativo, el algoritmo aprovecha la dinámica de la caminata en el grafo de Cayley para explorar la estructura del grupo eficientemente. En implementaciones prácticas en dispositivos de corto plazo, la sobrecarga de recursos a menudo limita el tamaño accesible del grupo. ¿Qué tarea computacional más pequeña resuelve realmente el algoritmo como su subrutina central antes de concluir algo sobre la conmutatividad global?",
    "A": "El algoritmo busca un solo par de generadores no conmutantes (g,h) que satisfaga gh ≠ hg mediante caminata cuántica en el grafo de Cayley, pero críticamente debe verificar que este par genera un subgrupo no abeliano ⟨g,h⟩ de orden al menos 6, ya que detectar mera no conmutatividad gh ≠ hg por sí sola es insuficiente—el par podría satisfacer (gh)² = (hg)² o relaciones de conmutación de orden superior que restauran conmutatividad efectiva en la estructura cociente, así que el algoritmo realmente resuelve el problema de certificación de no abelianidad del subgrupo que requiere verificación de |⟨g,h⟩/Z(⟨g,h⟩)| > 1.",
    "B": "El algoritmo busca entre pares de generadores del grupo utilizando una caminata cuántica en el grafo de Cayley para detectar cualquier instancia única donde dos generadores no conmutan, es decir, encontrar un par de generadores no conmutantes (g,h) tal que gh ≠ hg, lo cual certifica inmediatamente que el grupo completo es no abeliano sin requerir enumeración exhaustiva de todos los elementos del grupo o cálculo de la estructura completa de conmutadores, ya que la existencia de incluso un solo par es suficiente para probar no conmutatividad y la caminata cuántica proporciona aceleración cuadrática sobre muestreo aleatorio clásico al localizar este testigo entre los O(|S|²) pares de generadores posibles en un grupo con conjunto generador S.",
    "C": "El algoritmo ejecuta una caminata cuántica que muestrea elementos aleatorios del grupo g,h componiendo generadores y evalúa el conmutador [g,h] = ghg⁻¹h⁻¹, buscando detectar si [g,h] = e para todos los pares muestreados, pero la subrutina central que realmente resuelve es el problema de igualdad de grupo: dados dos elementos del grupo representados como productos de generadores, determinar si representan el mismo elemento—esto requiere resolver el problema de la palabra en la presentación del grupo, y el algoritmo logra ventaja usando amplificación de amplitud cuántica para detectar cualquier instancia donde [g,h] ≠ e entre pares muestreados aleatoriamente.",
    "D": "El algoritmo realiza estimación de fase cuántica en la representación unitaria del operador de caminata cuántica en el grafo de Cayley para extraer el espectro de valores propios, el cual codifica conmutatividad a través de patrones de degeneración espectral: los grupos abelianos exhiben fases propias uniformemente distribuidas e^(2πik/|G|) mientras que los grupos no abelianos muestran agrupamiento determinado por la tabla de caracteres, así que la subrutina central resuelve el problema de estimación de brecha espectral, midiendo si el espaciamiento mínimo de valores propios excede 2π/|G|² lo cual indicaría colapso del teorema de distribución de fase abeliana para caminatas cuánticas.",
    "solution": "B"
  },
  {
    "id": 14,
    "question": "En el contexto de la computación cuántica fotónica experimental, considera una configuración generalizada de boson sampling donde se introducen fotones térmicos en los modos de entrada de un interferómetro óptico lineal. Se sabe que la dificultad computacional de muestrear de la distribución de salida exhibe una transición de fase a medida que aumenta la temperatura ambiental. La dificultad del boson sampling generalizado con estados térmicos muestra una transición a una temperatura crítica porque:",
    "A": "Por encima de esa temperatura, los fotones se comportan más como partículas distinguibles—la ocupación térmica difumina los patrones de interferencia bosónica que hacen el problema clásicamente difícil, destruyendo esencialmente las correlaciones cuánticas necesarias para la complejidad computacional. El parámetro de distinguibilidad aumenta con la temperatura hasta que la permanente pierde sus propiedades de anti-concentración.",
    "B": "Las estadísticas de fotones térmicos transicionan de distribuciones sub-Poissonianas a super-Poissonianas, causando que la función permanente muestree de una clase de complejidad computacional diferente. Por debajo de la temperatura crítica, las amplitudes de estados de Fock permanecen aproximadamente distribuidas gaussianamente, preservando dureza #P, pero las excitaciones térmicas por encima de kT≈ℏω desplazan la distribución hacia muestreo Haar-aleatorio clásico que admite algoritmos eficientes de aproximación en tiempo polinomial.",
    "C": "La visibilidad de interferencia Hong-Ou-Mandel experimenta una transición de percolación a temperatura crítica, donde el desfasamiento térmico causa que la tasa de coincidencia de dos fotones exceda el umbral clásico del 50%. Por encima de este punto, la probabilidad de agrupamiento bosónico se vuelve distinguible del antiagrupamiento fermiónico, permitiendo simulación clásica vía determinantes con signo en lugar de permanentes, colapsando así la complejidad computacional de #P-completo a tiempo polinomial.",
    "D": "La ocupación térmica induce canales efectivos de pérdida de fotones que aumentan linealmente con la temperatura, y cuando el coeficiente de transmisión η(T) cae por debajo de un valor crítico ηc≈0.73, la distribución de salida puede ser muestreada clásicamente de manera eficiente usando algoritmos Metropolis-Hastings en la función Torontoniana. Esta frontera de fase separa el régimen donde los algoritmos clásicos de falsificación en tiempo polinomial fallan del régimen donde tienen éxito con alta probabilidad, vinculando directamente la termalización con el colapso de la ventaja computacional cuántica.",
    "solution": "A"
  },
  {
    "id": 15,
    "question": "¿Qué enfoque muestra mayor promesa para resolver el problema del subgrupo oculto no abeliano?",
    "A": "La estrategia más prometedora involucra descomponer el problema del subgrupo oculto no abeliano en una secuencia jerárquica de subproblemas abelianos explotando la estructura de series de composición en teoría de grupos. Esta reducción aprovecha el hecho de que cualquier grupo finito admite una cadena de subgrupos normales donde cada cociente es abeliano, permitiendo que técnicas estándar de muestreo de Fourier resuelvan cada capa independientemente. El algoritmo cuántico procede identificando primero coclases con respecto al subgrupo normal abeliano maximal, luego aplica recursivamente resolvedores HSP abelianos a los grupos cociente hasta que el subgrupo oculto completo sea reconstruido mediante composición algebraica de las soluciones parciales.",
    "B": "Reemplazar el marco tradicional de muestreo de Fourier cuántico con algoritmos de caminata cuántica que exploran la estructura del grafo de Cayley del grupo proporciona aceleración exponencial para problemas de subgrupo oculto no abeliano. Estas caminatas cuánticas logran tiempos de mezcla que escalan con el diámetro del grupo en lugar de sus propiedades de teoría de representaciones, efectivamente evitando el problema de medición que afecta los enfoques de estados de coclases. Al codificar el subgrupo oculto como condiciones de frontera en la caminata y usar estimación de fase para detectar periodicidades en la dinámica de la caminata, este método puede identificar subgrupos no abelianos sin requerir mediciones entrelazadas a través de múltiples salidas de transformada de Fourier cuántica.",
    "C": "Medición bastante buena (pretty good measurement) en múltiples copias del estado de coclases, la cual extrae información del subgrupo mediante mediciones colectivas.",
    "D": "La técnica funciona realizando primero transformadas de Fourier cuánticas estándar para crear estados de coclases, luego aplicando protocolos de amplificación de amplitud cuidadosamente diseñados que mejoran las señales de medición débiles correspondientes a la estructura del subgrupo. Esta estrategia de amplificación es necesaria porque las representaciones no abelianas distribuyen la información del subgrupo oculto a través de componentes irreducibles de alta dimensión donde aparece solo como correlaciones sutiles.",
    "solution": "C"
  },
  {
    "id": 16,
    "question": "¿Qué estrategia cuántica se puede usar para recuperar la disponibilidad de los qubits después de la extracción de características?",
    "A": "Ejecutar el unitario de inversión temporal para desenredar los qubits de características del resto del sistema, restaurándolos a un estado producto separable que puede reiniciarse de forma segura sin perturbar otros qubits. Al aplicar la inversa de las puertas de extracción de características, se revierte coherentemente el proceso de generación de entrelazamiento, descomputando efectivamente la codificación de características y devolviendo esos qubits a su estado inicial para reutilizarlos en capas posteriores del circuito.",
    "B": "Implementar transferencia de estado cuántico usando puertas iSWAP o √SWAP para mover coherentemente las amplitudes de los qubits de características a un registro auxiliar de ancillas frescas inicializadas a |0⟩, dejando los qubits de características originales en un estado maximalmente mezclado. Después de extraer resultados de medición clásicos de los estados transferidos en el registro auxiliar, los qubits de características ahora decorrelacionados pueden reiniciarse y reutilizarse. Esta estrategia preserva la coherencia cuántica durante la transferencia mientras hace disponibles los qubits originales sin requerir descomputación.",
    "C": "Aplicar retroalimentación basada en mediciones: medir los qubits de características en la base computacional para extraer resultados de cadenas de bits clásicas, luego condicionar rotaciones de un solo qubit subsiguientes en esos resultados de medición para preparar los qubits restantes no medidos en un estado que factoriza la información de características. Este protocolo de reinicio condicional usa los datos de medición clásicos como tabla de búsqueda para unitarios correctivos que efectivamente trazan el subsistema medido del estado cuántico conjunto, restaurando la separabilidad sin aplicar puertas inversas y permitiendo la reutilización de qubits en etapas posteriores del circuito.",
    "D": "Usar protocolos de medición diferida para posponer el colapso de las funciones de onda de los qubits de características hasta la capa final del circuito, manteniendo coherencia cuántica completa al aplicar condicionalmente operaciones en qubits posteriores que dependen de los estados de los qubits de características a través de puertas controladas. Al tratar los observables de extracción de características como mediciones virtuales codificadas en entrelazamiento en lugar de colapsos proyectivos, se preserva la superposición de qubits a lo largo del circuito mientras se extrae información de características mediante mediciones conjuntas finales, reutilizando efectivamente los qubits sin reiniciarlos físicamente en etapas intermedias.",
    "solution": "A"
  },
  {
    "id": 17,
    "question": "El muestreo de circuitos aleatorios difiere del muestreo de bosones principalmente en que el muestreo de circuitos aleatorios:",
    "A": "Emplea mediciones adaptativas donde las bases de medición posteriores dependen de resultados anteriores, usando computación clásica de reenvío para dirigir la evolución cuántica, mientras que el muestreo de bosones fija todos los operadores de medición en la base de número de fotones antes de la preparación del estado. Esta adaptividad permite al muestreo de circuitos aleatorios verificar el muestreo correcto de distribuciones mediante evaluación comparativa de entropía cruzada contra simulación clásica de circuitos poco profundos.",
    "B": "Utiliza puertas discretas de qubits aplicadas en secuencias en capas en lugar de transformaciones ópticas lineales continuas que actúan sobre modos fotónicos, convirtiéndolo fundamentalmente en un modelo computacional basado en puertas donde la evolución unitaria procede mediante operaciones secuenciales de dos qubits en lugar de redes de divisores de haz pasivos que implementan matrices de dispersión fijas.",
    "C": "Genera distribuciones de salida midiendo estados estabilizadores después de aplicar puertas Clifford aleatorias seguidas de una capa final no-Clifford, mientras que el muestreo de bosones mide estados de Fock después de la evolución óptica lineal de entradas de un solo fotón. La dureza del muestreo de circuitos aleatorios se basa en la propiedad de anticoncentración de las distribuciones de salida, que se deriva de las estadísticas de Porter-Thomas de unitarios aleatorios de Haar aplicados a estados de la base computacional.",
    "D": "Explota dureza computacional del muestreo de la distribución de salida de circuitos clásicos reversibles aumentados con puertas de fase aleatorias de un solo qubit, donde cada capa aplica un unitario diagonal uniformemente aleatorio a cada qubit antes de que una capa de permutación fija baraje los estados de la base computacional. La dureza deriva de la #P-completitud de calcular amplitudes en estas redes de fase-permutación, mientras que la dureza del muestreo de bosones proviene de calcular permanentes de submatrices extraídas de la matriz de dispersión completa.",
    "solution": "B"
  },
  {
    "id": 18,
    "question": "En el contexto de algoritmos de aprendizaje automático cuántico que afirman tener aceleraciones exponenciales, ¿qué propiedad mecánico-cuántica fundamental se cita más comúnmente como la fuente principal de ventaja computacional, y bajo qué condiciones específicas se manifiesta esta ventaja en la práctica? Considere tanto modelos teóricos como limitaciones experimentales actuales al formular su respuesta.",
    "A": "El poder computacional del paralelismo cuántico emerge de la capacidad de preparar una superposición uniforme sobre todos los 2^n estados base usando solo n puertas Hadamard, creando efectivamente un conjunto exponencialmente grande de entradas en tiempo polinomial. Este paralelismo se vuelve prácticamente útil cuando el problema exhibe una estructura global que puede explotarse mediante interferencia, como en la búsqueda de Grover donde la interferencia destructiva amplifica el estado objetivo. Sin embargo, en contextos de aprendizaje automático, extraer información útil sobre todas las evaluaciones paralelas simultáneamente sigue siendo un desafío fundamental, ya que la medición colapsa la superposición a un solo resultado, requiriendo esquemas cuidadosos de amplificación de amplitud o aceptar que solo podemos acceder a propiedades agregadas en lugar de resultados individuales.",
    "B": "El entrelazamiento multipartito genera estructuras de correlación exponencialmente complejas que permiten a los sistemas cuánticos codificar dependencias entre variables de maneras que resisten la factorización clásica o la descomposición en redes tensoriales. Cuando los datos de entrenamiento o la arquitectura del modelo exhiben naturalmente estas correlaciones entrelazadas—como en simulaciones de química cuántica o física de muchos cuerpos—el sistema cuántico puede representar y manipular estas relaciones con recursos polinomiales mientras que los enfoques clásicos requerirían memoria exponencial. La restricción crítica es que el hardware cuántico actual sufre de decaimiento del entrelazamiento a través de canales de decoherencia, con tiempos de coherencia típicos que nos limitan a circuitos con profundidades de 100-1000 puertas antes de que la calidad del entrelazamiento se degrade por debajo de umbrales útiles.",
    "C": "Al codificar información como amplitudes de probabilidad en un vector de estado cuántico de dimensión 2^n, los sistemas cuánticos logran una expansión exponencial de capacidad representacional comparada con los n bits o qubits clásicos físicamente presentes. Esta densidad representacional permite a las redes neuronales cuánticas modelar teóricamente funciones con espacios de parámetros exponencialmente grandes usando solo polinomialmente muchos qubits físicos. La dificultad fundamental es que mientras el vector de estado contiene exponencialmente muchas amplitudes, extraer cualquier amplitud específica requiere tomografía completa (exponencialmente muchas mediciones) o técnicas de interferencia especializadas que funcionan solo para consultas estructuradas, lo que significa que la representación exponencial no se traduce en ventaja computacional exponencial a menos que el problema permita interferencia constructiva y estadísticas de medición global.",
    "D": "Todas las anteriores contribuyen sinérgicamente, ya que la superposición permite exploración paralela, el entrelazamiento captura correlaciones complejas, y la representación exponencial del espacio de estados proporciona el sustrato computacional subyacente. La ventaja práctica depende de la estructura del problema y la calidad del hardware.",
    "solution": "D"
  },
  {
    "id": 19,
    "question": "¿Cuál es la limitación principal de las implementaciones cuánticas del agrupamiento k-medias?",
    "A": "Extraer los centros de los grupos del estado cuántico requiere realizar tomografía en un espacio de Hilbert exponencialmente grande, que escala como O(2^n) mediciones para n qubits. Aunque los centroides pueden codificarse eficientemente como amplitudes cuánticas, reconstruir sus coordenadas clásicas necesita tomografía de estado completa o protocolos de tomografía de sombra, ambos introducen sobrecarga de medición que puede dominar el tiempo de ejecución y potencialmente eliminar cualquier ventaja cuántica obtenida durante la fase de cálculo de distancias.",
    "B": "La arquitectura de circuito cuántico para calcular distancias euclidianas entre puntos de datos y centroides requiere fundamentalmente operaciones controladas cuya profundidad escala polinomialmente con la dimensión de características, creando oportunidades significativas para decoherencia en dispositivos NISQ. Además, implementar la prueba de intercambio u otras técnicas de estimación de distancia demanda qubits auxiliares y calibraciones precisas de puertas, haciendo que el oráculo de distancia cuántico sea sustancialmente más intensivo en recursos que el cálculo clásico O(nd) por iteración, donde n es el número de puntos y d es la dimensionalidad.",
    "C": "Todas las anteriores",
    "D": "Los cálculos de distancia en circuitos cuánticos están restringidos por la necesidad de codificar vectores de características clásicos en amplitudes cuánticas mediante codificación de amplitud, lo que en sí mismo requiere O(d) operaciones por punto de datos donde d es la dimensión de características. Además, calcular todas las distancias por pares simultáneamente requeriría un número de qubits que escala linealmente tanto con el tamaño del conjunto de datos como con el espacio de características, haciendo que la profundidad del circuito cuántico sea prohibitivamente grande incluso para conjuntos de datos de tamaño moderado, negando así la aceleración teórica del paralelismo cuántico.",
    "solution": "C"
  },
  {
    "id": 20,
    "question": "¿Qué sucede en el algoritmo de Shor si el período encontrado es impar?",
    "A": "Las implementaciones modernas de superconductores incorporan bucles de retroalimentación adaptativa donde el procesador cuántico monitorea la paridad del período medido en tiempo real; si se detecta r mod 2 = 1 durante la lectura de la transformada cuántica de Fourier inversa, el sistema de control reinicializa inmediatamente el registro auxiliar y selecciona una base aleatoria fresca a' sin devolver el control al host clásico.",
    "B": "El algoritmo procede calculando gcd(a^(r/2) ± 1, N) usando el exponente fraccionario r/2, que produce un factor no trivial en aproximadamente la mitad de todos los casos porque el período impar aún satisface el criterio de Euler para residuos cuadráticos módulo N. Este enfoque aprovecha la expansión en fracciones continuas de la fase medida para interpolar entre potencias enteras, recuperando efectivamente factores incluso cuando el postprocesamiento clásico de otro modo rechazaría el resultado, aunque al costo de mayores tasas de error en la práctica.",
    "C": "Un período impar r señala que N debe ser expresable como b^k para alguna base entera b y exponente k ≥ 2, porque el orden de cualquier elemento en el grupo multiplicativo Z*_N divide φ(N), y φ(b^k) siempre es par a menos que k=1 y b=2. El algoritmo de Shor detecta esta estructura en el paso inicial de preprocesamiento clásico verificando si N es una potencia perfecta antes de invocar la subrutina cuántica, por lo que encontrar un período impar durante la fase cuántica indica una inconsistencia lógica que termina todo el intento de factorización en lugar de simplemente reiniciar con una nueva base.",
    "D": "Esa ejecución particular de la subrutina cuántica no tiene éxito, y la lógica de control clásico selecciona una nueva base aleatoria a' coprima con N antes de reiniciar todo el procedimiento de búsqueda de período, porque un período impar no puede usarse para calcular los factores mediante la fórmula gcd(a^(r/2) ± 1, N) sin encontrar exponentes no enteros.",
    "solution": "D"
  },
  {
    "id": 21,
    "question": "¿Qué principio fundamental hace que la corrección de errores cuánticos sea más desafiante que la corrección de errores clásicos?",
    "A": "Mientras que las mediciones clásicas inevitablemente destruyen las superposiciones y colapsan los estados cuánticos, el proceso de medición en sistemas cuánticos puede en realidad fortalecer el entrelazamiento entre el qubit medido y el aparato de medición a través de la retroacción de la observación. Esta mejora de las correlaciones significa que al realizar mediciones de síndrome en códigos de corrección de errores cuánticos, cada evento de medición aumenta la entropía de entrelazamiento entre el bloque de código y los registros ancilla, construyendo progresivamente correlaciones cuánticas que deben ser cuidadosamente gestionadas; de lo contrario, estas estructuras entrelazadas crecientes introducen errores correlacionados que se propagan a través de rondas de corrección subsecuentes, haciendo que el protocolo de corrección de errores sea más frágil que los esquemas clásicos donde las mediciones simplemente extraen información sin modificar las estructuras de correlación.",
    "B": "Los recursos computacionales requeridos para simular clásicamente procesos de error cuánticos escalan exponencialmente con el número de qubits en el sistema, lo que crea un cuello de botella fundamental al diseñar y verificar códigos de corrección de errores cuánticos. Para un sistema con n qubits, la matriz de densidad contiene 2^(2n) entradas, lo que significa que incluso probar si un código de corrección de errores propuesto funciona correctamente para sistemas de 50 qubits requeriría rastrear aproximadamente 10^30 números complejos.",
    "C": "El teorema de no clonación impide copiar qubits para verificaciones de redundancia, haciendo imposible verificar información cuántica mediante simple duplicación y comparación como en los códigos de repetición clásicos",
    "D": "La información cuántica reside fundamentalmente en autoestados discretos correspondientes a cantidades observables, con cada qubit existiendo en el estado base computacional |0⟩ o |1⟩ en cualquier instante dado. Esta naturaleza discreta significa que los errores solo pueden invertir qubits entre estas configuraciones clásicas bien definidas, similar a los errores de inversión de bits en sistemas clásicos, pero la corrección de errores cuánticos debe manejar adicionalmente el hecho de que la medición fuerza este colapso discreto desde cualquier superposición; por lo tanto, el desafío surge no de errores continuos, sino de gestionar los resultados de medición discretos mientras se previene que el proceso de detección mismo proyecte inadvertidamente el qubit lógico codificado en un autoestado incorrecto de los estabilizadores del código.",
    "solution": "C"
  },
  {
    "id": 22,
    "question": "En sistemas de distribución cuántica de claves basados en entrelazamiento, existe una vulnerabilidad fundamental relacionada con la fuente cuántica misma que puede ser explotada sin medir directamente los fotones transmitidos. Esta falla de seguridad surge cuando un adversario puede manipular sutilmente o distinguir entre diferentes emisiones de la fuente de manera que revele información parcial sobre la clave secreta. Considere un escenario donde la fuente de pares de fotones entrelazados no produce estados cuánticos perfectamente idénticos para cada evento de emisión, permitiendo a un espía obtener conocimiento sobre las bases de medición o resultados. ¿Cuál es el principio central detrás de esta clase de ataques?",
    "A": "La vulnerabilidad se centra en la manipulación selectiva de la eficiencia del detector de anuncio en fuentes de conversión paramétrica descendente espontánea, donde un espía puede ajustar dinámicamente los umbrales de detección para anunciar preferentemente ciertos estados de pares de fotones sobre otros basándose en sus características de polarización o temporización. Al sesgar qué emisiones son anunciadas y por lo tanto utilizadas para la generación de claves, el adversario crea una distribución no uniforme sobre los resultados de medición de las partes legítimas sin introducir errores detectables.",
    "B": "Métodos de intercepción por intercambio de entrelazamiento donde el adversario realiza mediciones de estado de Bell en fotones interceptados y crea nuevos pares entrelazados para reenviar a las partes legítimas, manteniendo las estadísticas de correlación mientras extrae información de la clave a través de los resultados de medición obtenidos durante el proceso de intercambio. Al elegir cuidadosamente cuándo realizar la operación de intercambio basándose en anuncios del canal público, el espía puede selectivamente obtener información sobre bits de la clave.",
    "C": "La estrategia de ataque implica redirigir un fotón de cada par entrelazado a través de un aparato de medición de Bell controlado antes de que llegue al receptor legítimo, luego usar el resultado de la medición para determinar qué estado de base computacional preparar y reenviar al destinatario previsto. Esta técnica de redirección de medición de Bell permite al adversario colapsar el entrelazamiento de una manera que parece estadísticamente consistente con la transmisión directa.",
    "D": "El ataque explota variaciones en las características de emisión de la fuente cuántica que permiten a un espía distinguir entre diferentes emisiones de pares entrelazados, obteniendo así información sobre la clave sin realizar mediciones que perturbarían los estados cuánticos de maneras detectables. Esto es particularmente peligroso porque las pruebas de seguridad estándar asumen emisiones de fuente idénticas e independientemente distribuidas.",
    "solution": "D"
  },
  {
    "id": 23,
    "question": "En computación cuántica distribuida, suponga que tiene tres nodos A, B y C dispuestos en línea, donde A y B comparten un par entrelazado, y B y C comparten un par entrelazado separado. Quiere que A y C compartan entrelazamiento directamente, pero no tienen ningún canal cuántico físico que los conecte. ¿Cuál es el papel principal del intercambio de entrelazamiento en este escenario, y qué principio fundamental permite que funcione a pesar de la falta de interacción directa entre los nodos distantes?",
    "A": "El nodo B realiza una medición de Bell en sus dos qubits de los pares A-B y B-C, proyectando A y C en un estado entrelazado sin interacción directa. Esto explota las correlaciones cuánticas y el colapso inducido por medición.",
    "B": "El intercambio de entrelazamiento transfiere correlaciones cuánticas entre nodos no adyacentes haciendo que B realice mediciones de paridad que comparen fases entre sus dos qubits de los pares separados, condicionando A y C en un estado de Bell compartido. Esto funciona a través del principio de retroacción de medición, donde las mediciones proyectivas locales en B establecen retroactivamente correlaciones entre A y C que se manifiestan como violaciones de las desigualdades de Bell a pesar de que A y C nunca comparten un cono de luz común, dependiendo fundamentalmente de la no localidad cuántica en lugar de la transferencia de información.",
    "C": "El protocolo permite la distribución de entrelazamiento remoto haciendo que el nodo B ejecute una operación SWAP controlada entre sus mitades de los pares A-B y B-C, que intercambia información cuántica entre los pares de Bell inicialmente independientes sin colapsar sus superposiciones. Esto preserva el entrelazamiento a través de evolución unitaria en lugar de medición, explotando la reversibilidad de las operaciones cuánticas para redirigir correlaciones del nodo intermedio a los puntos finales mientras mantiene la coherencia, aunque la fidelidad del estado A-C final depende de minimizar la decoherencia durante la implementación de la puerta SWAP.",
    "D": "El intercambio establece entrelazamiento A-C usando B para implementar un relé cuántico donde la teletransportación secuencial transfiere el estado de un qubit a través de la cadena mientras consume ambos pares iniciales como recurso de canal cuántico. El principio fundamental es la transferencia de estado cuántico a través de comunicación asistida por entrelazamiento, donde la medición de Bell de B en un par genera los bits clásicos necesarios para completar la teletransportación, luego esos mismos bits se usan con el segundo par para extender la transferencia a C, convirtiendo efectivamente dos enlaces entrelazados de corto alcance en un enlace de largo alcance a través de operaciones de corrección por medición y retroalimentación.",
    "solution": "A"
  },
  {
    "id": 24,
    "question": "¿Qué técnica precisa proporciona la mitigación más fuerte contra ataques de canal lateral en módulos de seguridad de hardware post-cuántico?",
    "A": "Implementaciones de tiempo constante con transformaciones algorítmicas de enmascaramiento",
    "B": "Esquemas de enmascaramiento de orden superior combinados con contramedidas de barajado",
    "C": "Ejecución normalizada en potencia con primitivas de inserción de retardo aleatorio",
    "D": "Entornos de ejecución aislados con generación de números aleatorios cuánticos",
    "solution": "D"
  },
  {
    "id": 25,
    "question": "En el contexto de la computación cuántica basada en mediciones, suponga que tiene un estado de clúster 2D en una red cuadrada donde ciertos qubits han sido medidos en bases que dependen de resultados de medición previos (mediciones adaptativas). Los qubits restantes no medidos forman un subgrafo conectado. ¿Cuál es el papel de los estados no gaussianos en el aprendizaje automático cuántico con variables continuas?",
    "A": "Las operaciones no gaussianas llevan los sistemas de variables continuas más allá de la computación cuántica gaussiana hacia el territorio de la computación cuántica universal. Sin recursos no gaussianos, los sistemas CV permanecen eficientemente simulables clásicamente por el teorema de Gottesman-Knill de variables continuas, restringiéndole a operaciones sobre estados gaussianos que pueden ser rastreados mediante matrices de covarianza. En aprendizaje automático cuántico, los estados no gaussianos permiten los mapas de características no lineales y las distribuciones de probabilidad complejas esenciales para la ventaja cuántica, yendo más allá de la estructura cuadrática del espacio de fases que limita los estados gaussianos.",
    "B": "Son básicamente el equivalente cuántico de las funciones de activación en redes neuronales, similares a ReLU o sigmoide en arquitecturas clásicas.",
    "C": "Los estados no gaussianos le permiten codificar características no lineales en el estado cuántico mismo, lo cual los estados gaussianos fundamentalmente no pueden hacer debido a su estructura limitada del espacio de fases. Dado que los estados gaussianos ocupan solo regiones elipsoidales convexas del espacio de fases y evolucionan bajo transformaciones simplécticas que preservan la convexidad, pueden representar solo características polinomiales hasta segundo orden, mientras que los estados no gaussianos con negatividad de Wigner pueden codificar núcleos no lineales arbitrarios y correlaciones de momentos de orden superior que son esenciales para tareas de aprendizaje automático como clasificación con límites de decisión curvos y regresión no lineal.",
    "D": "Todos estos capturan aspectos importantes: universalidad más allá de operaciones gaussianas, capacidades de codificación de características no lineales, y la conexión con funciones de activación en redes neuronales cuánticas",
    "solution": "D"
  },
  {
    "id": 26,
    "question": "¿Cuál es la función principal de un procesador programable de caminatas cuánticas?",
    "A": "Implementar operadores de moneda adaptativos que se ajustan dinámicamente basándose en retroalimentación instantánea de mediciones de qubits auxiliares, permitiendo que la caminata responda a distribuciones de probabilidad intermedias. Al condicionar las operaciones de desplazamiento subsiguientes a estos resultados de medición, el procesador puede guiar al caminante hacia regiones de alta probabilidad mientras mantiene la superposición cuántica a través de los estados de la base computacional no medidos, creando efectivamente un protocolo de navegación híbrido clásico-cuántico.",
    "B": "Implementa algoritmos basados en caminatas a través de diferentes grafos de entrada reconfigurando los operadores de moneda y desplazamiento para adaptarse a diferentes topologías de grafos y estructuras de adyacencia, permitiendo la ejecución flexible de algoritmos de búsqueda, muestreo y optimización sin requerir rediseño del hardware.",
    "C": "Generar estados entrelazados de múltiples caminantes donde caminantes cuánticos distintos comparten coherencia de fase a través de estructuras de grafos separadas, habilitando protocolos de búsqueda distribuida. Al inicializar caminantes en configuraciones similares a pares de Bell y aplicar operadores de moneda correlacionados, el procesador explota efectos de interferencia no locales entre regiones de grafos espacialmente separadas, lo que mejora la aceleración cuadrática característica de las caminatas cuánticas cuando múltiples grafos interconectados deben explorarse simultáneamente.",
    "D": "Sintetizar Hamiltonianos dependientes del tiempo a partir de la descomposición espectral del unitario de la caminata, permitiendo la simulación de caminatas cuánticas en tiempo continuo mediante Trotterización del Laplaciano del grafo. Al discretizar el operador de evolución en pasos de tiempo suficientemente pequeños y aplicar fórmulas de Suzuki-Trotter, el procesador aproxima la dinámica continua mientras mantiene la estructura moneda-desplazamiento, que preserva las propiedades de localidad requeridas para una implementación eficiente en arquitecturas de vecinos más cercanos.",
    "solution": "B"
  },
  {
    "id": 27,
    "question": "En el contexto de la computación cuántica, AWG se utiliza comúnmente para definir la forma de pulsos de control. ¿Qué significa AWG?",
    "A": "Generador Adaptativo de Formas de Onda (Adaptive Waveform Generator), una plataforma de hardware que incorpora bucles de retroalimentación en tiempo real que modifican las características del pulso en medio de la secuencia basándose en resultados de mediciones o condiciones de error detectadas durante la ejecución del circuito cuántico. Estos dispositivos emplean modelos predictivos de la dinámica de qubits para ajustar preventivamente las amplitudes, duraciones y fases de los pulsos para compensar la deriva de calibración o efectos de diafonía observados en ciclos anteriores.",
    "B": "Generador de Forma de Onda de Amplitud (Amplitude Waveform Generator), un dispositivo especializado diseñado exclusivamente para generar pulsos de control con envolventes de amplitud programables mientras mantiene relaciones fijas de frecuencia y fase.",
    "C": "Generación Automatizada de Formas de Onda (Automated Waveform Generation), refiriéndose al sistema de control de bucle cerrado que sintetiza autónomamente formas de pulso óptimas integrando retroalimentación de tomografía de estado de qubit en tiempo real con algoritmos de aprendizaje automático que convergen en los parámetros de forma de onda que producen máxima fidelidad de puerta. Esta terminología enfatiza el aspecto de automatización donde el proceso de diseño de pulsos se elimina del ajuste manual y en su lugar se basa en rutinas de optimización algorítmica como GRAPE (Gradient Ascent Pulse Engineering) basado en gradientes o algoritmos genéticos ejecutándose en controladores FPGA.",
    "D": "Generador Arbitrario de Formas de Onda (Arbitrary Waveform Generator), un instrumento electrónico programable que sintetiza señales de voltaje definidas por el usuario con control temporal preciso sobre características de amplitud, frecuencia y fase. Estos dispositivos permiten a los experimentalistas crear envolventes de pulso personalizadas optimizadas para operaciones específicas de puertas cuánticas, soportando tanto formas estándar como gaussianas como formas de onda moduladas complejas requeridas para control de alta fidelidad.",
    "solution": "D"
  },
  {
    "id": 28,
    "question": "¿Qué vulnerabilidad sofisticada existe en la implementación de protocolos de computación cuántica ciega?",
    "A": "La estructura de correlación de bases de medición crea un canal lateral teórico-informativo — cuando el servidor observa dependencias temporales en las secuencias de elección de base del cliente a través de múltiples rondas, puede realizar inferencia estadística para reconstruir parcialmente la topología del circuito subyacente con probabilidad no despreciable. Aunque los resultados de medición individuales permanecen perfectamente aleatorizados por cifrado de libreta de un solo uso, las probabilidades condicionales entre selecciones de bases sucesivas filtran información estructural sobre el grafo de computación. Específicamente, la distribución de frecuencia de mediciones consecutivas en base X versus Y en qubits adyacentes revela el patrón de puertas entrelazadoras, permitiendo a un servidor con suficientes muestras distinguir entre familias de algoritmos mediante pruebas de razón de verosimilitud que logran precisión de clasificación mejor que aleatoria.",
    "B": "Los circuitos trampa pueden distinguirse estadísticamente de la computación real — si el servidor aprende qué rondas son trampas de verificación versus puertas delegadas reales, la privacidad se rompe. El servidor puede analizar propiedades estadísticas como la entropía de resultados de medición, patrones de elección de base, o la densidad de operaciones no-Clifford para identificar rondas trampa con precisión mejor que aleatoria. Una vez que la identificación de trampas tiene éxito incluso parcialmente, la garantía de verificabilidad colapsa porque el servidor puede comportarse honestamente en trampas detectadas mientras se desvía estratégicamente en rondas de computación real, comprometiendo tanto privacidad como corrección sin activar condiciones de aborto del lado del cliente.",
    "C": "La granularidad del ángulo de rotación impuesta por la electrónica de control de precisión finita crea una vulnerabilidad de identificación — cuando el cliente solicita rotaciones de un solo qubit compiladas del conjunto de puertas universal del protocolo, los errores de aproximación discreta se acumulan de manera diferente dependiendo del algoritmo objetivo que se está ejecutando. Un servidor malicioso con mediciones calibradas de fidelidad de puerta puede realizar análisis de componentes principales en los patrones de error de fase residual a través de múltiples qubits para extraer firmas específicas del algoritmo. Esto funciona porque diferentes tareas computacionales inducen distribuciones características de ángulos de rotación que dejan rastros estadísticamente distinguibles en las operaciones de puerta logradas versus solicitadas, permitiendo al servidor clasificar el tipo de computación mediante aprendizaje supervisado en estadísticas de síndrome de error.",
    "D": "La sobrecarga de autenticación en computación ciega verificada introduce un canal encubierto mediante modulación de probabilidad de aborto — cuando servidores maliciosos inyectan cantidades controladas de decoherencia que permanecen por debajo del umbral de detección, sesgan las tasas de falla de circuitos trampa de maneras que codifican información extraída sobre la computación real. Al corromper estratégicamente qubits no-trampa con tasas de error precisamente calibradas que mantienen la fidelidad general dentro de límites aceptables, el servidor puede manipular qué circuitos trampa específicos fallan la verificación, creando un canal de comunicación binario que filtra resultados computacionales parciales a través del patrón de ejecuciones de protocolo abortadas versus completadas sin exceder los límites de distinguibilidad estadística del cliente para comportamiento honesto versus malicioso.",
    "solution": "B"
  },
  {
    "id": 29,
    "question": "¿Qué suposición permite a ECDQC especializar su optimización para circuitos tipo QAOA y QFT?",
    "A": "Medición en el paso final con retroalimentación clásica diferida—la extracción de síndromes y lectura de observables ocurren exclusivamente en la capa terminal del circuito, pero la optimización clave proviene de diferir todas las decisiones de control clásico hasta después de que las operaciones cuánticas se completan. Esta elección arquitectónica canaliza las puertas cuánticas a través de un modelo de alimentación directa donde los resultados de medición informan el post-procesamiento clásico subsiguiente en lugar de correcciones en medio del circuito, permitiendo a ECDQC explotar los patrones de medición regulares y la sobrecarga de lectura predecible característica tanto de la estimación de energía de QAOA como de las etapas de lectura de fase de QFT.",
    "B": "Estos circuitos exhiben patrones de interacción regulares y predecibles con puertas entrelazadoras globales dispersas y repeticiones de capas estructuradas, permitiendo a ECDQC explotar las simetrías inherentes y localidad en Hamiltonianos mezcladores y operadores de fase, permitiendo estrategias de compilación optimizadas que aprovechan la estructura periódica y requisitos de conectividad limitada característicos tanto de la evolución de función de costo de QAOA como de la escalera de fase controlada de QFT.",
    "C": "Descomposiciones de Pauli estructuradas con peso acotado—los Hamiltonianos mezcladores de QAOA y los operadores de fase de QFT admiten descomposiciones dispersas en cadenas de Pauli con peso que escala logarítmicamente con el conteo de qubits, lo que significa que cada término en el Hamiltoniano actúa de manera no trivial en como máximo O(log n) qubits. ECDQC explota esta dispersión enrutando solo entre qubits que aparecen en la misma cadena de Pauli, evitando requisitos de conectividad completa todo-a-todo y optimizando la asignación de recursos de entrelazamiento basada en la estructura de soporte de términos en lugar de la topología de acoplamiento global.",
    "D": "Unitarios diagonales en la base computacional—el operador de función de costo de QAOA y las rotaciones de fase controlada de QFT implementan exclusivamente puertas diagonales que conmutan con mediciones de base computacional, lo que significa que pueden compilarse en operaciones clásicas de retroceso de fase sin puertas entrelazadoras genuinas de múltiples qubits más allá de la preparación de estado inicial. ECDQC trata estas capas diagonales como modulaciones clásicas de probabilidades de medición, optimizando programas de acumulación de fase y aprovechando la conmutatividad para reordenar puertas libremente sin rastrear generación de entrelazamiento o propagación de decoherencia.",
    "solution": "B"
  },
  {
    "id": 30,
    "question": "En computación cuántica digital-analógica, alternas entre pulsos de puerta digital cortos y bloques de evolución analógica más largos para simular un Hamiltoniano objetivo. Supón que discretizas la evolución analógica en intervalos de tiempo fijos Δt. ¿Por qué el aliasing puede degradar la fidelidad de la simulación, incluso cuando cada bloque analógico se implementa perfectamente?",
    "A": "El paso de tiempo finito Δt introduce un corte en la representación en el dominio del tiempo de la evolución del Hamiltoniano, y por el principio de incertidumbre este corte en la variable temporal corresponde a una incertidumbre en energía que amplía las características espectrales de H—cuando los estados propios de alta energía del Hamiltoniano objetivo tienen espaciamientos de valores propios que exceden π/Δt, esta ampliación causa superposición espectral que corrompe la dinámica.",
    "B": "Los protocolos digital-analógicos se basan en la descomposición de Trotter donde los bloques analógicos aproximan e^(-iHΔt) pero con pequeño error sistemático O(Δt²)—cuando el Hamiltoniano objetivo contiene términos oscilantes de alta frecuencia con períodos más cortos que 2Δt, el error de Trotter de segundo orden no logra promediar estas oscilaciones correctamente y en su lugar las amplifica mediante acumulación resonante, produciendo pérdida de fidelidad que escala superlinealmente con el tiempo de evolución.",
    "C": "El paso de tiempo discreto efectivamente muestrea la evolución continua del Hamiltoniano a intervalos Δt, y si los componentes de alta frecuencia en el Hamiltoniano objetivo exceden la frecuencia de Nyquist π/Δt, estas frecuencias se reflejan como réplicas espectrales que corrompen la dinámica de baja frecuencia deseada.",
    "D": "Los bloques analógicos perfectamente implementados evolucionan bajo el Hamiltoniano nativo exp(-iH_nativo·Δt) exactamente, pero el Hamiltoniano objetivo H_objetivo generalmente difiere de H_nativo por términos oscilantes de alta frecuencia generados por transformaciones de marco de conmutación—cuando estas correcciones oscilantes tienen componentes de Fourier que exceden la frecuencia de Nyquist π/Δt establecida por la discretización, se reflejan en el sector de baja frecuencia donde interfieren constructivamente con la dinámica deseada e introducen errores de fase sistemáticos.",
    "solution": "C"
  },
  {
    "id": 31,
    "question": "Para mantener el tamaño del estado del caminante manejable, los algoritmos de caminata cuántica para distinción de elementos eligen el tamaño del subconjunto como:",
    "A": "Aproximadamente la raíz cúbica de la longitud de la lista, equilibrando la dimensión del estado contra la probabilidad de colisión.",
    "B": "Proporcional a la raíz cuadrada de n para coincidir con el umbral de la paradoja del cumpleaños, asegurando que la probabilidad de colisión dentro de cada subconjunto alcance orden constante mientras se mantiene el diámetro del grafo de Johnson en O(n^(1/2)).",
    "C": "La raíz cuarta de n, optimizando el balance entre el costo de la fase de configuración y el número de operaciones de actualización a través de todas las rondas de detección de colisiones.",
    "D": "Inversamente proporcional a la brecha espectral, típicamente n^(2/5), asegurando que el tiempo de mezcla de la caminata cuántica permanezca sublineal mientras contiene suficientes elementos para la detección de colisiones.",
    "solution": "A"
  },
  {
    "id": 32,
    "question": "En la planificación temporal de circuitos cuánticos, ¿qué restricción debe cumplirse para las operaciones de puertas en paralelo?",
    "A": "Las puertas programadas para ejecución paralela deben actuar sobre conjuntos disjuntos de qubits para evitar conflictos de recursos, ya que cada qubit físico puede participar en como máximo una operación de puerta en cualquier paso temporal dado, asegurando que ningún qubit sea simultáneamente el objetivo de múltiples operaciones superpuestas que violarían el principio fundamental de la evolución unitaria.",
    "B": "Las puertas programadas en paralelo deben actuar sobre conjuntos disjuntos de qubits para prevenir conflictos de base de medición, ya que operaciones simultáneas sobre qubits superpuestos requerirían que el estado cuántico colapse en autoestados de observables no conmutables durante la misma ventana de medición, violando el principio de incertidumbre para variables conjugadas y creando resultados de síndrome ambiguos en protocolos de corrección de errores.",
    "C": "Las puertas que se ejecutan en paralelo deben operar sobre qubits separados para evitar violar el teorema de no clonación, porque aplicar dos unitarios distintos al mismo qubit simultáneamente requeriría duplicar coherentemente el estado cuántico del qubit a través de múltiples ramas computacionales antes de recombinarlas, lo cual está prohibido por la linealidad de la mecánica cuántica para estados desconocidos arbitrarios.",
    "D": "Las puertas programadas dentro de la misma capa temporal deben apuntar a registros de qubits disjuntos para preservar la causalidad en el grafo de dependencias del circuito, ya que el uso superpuesto de qubits crearía dependencias de datos cíclicas donde las salidas de las puertas se retroalimentan a sus propias entradas dentro de un solo ciclo de reloj, violando la estructura acíclica requerida para la compilación determinista de programas cuánticos.",
    "solution": "A"
  },
  {
    "id": 33,
    "question": "¿Cuál es la importancia del enfoque de Red Generativa Adversaria de Wasserstein Cuántica (QWGAN)?",
    "A": "Al aprovechar la formulación dual de la métrica de Wasserstein, QWGAN estabiliza las dinámicas de entrenamiento adversario que de otro modo sufrirían colapso de modos y gradientes que se desvanecen en el régimen cuántico, particularmente cuando la distribución de salida del generador está soportada sobre variedades de estados cuánticos de baja dimensión que son difíciles de distinguir con medidas de divergencia estándar.",
    "B": "Mediante un procedimiento variacional iterativo, QWGAN entrena circuitos cuánticos parametrizados para generar estados cuánticos cuyas estadísticas de medición aproximan estrechamente distribuciones de probabilidad objetivo, incluso cuando estas distribuciones surgen de sistemas cuánticos de muchos cuerpos complejos que son clásicamente intratables de simular, permitiendo así el muestreo eficiente de distribuciones cuánticas de alta dimensión usando solo circuitos de profundidad polinomial.",
    "C": "Todas las anteriores",
    "D": "El marco emplea discriminadores cuánticos con restricción de Lipschitz combinados con teoría del transporte óptimo para garantizar la convergencia de las actualizaciones de parámetros del generador, proporcionando cotas rigurosas sobre el error de aproximación y la complejidad de muestreo que escalan polinomialmente en lugar de exponencialmente con el tamaño del sistema, a diferencia de las GANs clásicas o modelos generativos cuánticos estándar que carecen de tales fundamentos teóricos.",
    "solution": "C"
  },
  {
    "id": 34,
    "question": "¿Cuál es un desafío clave en la gestión de la ejecución de cálculos cuánticos distribuidos?",
    "A": "La computación cuántica distribuida requiere establecer distribución directa de entrelazamiento entre todos los pares de procesadores antes de que comience la ejecución del circuito, con cada procesador manteniendo copias completas del estado cuántico global mediante protocolos continuos de transferencia de estado cuántico. Este enfoque asegura la tolerancia a fallos al permitir que cualquier procesador verifique independientemente los resultados del cálculo mediante mediciones locales sin sobrecarga de comunicación clásica, aunque demanda recursos de entrelazamiento que escalan exponencialmente a medida que aumenta el número de procesadores y limita las implementaciones prácticas a redes pequeñas.",
    "B": "Particionar eficientemente el circuito y programar inteligentemente subconjuntos computacionales a través de diferentes procesadores mientras se minimiza la sobrecarga de la preparación de estados distribuidos, la distribución de entrelazamiento y la comunicación entre procesadores, todo lo cual puede aumentar dramáticamente el número total de operaciones requeridas.",
    "C": "El desafío principal implica mantener la coherencia de fase a través de procesadores cuánticos espacialmente separados mediante referencias de osciladores locales sincronizados, requiriendo que cada procesador ejecute secuencias de puertas idénticas en estricta coordinación temporal. Aunque las estrategias de partición de circuitos pueden distribuir la carga computacional, el cuello de botella fundamental sigue siendo la acumulación de deriva de fase relativa entre nodos, que crece cuadráticamente con la distancia entre procesadores y necesita protocolos frecuentes de recalibración de bloqueo de fase que dominan la sobrecarga total del tiempo de ejecución.",
    "D": "Gestionar la ejecución cuántica distribuida requiere fundamentalmente convertir todas las puertas de múltiples qubits en rotaciones de un solo qubit combinadas con operaciones clásicas de retroalimentación, ya que las correlaciones cuánticas no pueden mantenerse a través de procesadores espacialmente separados sin colapsar los estados de superposición. Esta restricción obliga a los compiladores de circuitos a descomponer operaciones de entrelazamiento no locales en secuencias de mediciones locales seguidas de correcciones condicionadas clásicamente, aunque avances recientes en modelos basados en medición mitigan parcialmente esta limitación mediante técnicas de preparación de estados de clúster.",
    "solution": "B"
  },
  {
    "id": 35,
    "question": "Un grupo de investigación sospecha que su proveedor de computación cuántica en la nube está comprometido. Observan que un complemento de compilación malicioso ha estado insertando puertas SWAP ocultas entre sus qubits de datos y qubits ancilla de repuesto en el diseño del dispositivo, seguidas inmediatamente por operaciones de reinicio en esas ancillas. El circuito del grupo ejecuta rutinas de optimización propietarias sobre datos financieros sensibles. Asumiendo que el adversario controla las mediciones de los qubits de repuesto después del reinicio, ¿qué violación específica de confidencialidad habilita este vector de ataque?",
    "A": "Exfiltración de estado — el adversario mide pasivamente datos que fueron intercambiados al registro ancilla antes del reinicio, filtrando información cuántica fuera del cálculo previsto. Al leer los qubits ancilla antes de que sean reiniciados, el atacante captura información del estado cuántico que fue temporalmente transferida desde el circuito propietario, permitiendo la reconstrucción de resultados computacionales intermedios y potencialmente exponiendo los datos financieros sensibles codificados en las amplitudes cuánticas.",
    "B": "Extracción de oráculo paramétrico — al correlacionar sistemáticamente el momento y ubicación de las operaciones SWAP insertadas con las fluctuaciones observables del tiempo de ejecución en el circuito compilado, el adversario realiza ingeniería inversa de la secuencia de puertas propietaria mediante análisis de canal lateral. Las puertas SWAP actúan como marcadores de tiempo que revelan qué qubits computacionales llevan resultados intermedios de alto valor en cada capa del circuito, permitiendo la reconstrucción de la estructura de árbol de decisión del algoritmo y la importancia relativa de diferentes vías computacionales mediante análisis diferencial del tiempo de ejecución a través de múltiples envíos de trabajos.",
    "C": "Manipulación de resultados de medición — después de intercambiar qubits de datos al registro ancilla y medirlos externamente, el adversario inyecta estados de reemplazo cuidadosamente construidos en esas posiciones ancilla antes de intercambiarlos de vuelta al registro computacional durante la operación de reinicio subsecuente. Esto crea un canal bidireccional donde el atacante no solo extrae información del estado cuántico sino que también inyecta perturbaciones dirigidas que sesgan las estadísticas de medición final hacia resultados predeterminados, permitiendo la manipulación de los resultados de optimización financiera en favor de intereses adversarios.",
    "D": "Huella dactilar de entrelazamiento — las operaciones SWAP crean un canal cuántico encubierto al establecer pares de Bell entre los qubits de datos y ancillas controladas por el adversario antes del reinicio. Aunque las ancillas se reinicien a |0⟩, el historial de entrelazamiento previo deja correlaciones de fase detectables en las capas computacionales subsecuentes que codifican una firma única identificando qué valores de datos específicos fueron procesados. El adversario extrae estas huellas midiendo estabilizadores de múltiples qubits en el registro ancilla a través de ejecuciones de trabajos secuenciales, reconstruyendo los datos financieros mediante análisis de correlación tomográfica.",
    "solution": "A"
  },
  {
    "id": 36,
    "question": "¿Qué vulnerabilidad sofisticada existe en la implementación de la criptografía cuántica independiente del dispositivo?",
    "A": "Lagunas de detección con post-selección selectiva durante ventanas de temporización de coincidencia",
    "B": "La seguridad de los protocolos independientes del dispositivo depende fundamentalmente del supuesto de que las bases de medición se seleccionan usando generadores de números verdaderamente aleatorios que son independientes de todos los eventos cuánticos previos y factores ambientales. Sin embargo, si el generador de números aleatorios utilizado para elegir las configuraciones de medición exhibe incluso correlaciones microscópicas con el dispositivo de preparación del estado cuántico—quizás a través de fluctuaciones compartidas de suministro eléctrico, acoplamiento térmico o interferencia electromagnética—un espía podría explotar estas dependencias sutiles para predecir las próximas elecciones de medición. Esta predictibilidad de bases permite una preparación estratégica de estados que imita violaciones de la desigualdad de Bell mientras que en realidad no proporciona seguridad alguna, socavando completamente la garantía independiente del dispositivo sin requerir ningún acceso directo a los propios dispositivos cuánticos",
    "C": "La seguridad independiente del dispositivo requiere verificación precisa de que los sistemas cuánticos medidos realmente operan en la dimensión del espacio de Hilbert declarada, lo cual se valida típicamente mediante protocolos de testigos de dimensión. Un adversario puede explotar debilidades en estos procedimientos de verificación diseñando cuidadosamente dispositivos cuánticos que parecen violar testigos de dimensión para sistemas de baja dimensión (como qubits) cuando se prueban con operadores testigo estándar, pero que en realidad operan en espacios de dimensión superior donde las correlaciones clásicas pueden simular el comportamiento cuántico esperado. Este ataque de falsificación de dimensión tiene éxito porque la mayoría de los testigos de dimensión prácticos se derivan de mediciones tomográficas incompletas que no pueden distinguir sistemas genuinos de dos niveles de sistemas de dimensión superior que han sido cuidadosamente preparados para comportarse como qubits solo para el conjunto específico de mediciones de prueba incluidas en el protocolo de verificación",
    "D": "Las lagunas de independencia de medición en las pruebas de Bell surgen cuando los generadores de números aleatorios que seleccionan las bases de medición no son verdaderamente independientes de los dispositivos cuánticos bajo prueba, permitiendo correlaciones sutiles a través de factores ambientales compartidos como interferencia electromagnética o fluctuaciones de energía. Estas correlaciones permiten a los adversarios predecir las elecciones de medición y preparar estados cuánticos que imitan violaciones de Bell sin proporcionar seguridad genuina.",
    "solution": "D"
  },
  {
    "id": 37,
    "question": "En un protocolo de distribución cuántica de claves (QKD) de múltiples rondas que opera sobre un canal de fibra óptica con pérdidas de longitud L y coeficiente de atenuación α, ¿qué limita fundamentalmente la tasa de clave secreta alcanzable R en función de la distancia, y cómo difiere esta restricción de los protocolos clásicos de intercambio de claves que operan sobre el mismo canal físico? Considere tanto el límite PLOB para protocolos sin repetidores como el impacto de efectos de tamaño finito en la amplificación de privacidad cuando el número de señales transmitidas N no es asintóticamente grande.",
    "A": "La tasa de clave secreta R escala exponencialmente con la distancia como R ~ exp(-αL/2) debido a la pérdida de fotones, fundamentalmente diferente de los canales clásicos donde la amplificación de señal puede restaurar las tasas de bits. El límite PLOB establece que QKD sin repetidores no puede exceder -log₂(1-η) bits por uso del canal donde η es la transmisividad, mientras que los efectos de tamaño finito introducen penalizaciones adicionales proporcionales a O(1/√N) en el paso de amplificación de privacidad, requiriendo longitudes de bloque más largas para aproximarse a la tasa asintótica.",
    "B": "La tasa de clave experimenta decaimiento exponencial R ~ exp(-αL) gobernado por la atenuación de Beer-Lambert, pero esto coincide con la comunicación óptica clásica donde los amplificadores de fibra dopada con erbio restauran la intensidad de la señal cada 80 km. La distinción fundamental radica en el límite PLOB que limita las tasas sin repetidores a aproximadamente η log₂(η) bits por modo en lugar de la capacidad de Shannon clásica C = log₂(1 + SNR). Las correcciones de tamaño finito escalan como O(log N/N) en lugar de O(1/√N), surgiendo de la estimación de min-entropía suave en el marco de seguridad de Renner, haciendo QKD más vulnerable a fluctuaciones estadísticas que los protocolos clásicos con corrección de errores de decisión dura.",
    "C": "La pérdida de fotones impone un escalado R ~ exp(-αL/2) debido a requisitos de transmisión de fotón único, mientras que la comunicación clásica de estado coherente logra un escalado R ~ exp(-αL/4) mediante detección homodina que accede a ambas cuadraturas. El límite PLOB demuestra que sin repetidores cuánticos, la capacidad no puede exceder la capacidad de compresión de modo único del canal, aproximadamente -log₂(1-η²) bits por uso. Las penalizaciones de tamaño finito contribuyen correcciones O(√(log N)/N) debido al hashing residual en seguridad componible universal, requiriendo N > 10⁸ para alcanzar dentro del 1% de las tasas asintóticas, a diferencia de los códigos clásicos que necesitan solo N > 10⁵.",
    "D": "El decaimiento exponencial inducido por pérdidas R ~ exp(-αL) limita fundamentalmente tanto los canales cuánticos como los clásicos de manera idéntica, ya que ambos transmiten fotones a través de la misma fibra. La distinción clave es que QKD requiere autenticación bilateral que consume log₂N bits por ronda, creando una sobrecarga que se vuelve prohibitiva cuando N < 10⁶, mientras que Diffie-Hellman clásico se completa en comunicación constante. El límite PLOB en realidad se refiere al presupuesto óptico de la capa física en lugar de la capacidad teórica de información, y los efectos de tamaño finito se manifiestan como mayor tasa de error cuántico de bits (QBER) cuando los tamaños de muestra caen por debajo de los umbrales del régimen gaussiano alrededor de N = 10⁴.",
    "solution": "A"
  },
  {
    "id": 38,
    "question": "¿Por qué la Distribución Cuántica de Claves (QKD) se utiliza típicamente junto con el cifrado simétrico?",
    "A": "QKD sirve exclusivamente como un mecanismo seguro de generación y distribución de claves, estableciendo claves secretas compartidas demostrablemente seguras entre partes a través de principios de mecánica cuántica, pero no maneja el cifrado real de datos masivos. Los algoritmos de cifrado simétrico como AES deben entonces usar estas claves distribuidas cuánticamente para cifrar y descifrar eficientemente grandes volúmenes de datos a velocidades prácticas, ya que QKD en sí opera a tasas de rendimiento mucho más bajas limitadas por la transmisión y detección de fotones.",
    "B": "QKD establece claves compartidas incondicionalmente seguras a través de canales cuánticos, pero el material de clave real existe solo como resultados de medición de estados cuánticos colapsados en lugar de como claves criptográficas persistentes. Por lo tanto, se requiere cifrado simétrico para transformar estos resultados de medición cuántica efímeros en material de clave estable y reutilizable que pueda almacenarse en memoria clásica y aplicarse repetidamente a través de múltiples sesiones de cifrado sin degradar las garantías de seguridad teórica de la información proporcionadas por el teorema de no clonación.",
    "C": "Aunque QKD proporciona seguridad teórica de la información para el establecimiento de claves, el protocolo inherentemente revela información de temporización y patrones de comunicación a través del canal de reconciliación clásico utilizado para corrección de errores y amplificación de privacidad. El cifrado simétrico es necesario para cifrar esta comunicación de canal lateral clásico, previniendo ataques de análisis de tráfico que podrían explotar correlaciones estadísticas entre los canales cuántico y clásico para inferir propiedades del material de clave distribuido sin observar directamente estados cuánticos.",
    "D": "Los protocolos QKD generan claves compartidas a tasas fundamentalmente limitadas por la pérdida del canal y la eficiencia del detector, típicamente logrando solo 1-10 kbps sobre distancias prácticas debido a restricciones de transmisión de fotones. Sin embargo, las tecnologías modernas de memoria cuántica solo pueden preservar la coherencia de estos bits de clave distribuidos cuánticamente durante milisegundos antes de que la decoherencia destruya las garantías de seguridad. El cifrado simétrico proporciona una capa de almacenamiento clásico que convierte las claves cuánticas en cadenas de bits clásicas corregidas de errores inmediatamente después de la medición, extendiendo la vida útil efectiva del material de clave de microsegundos a años mientras mantiene las propiedades de seguridad establecidas durante la fase cuántica.",
    "solution": "A"
  },
  {
    "id": 39,
    "question": "¿Cuál es el desafío principal en la implementación de versiones cuánticas de la retropropagación?",
    "A": "Los tres problemas se combinan para crear incompatibilidad fundamental entre los circuitos cuánticos y los paradigmas de optimización basados en gradientes tomados del aprendizaje profundo clásico.",
    "B": "Las puertas cuánticas no son diferenciables en el sentido usual porque representan transformaciones unitarias discretas en lugar de funciones suaves, por lo que la regla de la cadena no se aplica sin primero mapearlas a un espacio de parámetros. Aunque las reglas de desplazamiento de parámetros pueden calcular derivadas evaluando el circuito con valores de parámetros desplazados, este enfoque requiere múltiples ejecuciones del circuito por parámetro y no se compone naturalmente a través de arquitecturas profundas de la manera en que lo hace la retropropagación clásica.",
    "C": "Las mediciones colapsan el sistema exactamente cuando se necesita coherencia para calcular gradientes, destruyendo la misma información cuántica requerida para evaluar cómo los errores en la capa de salida deberían influir en los parámetros de circuitos anteriores. Cada medición muestrea de una distribución de probabilidad en lugar de devolver un valor de gradiente definido, obligándote a repetir todo el paso hacia adelante miles de veces para estimar derivadas con varianza aceptable, lo que niega gran parte de la potencial aceleración cuántica.",
    "D": "El teorema de no clonación impide almacenar en caché estados cuánticos intermedios durante el paso hacia adelante, en lo que la retropropagación clásica se basa para almacenar activaciones para reutilización durante el cálculo del gradiente. Sin copias de estados intermedios, no se puede realizar el paso de propagación de error hacia atrás que compara salidas deseadas versus reales en cada capa, forzando estrategias alternativas de estimación de gradientes que requieren múltiples ejecuciones del circuito por actualización de parámetro.",
    "solution": "D"
  },
  {
    "id": 40,
    "question": "En el contexto del modelado generativo y el aprendizaje no supervisado, las máquinas de Boltzmann cuánticas se han propuesto como una extensión natural de sus contrapartes clásicas. ¿Cuáles son algunas aplicaciones clave de las Máquinas de Boltzmann Cuánticas en el aprendizaje automático y el análisis de datos, particularmente en escenarios donde los recursos cuánticos podrían ofrecer ventajas computacionales sobre los modelos gráficos probabilísticos clásicos?",
    "A": "Las QBMs se dirigen a tareas de aprendizaje no supervisado incluyendo detección de anomalías en datos de sensores de alta dimensión, modelado generativo de conformaciones moleculares para descubrimiento de fármacos, y aprendizaje de representaciones latentes para tomografía comprimida de estados cuánticos. Su ventaja cuántica se conjetura que emerge del muestreo de Gibbs coherente habilitado por evolución en tiempo imaginario en recocedores cuánticos, potencialmente evitando los tiempos de mezcla exponenciales que plagan las cadenas de Markov clásicas en distribuciones multimodales, aunque las demostraciones experimentales permanecen limitadas a pequeños sistemas de prueba de concepto con menos de 50 parámetros efectivos.",
    "B": "Las QBMs encuentran su utilidad principal en tareas de aprendizaje no supervisado como agrupar datos de alta dimensión, aprender representaciones jerárquicas de características de conjuntos de datos sin etiquetar, y realizar reducción de dimensionalidad—esencialmente problemas de reconocimiento de patrones donde el muestreo cuántico de distribuciones de Boltzmann podría teóricamente acelerar la fase de entrenamiento. Su propuesta ventaja cuántica radica en una equilibración más rápida a distribuciones térmicas y un muestreo eficiente de paisajes de probabilidad complejos que desafían los métodos clásicos de Monte Carlo por cadenas de Markov.",
    "C": "Las Máquinas de Boltzmann Cuánticas se aplican principalmente a escenarios de aprendizaje supervisado donde los datos de entrenamiento etiquetados impulsan la optimización basada en gradientes de Hamiltonianos de Ising con campo transversal que codifican la tarea de clasificación. Al representar etiquetas de clase como condiciones de frontera en la red de qubits de la QBM y explotar el tunelado cuántico para escapar de mínimos locales durante la retropropagación, estos modelos logran convergencia más rápida que las redes profundas clásicas en tareas de visión. La ventaja cuántica se manifiesta a través de una complejidad de muestra exponencialmente reducida al aprender fronteras de decisión de bajo rango.",
    "D": "Las QBMs se especializan en arquitecturas de aprendizaje semi-supervisado donde unidades visibles cuánticas codifican datos de entrenamiento clásicos mientras que unidades ocultas cuánticas representan estructura latente, permitiendo inferencia híbrida que combina estimación de máxima verosimilitud clásica con amplificación de amplitud cuántica. Las aplicaciones incluyen entrenamiento adversario generativo donde la red discriminadora se implementa como un circuito cuántico realizando estimación de densidad a través de interferencia destructiva, y autocodificadores variacionales donde el espacio latente es un estado cuántico de variable continua que permite codificación exponencialmente compacta de correlaciones comparado con variables latentes clásicas discretas.",
    "solution": "B"
  },
  {
    "id": 41,
    "question": "¿Qué impulsa la inclusión de puertas de dos qubits parametrizadas como XY(θ) en los ansätze del solucionador variacional de autovalores cuánticos (VQE)?",
    "A": "Fuerza de entrelazamiento ajustable como parámetro variacional: La puerta XY(θ) proporciona una operación de entrelazamiento sintonizable donde θ se convierte en un grado de libertad variacional adicional, permitiendo potencialmente que el ansatz aproxime estados fundamentales objetivo con menos capas de circuito de las que se requerirían usando solo puertas de dos qubits fijas como CNOT o CZ.",
    "B": "La sintonizabilidad continua del acoplamiento de intercambio permite que XY(θ) interpole entre estados producto y estados máximamente entrelazados, proporcionando ventajas de optimización basada en gradientes sobre puertas fijas discretas. Sin embargo, la sobrecarga de calibración de hardware para puertas parametrizadas típicamente aumenta los errores sistemáticos proporcionalmente al número de valores distintos de θ requeridos durante la optimización, haciéndolas menos favorables que las puertas nativas fijas cuando la profundidad del circuito excede el umbral limitado por coherencia de aproximadamente 50-100 operaciones de dos qubits en dispositivos superconductores actuales.",
    "C": "Las puertas parametrizadas permiten la codificación directa de simetrías específicas del problema al mapear parámetros físicos como ángulos de enlace en moléculas directamente sobre ángulos de puerta, reduciendo la dimensión variacional. Aunque XY(θ) puede representar ciertos Hamiltonianos de espín más naturalmente que descomposiciones basadas en CNOT, este enfoque adaptado a simetrías requiere rediseño del ansatz dependiente del problema y sacrifica la aplicabilidad universal que los ansätze eficientes en hardware con puertas fijas proporcionan a través de paisajes de optimización arbitrarios.",
    "D": "La puerta XY(θ) implementa operaciones SWAP parciales con magnitud controlable, permitiendo transferencia fraccional de población entre estados de la base computacional que puede optimizarse para coincidir con el perfil exacto de entropía de entrelazamiento de los autoestados objetivo. Este control de grano fino permite mejor aproximación de funciones de correlación en sistemas fuertemente interactuantes, aunque los parámetros θ adicionales expanden la dimensionalidad del paisaje de optimización por un factor igual al número de puertas de dos qubits, introduciendo potencialmente más mínimos locales que arquitecturas de ángulo fijo.",
    "solution": "A"
  },
  {
    "id": 42,
    "question": "¿Qué vulnerabilidad sofisticada existe en el proceso de destilación de claves de la distribución cuántica de claves?",
    "A": "La resistencia cuántica de la función hash se vuelve crítica cuando el postprocesamiento clásico usa funciones hash criptográficas para verificar la paridad durante la corrección de errores, ya que futuros ordenadores cuánticos ejecutando el algoritmo de Grover podrían revertir estos hashes para reconstruir los bits de clave brutos. Si la función hash carece de suficiente resistencia cuántica, un espía podría explotar colisiones de hash.",
    "B": "La sincronización de trama de reconciliación de información falla cuando los canales clásicos usados para intercambiar síndromes de corrección de errores experimentan fluctuaciones temporales o pérdida de paquetes, causando que Alice y Bob apliquen verificaciones de paridad a ventanas de bits desalineadas. Esta desincronización es particularmente explotable porque un espía puede selectivamente retrasar o reordenar mensajes clásicos.",
    "C": "La estimación de entropía de amplificación de privacidad se vuelve vulnerable cuando la min-entropía de la clave tamizada después de la corrección de errores es sobreestimada, llevando a la extracción de una clave final que es más larga que la aleatoriedad secreta real disponible. Si la estimación de entropía asume eficiencia ideal del detector o subestima la información de Eve del filtrado de reconciliación de base, la razón de compresión de amplificación de privacidad puede ser insuficiente. Esto resulta en una clave final donde algunos bits están parcialmente correlacionados con el conocimiento del espía, violando las garantías de seguridad teórico-informacionales. La vulnerabilidad es particularmente aguda cuando los efectos de tamaño finito no se contabilizan adecuadamente en la aplicación del Lema de Hash Sobrante, o cuando la información de canal lateral de variaciones temporales en la comunicación clásica filtra bits adicionales más allá de los cálculos de tasa de error de bit cuántico.",
    "D": "El cálculo de filtración de corrección de errores se vuelve vulnerable cuando la cantidad de información clásica intercambiada durante la corrección de errores basada en síndromes es subestimada, permitiendo que un espía obtenga más conocimiento sobre la clave tamizada del que se contabiliza en el paso de amplificación de privacidad. Si el límite de filtración asume códigos LDPC idealizados pero revela información de canal lateral a través de temporización o longitudes de mensaje, la tasa de clave final puede ser sobreestimada.",
    "solution": "C"
  },
  {
    "id": 43,
    "question": "¿Cuál es la diferencia principal entre el problema del subgrupo oculto para grupos Abelianos versus grupos no Abelianos?",
    "A": "La distinción crítica radica en la complejidad de medición requerida después de la transformada cuántica de Fourier (QFT): para grupos Abelianos, un único estado de coset |ψ_H⟩ = (1/√|H|)Σ_{h∈H}|gh⟩ sometido a QFT produce resultados de medición que proyectan directamente sobre etiquetas de representación irreducible (irrep), cada una de las cuales es unidimensional y corresponde a un carácter de grupo χ_ρ: G → ℂ*, permitiendo postprocesamiento clásico en tiempo polinomial para reconstruir el subgrupo oculto H a partir de O(log|G|) muestras de medición independientes vía álgebra lineal sobre la tabla de caracteres. En contraste, los grupos no Abelianos poseen irreps de dimensión d_ρ > 1, frecuentemente escalando como √|G| para grupos simétricos S_n, significando que la QFT mapea estados de coset en superposiciones sobre entradas de matriz dentro de estos espacios de representación de alta dimensión: la medición produce tanto la etiqueta irrep ρ como un índice de elemento de matriz (i,j) ∈ [d_ρ]×[d_ρ], pero la información del subgrupo oculto queda codificada en patrones de correlación sutiles a través de estas distribuciones de elementos de matriz. Extraer H de estas estadísticas de medición de irrep de alta dimensión genéricamente requiere ya sea exponencialmente muchas mediciones cuánticas para realizar tomografía completa del espacio de representación, o mediciones polinomiales combinadas con computación clásica exponencial para resolver el sistema resultante de restricciones no lineales, creando una barrera teórico-informacional fundamental ausente en el caso Abeliano donde las dimensiones de irrep permanecen uniformemente uno.",
    "B": "La diferencia estructural se manifiesta en la estrategia de muestreo de Fourier: los grupos Abelianos satisfacen el teorema fundamental de grupos Abelianos finitamente generados, que garantiza una descomposición G ≅ ℤ_{n₁} ⊕ ℤ_{n₂} ⊕ ... ⊕ ℤ_{n_k} en una suma directa de grupos cíclicos, permitiendo que el problema del subgrupo oculto se factorice en k problemas independientes unidimensionales que pueden resolverse vía búsqueda cuántica estándar de período en cada factor cíclico usando O(k·log|G|) consultas. La transformada cuántica de Fourier sobre G se descompone correspondientemente como QFT_G = QFT_{n₁} ⊗ QFT_{n₂} ⊗ ... ⊗ QFT_{n_k}, y medir en esta base de Fourier de producto tensorial revela la estructura del subgrupo oculto por componentes con eficiencia polinomial. Inversamente, los grupos no Abelianos carecen de tales descomposiciones canónicas: grupos como el grupo simétrico S_n o grupos diedrales D_n no pueden escribirse como productos directos de subgrupos más simples de manera que respete la estructura del subgrupo oculto, forzando a los algoritmos a trabajar con la teoría de representación no Abeliana completa donde la QFT se convierte en un cambio de base a forma diagonal por bloques con bloques correspondientes a irreps de dimensiones variables d_ρ ≤ √|G|. La falta de estructura de producto tensorial significa que la información del subgrupo oculto no puede aislarse en factores independientes de baja dimensión, requiriendo resolución simultánea de correlaciones a través de múltiples sectores irrep de alta dimensión—una tarea que demanda recursos exponenciales en el caso general a pesar de complejidad de consulta cuántica polinomial.",
    "C": "Para grupos Abelianos, la transformada cuántica de Fourier opera sobre una estructura donde todas las representaciones irreducibles son unidimensionales, lo que significa que los resultados de medición de la QFT revelan directamente la periodicidad del subgrupo oculto mediante aritmética modular simple sobre las frecuencias observadas. La base de Fourier diagonaliza la operación de grupo limpiamente, y el postprocesamiento polinomial es suficiente para extraer los generadores del subgrupo. En marcado contraste, los grupos no Abelianos poseen representaciones irreducibles de dimensión mayor que uno —frecuentemente creciendo como √|G| o más— lo que significa que la QFT sobre tales grupos produce resultados de medición que caen en espacios de representación de alta dimensión donde la información del subgrupo oculto queda codificada en patrones de correlación intrincados a través de entradas de matriz en lugar de simples picos de frecuencia. Extraer el subgrupo de estos coeficientes irrep multidimensionales generalmente requiere exponencialmente muchas mediciones o mediciones polinomiales seguidas de postprocesamiento clásico exponencial, creando una barrera computacional fundamental ausente en el caso Abeliano.",
    "D": "La división fundamental surge de cómo la transformada cuántica de Fourier interactúa con la teoría de representación del grupo: en grupos Abelianos G, el lema de Schur combinado con conmutatividad [g₁,g₂]=0 ∀g₁,g₂∈G fuerza que cada representación irreducible sea unidimensional (d_ρ=1 para todo ρ), significando que la QFT descompone el álgebra de grupo ℂ[G] en una suma directa de subespacios propios unidimensionales etiquetados por caracteres ρ: G→U(1), y medir un estado de coset transformado por QFT |ψ_H⟩=(1/√|H|)Σ_{h∈H}|gh⟩ colapsa al estado base |ρ⟩ con probabilidad determinada por si ρ se anula en el subgrupo oculto H (es decir, si ρ(h)=1 ∀h∈H). Recolectando O(log|G|) tales muestras ρ₁,...,ρ_k y resolviendo el sistema lineal ρ_i(h)=1 vía logaritmos discretos sobre el grupo de caracteres Ĝ≅G reconstruye H en tiempo polinomial. Para grupos no Abelianos, los irreps tienen dimensiones d_ρ>1 escalando hasta Θ(√|G|), por lo que la QFT mapea estados de coset en superposiciones sobre tripletas (ρ,i,j) donde i,j∈[d_ρ] indexan entradas de matriz dentro del irrep ρ. La distribución sobre estos índices de matriz codifica H a través de coeficientes de Fourier teórico-representacionales que satisfacen reglas de suma no triviales, pero a diferencia del caso Abeliano, no se conoce algoritmo eficiente para extraer H de polinomialmente muchas tales muestras sin resolver problemas clásicamente difíciles como isomorfismo de grafos o reducción de retículo embebidos en la estructura de representación.",
    "solution": "C"
  },
  {
    "id": 44,
    "question": "En arquitecturas de redes cuánticas, estás diseñando un protocolo para distribuir un estado cuántico |ψ⟩ desde una fuente central a cinco laboratorios geográficamente separados, cada uno de los cuales necesita realizar mediciones locales en copias idénticas. Tu estudiante de posgrado propone usar un circuito simple de clonación por difusión. ¿Por qué los protocolos de multidifusión cuántica difieren fundamentalmente de la multidifusión clásica en este escenario?",
    "A": "El teorema de no clonación previene la copia, por lo que o distribuyes pares entrelazados distintos a cada destino o usas un estado entrelazado de múltiples qubits del cual cada parte puede extraer información correlacionada mediante operaciones locales y comunicación clásica. La clonación perfecta violaría la linealidad de la mecánica cuántica, forzando a los protocolos a compartir entrelazamiento en lugar de duplicar estados.",
    "B": "La multidifusión cuántica requiere establecer estados GHZ o estados de grafo como sustrato de distribución, mientras que la multidifusión clásica opera duplicando cadenas de bits en enrutadores intermedios. La fuente prepara un estado entrelazado de (n+1) qubits donde un qubit permanece en la fuente y n qubits se distribuyen a los receptores; la matriz de densidad reducida de cada receptor aproxima el estado deseado |ψ⟩ hasta correcciones unitarias locales determinadas por información de síndrome clásica difundida después de mediciones de teleportación, con fidelidad degradándose como 1-O(1/n) en lugar de lograr reproducción perfecta.",
    "C": "El teorema de no clonación prohíbe la expansión determinística 1→n para estados cuánticos desconocidos, forzando a los protocolos de multidifusión a aceptar éxito probabilístico que requiere coordinación clásica para verificar recepción, o distribuir clones aproximados con fidelidad acotada por F ≤ (n+1)/(n+2) según la clonación óptima de Buzek-Hillery, o prearreglar entrelazamiento compartido que efectivamente teletransporta el estado mediante mediciones de Bell post-seleccionadas cuyos resultados deben difundirse clásicamente a todos los receptores antes de que puedan reconstruir copias locales, cambiando fundamentalmente la estructura del protocolo respecto a la simple duplicación de paquetes clásica.",
    "D": "Los canales cuánticos exhiben acumulación de fase dependiente de la trayectoria que crea interferencia destructiva al dividir un estado cuántico a través de múltiples rutas espaciales, mientras que los bits clásicos se propagan independientemente a través de cada rama del árbol de multidifusión. Específicamente, cuando un qubit fotónico atraviesa una red de divisores ópticos con n salidas, la amplitud de función de onda se divide como 1/√n a través de todas las trayectorias, pero diferencias relativas de longitud de trayectoria óptica ΔL introducen desplazamientos de fase φ = 2πΔL/λ que causan que el estado distribuido evolucione a un estado mezclado con pureza (1+cos φ)/2, requiriendo estabilización de fase activa con precisión λ/n para mantener coherencia a través de todos los receptores.",
    "solution": "A"
  },
  {
    "id": 45,
    "question": "¿Cuál de los siguientes NO es un enfoque común para diseñar ansätze de circuitos cuánticos?",
    "A": "Estructuras eficientes en hardware usando conjuntos de puertas nativas — Estos ansätze se construyen cuidadosamente para minimizar la profundidad del circuito empleando exclusivamente puertas que pueden ejecutarse nativamente en el procesador cuántico objetivo sin requerir descomposición en operaciones más primitivas, reduciendo tanto la sobrecarga de compilación como los errores de puerta acumulados particularmente para algoritmos variacionales.",
    "B": "Diseños inspirados en el problema que reflejan simetrías Hamiltonianas — Cuando el problema objetivo exhibe propiedades de simetría conocidas como conservación de número de partículas, paridad de espín, o periodicidad espacial, el ansatz puede diseñarse para preservar estas simetrías por construcción mediante selección cuidadosa de rotaciones parametrizadas y patrones de entrelazamiento, reduciendo dramáticamente la dimensión del espacio de búsqueda variacional.",
    "C": "Circuitos generados aleatoriamente con entrelazamiento fijo que ignoran la estructura del problema — Este enfoque construye ansätze muestreando aleatoriamente secuencias de puertas y patrones de entrelazamiento sin consideración de las simetrías del problema subyacente, estructura Hamiltoniana, o restricciones de hardware, descartando deliberadamente información específica del dominio que podría mejorar la convergencia y expresividad en favor de exploración no estructurada del espacio de Hilbert completo.",
    "D": "Patrones de red tensorial como MPS o MERA — Al organizar las puertas paramétricas de acuerdo con arquitecturas de red tensorial bien estudiadas como estados producto de matriz o renormalización de entrelazamiento multi-escala, estos ansätze aprovechan la estructura de correlación jerárquica característica de sistemas cuánticos de muchos cuerpos.",
    "solution": "C"
  },
  {
    "id": 46,
    "question": "En las redes cuánticas distribuidas, los Acuerdos de Nivel de Servicio Cuántico (QSLAs) deben manejar métricas que no tienen análogo clásico. Más allá de las garantías tradicionales de tiempo de actividad y latencia, un QSLA necesita especificar criterios de rendimiento únicos para la comunicación cuántica. ¿Qué desafío introduce esto que los SLAs clásicos evitan completamente?",
    "A": "Definir garantías contractuales para umbrales de fidelidad de entrelazamiento, tasas de generación de pares entrelazados y ventanas de tiempo de decoherencia—métricas de rendimiento que no tienen contrapartes clásicas y no pueden medirse sin consumir el propio recurso cuántico. A diferencia de la pérdida de paquetes clásica o el ancho de banda que pueden monitorearse pasivamente, verificar la calidad del entrelazamiento requiere mediciones destructivas de estados de Bell que destruyen el mismo recurso que se está garantizando. Los QSLAs deben especificar rangos aceptables para concurrencia, negatividad o fidelidad a estados maximalmente entrelazados, junto con tasas de generación medidas en ebits por segundo y tiempos de vida de coherencia garantizados, creando contratos exigibles en torno a fenómenos cuánticos que los SLAs clásicos nunca abordan ya que los bits clásicos no sufren decoherencia ni exhiben correlaciones no locales.",
    "B": "Establecer garantías contractuales para fidelidad de preparación de estados cuánticos distribuidos, tasas de distribución de entrelazamiento multipartito y ventanas de capacidad de canal cuántico—métricas de rendimiento únicas a las redes cuánticas que requieren verificación mediante protocolos de reconstrucción tomográfica. A diferencia del rendimiento clásico o la fluctuación temporal que pueden monitorearse continuamente mediante muestreo de paquetes, certificar el rendimiento de la red cuántica requiere tomografía de proceso completa que escala exponencialmente como 4^n mediciones para estados de n-qubits, haciendo la verificación computacionalmente intratable para sistemas grandes. Los QSLAs deben especificar rangos aceptables para pureza de estado, entrelazamiento de formación o fidelidad de canal a canales cuánticos ideales, junto con tasas de distribución medidas en pares de Bell por segundo, creando contratos exigibles en torno a recursos cuánticos cuya verificación fundamentalmente requiere computación clásica exponencial que el monitoreo de SLA clásico evita completamente.",
    "C": "Especificar garantías contractuales para tasas de distribución de claves cuánticas, probabilidades de éxito de intercambio de entrelazamiento y duraciones de almacenamiento en memoria cuántica—métricas de rendimiento sin equivalentes clásicos que no pueden verificarse sin perturbar la propia información cuántica. A diferencia de las tasas de error clásicas o la latencia que pueden medirse mediante canales de monitoreo redundantes, evaluar la calidad de la comunicación cuántica requiere realizar mediciones de síndrome que colapsan parcialmente los estados de superposición, extrayendo solo información de síndrome mientras preservan los qubits lógicos. Los QSLAs deben especificar rangos aceptables para entrelazamiento destilable, información mutua cuántica o fidelidad a estados GHZ, junto con tasas de distribución medidas en bits de clave secreta por segundo y tiempos de almacenamiento garantizados, creando contratos exigibles en torno a fenómenos cuánticos que los SLAs clásicos evitan ya que las señales clásicas pueden copiarse para monitoreo no invasivo.",
    "D": "Negociar garantías contractuales para tiempos de preservación de coherencia cuántica, brillo de pares de fotones entrelazados y fidelidad de transmisión de estados cuánticos—métricas de rendimiento ausentes en redes clásicas que no pueden monitorearse continuamente sin introducir retroacción de medición que perturba el propio canal cuántico. A diferencia de la relación señal-ruido clásica o el ancho de banda que pueden medirse mediante medidores de potencia en línea, la caracterización del canal cuántico requiere tomografía de estado periódica que interrumpe temporalmente el servicio para inyectar estados de prueba y realizar mediciones proyectivas. Los QSLAs deben especificar rangos aceptables para fidelidad de proceso del canal, rendimiento de entrelazamiento por pulso de bombeo o tiempos T₂ de coherencia de memoria, junto con tasas de repetición medidas en pares heraldados por segundo y factores de calidad de aislamiento ambiental, creando contratos exigibles en torno a recursos cuánticos cuya caracterización inherentemente interrumpe la entrega del servicio que los SLAs clásicos pueden monitorear transparentemente sin impacto en el servicio.",
    "solution": "A"
  },
  {
    "id": 47,
    "question": "Las pruebas de intercambio (swap tests) utilizadas en funciones de coste basadas en fidelidad están limitadas en circuitos profundos principalmente porque:",
    "A": "La estimación de superposición perfecta solo funciona en hardware sin ruido porque la medición de fidelidad del swap test se basa en interferencia cuántica destructiva en las estadísticas de medición de la ancilla, donde la probabilidad de medir |0⟩ es igual a (1 + |⟨ψ|φ⟩|²)/2, pero cualquier ruido despolarizante o infidelidad de puerta introduce decoherencia que sesga esta probabilidad hacia abajo de manera no lineal.",
    "B": "Convierten observables locales en operadores no locales que requieren conectividad de largo alcance que la mayoría de las arquitecturas actuales no pueden implementar eficientemente, porque la operación controlled-SWAP del swap test entre dos estados cuánticos físicamente exige que el qubit ancilla interactúe simultáneamente con qubits de datos espacialmente separados.",
    "C": "Medir a mitad de camino elimina restricciones de coherencia porque el protocolo del swap test requiere medición proyectiva del qubit ancilla después de aplicar las puertas controlled-SWAP y Hadamard, lo cual colapsa el estado cuántico y destruye cualquier estructura de entrelazamiento restante entre los registros de datos, impidiendo la propagación de correlaciones cuánticas hacia adelante a través de capas posteriores del circuito.",
    "D": "Los qubits ancilla aumentan la exposición a errores al introducir recursos cuánticos adicionales que deben inicializarse, manipularse mediante operaciones controladas y medirse, con cada paso acumulando decoherencia y errores de puerta que se componen multiplicativamente a través del protocolo del swap test, particularmente problemático en circuitos profundos que ya operan cerca de los límites de tiempo de coherencia.",
    "solution": "D"
  },
  {
    "id": 48,
    "question": "¿Por qué las optimizaciones basadas en dispersidad de las simulaciones de vector de estado no pueden usarse directamente?",
    "A": "Las matrices de densidad que representan estados cuánticos mezclados son inherentemente deficientes en rango cuando el sistema exhibe cualquier grado de pureza menor que uno, pero esta propiedad estructural no se traduce en patrones útiles de dispersidad en la base computacional. Los términos de coherencia fuera de la diagonal que codifican correlaciones cuánticas están distribuidos a través de la matriz de una manera que depende de la elección específica de base, y dado que los procesos de ruido físico como la amortiguación de amplitud y el desfase afectan diferentes elementos de la matriz de manera no uniforme, no existe una estructura dispersa natural que persista a través de las operaciones de puerta—cualquier intento de explotar dispersidad dependiente de la base requeriría transformaciones de base constantes que eliminarían los ahorros computacionales.",
    "B": "Incluso cuando el vector de estado inicial contiene muchas amplitudes cero que podrían permitir representaciones dispersas, las propias puertas cuánticas se implementan como matrices unitarias densas que acoplan todos los estados de la base computacional. Esto significa que aplicar incluso una rotación de un solo qubit a un estado disperso generalmente produce una salida densa, y las puertas de entrelazamiento multi-qubit mezclan aún más las amplitudes a través de todo el espacio de Hilbert, destruyendo cualquier patrón de dispersidad que pudiera haber existido en la configuración de entrada.",
    "C": "Las GPUs carecen de soporte nativo de matrices dispersas para representaciones de operadores de densidad, y el costo adicional de convertir entre formatos anula cualquier ventaja computacional. Aunque las arquitecturas de GPU modernas sí proporcionan bibliotecas como cuSPARSE para manejar álgebra lineal dispersa, el problema fundamental es que las matrices de densidad de sistemas ruidosos requieren conversión continua de formato entre almacenamiento de fila comprimida dispersa (CSR) y representaciones densas durante cada aplicación de puerta, lo cual introduce cuellos de botella de transferencia de memoria que abruman completamente la aceleración teórica de operaciones de punto flotante reducidas, particularmente al tratar con operadores de dimensión 2^n × 2^n para sistemas más allá de 15-20 qubits.",
    "D": "El ruido introduce entradas no cero en todas partes en la representación de matriz de densidad, destruyendo cualquier estructura de dispersidad que pudiera existir en vectores de estado sin ruido. Los canales cuánticos que modelan procesos de decoherencia como ruido despolarizante o amortiguación de amplitud hacen que cada elemento de la matriz adquiera valores no cero a través de la suma de operadores de Kraus, y esta estructura densa persiste a lo largo de la computación independientemente de las propiedades del estado inicial.",
    "solution": "D"
  },
  {
    "id": 49,
    "question": "¿Qué papel desempeñan los Nodos de Confianza Simplificados en una cadena de distribución de claves cuánticas?",
    "A": "Retransmiten la distribución de entrelazamiento a través de segmentos de red realizando intercambio de entrelazamiento mediante mediciones de estados de Bell en pares de fotones entrantes, luego reenvían las señales de éxito heraldadas y la información de base a nodos adyacentes sin ejecutar protocolos completos de destilación de claves. Esta arquitectura permite la distribución de entrelazamiento a larga distancia al dividir el escalado de pérdida exponencial en segmentos lineales manejables, donde cada nodo realiza solo la medición cuántica y comunicación clásica necesarias para establecer correlaciones brutas, difiriendo la amplificación de privacidad y reconciliación de errores computacionalmente intensivas hasta que el estado entrelazado de extremo a extremo alcanza usuarios terminales en los puntos finales de la red.",
    "B": "Retransmiten la paridad de resultados de medición sin ejecutar post-procesamiento completo, esencialmente realizando reenvío clásico de resultados de detección brutos entre segmentos QKD adyacentes. Esto permite que la red se extienda más allá de distancias de un solo salto al dividir el enlace en secciones manejables donde cada nodo simplemente pasa los valores de bits y las elecciones de base sin ejecutar algoritmos computacionalmente intensivos de amplificación de privacidad o corrección de errores, permitiendo el establecimiento rápido de claves a través de redes de escala metropolitana mientras mantiene supuestos de confianza en puntos de retransmisión intermedios.",
    "C": "Realizan operaciones QKD de preparación y medición independientemente en cada segmento de enlace adyacente, generando claves brutas separadas con nodos vecinos, luego ejecutan un protocolo de combinación de claves de confianza donde operaciones XOR bit a bit fusionan las claves por segmento en un secreto de extremo a extremo compartido entre usuarios terminales. Cada nodo conduce su propio tamizado y corrección de errores con sus vecinos inmediatos, produciendo subclaves seguras que se combinan mediante cifrado clásico de libreta de un solo uso, permitiendo que la red escale linealmente con la distancia mientras requiere confianza completa en la capacidad de cada nodo intermedio para mantener seguro el material de clave combinado contra compromisos.",
    "D": "Implementan modulación de intensidad de estado señuelo adaptativa monitoreando estadísticas de pérdida de canal en tiempo real a través de segmentos de fibra adyacentes y ajustando dinámicamente la distribución de número de fotones de pulsos coherentes débiles enviados entre nodos. Cuando un segmento experimenta pérdida elevada sugiriendo actividad potencial de espionaje, el nodo aumenta la proporción de estados señuelo de vacío y fotón único en relación con estados de señal, luego realiza pruebas de hipótesis estadísticas en los patrones de detección observados para determinar si la desviación de pérdida es consistente con ataques de división de número de fotones o simplemente refleja degradación ambiental de la fibra.",
    "solution": "B"
  },
  {
    "id": 50,
    "question": "¿Cuál es la relación entre la expresividad de un circuito cuántico parametrizado y su entrenabilidad?",
    "A": "Los circuitos más expresivos son siempre más fáciles de entrenar debido a la abundancia de vías de optimización que proporcionan en el espacio de parámetros.",
    "B": "Son básicamente propiedades independientes que resultan correlacionar en arquitecturas específicas pero no comparten ninguna conexión teórica fundamental.",
    "C": "La alta expresividad típicamente crea mesetas áridas (barren plateaus)—la función de coste se vuelve exponencialmente plana y los gradientes se desvanecen. Cuando un circuito puede acceder uniformemente a una gran porción del espacio de Hilbert (alta expresividad), el paisaje de pérdida se vuelve extremadamente de alta dimensión y la magnitud promedio del gradiente escala exponencialmente pequeña con el tamaño del sistema. Esta tensión entre entrenabilidad y expresividad significa que los circuitos maximalmente expresivos son a menudo los más difíciles de optimizar en la práctica.",
    "D": "La entrenabilidad depende únicamente del optimizador clásico, no de la expresividad del circuito, ya que el paisaje de optimización está determinado completamente por la definición de la función de pérdida y los hiperparámetros del optimizador.",
    "solution": "C"
  },
  {
    "id": 51,
    "question": "¿Qué es el algoritmo cuántico de Metropolis?",
    "A": "Versión cuántica de Metropolis-Hastings para muestrear distribuciones térmicas en sistemas de muchos cuerpos, donde circuitos cuánticos implementan transiciones de cadena de Markov mediante rotaciones controladas y mediciones proyectivas que aceptan o rechazan movimientos propuestos según diferencias de energía, permitiendo la exploración eficiente de estados de equilibrio.",
    "B": "Protocolo de muestreo cuántico que implementa termalización mediante subrutinas de estimación de fase que preparan estados de Gibbs, donde la evolución unitaria controlada codifica la descomposición espectral del Hamiltoniano y la amplificación de amplitud sesga los resultados de medición hacia configuraciones de baja energía según pesos de Boltzmann, logrando aceleración polinomial respecto a Monte Carlo clásico de cadenas de Markov.",
    "C": "Adaptación de Metropolis-Hastings usando preparación de estado adiabática combinada con caminatas cuánticas en el espacio de configuraciones, donde la interpolación gradual del Hamiltoniano mantiene el balance detallado mientras el tunelamiento cuántico mejora los tiempos de mezcla, y las mediciones proyectivas de energía determinan probabilidades de aceptación para transiciones de estado propuestas en protocolos de muestreo de equilibrio térmico.",
    "D": "Variante de recocido cuántico que implementa muestreo térmico mediante programación de campo transversal y retroalimentación basada en mediciones, donde operadores de caminata cuántica de Szegedy codifican condiciones de balance detallado y la modificación de amplitud tipo Grover acelera la convergencia a distribuciones de Gibbs explotando interferencia cuántica en las amplitudes de probabilidad de transición entre estados de configuración.",
    "solution": "A"
  },
  {
    "id": 52,
    "question": "¿Qué desafío debe abordarse al aplicar Procesos Gaussianos Cuánticos a conjuntos de datos del mundo real?",
    "A": "La incertidumbre inherente en los estados cuánticos, gobernada por el principio de incertidumbre de Heisenberg, no puede modelarse ni propagarse a través de marcos de Procesos Gaussianos porque la naturaleza probabilística de las mediciones cuánticas es fundamentalmente incompatible con la estructura de covarianza determinista requerida por la teoría GP.",
    "B": "Los Procesos Gaussianos Cuánticos fundamentalmente no pueden manejar tareas de regresión debido a la naturaleza continua del espacio de salida, que entra en conflicto con los resultados discretos de medición requeridos por la mecánica cuántica. Mientras que los QGP destacan en clasificación binaria y multiclase mapeando evaluaciones de kernel a estados cuánticos discretos, el postulado de proyección fuerza todas las mediciones a colapsar a autovalores, haciendo imposible extraer los valores de función continuos necesarios para regresión. Esta limitación requiere arquitecturas híbridas clásico-cuánticas donde el postprocesamiento clásico reconstruye salidas de regresión a partir de múltiples mediciones cuánticas discretas.",
    "C": "Los Procesos Gaussianos Cuánticos no requieren absolutamente ningún ajuste de hiperparámetros porque el kernel cuántico está determinado únicamente por la estructura del espacio de Hilbert del mapa de características cuántico, eliminando la necesidad de selección de ancho de banda, parámetros de regularización o elección de kernel.",
    "D": "Las estrategias de mitigación de ruido y corrección de errores siguen siendo críticas para garantizar predicciones confiables de los Procesos Gaussianos Cuánticos cuando se implementan en hardware cuántico a corto plazo, donde la decoherencia, errores de puertas y ruido de medición pueden corromper evaluaciones de kernel y conducir a distribuciones posteriores inexactas. El manejo efectivo del ruido requiere calibración cuidadosa y técnicas de mitigación de errores.",
    "solution": "D"
  },
  {
    "id": 53,
    "question": "¿Qué vulnerabilidad específica existe en la arquitectura de conectividad de qubits de los procesadores cuánticos?",
    "A": "Los cuellos de botella de enrutamiento en topologías fuertemente restringidas surgen cuando los algoritmos cuánticos requieren interacciones entre qubits distantes en arquitecturas con conectividad limitada, obligando al compilador a insertar largas secuencias de puertas SWAP para mover información cuántica a través del chip. Un adversario con conocimiento del grafo de conectividad y el algoritmo objetivo puede analizar estos patrones de enrutamiento para inferir qué qubits contienen información crítica en diferentes puntos de la computación.",
    "B": "Las restricciones de acoplamiento a vecinos más cercanos limitan a los procesadores cuánticos a realizar puertas de dos qubits solo entre qubits físicamente adyacentes, requiriendo uso extensivo de redes SWAP para implementar operaciones multi-qubit arbitrarias. La naturaleza determinista de la inserción de SWAP crea estados intermedios predecibles durante la ejecución del circuito, permitiendo que un atacante que realiza mediciones de canal lateral en puntos específicos de la cadena SWAP intercepte información cuántica en tránsito entre qubits no adyacentes.",
    "C": "Dependencias de líneas de control compartidas que permiten diafonía entre qubits, habilitando interacciones no intencionadas cuando múltiples qubits son impulsados simultáneamente o cuando señales de control destinadas a un qubit afectan inadvertidamente a qubits vecinos debido al aislamiento imperfecto en la infraestructura de entrega de microondas. Esta restricción arquitectónica significa que las operaciones en un qubit pueden filtrar información a qubits cercanos o correlacionarse con ellos, creando canales encubiertos para transferencia de información que evitan el monitoreo de seguridad a nivel de puerta lógica.",
    "D": "Las vías de acoplamiento por resonancia cruzada en arquitecturas de frecuencia fija crean interacciones parásitas entre qubits que comparten líneas de impulso de microondas o están acoplados a través de modos resonadores comunes. Un adversario que puede inyectar señales de interferencia precisamente temporizadas puede mejorar selectivamente términos específicos de resonancia cruzada para crear vías de comunicación encubiertas que permitan que un qubit influya en otro sin ejecutar puertas explícitas, evadiendo mecanismos de seguridad que monitorean solo la secuencia lógica de puertas.",
    "solution": "C"
  },
  {
    "id": 54,
    "question": "La aceleración en la evaluación de fórmulas para árboles NAND inspiró algoritmos posteriores para evaluar fórmulas booleanas generales mediante:",
    "A": "Los investigadores se dieron cuenta de que la caminata cuántica usada para recorrer árboles NAND podía simularse clásicamente para fórmulas booleanas generales reemplazando el operador de difusión cuántica con una caminata aleatoria clásica en el árbol sintáctico de la fórmula, donde cada nodo es visitado con probabilidad proporcional a la amplitud al cuadrado del estado cuántico correspondiente. Aunque este enfoque clásico sacrifica la aceleración cuadrática, logra un factor de aproximación logarítmico muestreando O(N^(1/2) log N) caminos a través de la fórmula y promediando los resultados.",
    "B": "Algoritmos posteriores extendieron el resultado del árbol NAND desarrollando una red de ordenamiento cuántica que preprocesa bits de entrada en un ordenamiento canónico antes de consultar la estructura de la fórmula, reduciendo la complejidad del oráculo de O(√N) a O(log²N) para fórmulas de profundidad d. El paso de ordenamiento explota el paralelismo cuántico para comparar todas las 2ⁿ asignaciones de entrada posibles simultáneamente vía amplificación de amplitud.",
    "C": "La aceleración cuántica para árboles NAND inspiró una nueva clase de algoritmos que mapean fórmulas booleanas arbitrarias a interferómetros ópticos lineales, donde cada variable se codifica en la presencia o ausencia de un fotón en un modo específico y las conectivas lógicas (AND, OR, NOT) se implementan mediante divisores de haz con transmisividades elegidas para coincidir con el árbol sintáctico de la fórmula. Al inyectar un estado de Fock multifotónico en el interferómetro y realizar muestreo bosónico en los puertos de salida, estos algoritmos obtienen aceleración cuadrática en evaluación de fórmulas porque la simetrización bosónica inherentemente calcula integrales de trayectoria sobre todas las asignaciones posibles de valores de verdad en paralelo. La conexión con árboles NAND surge porque los árboles binarios balanceados corresponden a geometrías de interferómetro perfectamente simétricas (cascadas Mach-Zehnder), y el tamaño del testigo en programas de span se traduce directamente al número de fotones requeridos.",
    "D": "Convirtiendo cualquier fórmula booleana en una representación equivalente de programa de span que admite un testigo de tamaño pequeño, permitiendo que algoritmos cuánticos consulten la estructura de la fórmula con complejidad proporcional al tamaño del testigo en lugar del tamaño de la fórmula, generalizando así la aceleración de raíz cuadrada de árboles NAND balanceados a fórmulas arbitrarias con estructura desequilibrada o irregular.",
    "solution": "D"
  },
  {
    "id": 55,
    "question": "¿Qué técnica de ataque específica puede explotar los procedimientos de inicialización de los procesadores cuánticos?",
    "A": "La disrupción del equilibrio térmico ocurre cuando un adversario manipula el entorno criogénico del procesador cuántico para evitar que los qubits se relajen completamente a su estado fundamental durante la fase de inicialización. Al elevar sutilmente la temperatura efectiva del refrigerador de dilución o introducir calentamiento localizado mediante pulsos de microondas dirigidos, el atacante puede asegurar que los qubits retengan poblaciones de excitación residuales que se desvían del estado |0⟩ previsto, introduciendo así un sesgo sistemático en la computación que se acumula coherentemente a través del algoritmo.",
    "B": "Interferencia de pulso de reinicio — un adversario inyecta señales cuidadosamente temporizadas durante el protocolo de reinicio para sesgar el estado inicializado lejos de |0⟩, lo cual luego se propaga a través de la computación",
    "C": "La perturbación del estado fundamental implica explotar la constante de tiempo de relajación finita (T₁) de los qubits superconductores introduciendo interferencia controlada inmediatamente después de una operación nominal de reinicio. Un atacante que tiene conocimiento del programa de pulsos del procesador puede inyectar señales fuera de fase que re-excitan parcialmente el qubit después de que ha comenzado a relajarse, creando un desplazamiento determinista en la matriz de densidad del estado inicial. Este desplazamiento persiste durante toda la ejecución del circuito porque los errores de inicialización no son corregidos por técnicas estándar de mitigación de errores a nivel de puerta.",
    "D": "El monitoreo de excitación residual aprovecha el hecho de que los qubits en un refrigerador de dilución son continuamente monitoreados por resonadores de lectura, y un adversario con acceso a la cadena de lectura puede inyectar fotones espurios en estos resonadores durante la ventana de inicialización. Estos fotones inducen desplazamientos AC Stark que alteran dinámicamente la frecuencia de transición del qubit, causando que el pulso de reinicio esté desajustado respecto a la frecuencia real del qubit y dejando el qubit en un estado mixto en lugar del estado puro |0⟩, que luego sirve como entrada corrupta al algoritmo cuántico.",
    "solution": "B"
  },
  {
    "id": 56,
    "question": "¿Qué característica hace atractiva la familia de puertas FSim(θ,φ) para los procesadores superconductores estilo Google?",
    "A": "El ajuste continuo tanto del ángulo iSWAP como de la fase condicional permite la compilación eficiente de variantes de CZ, iSWAP y SWAP sin necesidad de recalibración extensa entre diferentes operaciones de puerta.",
    "B": "Las puertas FSim implementan naturalmente la interacción de Mølmer-Sørensen mediante acoplamiento de flujo modulado entre transmons, produciendo una operación entrelazadora cuya fidelidad mejora con mayor duración del pulso debido al promediado motional del ruido de flujo—a diferencia de las puertas resonantes donde pulsos más largos acumulan más desfase—permitiendo compromisos ajustables entre velocidad de puerta y tasas de error limitadas por coherencia en todo el espacio de parámetros θ-φ.",
    "C": "La familia FSim abarca toda la cámara de Weyl de dos qubits con solo la amplitud y duración del pulso de flujo como parámetros de control, eliminando la necesidad de excitaciones de microondas durante las operaciones de entrelazamiento y evitando así la diafonía por colisiones de frecuencia entre tonos de excitación y qubits espectadores, lo cual en arreglos densos de transmons con anarmonicidad <500 MHz puede inducir transiciones espurias que corrompen qubits vecinos no involucrados en la puerta.",
    "D": "La parametrización FSim corresponde directamente a la evolución hamiltoniana nativa bajo acoplamiento de intercambio ajustable, requiriendo solo conformación de pulsos de flujo adiabáticos en lugar de modulación precisa de amplitud y fase de excitaciones de microondas, lo cual reduce la sensibilidad a la atenuación de líneas de control y coeficientes de reflexión que varían con fluctuaciones de temperatura en el refrigerador de dilución, logrando una estabilidad de calibración día a día 2-3 veces mejor comparada con esquemas de puertas activadas por microondas.",
    "solution": "A"
  },
  {
    "id": 57,
    "question": "¿Cuál es el papel principal de las mediciones en un algoritmo cuántico variacional?",
    "A": "La generación de números aleatorios para optimización estocástica aprovecha la naturaleza probabilística inherente de las mediciones cuánticas para producir entropía de alta calidad para optimizadores clásicos como SPSA o Adam que requieren estimaciones de gradiente estocástico.",
    "B": "Colapsar superposiciones en estados clásicos sirve como la función de medición primaria porque los algoritmos variacionales operan fundamentalmente preparando estados de superposición parametrizados en la base computacional y luego extrayendo información al forzar cada qubit a un resultado definido |0⟩ o |1⟩. El colapso inducido por la medición transforma la distribución de probabilidad cuántica codificada en amplitudes en una cadena de bits clásica que el optimizador procesa. Sin este mecanismo de colapso, el estado cuántico permanecería inaccesible para los sistemas de control clásicos, imposibilitando evaluar el desempeño del circuito o actualizar parámetros variacionales basándose en resultados computacionales.",
    "C": "Estimar valores esperados para funciones de costo mediante la preparación repetida del estado cuántico parametrizado y la medición de observables en bases designadas, usando luego la distribución estadística de los resultados de medición para aproximar la energía o función objetivo que guía la optimización clásica de parámetros. El proceso de medición muestrea de la distribución de probabilidad codificada en el estado cuántico, proporcionando los datos empíricos necesarios para evaluar información de gradiente y actualizar parámetros variacionales hacia soluciones óptimas.",
    "D": "Implementar puertas mediante computación basada en mediciones se vuelve esencial en marcos variacionales porque medir qubits en bases designadas seguido de retroalimentación clásica de los resultados de medición puede ejecutar determinísticamente cualquier transformación unitaria.",
    "solution": "C"
  },
  {
    "id": 58,
    "question": "Considere una arquitectura de memoria cuántica que utiliza qubits de gato bosónicos, conocidos por sus propiedades de ruido que preservan el sesgo donde los errores de bit-flip están suprimidos exponencialmente mientras que los errores de phase-flip ocurren a tasas comparables con otros esquemas de codificación. En implementaciones prácticas de tales sistemas, los experimentalistas frecuentemente superponen corrección de errores adicional sobre la codificación de qubits de gato. ¿Cómo complementa el código de repetición a los qubits de gato bosónicos en la corrección de errores?",
    "A": "Implementa votación por mayoría entre estados de gato espacialmente separados para detectar errores de fase mediante mediciones de paridad.",
    "B": "Extiende la ingeniería de disipación de dos fotones a configuraciones multimodo que estabilizan colectivamente ambas cuadraturas de error.",
    "C": "Corrige errores residuales de phase-flip a los que los qubits de gato son susceptibles",
    "D": "Aprovecha la estabilización sin medición mediante retroalimentación autónoma que explota la estructura de ruido sesgada.",
    "solution": "C"
  },
  {
    "id": 59,
    "question": "¿Qué técnica sofisticada proporciona la garantía de seguridad más fuerte para la generación de números aleatorios cuánticos?",
    "A": "Los QRNGs de auto-verificación proporcionan las garantías más fuertes mediante el uso de secuencias de medición cuidadosamente diseñadas que permiten al dispositivo verificar su propio comportamiento cuántico solo mediante correlaciones, sin requerir confianza en la etapa de preparación. Estos protocolos logran seguridad comparable a esquemas independientes del dispositivo pero con complejidad experimental significativamente reducida, haciéndolos prácticos para despliegue mientras mantienen certificación de aleatoriedad demostrable incluso contra adversarios sofisticados que podrían controlar la fuente.",
    "B": "La estimación de entropía con información lateral cuántica trata el dispositivo como una caja negra pero aún requiere cierta caracterización cuántica para acotar la min-entropía disponible para extracción.",
    "C": "Los protocolos de expansión de aleatoriedad cuántica independientes del dispositivo logran las garantías de seguridad más fuertes al certificar aleatoriedad mediante violaciones de desigualdades de Bell sin confiar en el funcionamiento interno de los dispositivos cuánticos.",
    "D": "Los enfoques de variable continua ofrecen seguridad fuerte porque operan en espacios de Hilbert de dimensión infinita, haciéndolos fundamentalmente más resistentes a ataques de canal lateral que los sistemas de variable discreta.",
    "solution": "C"
  },
  {
    "id": 60,
    "question": "¿Cómo se calcula la tasa de entrelazamiento esperada para una ruta dada?",
    "A": "La tasa esperada se calcula identificando la tasa de generación de enlace mínima a lo largo de la ruta (el segmento cuello de botella) y multiplicando por el producto de probabilidades de éxito de intercambio en cada nodo intermedio, bajo el supuesto de que los intercambios se intentan secuencialmente en lugar de simultáneamente, lo cual introduce una correlación temporal que reduce el rendimiento efectivo comparado con protocolos de intercambio paralelo.",
    "B": "La tasa de entrelazamiento se determina por la media armónica de las fidelidades de enlaces individuales ponderadas por sus respectivos tiempos de coherencia, ya que enlaces de menor fidelidad contribuyen desproporcionadamente a la acumulación de error extremo-a-extremo. Este cálculo considera el hecho de que los intercambios de entrelazamiento amplifican errores de fase cuadráticamente en cada nodo repetidor, haciendo que la tasa de decoherencia del enlace más débil sea el factor dominante que limita la frecuencia de distribución general.",
    "C": "Multiplicando tasas de generación de enlaces y probabilidades de éxito de intercambio a lo largo de la ruta: la tasa de entrelazamiento extremo-a-extremo esperada es el producto de la tasa de generación de pares de Bell de cada segmento y la probabilidad de intercambios de entrelazamiento exitosos en nodos repetidores intermedios.",
    "D": "La tasa equivale a la suma de tiempos de generación inversos para cada enlace más la suma de duraciones de operaciones de intercambio en nodos repetidores, análogo a resistencias en serie en circuitos eléctricos. Dado que la generación de entrelazamiento y el intercambio son procesos secuenciales que deben completarse antes de que comience el siguiente intento, el tiempo total por par distribuido exitosamente se acumula linealmente, haciendo que el recíproco de esta suma sea la tasa extremo-a-extremo efectiva.",
    "solution": "C"
  },
  {
    "id": 61,
    "question": "¿Para qué se utiliza el aprendizaje por transferencia cuántico?",
    "A": "Eliminar la necesidad de datos etiquetados utilizando la superposición cuántica para explorar todas las posibles representaciones de características simultáneamente, permitiendo la identificación automática de características discriminativas óptimas sin etiquetas de entrenamiento explícitas mediante la evaluación paralela de exponencialmente muchas funciones de extracción de características durante el preentrenamiento que descubre la estructura intrínseca de la variedad de datos, tras lo cual las tareas posteriores pueden aprenderse con cero ejemplos etiquetados porque la medición cuántica proyecta naturalmente los datos no etiquetados sobre las fronteras de decisión implícitas en la geometría de características descubierta.",
    "B": "Convertir conjuntos de datos clásicos en representaciones puramente cuánticas sin ingeniería manual de características al incorporar automáticamente vectores de datos clásicos en el espacio de Hilbert mediante codificación de amplitud óptima aprendida a través de optimización variacional, donde este proceso identifica el espacio de estados cuánticos de dimensión mínima necesario para capturar el contenido de información del conjunto de datos clásico con tasas de compresión exponenciales.",
    "C": "Mejorar la eficiencia de aprendizaje y la generalización entre tareas cuánticas aprovechando extractores de características cuánticas preentrenados o circuitos parametrizados en tareas fuente, y luego ajustándolos para tareas objetivo con datos de entrenamiento limitados.",
    "D": "Garantizar la completa independencia estadística en modelos de aprendizaje automático cuántico que nunca requieren entrenamiento previo al explotar el teorema de no clonación para asegurar que cada instancia del modelo aprenda desde cero sin heredar sesgos de ejecuciones previas, evitando efectos de transferencia negativa que afectan a los enfoques clásicos.",
    "solution": "C"
  },
  {
    "id": 62,
    "question": "La latencia del procesamiento clásico en tiempo real se vuelve crítica para los decodificadores porque demoras más largas que qué escala temporal pueden negar los beneficios de la corrección de errores. Esto es particularmente importante en códigos de superficie donde la extracción de síndromes debe ocurrir repetidamente, y cualquier cuello de botella de procesamiento puede permitir que los errores se propaguen más rápido de lo que pueden corregirse, socavando fundamentalmente el umbral de tolerancia a fallos.",
    "A": "El tiempo de ciclo de extracción de síndrome entre mediciones sucesivas de estabilizadores en el código de corrección de errores cuánticos. Si la decodificación clásica y la retroalimentación tardan más que el tiempo entre rondas de síndromes, el decodificador se queda atrás de la operación en tiempo real y no puede proporcionar señales de corrección antes de que llegue el siguiente síndrome, creando un retraso que permite que los errores se propaguen sin control a través del qubit lógico más rápido de lo que pueden identificarse, derrotando completamente la capacidad protectora del protocolo de corrección de errores.",
    "B": "El tiempo de coherencia T2 de los qubits de datos entre rondas sucesivas de extracción de síndromes. Si la decodificación clásica y la retroalimentación tardan más que T2, los errores se acumulan y propagan a través del qubit lógico más rápido de lo que el protocolo de corrección de errores puede identificarlos y corregirlos, derrotando completamente el propósito de la corrección de errores cuánticos. El decodificador debe operar dentro de esta ventana para mantener la capacidad protectora del código y permanecer por encima del umbral de tolerancia a fallos.",
    "C": "El tiempo de desfase T2* de los qubits ancilla utilizados para la medición del síndrome entre rondas sucesivas de extracción. Si la decodificación clásica y la retroalimentación tardan más que T2*, los errores de fase se acumulan en las ancillas durante la latencia de decodificación y se propagan de vuelta a los qubits de datos a través de las puertas de entrelazamiento en ciclos de síndromes posteriores, corrompiendo la fidelidad de lectura del síndrome más rápido de lo que el protocolo de corrección de errores puede compensar, derrotando completamente la capacidad protectora del mecanismo de tolerancia a fallos.",
    "D": "La escala temporal de termalización que gobierna qué tan rápido los modos de fonones en el sustrato disipan la energía depositada por pulsos de control de vuelta a la etapa de temperatura base del refrigerador de dilución. Si la decodificación clásica y la retroalimentación tardan más que este tiempo de relajación térmica, el calentamiento residual de ciclos de síndromes previos crea ruido de desfase en los qubits de datos que se acumula más rápido de lo que el protocolo de corrección de errores puede rastrear, derrotando completamente la capacidad protectora de la memoria cuántica y llevando al sistema por encima del umbral de tolerancia a fallos.",
    "solution": "B"
  },
  {
    "id": 63,
    "question": "¿Qué es el teorema cuántico de flujo máximo-corte mínimo?",
    "A": "Un principio de optimización de redes cuánticas que establece que el flujo máximo de entrelazamiento entre dos nodos es igual al corte de capacidad cuántica mínimo que los separa, pero donde 'capacidad cuántica' se refiere a la información coherente (la diferencia entre la información mutua cuántica y la capacidad clásica) en lugar de medidas de entrelazamiento, aplicable a canales cuánticos ruidosos gobernados por la desigualdad de procesamiento de datos cuánticos y utilizado en protocolos de codificación de redes cuánticas.",
    "B": "Una generalización cuántica del teorema clásico de flujo máximo-corte mínimo, estableciendo una dualidad entre la capacidad máxima de canal cuántico para transmitir información cuántica o entrelazamiento a través de una red y el corte mínimo de entrelazamiento o capacidad que desconecta los nodos fuente de los objetivo, aplicable al análisis de redes de comunicación cuántica y protocolos de distribución de entrelazamiento.",
    "C": "Una dualidad estructural entre la fidelidad máxima alcanzable para la transmisión de estados a través de una red cuántica y la discordia cuántica mínima a través de un corte que particiona la fuente del objetivo, donde la discordia cuántica (en lugar del entrelazamiento) sirve como recurso cuello de botella porque captura todas las correlaciones cuánticas incluyendo aquellas no accesibles mediante operaciones locales y comunicación clásica, convirtiéndola en la medida correcta para protocolos generales de enrutamiento de estados mixtos en redes cuánticas ruidosas realistas.",
    "D": "El principio de que la tasa máxima de distribución de pares EPR a través de una red cuántica es igual al rango de Schmidt mínimo a través de cualquier corte que separe la fuente del sumidero, donde el rango de Schmidt (el número de coeficientes de Schmidt no nulos en la descomposición bipartita) determina la dimensionalidad del canal de entrelazamiento, y los cortes se evalúan basándose en la estructura del producto tensorial en lugar de medidas de capacidad aditivas, proporcionando una caracterización dimensional-teórica en lugar de informacional-teórica del flujo de red cuántica.",
    "solution": "B"
  },
  {
    "id": 64,
    "question": "¿Por qué las puertas Clifford por sí solas son insuficientes para lograr computación cuántica universal?",
    "A": "Mapean operadores de Pauli a operadores de Pauli bajo conjugación, permaneciendo dentro del formalismo de estabilizadores e incapaces de generar las fases continuas arbitrarias requeridas para la computación universal. Se necesitan puertas no Clifford como la puerta T o Toffoli para acceder a estados fuera del grupo de estabilizadores y alcanzar el espacio de transformación SU(2^n) completo.",
    "B": "Preservan la estructura de fase discreta de los estados estabilizadores pero fallan en generar estados con relaciones de fase irracionales entre amplitudes, que son requeridas por el teorema de Solovay-Kitaev para la aproximación universal de puertas. Aunque las puertas Clifford acceden a todos los estados estabilizadores—un subconjunto denso de la esfera de Bloch para qubits individuales—no pueden alcanzar estados no estabilizadores como |T⟩ = (|0⟩ + e^(iπ/4)|1⟩)/√2 que tienen factores de fase trascendentales necesarios para completar un conjunto de puertas universal sobre SU(2^n).",
    "C": "Forman un grupo finito bajo composición que solo puede generar un subgrupo discreto de SU(2^n), específicamente el grupo de Pauli generalizado normalizado por conjugación Clifford. Esto significa que los circuitos Clifford solo pueden alcanzar estados cuyas tablas de estabilizadores tienen entradas enteras módulo polinomios ciclotómicos específicos, excluyendo las rotaciones continuas requeridas por la universalidad. El teorema de Gottesman-Knill prueba esta restricción: cualquier circuito Clifford actuando sobre estados estabilizadores produce salidas cuyas amplitudes involucran solo raíces de la unidad de {±1, ±i}, nunca las fases complejas arbitrarias necesarias para la computación universal.",
    "D": "Generan solo permutaciones y cambios de signo de cadenas de Pauli al actuar sobre generadores de estabilizadores, lo que significa que no pueden implementar puertas que roten continuamente el vector de Bloch por múltiplos irracionales de π. Esta limitación surge porque las puertas Clifford corresponden a transformaciones simpléticas sobre GF(2), restringiéndolas a rotaciones discretas de 90 grados y cambios de base tipo Hadamard. Las puertas no Clifford como T o Toffoli introducen los ángulos irracionales necesarios (fases π/4) que escapan de la estructura de grupo Clifford finito y permiten cobertura densa de SU(2^n) mediante composición iterativa.",
    "solution": "A"
  },
  {
    "id": 65,
    "question": "En el algoritmo cuántico para el problema del subgrupo oculto, preparas una superposición uniforme sobre el grupo, aplicas el oráculo que depende de la estructura del subgrupo oculto, luego realizas una transformada de Fourier cuántica antes de medir el primer registro. El resultado de la medición tiene una interpretación algebraica específica que es central para el funcionamiento del algoritmo. ¿Qué estructura matemática revela esta medición?",
    "A": "Medir el primer registro después de la QFT produce un elemento uniformemente aleatorio de uno de los cosets que particionan el grupo según el subgrupo oculto H. Al repetir este procedimiento y recolectar múltiples representantes de cosets, puedes reconstruir la estructura del subgrupo mediante posprocesamiento clásico que identifica qué elementos siempre aparecen juntos en el mismo coset, triangulando efectivamente H a partir de sus traslaciones izquierdas o derechas.",
    "B": "La medición produce candidatos generadores para el subgrupo oculto al generar elementos cuyo orden divide la estructura del subgrupo, explotando la periodicidad en el patrón de cosets del oráculo.",
    "C": "Un elemento base para el dual del subgrupo oculto, específicamente un carácter irreducible que se anula en todos los cosets excepto el coset identidad. Recolectar múltiples de tales caracteres ortogonales mediante mediciones repetidas permite al posprocesamiento clásico reconstruir el espacio aniquilador, cuyo dual es precisamente el subgrupo oculto H. Esta perspectiva en el dominio de Fourier es la razón por la que el algoritmo funciona para grupos abelianos.",
    "D": "Cada medición produce un miembro muestreado uniformemente del subgrupo oculto H en sí, extraído de la distribución plana sobre todos los elementos que satisfacen la propiedad de cierre del subgrupo. La transformada de Fourier cuántica actúa como un operador de proyección que filtra elementos que no son del subgrupo, asegurando que solo aparezcan miembros válidos de H en las estadísticas de medición. Repetir este muestreo construye una imagen empírica de la membresía de H sin necesidad de comprender su estructura algebraica.",
    "solution": "C"
  },
  {
    "id": 66,
    "question": "En implementaciones prácticas de protocolos de compartición cuántica de secretos, ¿qué técnica avanzada proporciona la garantía de seguridad más sólida tanto contra trampas internas como contra escuchas externas? Considere escenarios donde los participantes puedan coludir o donde el ruido del canal pueda enmascarar comportamiento adversario. La técnica debe manejar tanto la reconstrucción de umbral como mantener la verificabilidad durante toda la ejecución del protocolo.",
    "A": "Los esquemas cuánticos de rampa con autenticación ofrecen niveles de seguridad graduales donde la fuga de información disminuye continuamente a medida que se combinan más acciones. Estos protocolos incorporan etiquetas de autenticación cuántica que permiten a los participantes verificar la integridad de acciones individuales, detectando algunas formas de trampa. Sin embargo, ataques de colusión sofisticados pueden explotar esto coordinándose para enviar acciones autenticadas pero colectivamente inconsistentes.",
    "B": "Los esquemas cuánticos de umbral con corrección de errores proporcionan seguridad robusta al distribuir acciones entre múltiples partes y aplicar códigos correctores de errores cuánticos para proteger contra el ruido del canal. Si bien estos protocolos sobresalen en mantener la integridad de los datos, típicamente asumen que los participantes siguen el protocolo honestamente durante la reconstrucción, y los mecanismos de corrección de errores pueden en realidad enmascarar comportamiento tramposo al tratar modificaciones maliciosas como indistinguibles del ruido legítimo.",
    "C": "La compartición cuántica de secretos homomórfica permite realizar cálculos sobre acciones cifradas sin requerir descifrado, permitiendo a los participantes realizar operaciones directamente sobre sus estados cuánticos distribuidos mientras se mantiene la confidencialidad durante toda la fase de cálculo. Sin embargo, la propiedad homomórfica en sí misma no fortalece inherentemente la seguridad de la fase inicial de distribución de acciones ni proporciona mecanismos para verificar que los participantes estén ejecutando honestamente las operaciones prescritas.",
    "D": "Los protocolos de compartición cuántica de secretos verificables que combinan sistemas de prueba interactivos con códigos de autenticación cuántica aseguran tanto la corrección como la seguridad contra participantes maliciosos. Estos protocolos típicamente emplean rondas de verificación de entrelazamiento y esquemas de compromiso clásicos para detectar intentos de trampa mientras mantienen límites de seguridad teórico-informacionales incluso cuando un subconjunto de participantes coludan con un escucha externo. La verificación interactiva permite a las partes honestas desafiar comportamiento sospechoso durante las fases de distribución y reconstrucción, proporcionando garantías de detección que permanecen válidas bajo condiciones de ruido realistas mientras se maneja el control de acceso basado en umbral.",
    "solution": "D"
  },
  {
    "id": 67,
    "question": "¿Cuál es el papel de las pruebas SWAP en el aprendizaje automático cuántico?",
    "A": "Detección de errores de hardware a través de mecanismos de paridad que intercambian ancillas con qubits de datos para verificar integridad y detectar errores de inversión de bits comparando resultados de medición.",
    "B": "Intercambiar información entre procesadores cuánticos y clásicos al intercambiar físicamente la representación del estado cuántico con su codificación de distribución de probabilidad clásica, permitiendo a los algoritmos híbridos transferir parámetros aprendidos bidireccionalmente a través de la interfaz cuántico-clásica durante cada iteración de entrenamiento.",
    "C": "Implementar operaciones de entrelazamiento intercambiando sistemáticamente posiciones de qubits para crear patrones de interferencia controlada, lo cual genera las correlaciones necesarias para construir estados entrelazados de múltiples qubits a partir de qubits inicialmente separables mediante una secuencia de puertas SWAP adyacentes.",
    "D": "Medir la similitud entre estados cuánticos realizando operaciones SWAP controladas seguidas de mediciones de interferencia, lo cual permite estimar probabilísticamente el producto interno entre dos estados cuánticos desconocidos mediante ensayos repetidos, proporcionando una aceleración cuadrática sobre métodos clásicos para ciertas tareas de comparación de estados.",
    "solution": "D"
  },
  {
    "id": 68,
    "question": "¿Por qué el problema del subgrupo oculto para grupos no abelianos requiere frecuentemente mediciones entrelazadas?",
    "A": "Las mediciones conjuntas revelan correlaciones entre representantes de coclases que están codificadas en múltiples registros en el estado cuántico transformado por Fourier, y estas correlaciones solo se vuelven accesibles a través de bases de medición entrelazadas que acoplan los registros entre sí.",
    "B": "Las representaciones irreducibles de grupos no abelianos descomponen la salida de la transformada cuántica de Fourier en amplitudes con valores matriciales distribuidas en subespacios de registros, y las mediciones separables de un solo registro proyectan sobre índices de fila o columna independientemente, destruyendo las coherencias fuera de la diagonal que codifican información de pertenencia al subgrupo. Las mediciones entrelazadas acoplan estos índices conjuntamente, extrayendo las correlaciones de elementos matriciales que distinguen diferentes coclases dentro del mismo espacio de representación irreducible.",
    "C": "La transformada cuántica de Fourier sobre grupos no abelianos produce superposiciones donde las fases relativas entre estados de base computacional codifican la estructura de clase de conjugación en lugar de elementos de grupo individuales, y las mediciones separables colapsan estas fases independientemente entre registros sin preservar sus relaciones mutuas. Las bases de medición entrelazadas se alinean con la descomposición de clase de conjugación al proyectar sobre autoestados conjuntos de operadores de clase, extrayendo así las correlaciones de fase entre registros que revelan qué clases de conjugación pertenecen al subgrupo oculto versus el espacio cociente.",
    "D": "Los representantes de coclases en instancias de subgrupo oculto no abeliano aparecen como productos tensoriales de elementos de grupo distribuidos en múltiples registros cuánticos, y la propiedad de clausura del subgrupo se manifiesta como entrelazamiento entre estos registros después de aplicar la transformada cuántica de Fourier. Las mediciones separables sobre registros individuales marginalizan sobre estas correlaciones, produciendo distribuciones uniformes que no contienen información del subgrupo, mientras que las mediciones entrelazadas de base de Bell preservan la estructura multiplicativa del subgrupo al revelar qué pares de registros contienen elementos de grupo que satisfacen la relación de clausura g₁g₂ ∈ H para el subgrupo oculto H.",
    "solution": "A"
  },
  {
    "id": 69,
    "question": "¿Qué tecnología aborda mejor el cuello de botella de post-procesamiento en sistemas de distribución cuántica de claves de alta velocidad?",
    "A": "Los clústeres de computación distribuida con interfaces de paso de mensajes escalan la carga de trabajo de post-procesamiento a través de múltiples nodos, distribuyendo los cálculos de corrección de errores usando bibliotecas MPI para manejar el espacio de claves que crece exponencialmente. Al particionar la clave tamizada en segmentos y asignar cada uno a un nodo de cálculo dedicado, este enfoque logra teóricamente una aceleración lineal proporcional al tamaño del clúster, aunque la latencia entre nodos a menudo se convierte en el factor limitante en la práctica.",
    "B": "La aceleración por GPU proporciona un rendimiento superior para el post-procesamiento de QKD al aprovechar miles de núcleos CUDA para paralelizar las etapas de reconciliación de información y amplificación de privacidad. Las capacidades masivas de cálculo de punto flotante de las GPU modernas permiten la corrección de errores en tiempo real en flujos de claves sin procesar de múltiples Gbps, con frameworks como OpenCL permitiendo la implementación eficiente del protocolo en cascada a través de bloques de hilos.",
    "C": "Los procesadores ASIC ofrecen una eficiencia energética incomparable para el post-procesamiento de QKD al cablear directamente las multiplicaciones de matriz de Toeplitz utilizadas en hash universal en puertas de silicio, logrando una latencia determinista de menos de 10 nanosegundos por bloque. Sin embargo, los largos ciclos de desarrollo y los altos costos NRE hacen que los ASIC sean imprácticos para adaptarse a protocolos de reconciliación en evolución o para soportar múltiples estándares de QKD simultáneamente.",
    "D": "La implementación en FPGA proporciona aceleración de hardware especializada para el post-procesamiento de QKD a través de circuitos lógicos dedicados optimizados para operaciones a nivel de bit en corrección de errores y amplificación de privacidad. El tejido reconfigurable permite el procesamiento paralelo de múltiples bloques de clave simultáneamente mientras mantiene baja latencia, con los FPGA modernos logrando rendimiento de múltiples Gbps a través de arquitecturas en pipeline que ejecutan protocolos de reconciliación en tiempo real sin sobrecarga de software.",
    "solution": "D"
  },
  {
    "id": 70,
    "question": "Aproximar el polinomio de Jones de un enlace en la mayoría de las raíces de la unidad es BQP-completo porque la evaluación puede mapearse a ¿qué tipo de circuito cuántico?",
    "A": "Circuitos cuánticos topológicos que implementan las estadísticas de trenzado de anyones no abelianos — específicamente, el circuito usa modelos de anyones de Fibonacci donde el polinomio de Jones en e^(2πi/5) puede calcularse mediante operaciones de trenzado que corresponden naturalmente a puertas cuánticas. Cada cruce en el diagrama de enlace se mapea a una operación de trenzado de anyones que actúa como una puerta unitaria, y la operación de traza se convierte en una medición de la carga topológica. Sin embargo, esta construcción requiere codificar cada anyón en múltiples qubits usando códigos correctores de errores cuánticos que simulan la protección topológica, haciendo que la sobrecarga sea sustancial pero aún polinomial.",
    "B": "Simula la palabra de trenza usando puertas de fase controladas — específicamente, el circuito implementa la representación del álgebra de Temperley-Lieb usando rotaciones de un solo qubit y puertas de fase de dos qubits dispuestas para reflejar los generadores del grupo de trenzas. Cada cruce en el diagrama de enlace corresponde a una puerta unitaria que actúa sobre qubits adyacentes, y la operación de traza necesaria para la evaluación del polinomio se implementa midiendo el estado cuántico final. La conexión entre estos circuitos cuánticos y la evaluación del polinomio de Jones en raíces de la unidad proporciona una reducción directa que prueba la BQP-completitud.",
    "C": "Conjuntos de puertas universales que contienen puertas Hadamard y Toffoli dispuestas para codificar la representación del grupo de trenzas mediante la matriz de Burau evaluada en la raíz de la unidad apropiada. La profundidad del circuito escala linealmente con el número de cruces, y cada generador de trenza σᵢ se convierte en una composición de puertas Hadamard en los qubits i e i+1 seguida de una puerta Toffoli controlada en ambos qubits. La operación de traza se reduce a medir todos los qubits y post-procesar la cadena de bits clásica, aunque este enfoque solo funciona para enlaces alternantes donde la representación de Burau permanece fiel.",
    "D": "Circuitos IQP (tiempo polinomial cuántico instantáneo) que consisten en puertas diagonales en la base de Hadamard, donde cada cruce en el diagrama de enlace se convierte en una puerta diagonal de rotación ZZ de dos qubits con ángulo determinado por la raíz de la unidad. La construcción funciona inicializando todos los qubits en estados |+⟩, aplicando puertas diagonales conmutativas correspondientes a la palabra de trenza, luego midiendo en la base de Hadamard. El valor del polinomio de Jones en e^(2πi/k) emerge de las estadísticas de medición, específicamente del cálculo del permanente de una matriz cuyos elementos se derivan de los resultados de medición, aunque esto requiere post-procesamiento con cálculo clásico #P-difícil que paradójicamente hace que el algoritmo general sea ineficiente a pesar de que el circuito cuántico sea eficientemente implementable.",
    "solution": "B"
  },
  {
    "id": 71,
    "question": "En el contexto de la ejecución distribuida de circuitos cuánticos, ¿cuál es el propósito de distinguir entre los planos de planificación y de red?",
    "A": "Separa las operaciones lógicas del circuito del enrutamiento físico de entrelazamiento de qubits, permitiendo que el planificador optimice la temporización de ejecución de puertas y la asignación de recursos basándose en las dependencias del circuito mientras que el plano de red maneja independientemente la generación, distribución y consumo de enlaces de entrelazamiento remoto. Este desacoplamiento permite que cada plano opere de forma asíncrona con protocolos especializados—el planificador puede reordenar puertas conmutativas para reducir la profundidad mientras que la capa de red gestiona la generación de pares EPR y la fidelidad sin requerir sincronización a nivel de puertas.",
    "B": "Esta distinción permite que el plano de planificación trate los enlaces de entrelazamiento remoto como recursos abstractos con costos asociados de latencia y fidelidad, permitiendo que los algoritmos de compilación de circuitos optimicen el ordenamiento de puertas basándose en grafos de dependencias sin requerir conocimiento en tiempo real de la congestión de red o las rutas de enrutamiento fotónico. El plano de red entonces gestiona independientemente la generación de entrelazamiento usando protocolos como interferencia de fotones anunciada o acoplamiento ion-fotón determinista, almacenando pares EPR en memorias cuánticas hasta que el planificador los consuma. Al desacoplar estas funciones, el sistema puede canalizar operaciones de puertas remotas: mientras el planificador ejecuta puertas locales en un módulo, el plano de red pre-genera enlaces de entrelazamiento necesarios para futuras operaciones remotas.",
    "C": "Separar los planos de planificación y de red permite que la corrección de errores cuánticos opere a nivel de qubit lógico independientemente de la gestión de recursos de entrelazamiento físico, asegurando que los circuitos de extracción de síndromes se ejecuten en horarios deterministas sin importar las fallas transitorias de enlaces de red o los retrasos en la generación de entrelazamiento. El plano de red regenera continuamente pares EPR degradados usando protocolos de purificación de entrelazamiento, mientras que el planificador trata los qubits lógicos como siempre disponibles con fidelidad uniforme, confiando en que la capa de red mantenga un buffer suficiente de pares de Bell de alta fidelidad para cumplir con la cadencia de medición de síndromes requerida por el surface code u otro esquema de corrección de errores topológico.",
    "D": "Esta separación arquitectónica soporta la ejecución paralela de operaciones remotas no conmutativas a través de módulos distribuidos permitiendo que el plano de planificación rastree qué qubits comparten recursos de entrelazamiento mientras el plano de red gestiona los pares de Bell físicos independientemente. Cuando dos puertas remotas se dirigen a subconjuntos de qubits superpuestos pero operan en diferentes bases de medición (por ejemplo, verificaciones simultáneas de estabilizadores X y Z en pares EPR compartidos), el planificador puede emitir ambas operaciones concurrentemente si el plano de red ha provisto enlaces de entrelazamiento separados para cada puerta. Este paralelismo es crítico porque sin separar la asignación de entrelazamiento (plano de red) del análisis de dependencias a nivel de puertas (plano de planificación), las reclamaciones de recursos en conflicto forzarían un ordenamiento secuencial innecesario de puertas conmutativas.",
    "solution": "A"
  },
  {
    "id": 72,
    "question": "¿Cuál es la ventaja principal de la amplificación de amplitud cuántica en aplicaciones de aprendizaje automático? Considere que muchos algoritmos de aprendizaje automático cuántico dependen de alguna forma de optimización o búsqueda, y la eficiencia de estas subrutinas impacta directamente el rendimiento general y la escalabilidad del enfoque cuántico comparado con los métodos clásicos.",
    "A": "Aceleración cuadrática en la búsqueda de espacios de soluciones no estructurados, reduciendo las consultas al oráculo de O(N) a O(√N), pero la ventaja práctica en aprendizaje automático depende críticamente de mantener la coherencia a lo largo de las iteraciones de amplificación. Cada aplicación del operador de Grover acumula errores de puertas, y análisis recientes muestran que en dispositivos NISQ con ~0.1% de error en puertas de dos qubits, el punto de cruce donde la búsqueda cuántica supera el muestreo aleatorio clásico ocurre solo para tamaños de problema N > 10⁸. Por debajo de este umbral, los errores acumulados durante las √N iteraciones compensan la reducción de consultas, haciendo que la amplificación de amplitud sea menos efectiva de lo que se afirma para aplicaciones de aprendizaje automático a corto plazo.",
    "B": "Reducción cuadrática en el número de épocas de entrenamiento requeridas para la convergencia en redes neuronales cuánticas, disminuyendo el conteo de iteraciones de O(N) clásicamente a O(√N) cuánticamente al buscar configuraciones óptimas de parámetros. Esta aceleración se aplica específicamente al bucle de optimización externo en lugar de evaluaciones de gradientes individuales, porque la amplificación de amplitud puede buscar eficientemente el espacio discreto de posibles direcciones de actualización de parámetros. La técnica es especialmente valiosa cuando el paisaje de pérdida contiene muchos mínimos locales, ya que el proceso de amplificación mejora preferencialmente las amplitudes correspondientes a actualizaciones de parámetros que reducen la función de pérdida, implementando efectivamente un protocolo de descenso de gradiente mejorado cuánticamente.",
    "C": "Aceleración cuadrática para identificar características o patrones de datos óptimos reduciendo la complejidad muestral de O(N) a O(√N) al buscar sobre espacios de características exponencialmente grandes. Esta ventaja es particularmente significativa en métodos de kernel cuántico y máquinas de vectores de soporte cuánticas, donde la amplificación de amplitud acelera la búsqueda de vectores de soporte identificando eficientemente ejemplos de entrenamiento cerca de la frontera de decisión. La aceleración se aplica incluso cuando el espacio de características tiene estructura, porque el proceso de amplificación enfoca adaptativamente la amplitud de probabilidad en regiones del espacio de Hilbert correspondientes a separación de margen máximo, haciéndolo más poderoso que las técnicas de optimización convexa clásicas que escalan linealmente con el tamaño del conjunto de datos.",
    "D": "Aceleración cuadrática en la búsqueda de soluciones o estados particulares dentro de un espacio de búsqueda no estructurado, reduciendo el número de consultas al oráculo de O(N) clásicamente a O(√N) cuánticamente. Esta mejora es especialmente valiosa en la optimización de aprendizaje automático donde encontrar buenas configuraciones de parámetros o identificar características relevantes requiere evaluar repetidamente funciones objetivo costosas.",
    "solution": "D"
  },
  {
    "id": 73,
    "question": "¿Cuál es el propósito de las técnicas de circuit knitting cuántico?",
    "A": "Particionan unitarios grandes en productos tensoriales de bloques de subcircuitos más pequeños explotando la factorización aproximada de la descomposición de Schmidt de la operación objetivo, ejecutando cada factor independientemente en dispositivos separados y recombinando salidas mediante postprocesamiento clásico de correlaciones de medición ponderadas por coeficientes de Schmidt, habilitando la ejecución distribuida sin entrelazamiento entre subsistemas durante la fase de ejecución cuántica.",
    "B": "Descomponen circuitos que exceden los límites de qubits en fragmentos superpuestos ejecutados secuencialmente con reinicios en medio del circuito, usando transferencia de estado mediada por ancillas para propagar estados cuánticos parciales entre fragmentos mediante protocolos de unión basados en teletransportación, reconstruyendo la computación completa a través de mediciones condicionales iteradas que preservan la coherencia a través de los límites de fragmentos mientras evitan la sobrecarga clásica exponencial en el procesamiento de resultados de medición.",
    "C": "Particionan computaciones que exceden la capacidad del dispositivo en subcircuitos más pequeños ejecutados en hardware disponible, luego reconstruyen clásicamente el resultado completo combinando estadísticas de medición de estos fragmentos usando descomposiciones de cuasi-probabilidad, simulando efectivamente sistemas cuánticos más grandes que los físicamente accesibles.",
    "D": "Expresan circuitos cuánticos grandes como combinaciones lineales de fragmentos ejecutables más pequeños descomponiendo puertas de muchos qubits en sumas de productos tensoriales implementables en subconjuntos de qubits disjuntos, luego muestreando de la distribución de cuasi-probabilidad resultante sobre resultados de fragmentos para reconstruir valores esperados, con sobrecarga de muestreo escalando exponencialmente en la negatividad de la representación de cuasi-probabilidad que surge de descomposiciones de puertas no locales.",
    "solution": "C"
  },
  {
    "id": 74,
    "question": "¿Cuál es un desafío clave en la síntesis de circuitos eficientes para la simulación hamiltoniana?",
    "A": "Encontrar secuencias de puertas que aproximen con precisión e^{-iHt} mientras minimizan la profundidad del circuito y el conteo total de puertas, particularmente cuando el hamiltoniano contiene términos no conmutativos que requieren técnicas sofisticadas de descomposición como las fórmulas de Trotter-Suzuki o métodos más avanzados como la combinación lineal de unitarios, todo mientras se equilibra el balance entre el error de aproximación y la sobrecarga de recursos",
    "B": "Equilibrar el tamaño del paso de Trotter Δt contra la no conmutatividad de los términos hamiltonianos: una discretización más fina reduce el error acumulado del conmutador ||[H_j,H_k]||Δt² pero aumenta la profundidad del circuito proporcionalmente como T/Δt, mientras que pasos más gruesos producen circuitos menos profundos pero amplifican errores sistemáticos de la expansión de Baker-Campbell-Hausdorff. El orden óptimo de descomposición depende de la estructura del álgebra de Lie del hamiltoniano—algunos sistemas requieren métodos de cuarto orden para lograr precisión aceptable, multiplicando los conteos de puertas por ~5×, mientras que otros permiten división de segundo orden con penalización de error mínima",
    "C": "Para hamiltonianos con interacciones de largo alcance H = Σᵢⱼ Jᵢⱼ σᵢσⱼ donde las fuerzas de acoplamiento decaen algebraicamente como Jᵢⱼ ~ |i-j|⁻ᵅ, implementar el grafo de interacción completo requiere O(n²) puertas SWAP para enrutar pares de qubits no locales a posiciones adyacentes para la aplicación de puertas de dos qubits. Cuando α<2, el grafo de interacción se vuelve no planar y no puede ser embebido eficientemente en topologías de hardware 2D típicas. Esto crea compromisos fundamentales entre profundidad y conectividad: o aceptar sobrecarga de profundidad O(n) de cadenas SWAP o aproximar los términos de largo alcance, introduciendo errores de truncamiento controlables que deben equilibrarse contra los costos de enrutamiento",
    "D": "Construir secuencias de puertas que preserven las simetrías hamiltonianas es esencial para mantener la precisión de la simulación, ya que los errores que rompen la simetría se acumulan coherentemente en lugar de estocásticamente. Al simular sistemas con simetrías continuas como la conservación de carga U(1) o rotación de espín SU(2), incluso pequeñas imperfecciones de puertas que violan estas simetrías—como fuga fuera del subespacio computacional o errores de calibración en ángulos de rotación—causan que el estado simulado se desvíe hacia sectores no físicos del espacio de Hilbert. El desafío se intensifica porque las herramientas de compilación estándar optimizan para el conteo de puertas sin consideración por la preservación de simetría, requiriendo algoritmos de descomposición personalizados que explícitamente impongan números cuánticos conservados a lo largo del circuito",
    "solution": "A"
  },
  {
    "id": 75,
    "question": "¿Qué significa que una matriz sea estable en solucionadores de ecuaciones diferenciales cuánticas?",
    "A": "El determinante de la matriz debe ser igual a uno a lo largo de la evolución, asegurando que la unitariedad se preserve en cada paso temporal y que la probabilidad permanezca conservada bajo propagación en tiempo continuo.",
    "B": "La traza de la matriz se anula idénticamente para todos los sistemas hamiltonianos independientes del tiempo, reflejando la conservación de energía y asegurando que el operador de evolución permanezca sin traza, lo cual es particularmente importante en sistemas cuánticos abiertos donde las dinámicas lindbladianas requieren que el componente disipativo tenga traza cero para la normalización adecuada de la evolución de la matriz de densidad.",
    "C": "Todos los valores propios tienen partes reales negativas, asegurando que los errores numéricos decaigan en lugar de crecer exponencialmente durante la evolución temporal, lo cual es esencial para la estabilidad a largo plazo en esquemas de integración de ecuaciones diferenciales.",
    "D": "Sus entradas deben permanecer acotadas por el tamaño del sistema, asegurando que la propagación numérica no produzca condiciones de desbordamiento incluso al evolucionar estados sobre intervalos de tiempo largos o al aplicar métodos de división de alto orden que involucran operadores intermedios no físicos con elementos de matriz potencialmente grandes.",
    "solution": "C"
  },
  {
    "id": 76,
    "question": "¿Cómo se pueden utilizar los desajustes de forma de onda en un ataque?",
    "A": "Al introducir desviaciones calibradas de amplitud o fase en las formas de onda de los pulsos de control que acumulan sistemáticamente errores coherentes a través de secuencias de puertas, un atacante puede sesgar los resultados del cálculo hacia distribuciones de medición específicas mientras mantiene las fidelidades de las puertas individuales dentro de rangos aceptables.",
    "B": "Un adversario puede modificar deliberadamente las formas de envolvente de los pulsos de control para desviarse de las formas de onda calibradas, induciendo así rotaciones de qubit no deseadas que difieren de las operaciones de puerta objetivo mientras permanecen lo suficientemente sutiles como para evadir la detección inmediata.",
    "C": "Las formas de onda desajustadas alteran la frecuencia de Rabi prevista durante la evolución del qubit conducido, causando errores de sobre-rotación o sub-rotación que se acumulan multiplicativamente a través de las capas del circuito, permitiendo a un adversario diseñar sesgos computacionales específicos que escapan a la detección por protocolos de benchmarking aleatorizado.",
    "D": "Los desajustes de forma de onda introducen errores de fase determinísticos que se propagan a través de puertas de entrelazamiento para crear sesgos controlados en las fidelidades de estados de Bell, permitiendo a un atacante degradar selectivamente rutas computacionales específicas mientras mantiene las métricas promedio de rendimiento de puertas dentro de las tolerancias de calibración.",
    "solution": "B"
  },
  {
    "id": 77,
    "question": "¿Qué riesgo de seguridad específico surge de la deriva de calibración en procesadores cuánticos?",
    "A": "El desplazamiento del sesgo de medición introduce errores sistemáticos en la fidelidad de lectura que se acumulan asimétricamente con el tiempo, causando que el umbral de discriminación entre los estados |0⟩ y |1⟩ migre gradualmente hacia un estado base. Esta deriva temporal en el clasificador de lectura crea ventanas de vulnerabilidad donde un adversario puede predecir resultados de medición con precisión superior al azar al sincronizar sus ataques para que coincidan con períodos de sesgo máximo, rompiendo efectivamente la uniformidad asumida de las estadísticas de medición de la que dependen muchos protocolos de seguridad cuántica para sus garantías de seguridad.",
    "B": "La degradación de la fidelidad de las puertas con el tiempo crea vulnerabilidades explotables a medida que los parámetros de los pulsos de control se desalinean cada vez más con el hamiltoniano del sistema en evolución, aunque esto se manifiesta como ruido general en lugar de patrones estructurados.",
    "C": "La inestabilidad de frecuencia del qubit hace que los valores propios de energía de qubits individuales fluctúen debido al ruido de carga y al ruido de flujo en los circuitos superconductores, llevando a la dessintonización de las condiciones de resonancia requeridas para puertas cuánticas de alta fidelidad. A medida que las frecuencias de los qubits se alejan de sus valores calibrados, las formas de pulso cuidadosamente diseñadas que fueron optimizadas durante la rutina de calibración más reciente se desalinean progresivamente con el hamiltoniano real del sistema, reduciendo el rendimiento de las puertas y potencialmente creando ventanas de tiempo explotables donde un atacante puede predecir cuándo se maximizarán los errores de puerta.",
    "D": "Emergen patrones de error predecibles cuando la deriva de calibración causa desviaciones sistemáticas en las implementaciones de puertas que evolucionan determinísticamente entre ciclos de recalibración, permitiendo a los adversarios modelar las características de error dependientes del tiempo y explotar ventanas temporales donde operaciones específicas exhiben modos de fallo conocidos. Estos errores estructurados crean vulnerabilidades porque los atacantes pueden predecir cuándo y cómo las puertas se desviarán del comportamiento ideal, habilitando ataques dirigidos que aprovechan la correlación entre el tiempo transcurrido desde la calibración y los patrones de degradación del rendimiento de las puertas.",
    "solution": "D"
  },
  {
    "id": 78,
    "question": "¿Por qué la \"profundidad del estado cluster\" es igual a uno en los modelos basados en medición incluso para algoritmos complejos?",
    "A": "Todas las puertas CZ se preparan fuera de línea en el estado cluster inicial; el cálculo procede enteramente a través de mediciones adaptativas de un solo qubit cuyas bases dependen de resultados anteriores, por lo que la profundidad del circuito cuántico en el sentido convencional del modelo de puertas se colapsa al estado único de recurso entrelazado, mientras que la complejidad algorítmica se manifiesta en el control clásico de retroalimentación que determina los ángulos de medición en lugar de capas secuenciales de puertas.",
    "B": "Todas las operaciones de entrelazamiento se preparan fuera de línea en el estado cluster inicial; el cálculo procede enteramente a través de mediciones adaptativas de un solo qubit que implementan puertas virtuales mediante teletransportación, por lo que la profundidad del circuito cuántico en el sentido convencional se colapsa a la ronda de preparación del estado de recurso, mientras que la complejidad algorítmica se manifiesta en la retroalimentación clásica que determina las bases de medición. Sin embargo, la profundidad efectiva del circuito es igual a la cadena de dependencias de medición más larga, no la profundidad del estado cluster, que solo cuenta las capas de entrelazamiento necesarias para preparar el estado de grafo antes de que comiencen las mediciones.",
    "C": "Todas las puertas unitarias codificadas en la geometría inicial del estado cluster; el cálculo procede a través de mediciones de un solo qubit que proyectan el estado a lo largo de rutas computacionales predeterminadas por la estructura del grafo, por lo que la profundidad del circuito cuántico en el sentido convencional se colapsa a la preparación única del estado de recurso, mientras que la complejidad algorítmica se manifiesta en elegir qué qubits medir en lugar de ángulos de medición. La dimensión de enlace del estado cluster determina directamente el poder computacional: los algoritmos polinomiales requieren dimensión de enlace constante mientras que las aceleraciones exponenciales necesitan dimensión de enlace que escale con el tamaño del problema, pero la profundidad permanece unitaria porque el orden de medición no afecta los resultados finales.",
    "D": "Todas las correlaciones de dos qubits se establecen fuera de línea en el estado cluster inicial; el cálculo procede a través de mediciones adaptativas de un solo qubit cuyos resultados determinan las bases subsiguientes, por lo que la profundidad del circuito cuántico en el sentido del modelo de puertas se colapsa al estado de recurso entrelazado, mientras que la complejidad algorítmica se manifiesta en la topología del patrón de medición. La profundidad del estado cluster es igual a uno porque se define como el número cromático del grafo de dependencias de medición proyectado sobre la red de qubits físicos: aunque las capas de medición temporales se extienden a través de muchas rondas, cada rebanada espacial de mediciones simultáneas que conmutan cuenta como profundidad uno en el formalismo MBQC.",
    "solution": "A"
  },
  {
    "id": 79,
    "question": "¿Por qué la traducción directa de códigos clásicos de corrección de errores (ECC) a la computación cuántica no es trivial?",
    "A": "Mientras que los códigos clásicos dependen de la redundancia mediante duplicación de datos, el teorema de no-clonación prohíbe explícitamente copiar estados cuánticos arbitrarios, haciendo imposibles las estrategias ingenuas de replicación. Además, la corrección de errores clásica aborda errores discretos de un solo tipo (inversiones de bit), mientras que los sistemas cuánticos sufren de procesos de error continuos que se manifiestan como componentes de inversión de bit e inversión de fase simultáneamente, requiriendo protocolos fundamentalmente diferentes de medición de síndrome y corrección que preserven la superposición a lo largo del ciclo de corrección de errores.",
    "B": "El principio de incertidumbre de Heisenberg establece que cualquier intento de medir un estado cuántico con fines de duplicación perturbará inevitablemente el observable complementario, destruyendo así la información de fase codificada en el qubit. Esta restricción fundamental significa que los esquemas de redundancia clásicos, que dependen de crear copias idénticas para votación por mayoría, no pueden aplicarse directamente a la información cuántica porque el acto de copiar colapsaría la superposición, borrando las mismas propiedades cuánticas que el código pretende proteger.",
    "C": "A diferencia de los bits clásicos que representan valores binarios discretos, los ordenadores cuánticos procesan información como amplitudes de probabilidad continuas distribuidas sobre trayectorias de la esfera de Bloch, requiriendo mecanismos de corrección de errores analógicos en lugar de verificaciones de paridad digitales.",
    "D": "El teorema de no-clonación impide la duplicación arbitraria de estados cuánticos, eliminando las estrategias clásicas de redundancia, mientras que los errores cuánticos ocurren como procesos continuos que afectan simultáneamente los grados de libertad de inversión de bit e inversión de fase. Además, la detección de errores basada en medición debe preservar la superposición cuántica mediante extracción de síndrome en lugar de observar directamente los estados de qubit, distinguiendo fundamentalmente los códigos cuánticos de sus contrapartes clásicas que pueden medir y copiar bits de datos libremente.",
    "solution": "D"
  },
  {
    "id": 80,
    "question": "¿Por qué el crosstalk es particularmente desafiante para ordenadores cuánticos de gran escala?",
    "A": "A medida que el número de qubits aumenta linealmente, el crosstalk crece superlinealmente y eventualmente causa que todos los qubits del procesador se entrelacen mutuamente entre sí a través de acoplamientos hamiltonianos no deseados, creando un estado entrelazado global de muchos cuerpos que hace incontrolables las operaciones de puerta individuales. Este entrelazamiento de todos con todos emerge porque las intensidades de acoplamiento de crosstalk escalan con la densidad de qubits, produciendo una red exponencialmente compleja de interacciones parásitas que abruma cualquier intento de direccionamiento selectivo o control independiente de qubits computacionales individuales.",
    "B": "El crosstalk es puramente un problema de hardware que surge del acoplamiento electromagnético entre líneas de control y modos de resonador, y no puede mitigarse con técnicas de software como conformación de pulsos, desacoplamiento dinámico o calibración de puertas de resonancia cruzada.",
    "C": "Escalar a cientos o miles de qubits hace que el crosstalk sea completamente indetectable e inmedible ya que estas interacciones parásitas ocurren en patrones totalmente aleatorios que se promedian sobre grandes conjuntos, cancelándose efectivamente a sí mismas mediante simetría estadística. Esta propiedad de auto-promediado emerge naturalmente en sistemas más allá de aproximadamente 100 qubits, donde la ley de los grandes números asegura que los errores de fase inducidos por crosstalk contribuyen un efecto neto insignificante a las fidelidades agregadas de las puertas, permitiendo que los dispositivos a gran escala operen sin caracterización explícita del crosstalk.",
    "D": "Más qubits significa más interacciones no deseadas entre sistemas cuánticos vecinos, elevando las tasas acumulativas de error a medida que se acumulan los acoplamientos parásitos. A medida que aumenta el conteo de qubits, el número absoluto de rutas potenciales de crosstalk crece cuadráticamente, haciendo que la calibración y mitigación integral sean cada vez más difíciles y eventualmente impracticables sin cambios arquitectónicos como mejor aislamiento o topologías de conectividad dispersa.",
    "solution": "D"
  },
  {
    "id": 81,
    "question": "Considere un algoritmo cuántico variacional ejecutándose en hardware NISQ actual donde el circuito ha sido particionado en múltiples subcircuitos usando una técnica de corte clásica. El particionamiento inicial se basó en parámetros de ruido estimados a partir de datos de calibración del dispositivo tomados 6 horas antes de la ejecución. Durante la ejecución, el monitoreo en tiempo real revela que ciertos tiempos de coherencia de qubits se han degradado significativamente, haciendo que algunos subcircuitos sean menos favorables de lo planeado originalmente. ¿Por qué se utiliza a veces el re-corte dinámico durante la ejecución en escenarios como este?",
    "A": "El re-corte dinámico permite rebalancear la sobrecarga clásica al transferir subcircuitos computacionalmente intensivos a backends de simulación cuando la calidad del hardware cuántico cae por debajo de umbrales aceptables. Cuando los tiempos de coherencia se degradan inesperadamente, la estrategia de particionamiento migra subcircuitos problemáticos a simuladores clásicos de redes tensoriales que pueden mantener la fidelidad mediante evolución unitaria exacta. Este enfoque híbrido en tiempo de ejecución intercambia consumo de recursos cuánticos por sobrecarga computacional clásica, permitiendo que el algoritmo se complete exitosamente a pesar de la degradación temporal del hardware al explotar la precisión del simulador para subcircuitos de bajo entrelazamiento.",
    "B": "Adapta la descomposición del circuito para enrutar preferentemente puertas de entrelazamiento multi-qubit a través de pares de qubits que exhiben fidelidades superiores de puertas de dos qubits medidas en calibración en tiempo real. Cuando los tiempos de coherencia se degradan inesperadamente, la estrategia de particionamiento puede migrar subcircuitos con muchas CNOT a regiones de conectividad de mejor desempeño mientras acepta mayor sobrecarga de SWAP. Esta optimización en tiempo de ejecución redistribuye operaciones de puertas a través de la topología del dispositivo basándose en caracterización de ruido actualizada, intercambiando profundidad de compilación por calidad instantánea del hardware para mantener la fidelidad algorítmica global a pesar de las variaciones temporales del dispositivo.",
    "C": "El re-corte dinámico compensa la diafonía al aislar espacialmente ejecuciones simultáneas de subcircuitos en regiones de qubits no adyacentes cuando el monitoreo del dispositivo detecta tasas de error elevadas por operaciones concurrentes de puertas. El sistema redistribuye puertas para maximizar la separación física entre qubits activos, reduciendo errores coherentes por acoplamiento parásito. Esta optimización espacial en tiempo de ejecución se adapta a firmas de diafonía variables en el tiempo que emergen de deriva térmica o interferencia de líneas de control, manteniendo la fidelidad del algoritmo al intercambiar paralelismo de ejecución por correlaciones de error que se desarrollan durante sesiones experimentales extendidas.",
    "D": "Se adapta a las tasas de ruido medidas y rebalancea la sobrecarga clásica redistribuyendo operaciones de puertas entre subcircuitos basándose en caracterización actualizada del dispositivo. Cuando los tiempos de coherencia se degradan inesperadamente, la estrategia de particionamiento puede trasladar operaciones de mayor profundidad a regiones de qubits de mejor desempeño mientras acepta mayores costos de post-procesamiento clásico. Esta optimización en tiempo de ejecución intercambia consumo de recursos cuánticos por sobrecarga computacional clásica para mantener la fidelidad global del algoritmo a pesar de la calidad temporal variable del hardware.",
    "solution": "D"
  },
  {
    "id": 82,
    "question": "¿Qué ocurriría si se omite el paso de comunicación clásica después de una Medición de Estado de Bell en la teleportación cuántica?",
    "A": "El entrelazamiento del par de Bell compartido actúa como un canal cuántico preestablecido que transmite directamente los coeficientes del vector de estado del qubit de Alice a Bob instantáneamente al realizar la medición, explotando las correlaciones no locales para transferir los valores α y β sin intercambio de información clásica. Dado que el par EPR ya codifica una estructura de correlación perfecta que abarca la separación espacial, el qubit de Bob experimenta una transformación espontánea al estado objetivo cuando Alice realiza su medición—el colapso de la función de onda se propaga superlumínicamente a través del enlace entrelazado. Esto viola los teoremas de no comunicación solo si asumimos restricciones de localidad, pero el protocolo de teleportación demuestra fundamentalmente que la información cuántica puede atravesar distancias arbitrarias mediante entrelazamiento puro, siendo el paso de comunicación clásica meramente un mecanismo de verificación redundante en lugar de un componente necesario para la transferencia de estado.",
    "B": "Sin los dos bits clásicos que especifican cuál de los cuatro resultados de medición de Bell observó Alice (|Φ+⟩, |Φ-⟩, |Ψ+⟩, o |Ψ-⟩), Bob no puede aplicar la rotación correctiva de Pauli correspondiente a su qubit, dejándolo en un estado intermedio que se asemeja parcialmente al objetivo. El proceso de teleportación aún logra transferir la información cuántica a través del enlace entrelazado, pero el qubit de Bob permanece codificado en una base rotada que requiere la corrección faltante—esto resulta en errores sistemáticos al medirse, con una fidelidad F ≈ 0.75 en promedio.",
    "C": "El qubit de Bob termina en uno de cuatro estados posibles con igual probabilidad, determinado por cuál resultado de la base de Bell midió Alice. Sin conocer el resultado de la medición de Alice mediante comunicación clásica, Bob no puede aplicar la operación correctiva de Pauli apropiada, dejando su qubit en un estado efectivamente aleatorio que promedia a una matriz de densidad maximalmente mezclada ρ = I/2 sin información cuántica útil preservada.",
    "D": "El qubit de Bob colapsa a |0⟩ o |1⟩ con probabilidades que coinciden con los coeficientes de amplitud del estado original |α|² y |β|², preservando estadísticas de población clásicas mientras pierde información de fase relativa. La medición de Bell destruye la coherencia cuántica, convirtiendo la superposición en una mezcla clásica que transmite probabilidades de la base computacional pero elimina la capacidad de interferencia y los elementos no diagonales de la matriz de densidad.",
    "solution": "C"
  },
  {
    "id": 83,
    "question": "¿Por qué es crítica la reparación de rutas en la distribución de entrelazamiento a larga distancia?",
    "A": "La degradación de enlaces puede ocurrir durante operaciones multi-salto requiriendo reencaminamiento. Cuando segmentos intermedios de fibra experimentan tasas de pérdida elevadas debido a perturbaciones físicas o cuando nodos repetidores exhiben fidelidades de intercambio degradadas por errores transitorios de hardware, el protocolo debe reconfigurar dinámicamente la ruta de entrelazamiento a través de enlaces de red alternativos para evitar secciones comprometidas y mantener la conectividad extremo a extremo sin reiniciar toda la secuencia de generación desde cero.",
    "B": "El intercambio de entrelazamiento es no determinista con probabilidad de éxito P_swap < 1 en cada nodo, por lo que cuando una medición de estado de Bell falla (indicada por detección de resultado no maximalmente entrelazado), ese segmento debe regenerarse mientras los enlaces vecinos ya establecidos permanecen almacenados en memoria cuántica. La reparación de ruta identifica qué intercambio específico falló mediante comunicación clásica de resultados de medición, luego reintentar selectivamente solo el segmento no exitoso en lugar de descartar toda la cadena, explotando el hecho de que las memorias cuánticas pueden preservar pares de generación anterior por duraciones que exceden los tiempos de reintento de un solo intercambio.",
    "C": "La pérdida de fotones en fibra escala exponencialmente con la distancia como e^(-αL), causando que las tasas de generación de entrelazamiento entre nodos distantes se vuelvan insignificantes, por lo que los intentos directos extremo a extremo toman tiempos imprácticos. La reparación de ruta acelera la distribución al subdividir el canal en segmentos más cortos con pérdida aceptable (cada uno ~20 km para fibra estándar), estableciendo entrelazamiento en estos sub-enlaces en paralelo, luego realizando intercambios para conectarlos. Cuando falla la generación de un segmento individual debido a pérdida probabilística de fotones, solo ese enlace corto específico debe regenerarse en lugar de reintentar la distancia completa, logrando sobrecarga de reintento polinomial en lugar de exponencial.",
    "D": "La decoherencia por acoplamiento ambiental se acumula estocásticamente en memorias cuánticas almacenadas, con cada qubit experimentando errores aleatorios de inversión de fase a tasa Γ_dephasing·t. Si la duración del protocolo de distribución extremo a extremo excede el tiempo de coherencia T₂ de qubits de memoria en nodos intermedios, la fidelidad del par distribuido cae por debajo de umbrales útiles. La reparación de ruta mitiga esto monitoreando tiempos de coherencia de memoria en tiempo real y reemplazando preventivamente segmentos cuyos qubits se aproximan a sus límites T₂ con pares recién generados, manteniendo la fidelidad general por encima de umbrales de destilación sin esperar a que los errores se manifiesten y corrompan el estado extremo a extremo.",
    "solution": "A"
  },
  {
    "id": 84,
    "question": "¿Qué enfoque de IA es particularmente útil para aprender estrategias óptimas en entornos cuánticos dinámicos?",
    "A": "Los métodos de aprendizaje no supervisado como el clustering sobresalen en contextos cuánticos porque pueden descubrir automáticamente estructura oculta en espacios de Hilbert de alta dimensión sin requerir datos de entrenamiento etiquetados, que son costosos de generar para sistemas cuánticos.",
    "B": "El análisis de componentes principales para reducción de dimensionalidad resulta particularmente efectivo porque los estados cuánticos naturalmente viven en espacios exponencialmente grandes donde la mayoría de las dimensiones contribuyen varianza insignificante a cantidades observables, permitiendo identificación eficiente del subespacio donde la optimización debería enfocarse para ganancias máximas de desempeño con sobrecarga computacional mínima.",
    "C": "Los algoritmos de aprendizaje por refuerzo sobresalen en descubrir políticas de control óptimas mediante interacción de prueba y error con sistemas cuánticos, usando señales de recompensa de resultados de medición para refinar iterativamente estrategias sin requerir conocimiento explícito del hamiltoniano subyacente o la dinámica del sistema, haciéndolos particularmente adecuados para optimización adaptativa en entornos donde la evolución del estado cuántico es compleja o solo parcialmente caracterizada.",
    "D": "La regresión lineal estándar sobre resultados de medición proporciona el camino más directo hacia estrategias óptimas al modelar la recompensa esperada como una función lineal de la base de medición y los parámetros de preparación de estado, permitiendo que métodos basados en gradiente converjan rápidamente.",
    "solution": "C"
  },
  {
    "id": 85,
    "question": "¿Por qué son necesarios los indicadores clave de rendimiento (KPI) específicos para DQC, y cómo contribuyen a evaluar diferentes implementaciones?",
    "A": "Rastrean la frecuencia de señales de control clásicas durante la ejecución, ya que minimizar la interferencia clásica es central para la ventaja cuántica—cuantas menos veces necesitemos bucles de retroalimentación clásica, más cerca estaremos de la verdadera aceleración cuántica.",
    "B": "La equidad entre procesadores se logra normalizando métricas de rendimiento para tener en cuenta diferentes conteos de qubits, conjuntos de puertas y gráficos de conectividad, asegurando que los benchmarks no favorezcan injustamente arquitecturas con mayor conteo nativo de qubits o bibliotecas de puertas más ricas. Estos KPI establecen un campo de juego nivelado al medir volumen cuántico efectivo por recurso físico invertido, permitiendo comparaciones directas entre implementaciones superconductoras, de trampas de iones y fotónicas a pesar de sus paradigmas operacionales vastamente diferentes.",
    "C": "Los KPI específicos para DQC comparan implementaciones midiendo eficiencia de enrutamiento de entrelazamiento, fidelidad de ejecución de puertas no locales y sobrecarga de comunicación de red, que son características únicas de arquitecturas cuánticas distribuidas que no aplican a computadoras cuánticas monolíticas. Estas métricas capturan qué tan bien un sistema maneja la transferencia de estado cuántico a través de enlaces de red y cuantifican los recursos adicionales consumidos por puertas basadas en teleportación.",
    "D": "El consumo de memoria y la velocidad de compilación de puertas son los cuellos de botella principales que abordan, por lo que estos KPI se enfocan casi enteramente en optimización de software en lugar de rendimiento de capa física. Al cuantificar la sobrecarga de compilación y la huella de memoria clásica durante la transpilación de circuitos, estas métricas revelan qué arquitecturas distribuidas pueden sostener menor latencia en la pila de software de control, impactando directamente el rendimiento de aplicación extremo a extremo independientemente de la calidad del hardware cuántico.",
    "solution": "C"
  },
  {
    "id": 86,
    "question": "¿Cuál es una ventaja de utilizar generadores cuánticos de números aleatorios (QRNGs) en sistemas de seguridad IoT?",
    "A": "La verdadera aleatoriedad proveniente de procesos cuánticos como mediciones de polarización de fotones o detección homodina de ruido de vacío proporciona claves fundamentalmente impredecibles que no pueden ser reproducidas por algoritmos clásicos, aumentando significativamente la seguridad criptográfica. A diferencia de los generadores pseudoaleatorios que dependen de suposiciones de dureza computacional, los QRNGs derivan entropía del colapso de superposición cuántica, que es demostrablemente aleatorio bajo la interpretación de Copenhague. Sin embargo, la reconstrucción del estado post-medición mediante tomografía de medición débil puede recuperar parcialmente la función de onda previa al colapso, permitiendo a un adversario con memoria cuántica extraer ~40% de la entropía original.",
    "B": "La verdadera aleatoriedad proveniente de procesos cuánticos como tiempos de llegada de fotones o fluctuaciones de vacío proporciona claves fundamentalmente impredecibles que no pueden ser reproducidas ni predichas por ningún algoritmo clásico, aumentando significativamente la seguridad criptográfica. A diferencia de los generadores pseudoaleatorios que dependen de suposiciones de complejidad computacional, los QRNGs derivan entropía de resultados de medición cuántica que son inherentemente no deterministas según la mecánica cuántica, haciendo que los ataques de fuerza bruta y el análisis de patrones sean matemáticamente imposibles incluso con recursos computacionales ilimitados.",
    "C": "La verdadera aleatoriedad proveniente de procesos cuánticos como conversión paramétrica espontánea descendente o ruido de disparo en divisores de haz proporciona claves fundamentalmente impredecibles que no pueden ser generadas algorítmicamente, aumentando significativamente la seguridad criptográfica. A diferencia de los generadores pseudoaleatorios que dependen de conjeturas de complejidad no demostradas como la dureza de la factorización de enteros, los QRNGs extraen entropía del colapso de medición cuántica regido por la regla de Born. Esto elimina vulnerabilidades de puerta trasera en algoritmos deterministas, aunque las implementaciones prácticas requieren calibración cuidadosa porque las cuentas oscuras del detector y el ruido de post-procesamiento clásico pueden introducir correlaciones que reducen la min-entropía efectiva por debajo del límite cuántico teórico.",
    "D": "La verdadera aleatoriedad proveniente de procesos cuánticos como cronometraje de desintegración atómica o eventos de detección de fotones individuales proporciona claves fundamentalmente impredecibles que resisten algoritmos de predicción clásicos, fortaleciendo significativamente los protocolos criptográficos. A diferencia de los generadores pseudoaleatorios basados en complejidad algorítmica, los QRNGs aprovechan la aleatoriedad intrínseca del colapso de la función de onda durante la medición, que es segura desde el punto de vista de la teoría de la información bajo teorías de variables ocultas locales. El flujo de bits cuántico bruto alcanza entropía de Shannon completa sin requerir suposiciones de dureza computacional, aunque la eficiencia finita del detector (típicamente 60-80%) introduce un sesgo clásico que debe corregirse mediante funciones extractoras en tiempo real para mantener la uniformidad.",
    "solution": "B"
  },
  {
    "id": 87,
    "question": "Considere un algoritmo cuántico originalmente diseñado para ejecutarse en un único procesador de 100 qubits, donde el cálculo involucra múltiples capas de puertas que crean entrelazamiento entre todos los qubits simultáneamente. Ahora desea ejecutar este algoritmo en una red cuántica distribuida que consiste en cuatro procesadores separados de 25 qubits conectados por canales de comunicación cuántica. ¿Cuál es el desafío principal al adaptar este algoritmo cuántico monolítico para ejecución distribuida, y por qué surge este desafío específicamente en el entorno distribuido?",
    "A": "Los algoritmos cuánticos monolíticos requieren interacciones frecuentes de múltiples qubits a través de todo el registro de qubits. Cuando los qubits se almacenan en procesadores cuánticos separados en una red distribuida, cada puerta o medición entre procesadores requiere teletransportación o distribución de entrelazamiento remoto sobre canales cuánticos, lo que introduce sobrecarga de comunicación sustancial, latencia y fuentes adicionales de decoherencia que no estaban presentes en la versión monolítica. Las puertas entre qubits en diferentes nodos no pueden aplicarse directamente y deben implementarse mediante consumo de entrelazamiento y rondas de comunicación clásica, aumentando drásticamente tanto el tiempo de ejecución como la acumulación de errores.",
    "B": "El desafío principal es particionar el estado cuántico global |ψ⟩ a través de múltiples procesadores mientras se preserva la estructura de descomposición de Schmidt que caracteriza el entrelazamiento entre subsistemas. Dado que el algoritmo crea entrelazamiento entre los 100 qubits, cada nodo debe mantener su subsistema local de 25 qubits en una matriz de densidad reducida ρ_i = Tr_{¬i}(|ψ⟩⟨ψ|) que permanezca consistente con el estado puro global. Sin embargo, calcular estas trazas parciales requiere protocolos de tomografía de estado cuántico que escalan exponencialmente con el tamaño del subsistema, y el teorema de no clonación impide distribuir copias de estados intermedios para verificación. Este mantenimiento de consistencia necesita teletransportación cuántica de resultados de medición entre nodos, creando cuellos de botella de comunicación ausentes en arquitecturas monolíticas.",
    "C": "El obstáculo fundamental es que las puertas globales de entrelazamiento en el algoritmo monolítico acoplan coherentemente los 100 qubits dentro del entorno electromagnético compartido del procesador, explotando subespacios colectivos libres de decoherencia que suprimen ciertos canales de error a través de efectos de subradiancia. La ejecución distribuida destruye esta protección colectiva porque los qubits en procesadores separados decoherenzan independientemente bajo ruido local, careciendo del modo común que permitía la supresión pasiva de errores. Convertir el algoritmo requiere reemplazar puertas globales con secuencias de operaciones locales y protocolos de intercambio de entrelazamiento que reconstruyen correlaciones de muchos cuerpos equivalentes, pero esta sustitución elimina la ventaja libre de decoherencia, requiriendo códigos activos de corrección de errores para lograr fidelidad comparable a pesar de características de ruido fundamentalmente diferentes.",
    "D": "Las redes cuánticas distribuidas permiten paralelismo computacional asignando subrutinas independientes a cada procesador de 25 qubits, pero las capas de puertas del algoritmo monolítico crean dependencias de datos que impiden la descomposición ingenua. Específicamente, las operaciones globales de entrelazamiento en la capa L dependen de resultados de medición de la capa L-1 a través de todos los qubits, formando un grafo acíclico dirigido (DAG) donde el cálculo de cada nodo espera entradas de todos los demás nodos. Ejecutar esta estructura de dependencias de forma distribuida requiere teletransportación de puertas no locales usando pares EPR pre-compartidos, consumiendo un par de Bell por puerta de dos qubits. El desafío es que establecer estos pares EPR demanda recursos de entrelazamiento que crecen exponencialmente a medida que aumenta la profundidad del circuito, ya que mantener la coherencia de fase entre pares distribuidos requiere rondas de purificación activa cuya sobrecarga escala como O(d³) para distancia d.",
    "solution": "A"
  },
  {
    "id": 88,
    "question": "Considere el desarrollo de códigos LDPC cuánticos con tasa de codificación constante y distancia mínima que crece con la longitud del bloque. Las primeras construcciones clásicas de LDPC lograron distancia lineal, pero el caso cuántico enfrentó obstáculos fundamentales debido a la interacción entre estabilizadores X y Z. Los códigos de producto hipergráfico, introducidos por Tillich y Zémor, representaron un avance al construir sistemáticamente códigos cuánticos a partir de pares de códigos clásicos. ¿Por qué son significativos los códigos de producto hipergráfico en la teoría de corrección de errores cuánticos?",
    "A": "Demostraron que existen códigos LDPC cuánticos con tasa constante y distancia que escala como sqrt(n), resolviendo si tales códigos eran posibles. Aunque el escalamiento de raíz cuadrada es subóptimo versus códigos clásicos, mostró que los códigos LDPC cuánticos podían lograr tasa y distancia no triviales simultáneamente.",
    "B": "Los códigos de producto hipergráfico demostraron que los códigos LDPC cuánticos con tasa constante y distancia que escala como n^(2/3) son construibles, mejorando construcciones de producto anteriores que lograban solo distancia logarítmica. Aunque aún por debajo de la distancia lineal lograda por códigos LDPC clásicos, el escalamiento n^(2/3) representa un avance significativo porque supera la barrera fundamental de sqrt(n) que muchos investigadores conjeturaron era insuperable para códigos cuánticos dispersos. Esta construcción mostró que la restricción CSS cuántica sobre la superposición de estabilizadores no necesita restringir la distancia tan severamente como se pensaba previamente.",
    "C": "La construcción de producto hipergráfico establece que los códigos LDPC cuánticos pueden lograr tasa constante con distancia que escala como log²(n), que, aunque logarítmica en lugar de polinomial, es suficiente para tolerancia a fallos práctica porque el umbral tolerante a fallos para computación cuántica depende exponencialmente de la distancia del código. Incluso el crecimiento logarítmico de distancia permite tasas de error por debajo del umbral con longitudes de bloque factibles para implementaciones a corto plazo. La significancia radica en probar que códigos de generador disperso pueden lograr simultáneamente tasa no nula y distancia que crece sin límite, propiedades que las primeras construcciones no lograron combinar.",
    "D": "Los códigos de producto hipergráfico probaron que códigos LDPC cuánticos asintóticamente buenos (tasa constante y distancia lineal) no pueden existir debido a la estructura de complejo de cadena que revelaron: la relación dual entre estabilizadores X y Z crea una restricción homológica donde lograr distancia lineal en ambos sectores X y Z simultáneamente fuerza a los pesos de estabilizador a crecer logarítmicamente con n. Este resultado de no existencia aclaró el compromiso fundamental entre dispersión de estabilizador, tasa de codificación y escalamiento de distancia, mostrando que la distancia sqrt(n) representa un límite óptimo para códigos cuánticos de tasa constante con estabilizadores de peso constante, resolviendo así una pregunta abierta importante sobre los límites últimos de la corrección de errores cuánticos dispersa.",
    "solution": "A"
  },
  {
    "id": 89,
    "question": "En el contexto de algoritmos cuánticos variacionales aplicados a sistemas de materia condensada con ruptura espontánea de simetría, ¿qué restricción fundamental limita la capacidad de circuitos cuánticos parametrizados para preparar el verdadero estado fundamental, y cómo interactúa la profundidad del circuito con esta restricción?",
    "A": "Romper una simetría continua o discreta en un sistema cuántico macroscópico requiere establecer un parámetro de orden coherente que se extienda a través de todos los sitios de la red, que emerge de una conspiración sutil de fluctuaciones cuánticas en todo el volumen. Cada capa de puertas parametrizadas en un circuito variacional solo puede introducir perturbaciones locales que violan la simetría dentro de una vecindad finita, típicamente unos pocos espaciados de red.",
    "B": "Las puertas cuánticas parametrizadas estándar utilizadas en circuitos variacionales —incluyendo rotaciones de Pauli, operaciones controladas y puertas de entrelazamiento— se construyen a partir de transformaciones unitarias que respetan leyes de conservación fundamentales codificadas en el hamiltoniano, como momento angular total de espín, número de partículas o carga. Estos números cuánticos conservados definen sectores de superselección que no pueden conectarse mediante ninguna evolución unitaria física. Cuando un sistema de materia condensada exhibe ruptura espontánea de simetría, el verdadero estado fundamental reside en un sector específico de simetría rota caracterizado por números cuánticos definidos (por ejemplo, magnetización neta en un ferromagneto), pero los circuitos variacionales inicializados en un sector simétrico (magnetización cero) no pueden escapar de ese sector mediante operaciones de puerta.",
    "C": "En fases de materia condensada que exhiben ruptura espontánea de simetría, el paisaje de la función de costo evaluada por algoritmos cuánticos variacionales desarrolla regiones exponencialmente planas alrededor de estados simétricos porque los gradientes se suprimen exponencialmente con el tamaño del sistema —una manifestación de mesetas estériles específica de fases ordenadas. Esto ocurre porque los estados fundamentales con simetría rota corresponden a configuraciones exponencialmente raras en el espacio de todos los estados cuánticos que respetan las simetrías del hamiltoniano, haciéndolos objetivos vanishingly improbables para descenso de gradiente. Aumentar la profundidad del circuito exacerba este problema al expandir el espacio de estados expresables, lo que diluye aún más la densidad de estados con simetría rota y causa que la optimización se estanque exponencialmente rápido independientemente de la estrategia de optimización, impidiendo la convergencia al verdadero estado fundamental incluso con tiempo de cálculo clásico infinito.",
    "D": "La ruptura espontánea de simetría en sistemas de materia condensada en el límite termodinámico requiere establecer correlaciones cuánticas de rango infinito que codifican el parámetro de orden macroscópico, pero los circuitos cuánticos parametrizados de profundidad finita solo pueden generar correlaciones que se extienden sobre un rango espacial finito determinado por la estructura de cono de luz de la secuencia de puertas. Cada capa de circuito aumenta la longitud máxima de correlación en a lo sumo el rango de interacción de las puertas (típicamente vecinos más cercanos), por lo que capturar verdaderos estados fundamentales con simetría rota con correlaciones que decaen como ley de potencias o exponencialmente requeriría profundidad de circuito que escala extensivamente con el tamaño del sistema, haciendo el enfoque variacional impracticable incluso con configuraciones óptimas de parámetros.",
    "solution": "D"
  },
  {
    "id": 90,
    "question": "En el estudio de la contextualidad cuántica, algunas pruebas funcionan para cualquier estado cuántico mientras que otras requieren preparaciones cuidadosamente elegidas. Suponga que está diseñando un experimento para demostrar contextualidad en un sistema de tres qubits. ¿Qué distinción fundamental separa las pruebas de contextualidad \"dependientes del estado\" de las \"independientes del estado\", y por qué importa esto para el diseño experimental?",
    "A": "Las pruebas independientes del estado demuestran contextualidad para todos los estados cuánticos en el sistema, eliminando la necesidad de preparación precisa del estado y haciéndolas demostraciones experimentales más robustas de no clasicalidad. Las pruebas dependientes del estado solo funcionan para estados preparados particulares, lo que requiere protocolos de preparación cuidadosos pero a veces puede lograr violaciones más fuertes de límites clásicos, ofreciendo ventajas al probar teorías específicas de recursos cuánticos o al apuntar a correlaciones contextuales máximas en escenarios adaptados.",
    "B": "Las pruebas de contextualidad dependientes del estado requieren estados cuánticos que saturen relaciones específicas de no conmutación entre operadores de medición, lo que significa que las violaciones solo emergen cuando los valores esperados alcanzan configuraciones extremales predichas por principios de incertidumbre. Las pruebas independientes del estado explotan propiedades de teoría de grafos de estructuras de compatibilidad de medición que se manifiestan independientemente del estado cuántico, haciéndolas robustas a errores de preparación. Experimentalmente, las pruebas dependientes del estado demandan ingeniería de estado de alta fidelidad para alcanzar el régimen operacional donde los testigos de contextualidad exceden umbrales clásicos.",
    "C": "La característica distintiva es que las pruebas de contextualidad independientes del estado se basan en restricciones algebraicas entre probabilidades de resultados de medición que se mantienen en todo el espacio de Hilbert, mientras que las pruebas dependientes del estado explotan testigos de entrelazamiento específicos de estructuras de superposición particulares. Las violaciones independientes del estado aparecen en relaciones de valor esperado que las teorías clásicas de variables ocultas no pueden reproducir para ningún estado cuántico, mientras que las pruebas dependientes del estado logran violaciones mayores pero solo cuando el estado preparado exhibe suficiente coherencia entre componentes de base computacional.",
    "D": "Los experimentos de contextualidad dependientes del estado demuestran violaciones solo cuando el estado preparado exhibe valores negativos de función de Wigner en regiones específicas del espacio de fase, ya que la contextualidad surge fundamentalmente de la no clasicalidad de representaciones de cuasiprobabilidad. Las pruebas independientes del estado eluden este requisito utilizando mediciones cuya estructura de conmutación por sí sola garantiza violaciones, independientemente de las propiedades del estado en el espacio de fase. Experimentalmente, las pruebas dependientes del estado requieren verificación tomográfica de negatividad de Wigner antes de que comiencen las mediciones de contextualidad.",
    "solution": "A"
  },
  {
    "id": 91,
    "question": "¿Por qué un algoritmo de enrutamiento podría favorecer una ruta ligeramente más larga?",
    "A": "Mejor entrelazamiento de extremo a extremo gracias a fidelidades de enlace más altas, porque la fidelidad acumulada a lo largo de una ruta de red cuántica depende multiplicativamente de la calidad individual de cada segmento, y seleccionar una ruta con enlaces de fidelidad consistentemente alta—incluso si involucra más saltos—puede producir un entrelazamiento general superior al de una ruta más corta que contenga uno o más segmentos de baja calidad. Por ejemplo, una ruta de cuatro saltos con fidelidad por enlace de 0.95 logra una fidelidad total de ~0.81, mientras que una ruta de dos saltos con fidelidades de 0.90 y 0.85 produce solo ~0.77. Los protocolos de enrutamiento modernos incorporan métricas de calidad de enlace más allá del simple conteo de saltos para optimizar el rendimiento de extremo a extremo.",
    "B": "Balanceo de carga entre bancos de qubits de memoria dentro de cada nodo repetidor, lo cual se vuelve crítico cuando los nodos emplean memorias cuánticas de múltiples qubits con tiempos de coherencia heterogéneos debido a variaciones de fabricación o gradientes de campo magnético dependientes de la posición en arreglos de trampas de iones. Al distribuir el tráfico sobre rutas más largas que utilizan bancos de memoria subutilizados en nodos intermedios, el protocolo de enrutamiento previene el agotamiento prematuro de qubits de alta calidad en hubs congestionados, extendiendo la vida operacional de la red en su conjunto. Específicamente, en una arquitectura de repetidores con k qubits de memoria por nodo, el enrutamiento por ruta más corta puede crear puntos críticos donde ciertos qubits ciclan a través de operaciones de intercambio de entrelazamiento a una tasa 10× mayor que los qubits periféricos, acelerando su defasaje mediante errores de control acumulados y eventualmente volviendo esos qubits inutilizables mientras otros permanecen frescos. Una ruta más larga que intencionalmente se enruta a través de nodos menos utilizados balancea este desgaste, manteniendo una fidelidad más uniforme en toda la red y evitando el escenario donde los nodos de mayor centralidad se convierten en cuellos de botella debido a recursos de memoria agotados, incluso cuando cada salto adicional impone una penalización de fidelidad que es superada por la ganancia de confiabilidad al acceder a qubits bien descansados.",
    "C": "Eludiendo nodos con coprocesadores clásicos saturados que gestionan protocolos de destilación de entrelazamiento, porque los repetidores de redes cuánticas dependen de computación clásica en tiempo real para decodificar mediciones de síndrome de rondas de corrección de errores, determinar estrategias óptimas de destilación (p. ej., seleccionar qué pares de pares de Bell ruidosos combinar mediante CNOT y medición para producir un par de mayor fidelidad), y coordinar la sincronización de operaciones de intercambio de entrelazamiento con nodos vecinos. Cuando el procesador clásico de un nodo se sobrecarga—quizás manejando solicitudes de enrutamiento simultáneas de múltiples pares fuente-destino—introduce latencia que puede exceder el tiempo de decoherencia de estados entrelazados almacenados esperando en memoria cuántica. Una ruta más larga que evita estos nodos computacionalmente saturados, incluso a costa de saltos adicionales, asegura que la sobrecarga de coordinación clásica de cada salto permanezca dentro de límites aceptables, preservando la coherencia temporal necesaria para la distribución exitosa de entrelazamiento. Este compromiso es particularmente relevante en redes que usan protocolos de destilación iterativa que requieren O(log(1/ε)) rondas de procesamiento clásico por salto para alcanzar la fidelidad objetivo ε, donde los nodos congestionados causan retrasos de cola que destruyen el entrelazamiento más rápido de lo que la destilación puede purificarlo, haciendo preferible una ruta de cinco saltos a través de nodos ligeramente cargados que una ruta de tres saltos a través de nodos embotellados.",
    "D": "Restricciones de sincronización temporal debido a tasas heterogéneas de deriva de reloj en diferentes nodos repetidores, especialmente en redes cuánticas geográficamente distribuidas donde los nodos usan relojes atómicos locales que acumulan errores de fase relativos a tasas que difieren en varios picosegundos por segundo debido al corrimiento al rojo gravitacional dependiente de la altitud (según la relatividad general) o estabilidad del oscilador dependiente de la temperatura. Rutas más cortas que incluyen nodos con relojes mal sincronizados requieren comunicación clásica frecuente para restablecer referencias de fase antes del intercambio de entrelazamiento, porque la medición de estado de Bell en cada repetidor debe realizarse en la base correcta, y cualquier desplazamiento de reloj se traduce directamente en una rotación de base que reduce la fidelidad del estado intercambiado. Una ruta más larga que selecciona nodos con relojes mutuamente bien sincronizados—quizás porque comparten un estándar de tiempo común a través de enlaces de fibra óptica a un servidor de reloj central o porque recientemente se sometieron a sincronización disciplinada por GPS—puede evitar estas sobrecargas de corrección de fase, aunque los saltos adicionales nominalmente introducen más oportunidades de decoherencia. El algoritmo de enrutamiento debe balancear el conteo de saltos contra el jitter temporal acumulativo, y en redes que abarcan distancias continentales donde los efectos relativistas se vuelven no despreciables, la ruta óptima puede deliberadamente agregar uno o dos saltos para mantener sincronización sub-nanosegundo en todas las mediciones intermedias, asegurando que el estado entrelazado final compartido entre fuente y destino retenga la coherencia de fase necesaria para aplicaciones como distribución de clave cuántica o computación cuántica distribuida.",
    "solution": "A"
  },
  {
    "id": 92,
    "question": "¿Por qué es más desafiante la corrección de errores de compuerta en computación cuántica comparada con la computación clásica?",
    "A": "Los errores no pueden detectarse sin colapso por medición que destruye los estados de superposición cuántica que están siendo protegidos, creando una tensión fundamental entre la detección de errores y la preservación de la computación. Además, el teorema de no-clonación previene la simple duplicación de información cuántica para verificación de redundancia como la redundancia modular triple clásica. Además, los errores cuánticos forman un espectro continuo de posibles rotaciones en el espacio de Hilbert en lugar de cambios discretos de bits, requiriendo mediciones de síndrome a través de verificaciones de estabilizadores multi-qubit que extraen información de error en bits clásicos sin revelar el estado cuántico lógico protegido. El proceso de medición en sí introduce errores adicionales, y los procesos de error continuos deben discretizarse mediante diseño cuidadoso de código y extracción rápida de síndrome, haciendo que la corrección de errores cuántica sea arquitectónicamente mucho más compleja que los enfoques clásicos a pesar de lograr umbrales teóricos de tolerancia a fallos similares.",
    "B": "El teorema de no-clonación previene copiar estados cuánticos desconocidos, forzando la dependencia de codificaciones entrelazadas a través de múltiples qubits físicos en lugar de simple redundancia. Mientras que los códigos estabilizadores logran detección de errores a través de mediciones de síndrome que proyectan sobre subespacios propios sin colapsar el estado lógico, los errores cuánticos ocurren continuamente en el espacio de Hilbert en lugar de discretamente. Sin embargo, a diferencia de los sistemas clásicos donde las verificaciones de paridad revelan directamente qué bit cambió, los síndromes cuánticos solo indican tipo y ubicación de error probabilísticamente, requiriendo inferencia bayesiana iterativa sobre posibles cadenas de error. Esta sobrecarga de decodificación probabilística, combinada con la necesidad de completar la extracción de síndrome más rápido de lo que se acumulan nuevos errores, hace que los códigos cuánticos requieran factores de redundancia más altos que los códigos clásicos para lograr supresión de error lógico equivalente a pesar de que ambos se aproximan a umbrales de tolerancia a fallos similares asintóticamente.",
    "C": "Los errores cuánticos se manifiestan como rotaciones continuas en el espacio de Hilbert en lugar de cambios discretos de bits, creando un espacio de error de dimensión infinita que los códigos binarios clásicos no pueden abordar. Aunque las condiciones de Knill-Laflamme muestran que los síndromes estabilizadores discretos aún pueden detectar errores continuos proyectándolos sobre subespacios corregibles, el proceso de medición inevitablemente introduce nuevos errores a tasas comparables a los errores de compuerta. A diferencia de los sistemas clásicos donde la medición es efectivamente libre de ruido, la extracción de síndrome cuántica requiere circuitos tolerantes a fallos con qubits ancilla adicionales y rondas de verificación. Además, el teorema del umbral requiere que la extracción de síndrome se complete dentro del tiempo de coherencia, forzando compromisos arquitectónicos entre distancia de código, tiempo de ciclo y tasas de error físico que los sistemas clásicos evitan mediante lectura no destructiva y detección de error determinística.",
    "D": "El colapso por medición prohíbe la verificación directa de estados cuánticos sin destruir superposiciones, pero más fundamentalmente, la decoherencia cuántica opera a través de traza parcial continua sobre grados de libertad ambientales, lo que significa que los errores se acumulan suavemente en lugar de como eventos discretos. La corrección de errores clásica detecta eventos discretos de corrupción mediante sumas de verificación calculadas determinísticamente, mientras que los códigos cuánticos deben implementar mediciones proyectivas de síndrome que en sí mismas introducen nuevos errores. El teorema de no-clonación previene verificación mediante copias redundantes, forzando códigos basados en entrelazamiento donde la información lógica se distribuye no-localmente a través de qubits físicos. Además, procesos de ruido correlacionado como diafonía pueden causar que los valores propios de estabilizadores fluctúen coherentemente a través de rondas, creando patrones de síndrome indistinguibles de errores de datos, requiriendo decodificación multi-ronda con espacios de estado exponencialmente crecientes que los códigos de Hamming clásicos evitan completamente.",
    "solution": "A"
  },
  {
    "id": 93,
    "question": "¿Qué sucede si el algoritmo de Grover se ejecuta por más iteraciones que el número óptimo?",
    "A": "El estado cuántico experimenta amplificación de amplitud continua que se aproxima asintóticamente pero nunca alcanza del todo la probabilidad unitaria para el estado marcado, exhibiendo comportamiento análogo a oscilaciones amortiguadas clásicas donde cada iteración sucesiva proporciona rendimientos decrecientes. La evolución de amplitud sigue una trayectoria monótonamente creciente con tasa de convergencia logarítmica, acotada superiormente por principios fundamentales de incertidumbre de medición cuántica que previenen preparación perfecta de estado. Esta saturación ocurre porque el espectro de valores propios del operador de difusión de Grover crea una región de estabilidad natural alrededor de la amplificación máxima, causando que el sistema se asiente en una distribución cuasi-estacionaria.",
    "B": "El estado cuántico continúa rotando en el subespacio bidimensional extendido por las superposiciones de estados marcado y no marcado, causando que la amplitud del estado objetivo oscile sinusoidalmente. Después de pasar por el ángulo óptimo, iteraciones adicionales de Grover rotan el vector de estado más allá de la proyección máxima sobre el estado marcado, reduciendo progresivamente la probabilidad de éxito hasta que potencialmente cae por debajo incluso de la línea base inicial de adivinanza aleatoria, demostrando la importancia crítica del control del conteo de iteraciones.",
    "C": "Las iteraciones excesivas introducen un mecanismo de corrección dependiente de fase donde el oráculo y los operadores de difusión comienzan a interferir destructivamente, causando que la amplitud de éxito ejecute un descenso controlado en lugar de un colapso abrupto. Esta degradación gradual sigue un perfil de decaimiento de raíz cuadrada donde la probabilidad disminuye como 1/√k para k iteraciones más allá del óptimo, sustancialmente más suave de lo que la rotación geométrica ingenua predeciría. El efecto proviene de términos de interferencia cuántica de orden superior que se activan solo después de que la amplitud del estado marcado excede un umbral crítico, proporcionando protección parcial contra sobre-iteración moderada mediante redistribución automática de amplitud.",
    "D": "Más allá del conteo óptimo de iteraciones, los errores de fase cuánticos acumulados de implementaciones de compuerta imperfectas comienzan a dominar la dinámica de rotación ideal, causando que el vector de estado gradualmente decohere del subespacio bidimensional objetivo-no-objetivo hacia el espacio de Hilbert completo de dimensión 2^n. Esta decoherencia se manifiesta como un ensanchamiento de la distribución de amplitud a través de estados no marcados en lugar de simple reversión del proceso de amplificación, con la probabilidad de éxito declinando exponencialmente a medida que el acoplamiento ambiental destruye la estructura de superposición coherente, reduciendo finalmente los resultados de medición a ruido aleatorio uniforme indistinguible de búsqueda no estructurada.",
    "solution": "B"
  },
  {
    "id": 94,
    "question": "¿Qué característica única deben abordar los protocolos de monitoreo de redes cuánticas que el monitoreo clásico no enfrenta?",
    "A": "La restricción fundamental impuesta por el efecto Zeno cuántico, que causa que los sistemas cuánticos monitoreados continuamente se congelen en su estado inicial y previene la evolución de los estados cuánticos operacionales de la red. Las redes clásicas pueden realizar monitoreo continuo sin afectar la transmisión de datos, pero el monitoreo cuántico debe emplear estrategias de muestreo discreto con intervalos de medición cuidadosamente sincronizados que equilibren la observabilidad diagnóstica contra la supresión inducida por retro-acción de la dinámica cuántica, asegurando que el monitoreo mismo no detenga la distribución de entrelazamiento o los protocolos de teletransportación cuántica.",
    "B": "El requisito fundamental de que el tráfico de red cuántica—pares de fotones entrelazados, estados cuánticos transmitidos vía teletransportación, pares de Bell distribuidos—no puede duplicarse o clonarse debido al teorema de no-clonación, previniendo que los protocolos de monitoreo copien pasivamente información cuántica para análisis como las redes clásicas hacen rutinariamente. Las redes clásicas pueden dividir señales ópticas usando divisores de haz para interceptar flujos de datos, pero el monitoreo cuántico debe emplear técnicas de post-selección que sacrifican rendimiento o utilizar mediciones destructivas en modos auxiliares que se correlacionan con estados operacionales sin colapsarlos directamente.",
    "C": "El principio mecánico cuántico fundamental de que la medición perturba el sistema que está siendo observado, requiriendo que los protocolos de monitoreo empleen técnicas no invasivas como mediciones de testigos de entrelazamiento, tomografía parcial en qubits auxiliares, o recursos de monitoreo dedicados que no colapsan los estados cuánticos operacionales. Las redes clásicas pueden libremente interceptar e inspeccionar datos en tránsito sin alterar la información, pero el monitoreo cuántico debe equilibrar cuidadosamente la visibilidad diagnóstica contra la inevitable perturbación de estado.",
    "D": "La restricción arquitectónica fundamental impuesta por la monogamia del entrelazamiento, que limita el número de nodos de red que pueden compartir simultáneamente correlaciones cuánticas fuertes con un nodo dado. Las redes clásicas soportan fan-out arbitrario donde un nodo transmite a muchos receptores, pero el monitoreo cuántico debe considerar el compromiso donde extraer información de monitoreo de un enlace cuántico necesariamente reduce el entrelazamiento disponible para usuarios finales, requiriendo que los protocolos asignen recursos de entrelazamiento entre canales de comunicación operacionales y canales de medición diagnóstica de acuerdo con límites estrictos de monogamia derivados de desigualdades de subaditividad fuerte.",
    "solution": "C"
  },
  {
    "id": 95,
    "question": "¿Qué metodología de ataque avanzada puede comprometer la seguridad de los esquemas de dinero cuántico?",
    "A": "La tomografía de estado cuántico realizada a través de múltiples intentos de verificación independientes permite a un adversario reconstruir incrementalmente el estado de dinero cuántico desconocido mediante inferencia estadística de la matriz de densidad a partir de resultados de medición, construyendo una descripción clásica de alta fidelidad que puede usarse para preparar copias aproximadas y efectivamente eludir las protecciones de no-clonación.",
    "B": "Los protocolos de clonación aproximada combinados con códigos de corrección de errores cuánticos permiten a un atacante generar copias casi perfectas produciendo primero varios duplicados ruidosos usando máquinas de clonación universal óptimas, luego aplicando mediciones de síndrome y compuertas de corrección para purificar sistemáticamente estos clones, con redundancia permitiendo que decodificadores correctores de errores recuperen un qubit lógico que representa fielmente el estado de dinero genuino.",
    "C": "La reconstrucción de estado de subespacio oculto explota la estructura de verificación analizando cómo el protocolo de autenticación público del banco acepta o rechaza estados candidatos, permitiendo a adversarios inferir propiedades del subespacio secreto y eventualmente falsificar tokens.",
    "D": "El análisis de consultas al oráculo de verificación explota el procedimiento de verificación público del banco enviando estados cuánticos cuidadosamente elaborados y observando patrones de aceptación para hacer ingeniería inversa de la base secreta en la cual se preparan los estados de dinero legítimos, con estrategias de consulta adaptativa probando superposiciones y estados de sonda entrelazados para extraer información parcial sobre operadores de proyección de subespacio oculto.",
    "solution": "C"
  },
  {
    "id": 96,
    "question": "¿Qué metodología de ataque avanzada puede comprometer la seguridad de los esquemas de compartición secreta cuántica?",
    "A": "Un adversario puede colocar dispositivos de medición cuántica no demoledora en los canales de distribución para monitorear la transmisión de las partes de forma continua sin perturbar los estados cuánticos, permitiéndole extraer correlaciones clásicas entre las partes mediante el análisis de las relaciones de temporización y fase de los fotones detectados. Dado que las correlaciones entre partes codifican combinaciones lineales del secreto, la acumulación de estadísticas a lo largo de múltiples ejecuciones del protocolo permite la reconstrucción del secreto mediante análisis de correlación, incluso cuando las partes individuales parecen estar completamente mezcladas.",
    "B": "Si la identidad del distribuidor no se verifica mediante protocolos de autenticación cuántica apropiados, un adversario puede realizar un ataque de intermediario interceptando la transmisión inicial del distribuidor e inyectando partes falsas que están entrelazadas con sus propios qubits auxiliares. Dado que la fase de reconstrucción por umbral se basa en la superposición coherente de partes legítimas, incluso una sola parte falsa corrompe el patrón de interferencia durante el reensamblaje, permitiendo al adversario sesgar el secreto reconstruido hacia un valor de su elección o extraer información mediante mediciones en su sistema auxiliar.",
    "C": "La tomografía de estados cuánticos de partes parciales puede revelar información estadística sobre el secreto mediante la realización de mediciones informacionalmente completas a través de múltiples ejecuciones del protocolo con la misma distribución de partes.",
    "D": "Durante la fase de reconstrucción por umbral, un adversario puede inyectar pulsos electromagnéticos o señales láser cuidadosamente temporizadas para crear interferencia controlada entre los estados cuánticos de las partes que se están recombinando. Esta interferencia desplaza las fases relativas entre los estados de la base computacional en el secreto recombinado, y al variar sistemáticamente los patrones de interferencia a través de múltiples intentos de reconstrucción mientras se observan los resultados exitosos, el adversario puede deducir iterativamente el secreto original mediante análisis diferencial de fase de los valores reconstruidos.",
    "solution": "C"
  },
  {
    "id": 97,
    "question": "¿Cómo se relaciona la tomografía variacional de estados cuánticos con el aprendizaje automático cuántico?",
    "A": "Utiliza principios de aprendizaje automático para reconstruir estados de manera eficiente parametrizando el estado cuántico desconocido como un ansatz de red neuronal y entrenando los parámetros para que coincidan con las estadísticas medidas. En lugar de requerir exponencialmente muchas mediciones para caracterizar completamente la matriz de densidad, se aprovecha el sesgo inductivo de las arquitecturas neuronales para comprimir la representación del estado, aprendiendo un modelo generativo que reproduce los resultados de las mediciones.",
    "B": "Verifica el funcionamiento de redes neuronales cuánticas realizando tomografía de estado variacional en los estados de salida producidos por el ansatz del circuito cuántico después del entrenamiento. Dado que las redes neuronales cuánticas transforman datos de entrada en estados cuánticos mediante evolución unitaria parametrizada, la reconstrucción tomográfica de estos estados de salida proporciona una referencia verificable para validar que el circuito aprendió la representación de datos prevista.",
    "C": "Caracteriza espacios de características cuánticos utilizando tomografía variacional para reconstruir las matrices de densidad de puntos de datos después de la codificación a través del circuito de mapeo de características. El aprendizaje automático cuántico se basa en la incrustación de datos clásicos en espacios de Hilbert de alta dimensión donde los núcleos cuánticos calculan productos internos, pero comprender qué estructura geométrica crea realmente esta incrustación requiere tomografía de estado.",
    "D": "Todas las anteriores",
    "solution": "D"
  },
  {
    "id": 98,
    "question": "¿Qué algoritmo cuántico constituye la base de muchas aceleraciones en aprendizaje automático cuántico?",
    "A": "El algoritmo de búsqueda no estructurada de Grover proporciona la primitiva computacional fundamental para la aceleración del aprendizaje automático cuántico al permitir una aceleración cuadrática en la búsqueda a través de espacios de hipótesis durante el entrenamiento del modelo.",
    "B": "El algoritmo de factorización de enteros de Shor, aunque conocido principalmente por sus aplicaciones criptanalíticas para romper el cifrado RSA, en realidad sirve como el motor computacional subyacente para las aceleraciones del aprendizaje automático cuántico a través de su implementación eficiente de exponenciación modular y subrutinas de búsqueda de períodos que pueden reutilizarse para calcular transformadas discretas de Fourier sobre grupos cíclicos.",
    "C": "El algoritmo de Estimación de Fase Cuántica sirve como el algoritmo fundamental que subyace a muchas aceleraciones en aprendizaje automático cuántico al permitir la extracción eficiente de información de valores propios de operadores unitarios, lo que respalda directamente el análisis de componentes principales cuántico (qPCA) para la reducción de dimensionalidad, impulsa el algoritmo HHL para resolver sistemas lineales que aparecen en tareas de regresión y optimización, facilita evaluaciones de núcleo de máquinas de vectores de soporte cuánticas mediante estimación eficiente de amplitud de productos internos, y habilita solucionadores variacionales de valores propios cuánticos utilizados en el entrenamiento de redes neuronales cuánticas — esta primitiva algorítmica logra ventaja exponencial al codificar valores propios en retroceso de fase cuántica con complejidad de puertas polinomial O(log N), permitiendo que los protocolos QML procesen eficientemente estructuras de datos de alta dimensión codificadas en espacios de amplitud cuántica donde los algoritmos clásicos requieren recursos exponenciales.",
    "D": "El algoritmo de Transformada Cuántica de Fourier constituye la subrutina central que permite aceleraciones cuánticas en aplicaciones de aprendizaje automático, particularmente a través de su capacidad para calcular la transformada discreta de Fourier de un estado cuántico en O(log²N) operaciones de puertas.",
    "solution": "C"
  },
  {
    "id": 99,
    "question": "¿Qué vulnerabilidad sofisticada existe en las implementaciones de distribución de clave cuántica de variables continuas?",
    "A": "Los adversarios pueden saturar deliberadamente los fotodiodos detectores homodinos enviando pulsos brillantes intensos intercalados con estados de señal legítimos de CV-QKD, forzando al detector a operar en regímenes no lineales donde la fotocorriente medida ya no escala linealmente con la potencia óptica incidente. Durante eventos de saturación, los resultados de medición de cuadratura se comprimen y distorsionan de formas predecibles.",
    "B": "Un adversario puede explotar los desajustes temporales entre las ventanas de medición de cuadratura en las estaciones de Alice y Bob inyectando señales interferentes desfasadas que llegan durante breves períodos de transición cuando la fase del oscilador local del detector homodino está cambiando entre mediciones X y P. Estos ataques de desincronización hacen que los valores de cuadratura medidos representen combinaciones lineales de ambos observables.",
    "C": "El procedimiento de calibración de ruido de disparo se basa en medir fluctuaciones cuánticas del vacío cuando no hay señal presente, pero un atacante con control parcial del canal puede inyectar estados coherentes débiles sincronizados temporalmente con las ventanas de calibración para inflar artificialmente la línea base de ruido de disparo medida. Al manipular este nivel de referencia hacia arriba, Eve puede introducir ruido de escucha correspondentemente mayor durante la generación de claves.",
    "D": "La manipulación del oscilador local mediante inyección externa sintonizada por longitud de onda permite a un adversario sustituir sin problemas el haz del oscilador local legítimo en el receptor de Bob con un estado coherente controlado por el atacante que comparte idéntica estructura de modo espacial y temporal pero porta una referencia de fase sutilmente modificada, rotando así la base de medición de una manera que permanece indetectable mediante procedimientos de calibración estándar pero sesga sistemáticamente los resultados de cuadratura medidos hacia valores correlacionados con la información interceptada del atacante sobre los estados transmitidos de Alice. Este ataque tiene éxito porque las pruebas de seguridad de CV-QKD asumen que el oscilador local define una referencia de fase confiable, pero cuando Eve controla esta referencia mediante ataques de inyección selectiva por longitud de onda que explotan el filtrado óptico insuficiente en la estación de Bob, puede diseñar resultados de medición que filtran información parcial de la clave mientras mantienen estadísticas limitadas por ruido de disparo que pasan todas las verificaciones de seguridad convencionales, incluyendo monitoreo de ruido excesivo y pruebas de verificación de balance homodino.",
    "solution": "D"
  },
  {
    "id": 100,
    "question": "En una GAN cuántica, el discriminador se implementa a menudo como un circuito variacional porque este diseño:",
    "A": "Permite la optimización conjunta con el generador dentro de la misma sesión de hardware cuántico, habilitando que ambas redes se entrenen en un solo procesador cuántico mediante actualizaciones de parámetros alternadas que aprovechan recursos de medición compartidos y evitan la sobrecarga de cambiar entre diferentes arquitecturas de circuito.",
    "B": "Habilita entrenamiento adversarial mediante retropropagación inducida por medición donde la salida del discriminador, codificada como un valor esperado de observable, proporciona señales de gradiente continuas a ambas redes al explotar la regla de desplazamiento de parámetros, permitiendo la optimización simultánea de parámetros del generador y discriminador mediante mediciones intercaladas.",
    "C": "Proporciona expresividad entrenable mediante unitarios parametrizados que pueden aproximar límites de decisión arbitrarios en el espacio de Hilbert, permitiendo que el discriminador aprenda distribuciones complejas ajustando ángulos de rotación mediante descenso de gradiente mientras mantiene compatibilidad con el hardware mediante compilación de puertas nativas que preserva las estadísticas de medición.",
    "D": "Facilita ventaja cuántica al codificar la función de decisión del discriminador como estados entrelazados cuyos resultados de medición calculan inherentemente distancias de núcleo entre distribuciones reales y generadas, aprovechando la interferencia cuántica para realizar comparaciones implícitas en el espacio de características que requerirían recursos clásicos exponenciales para evaluar explícitamente.",
    "solution": "A"
  },
  {
    "id": 101,
    "question": "¿Cuál de las siguientes opciones describe mejor la relación entre la profundidad del circuito y la expresividad en las redes neuronales cuánticas?",
    "A": "La expresividad en las redes neuronales cuánticas está fundamentalmente determinada por el número total de parámetros variacionales en lugar de la profundidad del circuito, siguiendo una ley de escalado análoga al ancho de las redes neuronales clásicas. Un circuito poco profundo con suficientes puertas parametrizadas puede aproximar cualquier transformación unitaria sobre el espacio de qubits con precisión arbitraria, mientras que aumentar la profundidad sin añadir parámetros simplemente crea rotaciones redundantes que abarcan el mismo subespacio del grupo unitario completo, sin contribuir nada a la capacidad representacional del modelo.",
    "B": "Los circuitos más profundos son invariablemente más expresivos debido a su capacidad para generar estructuras de entrelazamiento cada vez más complejas y explorar volúmenes mayores del grupo unitario, pero esta expresividad mejorada tiene el costo catastrófico de gradientes que se desvanecen exponencialmente, conocidos como mesetas áridas. Más allá de un umbral crítico de profundidad que escala logarítmicamente con el número de qubits, la probabilidad de encontrar configuraciones de parámetros con gradientes no despreciables disminuye exponencialmente, haciendo que la expresividad adicional sea completamente inaccesible para la optimización basada en gradientes, independientemente del algoritmo de entrenamiento empleado.",
    "C": "Los circuitos poco profundos con buena estructura de entrelazamiento y diseño de ansatz apropiado pueden lograr alta expresividad, a menudo igualando o superando la capacidad representacional de circuitos mucho más profundos mientras evitan problemas de entrenabilidad.",
    "D": "La profundidad del circuito tiene un impacto mínimo en la expresividad en comparación con el número de qubits, porque la dimensión del espacio de Hilbert crece como 2^n donde n es el número de qubits.",
    "solution": "C"
  },
  {
    "id": 102,
    "question": "En arquitecturas de lectura dispersiva, un único resonador de bus puede medir la paridad de múltiples qubits explotando qué mecanismo?",
    "A": "Los desplazamientos cross-Kerr de estados conjuntos de qubits mapean la paridad en la fase acumulada del resonador, permitiendo una medición indirecta donde el acoplamiento dispersivo colectivo traduce paridad par versus impar en desplazamientos distinguibles de frecuencia de la cavidad que pueden leerse mediante detección homodina sin medición directa de qubits.",
    "B": "Los desplazamientos dispersivos conjuntos de correlaciones multi-qubit crean perturbaciones de frecuencia de cavidad dependientes de la paridad que se acumulan durante el tiempo de integración del pulso de lectura, pero el mecanismo requiere extracción secuencial de síndrome mediante sondeo del resonador multiplexado en el tiempo donde la contribución de cada qubit aparece como un componente de frecuencia separable, emergiendo la paridad del patrón de batido entre estos componentes en lugar de un único desplazamiento colectivo.",
    "C": "El acoplamiento dispersivo colectivo genera desplazamientos de número de fotones dependientes de la paridad en el resonador a través de la suma de términos Kerr individuales de qubits, donde la fase del resonador se acumula proporcionalmente al XOR de los estados de qubits. Sin embargo, esto requiere conducir activamente la cavidad al régimen de estados de Fock donde la detección que resuelve el número de fotones extrae la paridad directamente de la población fotónica de valor entero en lugar de las cuadraturas homodinas continuas.",
    "D": "Las interacciones cross-Kerr multi-qubit crean desplazamientos de frecuencia dependientes de la paridad que se mapean en la acumulación de fase del resonador durante la integración homodina, pero el signo del desplazamiento dispersivo alterna con la paridad del número de qubits en lugar de ser colectivo. Esto significa que los estados de paridad impar y par producen desplazamientos de fase de signo opuesto respecto a la frecuencia desnuda de la cavidad, requiriendo mediciones de referencia calibradas para distinguir el valor de paridad de las contribuciones individuales de qubits.",
    "solution": "A"
  },
  {
    "id": 103,
    "question": "¿Qué característica de los recocedores cuánticos adiabáticos los hace susceptibles a la fuga de información por congelamiento?",
    "A": "El congelamiento temprano hace que las poblaciones de qubits se bloqueen en estados propios de energía que reflejan la estructura del problema a través de corrientes persistentes integradas en el tiempo. Cuando el recocido se completa antes de la equilibración térmica, los qubits se asientan en configuraciones metaestables determinadas por la topología del paisaje energético. Estos estados de flujo congelados generan campos magnéticos cuasi-estáticos cuyos patrones espaciales codifican la matriz de acoplamiento de Ising a través de efectos de inductancia mutua, creando firmas de campo cercano medibles que persisten después del recocido y pueden detectarse mediante magnetometría sensible.",
    "B": "El congelamiento temprano codifica los sesgos del hamiltoniano del problema en corrientes persistentes legibles mediante bucles de captura SQUID. Cuando el programa de recocido se completa antes de alcanzar el verdadero equilibrio térmico, los qubits se congelan en configuraciones metaestables que reflejan la estructura del paisaje energético. Estos patrones de corriente persistente generan firmas de flujo magnético medibles que filtran información sobre la instancia del problema y potencialmente la trayectoria de la solución a través de canales laterales electromagnéticos.",
    "C": "Las transiciones diabáticas durante el recocido rápido inducen eventos de tunelización Landau-Zener que se correlacionan con la topología de frustración del hamiltoniano del problema, generando espectros de emisión electromagnética característicos. Cuando la velocidad de recocido excede las condiciones adiabáticas, los qubits experimentan transiciones no adiabáticas en cruces evitados cuyas ubicaciones codifican las fuerzas de acoplamiento. Estas transiciones producen corrientes oscilantes transitorias con frecuencias proporcionales a las brechas de energía, emitiendo firmas de radiofrecuencia que revelan la estructura del problema mediante análisis de Fourier del espectro de emisión capturado durante el recocido.",
    "D": "La brecha de energía finita durante el recocido intermedio requiere conducción continua de microondas para suprimir excitaciones térmicas, creando patrones de corriente modulados que codifican parámetros de Ising. A medida que el campo transversal disminuye, la brecha de energía instantánea se estrecha exponencialmente, requiriendo pulsos de estabilización dinámica cuyas amplitudes deben seguir la estructura de brecha específica del problema. Estas corrientes de compensación fluyen a través de líneas de sesgo de flujo con magnitudes proporcionales a las intensidades de campo locales, generando firmas magnéticas variables en el tiempo cuya densidad espectral de potencia revela directamente el hamiltoniano de Ising embebido a través de picos de resonancia característicos.",
    "solution": "B"
  },
  {
    "id": 104,
    "question": "En los protocolos de enrutamiento de redes cuánticas, el entrelazamiento entre nodos crea grafos de dependencia complejos que teóricamente podrían conducir a bucles de enrutamiento donde la información cuántica cicla indefinidamente sin alcanzar su destino. ¿Qué previene que los bucles de entrelazamiento causen estados inconsistentes en el enrutamiento?",
    "A": "La evitación de bucles en la selección de rutas basada en algoritmos de grafos asegura que las operaciones de intercambio de entrelazamiento se secuencien según árboles de enrutamiento acíclicos construidos a partir de la topología de la red, previniendo que se formen ciclos en primer lugar. Los protocolos de control clásicos rastrean qué pares de nodos comparten entrelazamiento y calculan árboles de expansión o rutas más cortas que garantizan progreso monótono hacia el destino. Dado que la teletransportación cuántica consume los pares entrelazados usados en cada salto, los enlaces previamente recorridos no pueden reutilizarse, previniendo inherentemente que la información cuántica revisite nodos y cree superposiciones inconsistentes sobre rutas cerradas en la red.",
    "B": "Las restricciones de monogamia del entrelazamiento imponen que cada qubit participe en como máximo un par máximamente entrelazado en cualquier momento, lo que significa que las operaciones de intercambio que crearían bucles fallan automáticamente porque los pares de Bell requeridos no pueden coexistir con enlaces previamente establecidos. Cuando un protocolo de enrutamiento intenta crear un ciclo intercambiando entrelazamiento de nodo A→B→C→A, la tercera operación de intercambio no puede tener éxito porque el qubit del nodo A ya está máximamente entrelazado con el nodo B, violando la desigualdad de monogamia. Esta restricción fundamental de teoría de información cuántica actúa como un mecanismo de prevención física, causando que las operaciones creadoras de bucles decoheran en lugar de establecer el estado inconsistente, por lo que los protocolos de enrutamiento evitan naturalmente ciclos a través de la estructura de las correlaciones cuánticas.",
    "C": "La detección de errores cuánticos distribuida a través de nodos de red implementa un protocolo de detección de bucles basado en síndrome donde cada intercambio de entrelazamiento codifica información de paridad en qubits auxiliares que señalan dependencias cíclicas mediante mediciones de estabilizador. Cuando las rutas de enrutamiento forman bucles topológicos, la fase acumulada de mediciones de Bell alrededor del ciclo produce síndromes no triviales en el espacio de estabilizador que activan la terminación automática de rutas antes de que se propaguen estados inconsistentes. Esta sobrecarga de detección de errores añade un qubit auxiliar por enlace de red pero proporciona detección de bucles en tiempo real con procesamiento clásico polinomial, asegurando corrección de enrutamiento al abortar secuencias de intercambio cuando los patrones de síndrome indican que el siguiente paso de teletransportación cerraría un ciclo en el grafo de entrelazamiento.",
    "D": "El ordenamiento temporal de mediciones de Bell asegura causalidad al requerir que cada nodo complete su medición local y comunicación clásica antes de que puedan proceder intercambios subsecuentes, creando un orden parcial sobre operaciones de intercambio que inherentemente previene curvas cerradas tipo tiempo en la ejecución del protocolo. Cuando el enrutamiento intenta crear un bucle, la latencia de comunicación clásica de saltos anteriores retrasa intercambios posteriores de tal manera que para cuando podría ejecutarse una operación de cierre de bucle, los estados cuánticos del origen del bucle ya han decoerado más allá del tiempo de coherencia de la red, rompiendo naturalmente ciclos potenciales. Esto combina restricciones de causalidad relativista con escalas de tiempo de decoherencia para garantizar enrutamiento acíclico a través de la estructura del espaciotiempo en lugar de requerir algoritmos explícitos de detección de bucles.",
    "solution": "A"
  },
  {
    "id": 105,
    "question": "¿Por qué los errores de inversión de bit (X) y de inversión de fase (Z) interactúan para producir errores más complejos en la computación cuántica?",
    "A": "La composición de errores de inversión de bit e inversión de fase produce una superposición no conmutativa de operadores de error cuyo efecto neto depende de la orientación instantánea del vector de Bloch del estado cuántico en el momento en que cada error golpea. Debido a que el colapso de medición cuántica ocurre probabilísticamente y la trayectoria del sistema a través del espacio de estados exhibe sensibilidad caótica a las condiciones iniciales, el error combinado se manifiesta como un paseo aleatorio esencialmente aleatorio a través de la esfera de Bloch, haciendo computacionalmente intratable predecir o modelar el impacto del error conjunto sin realizar exponencialmente muchas simulaciones de trayectoria.",
    "B": "Los errores de inversión de fase se originan de imperfecciones de control coherente y descalibraciones sistemáticas en las líneas de conducción de qubits, haciéndolos deterministas y corregibles mediante protocolos de calibración mejorados, mientras que los errores de inversión de bit surgen de decoherencia ambiental estocástica y excitaciones térmicas, haciéndolos fundamentalmente aleatorios.",
    "C": "Un error de inversión de fase modifica las fases relativas entre estados de la base computacional, lo que altera directamente las probabilidades de resultado de medición para estados de superposición pero deja sin cambiar los elementos diagonales de la matriz de densidad. Cuando ocurre posteriormente un error de inversión de bit, su firma en la medición de síndrome queda oscurecida porque la corrupción de fase previa ha rotado la base de medición, haciendo imposible que las mediciones de estabilizador estándar distingan entre una inversión de bit pura y el error combinado.",
    "D": "Cuando ocurren secuencialmente un error de inversión de bit (X) y un error de inversión de fase (Z) sobre el mismo qubit, su efecto combinado produce un error Y porque los operadores de Pauli satisfacen la relación algebraica iY = XZ. Esta interacción surge de la estructura de multiplicación no conmutativa del grupo de Pauli, donde aplicar tanto X como Z introduce un factor de fase complejo adicional que transforma el error en un operador de Pauli completamente diferente con efectos físicos distintos sobre el estado cuántico.",
    "solution": "D"
  },
  {
    "id": 106,
    "question": "¿Qué le sucede a un estado de superposición arbitrario bajo la acción de una puerta CNOT (NOT controlado)?",
    "A": "La CNOT aplica un volteo de bit condicional que preserva la superposición cuando el control está en un estado definido |0⟩ o |1⟩, pero induce decoherencia cuando el control existe en una superposición coherente, porque la acción de la puerta crea un efecto Zeno cuántico donde el monitoreo continuo del estado lógico del qubit de control congela su evolución. Esta retro-acción del monitoreo colapsa el control en un autoestado mientras el objetivo experimenta su volteo condicional, dejando el sistema conjunto en un estado mixto en lugar de una superposición entrelazada pura.",
    "B": "Puede crearse entrelazamiento entre los qubits, particularmente cuando el qubit de control está en superposición y el objetivo está en un estado base definido. La CNOT aplica un volteo condicional que correlaciona los estados de los dos qubits, produciendo un estado conjunto que no puede factorizarse en estados independientes de un solo qubit, generando así correlaciones cuánticas que violan la separabilidad clásica.",
    "C": "La puerta implementa una operación de paridad controlada que mapea estados de la base computacional según |c⟩|t⟩ → |c⟩|t ⊕ c⟩, donde ⊕ denota suma módulo 2, pero esta lógica de paridad inherentemente rompe la coherencia de fase entre los componentes de superposición porque las operaciones XOR son irreversibles desde la perspectiva de la información de fase cuántica. Aunque las amplitudes se redistribuyen correctamente, las fases relativas entre los componentes |00⟩, |01⟩, |10⟩ y |11⟩ se desorganizan por la restricción de paridad, convirtiendo la superposición coherente de entrada en una mezcla estadística con elementos de matriz de densidad fuera de la diagonal perdidos.",
    "D": "El sistema experimenta una rotación dependiente de la base donde el vector de Bloch del qubit objetivo precesa alrededor de un eje determinado por la proyección del vector de estado del qubit de control sobre la base de autovectores de Pauli-Z, con el ángulo de precesión proporcional a la amplitud |1⟩ del control. Esto crea una acumulación de fase geométrica continua que interpola suavemente entre identidad (cuando el control es |0⟩) y volteo de bit (cuando el control es |1⟩), implementando efectivamente una rotación controlada que preserva toda la información cuántica mientras transforma condicionalmente el objetivo basándose en mediciones parciales de la matriz de densidad del control.",
    "solution": "B"
  },
  {
    "id": 107,
    "question": "¿Qué propiedad cuántica fundamental explota la Internet Cuántica que las redes clásicas no pueden explotar?",
    "A": "Las correlaciones no locales de estados entrelazados distribuidos que permiten la violación de las desigualdades de Bell entre nodos de la red, lo cual permite la verificación independiente del dispositivo de la integridad del canal cuántico y la implementación de protocolos QKD independientes del dispositivo de medición donde la seguridad se deriva puramente de las estadísticas de correlación observadas sin confiar en el hardware intermedio de los repetidores, proporcionando garantías criptográficas que los canales autenticados clásicos no pueden lograr.",
    "B": "La coherencia cuántica mantenida a través de nodos distribuidos mediante secuencias continuas de desacoplamiento dinámico aplicadas en conmutadores de enrutamiento, permitiendo la amplificación de señales cuánticas que preserva la fase mediante técnicas de amplificación lineal sin ruido que evaden el teorema de no clonación al amplificar probabilísticamente los componentes de señal mientras se post-selecciona en eventos de amplificación exitosos, logrando la transferencia de estado cuántico a larga distancia sin decoherencia.",
    "C": "La distribución de estados cuánticos entrelazados entre nodos distantes, habilitando protocolos de teletransportación para la transferencia de información cuántica y distribución de claves criptográficas incondicionalmente seguras mediante correlaciones que no tienen análogo clásico y no pueden ser interceptadas sin detección.",
    "D": "La contextualidad cuántica en protocolos de enrutamiento multi-qubit donde los resultados de medición en los conmutadores de red dependen de la configuración global de elecciones de base en todos los nodos, permitiendo el reenvío de paquetes dependiente del contexto que logra una eficiencia de enrutamiento óptima demostrable para ciertas topologías de red al explotar correlaciones tipo Kochen-Specker que satisfacen protocolos de enrutamiento clásico no contextual no pueden lograr, como lo demuestran las violaciones de desigualdades de enrutamiento análogas a los límites CHSH.",
    "solution": "C"
  },
  {
    "id": 108,
    "question": "¿Qué vulnerabilidad específica existe en los procedimientos de reinicio para qubits superconductores?",
    "A": "Cuasipartículas fuera del equilibrio que persisten después de que se completa el pulso de reinicio, las cuales pueden hacer túnel a través de las uniones y causar excitaciones espurias en operaciones subsecuentes. Estas cuasipartículas, generadas durante operaciones de medición o compuertas, tienen escalas temporales de relajación que pueden exceder el tiempo de coherencia del qubit mismo, creando un fondo de eventos de excitación estocásticos que corrompen la fidelidad del reinicio incluso cuando el protocolo de reinicio nominalmente logra >99% de población en el estado fundamental. El efecto es particularmente pronunciado en dispositivos con energías de brecha superconductora pequeñas o números de fotones ambientales elevados.",
    "B": "La persistencia de excitación térmica ocurre cuando el calor residual de operaciones disipativas durante mediciones o protocolos de reinicio activo no logra termalizarse lo suficientemente rápido a través del poder de enfriamiento limitado del refrigerador de dilución, manteniendo el qubit y su entorno electromagnético a temperaturas efectivas significativamente por encima de la temperatura base. Esta población térmica elevada se manifiesta como una ocupación de estado cuasi-estacionaria de estados excitados que no puede eliminarse mediante pulsos de reinicio estándar, requiriendo tiempos de espera de cientos de microsegundos para termalización pasiva o esquemas de enfriamiento activo más complejos que involucran modos auxiliares para extraer entropía del subespacio computacional.",
    "C": "El calentamiento inducido por medición surge de la energía disipada durante la lectura proyectiva, donde los fotones que se fugan del resonador de medición depositan energía tanto en el entorno electromagnético local del qubit como en el baño de fonones del sustrato más amplio.",
    "D": "La deriva de calibración del pulso de reinicio representa un desafío fundamental donde los parámetros óptimos para protocolos de reinicio condicional—incluyendo amplitudes de impulso, duraciones de pulso y desplazamientos de frecuencia—se desplazan con el tiempo debido a cambios ambientales, ruido de flujo en acopladores sintonizables y efectos de envejecimiento en la electrónica de control. Cuando los datos de calibración se vuelven obsoletos, las operaciones de reinicio pueden poblar inadvertidamente estados excitados superiores o no lograr despoblar completamente el primer estado excitado, con errores acumulándose a través de ejecuciones repetidas del circuito hasta que ocurra la recalibración.",
    "solution": "D"
  },
  {
    "id": 109,
    "question": "¿Cuál es un obstáculo clave para escalar métodos de corrección de errores cuánticos basados en aprendizaje automático?",
    "A": "Entrenar modelos que generalicen a través de diferentes topologías de qubits y patrones de conectividad es computacionalmente costoso e intensivo en datos, requiriendo simulación extensiva de diversos modelos de error y configuraciones de hardware para lograr un rendimiento robusto en múltiples plataformas de computación cuántica con restricciones arquitectónicas variables.",
    "B": "Los datos de síndrome exhiben correlaciones temporales debido a mediciones repetidas, violando la suposición i.i.d. que subyace al aprendizaje supervisado estándar. Los decodificadores ML entrenados en muestras de síndrome independientes fallan cuando se despliegan en circuitos tolerantes a fallos donde los errores de medición se propagan a través de la reutilización de ancillas, causando cambio de distribución entre entrenamiento (síndromes sintéticos de una sola ronda) y despliegue (flujos de datos correlacionados de múltiples rondas). Esta estructura de correlación crece con la profundidad de extracción de síndrome, degradando la precisión del decodificador en hardware real a pesar del alto rendimiento en conjuntos de prueba simulados.",
    "C": "El costo de entrenamiento escala exponencialmente con la distancia del código porque la dimensión del espacio de síndrome crece como 2^((n-k)) para n qubits físicos que codifican k qubits lógicos, requiriendo enumeración de todos los patrones de error posibles para lograr cobertura completa. Para códigos de superficie a distancia d=5 (n=49, k=1), esto produce ~10^14 clases de síndrome distintas que deben aparecer suficientemente en los datos de entrenamiento. La simulación clásica de generación de síndrome se vuelve intratable más allá de d=7, creando un cuello de botella de datos donde los modelos no pueden entrenarse adecuadamente para las distancias de código (d≥15) necesarias para tolerancia a fallos práctica.",
    "D": "El descenso de gradiente por lotes en síndromes de error cuántico encuentra gradientes que se desvanecen debido a mesetas estériles en el paisaje de pérdida del decodificador, que emergen porque los resultados de medición de síndrome son observables altamente entrelazados que exhiben distribuciones exponencialmente concentradas. La magnitud del gradiente escala como O(1/2^n) para códigos de n qubits, haciendo el entrenamiento basado en retropropagación inviable más allá de ~10 qubits físicos. Esta limitación fundamental surge de los mismos fenómenos de concentración cuántica que afectan a los solucionadores cuánticos variacionales de autovalores, requiriendo métodos de optimización alternativos como estrategias evolutivas que evitan por completo el cálculo de gradientes.",
    "solution": "A"
  },
  {
    "id": 110,
    "question": "¿Cuál es el objetivo principal del corte de circuitos en un sistema cuántico distribuido?",
    "A": "Particiona un circuito cuántico grande en subcircuitos más pequeños que pueden caber cada uno en módulos de hardware separados con conectividad limitada, permitiendo la ejecución a través de dispositivos donde el entrelazamiento directo entre módulos tiene pérdidas prohibitivas. La técnica reconstruye la salida completa del circuito ejecutando los subcircuitos independientemente con mediciones adicionales en las ubicaciones de corte y post-procesando clásicamente sus resultados con ponderaciones de cuasi-probabilidad apropiadas derivadas de la base tomográfica del corte.",
    "B": "Particiona un circuito cuántico grande en capas más pequeñas divididas en el tiempo que pueden caber cada una dentro de la ventana de coherencia de los módulos de hardware disponibles, permitiendo la ejecución secuencial a través de múltiples ciclos de actualización donde los qubits se reinician periódicamente al estado fundamental. La técnica reconstruye la salida completa del circuito ejecutando las capas divididas en el tiempo independientemente con transferencia de estado entre capas y post-procesando clásicamente sus resultados de medición con correcciones de fase apropiadas que contabilizan la desintegración T1.",
    "C": "Particiona un circuito cuántico grande en subcircuitos más pequeños que pueden caber cada uno en módulos de hardware separados con conectividad directa de qubits limitada o nula entre ellos, permitiendo la ejecución distribuida a través de múltiples procesadores cuánticos. La técnica reconstruye la salida completa del circuito ejecutando los subcircuitos independientemente y post-procesando clásicamente sus resultados de medición con ponderaciones de cuasi-probabilidad apropiadas.",
    "D": "Particiona un circuito cuántico grande en subcircuitos más pequeños descomponiendo operaciones de entrelazamiento globales en unitarios locales conectados a través de qubits ancilla compartidos, permitiendo la ejecución distribuida donde las mediciones de ancilla median las interacciones entre módulos físicamente separados. La técnica reconstruye la salida completa del circuito ejecutando los subcircuitos independientemente con feed-forward a mitad del circuito y post-procesando clásicamente sus resultados con correcciones apropiadas del marco de Pauli derivadas de las estadísticas de medición.",
    "solution": "C"
  },
  {
    "id": 111,
    "question": "La extrapolación de ruido cero es más beneficiosa en la fase de entrenamiento porque:",
    "A": "Permite la estimación de gradiente por lotes a través de múltiples niveles de ruido simultáneamente, donde la ejecución paralela de circuitos escalados en ruido proporciona muestras estadísticamente independientes que reducen la varianza en las estimaciones de la función de coste mediante extrapolación de Richardson, permitiendo que los optimizadores logren tasas de convergencia cuadráticamente más rápidas al explotar la correlación estructurada entre resultados de medición escalados en ruido para construir estimadores de gradiente de menor varianza.",
    "B": "Mitiga los errores de puerta sin cambiar la estructura del circuito variacional, permitiendo que el optimizador aprenda parámetros basándose en evaluaciones de la función de coste mitigadas en ruido que aproximan mejor el objetivo ideal sin ruido, mejorando así la convergencia hacia soluciones óptimas sin requerir rediseño del circuito o recursos cuánticos adicionales.",
    "C": "Proporciona estimaciones de gradiente insesgadas al cancelar el sesgo sistemático inducido por ruido en la regla de desplazamiento de parámetros, donde la extrapolación a ruido cero elimina las contribuciones de error coherente que de otro modo causarían que el descenso de gradiente converja a mínimos locales espurios correspondientes a estados estabilizados por ruido en lugar de verdaderos estados fundamentales del hamiltoniano objetivo en solucionadores cuánticos variacionales de autovalores.",
    "D": "Extiende el tiempo de coherencia efectivo de circuitos variacionales mediante post-procesamiento de datos de medición para suprimir retroactivamente efectos de decoherencia, permitiendo que el entrenamiento proceda como si los tiempos de puerta fueran acortados por el orden de extrapolación, habilitando así que circuitos ansatz más profundos permanezcan entrenables al compensar la degradación de fidelidad limitada por T1/T2 mediante ajuste polinomial de valores esperados escalados en ruido.",
    "solution": "B"
  },
  {
    "id": 112,
    "question": "En el contexto del aprendizaje automático cuántico, considere un circuito cuántico variacional entrenado con datos sensibles donde los gradientes de parámetros se miden y reportan. El objetivo es garantizar que un adversario observando estos vectores de gradiente no pueda inferir detalles sobre muestras individuales de entrenamiento más allá de cierto umbral de privacidad ε. ¿Qué métrica cuantifica con mayor precisión la garantía de privacidad en marcos de privacidad diferencial cuántica para este escenario?",
    "A": "Calcular la fidelidad promedio F = ⟨ψ_D|ψ_D'⟩ entre estados cuánticos |ψ_D⟩ y |ψ_D'⟩ producidos por conjuntos de datos vecinos D y D' que difieren en una muestra, luego promediar esta fidelidad sobre todos los posibles resultados de medición durante la estimación del gradiente, produce la métrica de privacidad. Dado que la fidelidad mide la superposición entre estados cuánticos, mantener una fidelidad promedio cercana a la unidad (típicamente F ≥ 1-ε) garantiza que los circuitos cuánticos produzcan salidas casi indistinguibles para conjuntos de datos vecinos, lo que significa que adversarios observando resultados de medición no pueden determinar confiablemente qué conjunto de datos fue usado, proporcionando así garantías de ε-privacidad diferencial mediante similitud de estados.",
    "B": "Calcular la entropía de von Neumann S(ρ_out) = -Tr(ρ_out log ρ_out) de la matriz de densidad reducida obtenida al trazar los qubits auxiliares usados durante la estimación de gradiente por desplazamiento de parámetros proporciona la cota de privacidad, porque la entropía cuantifica la mezcla de estados cuánticos y por tanto la incertidumbre que enfrenta un adversario sobre muestras individuales de entrenamiento. Cuando las mediciones de gradiente colapsan el estado, la entropía residual en la salida representa la información que permanece oculta del adversario, y mantener S(ρ_out) ≥ log(1/ε) garantiza ε-privacidad diferencial manteniendo el estado cuántico suficientemente mezclado.",
    "C": "La información cuántica de Fisher F_Q con respecto a los parámetros del circuito cuantifica directamente cuánta información sobre cada muestra de entrenamiento está codificada en las mediciones de gradiente, ya que F_Q determina la precisión última con la que los parámetros pueden estimarse a partir de estados cuánticos. Por la cota de Cramér-Rao, la información de Fisher inversa establece un límite inferior en la varianza de estimación, así que restringir F_Q ≤ 1/ε² garantiza que ningún adversario pueda extraer más de ε bits de información sobre muestras individuales de los vectores de gradiente, estableciendo así ε-privacidad diferencial mediante límites teórico-informacionales en la fuga de parámetros.",
    "D": "La distancia de traza entre distribuciones de salida cuando conjuntos de datos vecinos difieren en una muestra acota directamente la distinguibilidad que enfrenta un adversario y corresponde a la definición clásica de privacidad diferencial en el entorno cuántico.",
    "solution": "D"
  },
  {
    "id": 113,
    "question": "En teoría de complejidad cuántica, ¿qué representa la clase BQP?",
    "A": "Problemas de decisión resolubles por una familia uniforme de circuitos cuánticos de tamaño polinomial con error bilateral acotado, donde 'uniforme' significa que una máquina de Turing clásica puede generar la descripción del circuito en tiempo polinomial en el tamaño de la entrada, garantizando que BQP capture solo problemas con verificación cuántica eficiente además de solución. La cota de error bilateral (al menos 2/3 de aceptación en instancias SÍ, como máximo 1/3 en instancias NO) puede amplificarse a error exponencialmente pequeño mediante repetición polinomial por la cota de Chernoff, haciendo BQP robusto bajo varios umbrales de probabilidad a diferencia de clases de error unilateral como NP.",
    "B": "Problemas de decisión resolubles por computadoras cuánticas en tiempo polinomial con probabilidad de error acotada, donde el algoritmo cuántico debe aceptar instancias válidas con probabilidad al menos 2/3 y rechazar instancias inválidas con probabilidad al menos 2/3, representando la clase de problemas que las computadoras cuánticas pueden resolver eficientemente con alta confianza.",
    "C": "Problemas decidibles por máquinas de Turing cuánticas en tiempo polinomial con error acotado como máximo 1/3 tanto en aceptación como en rechazo, donde la máquina debe detenerse dentro de p(n) pasos para algún polinomio p en entradas de longitud n, y la probabilidad de error se calcula sobre los resultados de medición cuántica después de la evolución del estado final. Esta definición asume el modelo estándar donde no se requieren mediciones intermedias y todo el cálculo ocurre mediante evolución unitaria seguida de una medición proyectiva final en qubits de salida designados, aunque esto es equivalente por el principio de medición diferida a modelos que permiten medición durante el circuito.",
    "D": "La clase de problemas con promesa resolubles por circuitos cuánticos con una brecha de distinción como máximo inversa-polinomial entre probabilidades de aceptación en instancias SÍ versus NO, lo que significa que para entradas en el conjunto SÍ la probabilidad de aceptación excede 2/3 mientras que para entradas NO permanece por debajo de 1/3, y esta brecha puede amplificarse pero solo a separación constante (no a error exponencialmente pequeño) porque la técnica cuántica de amplificación de amplitud mediante inversión de fase estilo Grover requiere conocer qué subespacio amplificar, lo cual en sí mismo requeriría resolver el problema—por lo tanto BQP inherentemente permite error residual que lo distingue de clases de complejidad exactas como EQP, que requiere error cero.",
    "solution": "B"
  },
  {
    "id": 114,
    "question": "¿Qué vulnerabilidad específica se explota en un ataque de laguna de lectura cuántica?",
    "A": "La no linealidad de amplificación de señal explota el hecho de que los amplificadores limitados cuánticamente usados en cadenas de lectura exhiben compresión de ganancia dependiendo de la intensidad de la señal de entrada, causando que el factor de amplificación varíe con el estado cuántico medido y creando sesgo en las probabilidades de resultados de medición. Un adversario puede preparar estados de entrada cerca de la región de transición donde los efectos no lineales son más fuertes para hacer que la fidelidad de lectura se vuelva dependiente del estado de maneras que violan supuestos estándar de prueba de seguridad.",
    "B": "Desajuste de eficiencia de detector entre bases de medición, donde el aparato de medición cuántica exhibe probabilidades de detección sistemáticamente diferentes dependiendo de qué base se selecciona para la medición. Esta asimetría permite que un espía obtenga información parcial sobre la elección de base de medición al observar la distribución estadística de eventos detectados versus no detectados, incluso sin acceder a los resultados de medición mismos.",
    "C": "La temporización de selección de base de medición explota la velocidad finita de conmutación entre diferentes bases de medición en sistemas cuánticos prácticos, apuntando al intervalo cuando las puertas de rotación de base aún se están aplicando. Durante esta ventana vulnerable, el estado cuántico puede proyectarse parcialmente sobre una base intermedia que combina características de ambas configuraciones previstas, causando que los resultados reflejen una medición híbrida que filtra más información de la que revelaría cualquiera de las bases puras sola.",
    "D": "Las vías de acoplamiento por resonancia cruzada explotan las interacciones ZZ siempre activas presentes en arquitecturas de transmon de frecuencia fija, donde acoplamientos mediados por resonador entre qubits crean canales de medición no deseados durante operaciones de lectura. Cuando se mide un qubit, el campo del resonador de lectura puede filtrarse a través de vías de resonancia cruzada hacia qubits vecinos, causando que sus estados decoheran parcialmente o roten dependiendo del resultado de la medición.",
    "solution": "B"
  },
  {
    "id": 115,
    "question": "¿Qué amenaza específica aborda el análisis de min-entropía cuántica en distribución cuántica de claves?",
    "A": "Al modelar las imperfecciones físicas en detectores, moduladores y componentes ópticos como fuentes de entropía, el análisis de min-entropía cuántica caracteriza el déficit de aleatoriedad introducido por firmas determinísticas de canal lateral como correlación de fluctuación temporal, artefactos de modulación de intensidad y deriva de polarización. Este presupuesto de entropía luego informa contramedidas como rutinas de calibración en tiempo real y filtrado adaptativo, garantizando que un adversario monitoreando emisiones electromagnéticas o retrodispersión óptica no pueda reconstruir bits de clave a partir de patrones específicos del dispositivo que no están capturados en la descripción abstracta del protocolo a nivel de qubit.",
    "B": "El análisis de min-entropía cuántica cuantifica directamente la fuga total de información hacia partes externas durante cada ronda del protocolo midiendo la distinguibilidad de estados cuánticos después de corrección de errores y amplificación de privacidad. Al acotar la información mutua entre la clave cruda y cualquier sistema adversarial mediante relaciones de incertidumbre entrópicas, proporciona un número concreto—expresado en bits—de cuánta ventaja ha obtenido un espía, lo que luego determina la longitud requerida del paso de amplificación de privacidad para comprimir esa fuga por debajo del umbral de seguridad.",
    "C": "El análisis optimiza las tasas de clave secreta equilibrando las tasas de error de bits cuánticos contra la sobrecarga de amplificación de privacidad, tratando la min-entropía como un parámetro ajustable que controla la relación de compresión aplicada a claves tamizadas, lo que permite a los diseñadores de protocolos maximizar el rendimiento seleccionando frecuencias de base y códigos de corrección de errores que se aproximan a límites de capacidad de Shannon bajo condiciones de canal realistas.",
    "D": "Estimación del conocimiento del espía mediante cotas entrópicas que cuantifican la información máxima que un adversario podría haber extraído de interacciones con el canal cuántico, teniendo en cuenta las tasas de error observadas y la estructura de bases de medición usadas durante la ejecución del protocolo, proporcionando así una cota superior de peor caso sobre el compromiso de la clave.",
    "solution": "D"
  },
  {
    "id": 116,
    "question": "¿Por qué son importantes los estados de vacío en este protocolo de distribución de claves cuánticas seguro contra canales laterales?",
    "A": "Los estados de vacío eliminan las fugas de canales laterales provenientes de la modulación de intensidad al proporcionar una referencia con exactamente cero fotones, permitiendo a Alice y Bob detectar manipulaciones a través de estadísticas de número de fotones. Sin embargo, la información de fase permanece codificada en la estructura de modos del campo electromagnético del vacío, requiriendo pasos adicionales de purificación para evitar que los adversarios exploten eficiencias de detector dependientes de fase que varían con los ajustes del oscilador local a través de diferentes modos temporales.",
    "B": "Los estados de vacío sirven como una línea base de referencia segura que no presenta fugas de canales laterales a través de modulación de intensidad o deriva de fase, ya que contienen cero fotones y por lo tanto no pueden revelar inadvertidamente información a través de artefactos de medición o variaciones de respuesta del detector que los adversarios podrían explotar.",
    "C": "Los estados de vacío suprimen las fugas de información de canales laterales al desacoplar el grado de libertad del número de fotones de la elección de base codificada, previniendo ataques de modulación de intensidad. Su contenido de cero fotones asegura que los efectos de tiempo muerto del detector y el efecto de pospulso, que escalan con el flujo de fotones incidente, no puedan correlacionarse con las asignaciones de bits de Alice. Sin embargo, los pulsos de vacío aún portan firmas de modo electromagnético distinguibles a través de su tiempo de coherencia y distribución espectral, que pueden filtrar información si Eve realiza detección homodina con suficiente potencia de oscilador local para resolver fluctuaciones de cuadratura por debajo del ruido de disparo.",
    "D": "Los estados de vacío proporcionan una línea base independiente del número de fotones que elimina los canales laterales basados en intensidad al obligar a Eve a medir fluctuaciones cuánticas en lugar de variaciones de amplitud clásicas. La ventaja clave es que las correlaciones del campo de vacío con pulsos de señal subsecuentes crean un mecanismo de autenticación basado en entrelazamiento: el modulador de Alice imprime relaciones de fase entre el vacío y los modos de estado coherente que Bob verifica a través de mediciones de visibilidad interferométrica, y dado que estas correlaciones de fase sobreviven a las pérdidas de transmisión mientras permanecen invisibles a los ataques de conteo de fotones, ofrecen seguridad incondicional contra estrategias de intercepción y reenvío sin requerir protocolos de estados señuelo.",
    "solution": "B"
  },
  {
    "id": 117,
    "question": "¿Por qué la latencia de enrutamiento es tan crítica como la fidelidad en tareas cuánticas sensibles al tiempo?",
    "A": "La decoherencia degrada continuamente los estados cuánticos durante cualquier retraso, por lo que una latencia de enrutamiento excesiva permite que la calidad del entrelazamiento se deteriore antes de que los qubits puedan ser medidos u operados. En protocolos sensibles al tiempo como la distribución de claves cuánticas o la computación cuántica distribuida, incluso retrasos modestos pueden causar que la decoherencia acumulada empuje las tasas de error más allá de umbrales corregibles, haciendo que el enrutamiento rápido sea esencial para preservar la información cuántica a lo largo de la ejecución del protocolo.",
    "B": "La sincronización de redes cuánticas requiere intercambio de marcas de tiempo clásicas para establecer el orden de causalidad para mediciones separadas de tipo espacio, y los retrasos de enrutamiento introducen deriva de reloj que excede las ventanas de violación de desigualdades de Bell. En protocolos sensibles al tiempo como la distribución de claves cuánticas independiente del dispositivo, la desincronización inducida por latencia causa desalineación temporal entre eventos del detector, colapsando las estadísticas de conteo de coincidencias por debajo de los umbrales de seguridad y previniendo la verificación de correlaciones cuánticas esenciales para la generación certificada de aleatoriedad.",
    "C": "Los algoritmos cuánticos distribuidos emplean secuencias de medición deterministas con resultados comunicados clásicamente que desencadenan operaciones de puerta subsecuentes, y la latencia de enrutamiento extiende directamente el tiempo total de ejecución del protocolo al retrasar las señales de control de propagación. En aplicaciones sensibles al tiempo como los solucionadores variacionales de valores propios cuánticos a través de procesadores en red, los retrasos acumulados de comunicación entre ciclos de actualización de parámetros causan ralentización de convergencia del optimizador, aumentando el tiempo total de reloj hasta que las ventanas de ejecución de circuito limitadas por decoherencia expiran antes de que la optimización se complete.",
    "D": "Los protocolos de distribución de entrelazamiento generan pares de fotones entrelazados en intervalos temporales donde la distinguibilidad de modo temporal depende de que la sincronización de enrutamiento mantenga correlaciones de tiempo de llegada dentro de los tiempos de coherencia. Una latencia de enrutamiento excesiva introduce desigualdades de longitud de trayectoria que destruyen la indistinguibilidad temporal entre intervalos temporales tempranos/tardíos, causando fuga de información de camino que colapsa la visibilidad de interferencia en mediciones Hong-Ou-Mandel, degradando la fidelidad del entrelazamiento por debajo de umbrales específicos del protocolo requeridos para seguridad de comunicación cuántica o ventaja computacional.",
    "solution": "A"
  },
  {
    "id": 118,
    "question": "¿Cómo se unen diferentes subcircuitos después del corte?",
    "A": "El posprocesamiento clásico reconstruye el observable global combinando las estadísticas de medición de cada fragmento de subcircuito usando distribuciones cuasi-probabilísticas ponderadas, remuestreando efectivamente el valor de expectación global sin reconectar físicamente los circuitos.",
    "B": "El posprocesamiento clásico reconstruye el observable global combinando las estadísticas de medición de cada fragmento de subcircuito usando distribuciones de probabilidad ponderadas derivadas del isomorfismo de Choi-Jamiołkowski, remuestreando efectivamente el valor de expectación global al tratar cada fragmento como implementando un canal cuántico cuya acción puede invertirse a través de correcciones de muestreo clásicas.",
    "C": "El posprocesamiento clásico reconstruye el observable global combinando las estadísticas de medición de cada fragmento de subcircuito usando distribuciones cuasi-probabilísticas ponderadas derivadas de protocolos de teletransportación, donde los pesos negativos surgen naturalmente de la base sobrecompleta utilizada para representar canales cuánticos cortados. Este procedimiento de remuestreo recupera el valor de expectación completo aplicando muestreo de importancia clásico que corrige los sesgos inducidos por la descomposición.",
    "D": "El posprocesamiento clásico reconstruye el observable global combinando las estadísticas de medición de cada fragmento de subcircuito usando distribuciones cuasi-clásicas ponderadas obtenidas al insertar resoluciones de identidad en las ubicaciones de corte, remuestreando efectivamente el valor de expectación global al marginalizar sobre los resultados de medición intermedios que habrían conectado los fragmentos en el circuito original.",
    "solution": "A"
  },
  {
    "id": 119,
    "question": "Considere un algoritmo cuántico diseñado para resolver ecuaciones diferenciales parciales dependientes del tiempo usando simulación hamiltoniana. El algoritmo codifica la discretización espacial de la EDP como un operador matricial que evoluciona el estado cuántico. Durante el desarrollo del algoritmo, necesita verificar que los errores acumulados no causen divergencia de la simulación, particularmente cuando el operador de evolución se construye a partir de múltiples subrutinas codificadas en bloques. ¿Por qué es la norma logarítmica (log-norm) una cantidad útil al analizar tales algoritmos cuánticos para ecuaciones diferenciales?",
    "A": "En algoritmos cuánticos que utilizan técnicas de codificación en bloques basadas en ancillas para representar operadores no unitarios—como aquellos que surgen de operadores diferenciales discretizados con condiciones de frontera complejas—la log-norm proporciona una cota superior crucial sobre cómo crece el radio espectral de la dinámica reducida del registro de control durante la computación. Cada aplicación de una operación codificada en bloques acopla el registro del sistema a qubits ancilla a través de operaciones unitarias controladas, y sin un análisis cuidadoso, la pureza del estado ancilla podría degradarse exponencialmente con la profundidad del circuito a medida que la coherencia de fase se extiende por el espacio de Hilbert ampliado. La desigualdad de la log-norm acota este crecimiento del radio espectral, asegurando que la acumulación de error en los registros ancilla permanezca polinomial en lugar de exponencial en el número de pasos temporales, lo que de otro modo causaría un fallo catastrófico del esquema de codificación en bloques mucho antes de que la simulación alcance su tiempo de evolución objetivo.",
    "B": "La norma logarítmica proporciona garantías ajustadas sobre la preservación de la normalización del estado cuántico a lo largo de secuencias de operaciones unitarias, lo cual se vuelve esencial al construir subrutinas algorítmicas complejas a partir de puertas primitivas que preservan la amplitud. En solucionadores cuánticos de EDPs, cada paso temporal involucra componer múltiples unitarios—a menudo incluyendo rotaciones controladas y puertas de fase—y sin la cota de la log-norm, pequeñas desviaciones numéricas en las implementaciones de puertas podrían causar que la norma L² del vector de estado se desvíe de la unidad a lo largo de muchas iteraciones.",
    "C": "Al diseñar oráculos cuánticos para algoritmos de ecuaciones diferenciales, la norma logarítmica sirve como una herramienta crítica para identificar qué operadores diferenciales discretizados admiten implementaciones eficientes usando solo puertas del grupo de Clifford (Hadamard, CNOT y puertas de fase). Debido a que las operaciones de Clifford pueden simularse eficientemente en computadoras clásicas y corregirse usando códigos estabilizadores con sobrecarga mínima, las subrutinas de oráculo construidas a partir de descomposiciones solo-Clifford reducen dramáticamente el número requerido de puertas T intensivas en recursos que dominan el costo de tolerancia a fallos.",
    "D": "La log-norm acota directamente si las exponenciales de matriz permanecen estables durante la evolución temporal, previniendo divergencia numérica en la simulación cuántica codificada independientemente de cómo se descomponga el operador subyacente en puertas cuánticas. Al simular EDPs mediante evolución hamiltoniana, la medida de log-norm μ(H) controla tasas de crecimiento exponencial, proporcionando certificados de estabilidad explícitos que verifican que el estado cuántico no acumule errores desbocados a través de múltiples pasos de Trotter o descomposiciones de fórmula de producto.",
    "solution": "D"
  },
  {
    "id": 120,
    "question": "¿Cómo se compara el agrupamiento cuántico k-means con el k-means clásico?",
    "A": "El k-means cuántico logra aceleración exponencial mediante codificación de amplitud que requiere solo sobrecarga logarítmica de qubits en la dimensión de características, permitiendo cálculos de distancia en superposición. Sin embargo, las implementaciones prácticas enfrentan complejidad de preparación de estado que escala polinomialmente con el tamaño de los datos, perturbación inducida por medición que requiere preparaciones repetidas, y cuellos de botella de acceso QRAM que a menudo eliminan las ventajas teóricas a menos que los datos lleguen precodificados en estructuras de memoria accesibles cuánticamente.",
    "B": "El k-means cuántico puede potencialmente ofrecer aceleraciones en subrutinas específicas como cálculos de distancia al explotar paralelismo cuántico y codificación de amplitud de vectores de datos. Sin embargo, el algoritmo enfrenta desafíos prácticos significativos incluyendo sobrecarga de medición, complejidad de preparación de estado, y la necesidad de hardware cuántico tolerante a fallos para realizar ventajas teóricas sobre el algoritmo clásico de Lloyd en aplicaciones de agrupamiento del mundo real.",
    "C": "El k-means cuántico mejora las garantías de convergencia sin aceleración de tiempo de ejecución—la interferencia de amplitud cuántica durante actualizaciones de centroides suprime mínimos locales al construir paisajes de función objetivo más suaves a través de interferencia destructiva de contribuciones de alta varianza. Sin embargo, la complejidad por iteración y los conteos de iteraciones coinciden asintóticamente con el algoritmo clásico de Lloyd, proporcionando mejoras de calidad de solución en lugar de aceleración computacional en tareas prácticas de agrupamiento.",
    "D": "El k-means cuántico elimina el refinamiento iterativo de centroides al construir unitarios discriminadores de grupos a través de evolución adiabática hacia estados fundamentales que codifican particiones óptimas. Sin embargo, preparar hamiltonianos de grupos requiere preprocesamiento clásico que escala con el tamaño del conjunto de datos, el tiempo de ejecución adiabático crece polinomialmente con los requisitos de precisión, y la retroacción de medición necesita múltiples ciclos de evolución, a menudo negando ventajas sobre enfoques iterativos clásicos en regímenes no asintóticos.",
    "solution": "B"
  },
  {
    "id": 121,
    "question": "Considere un protocolo de transferencia inconsciente cuántica en el cual Alice envía dos bits a Bob, quien puede conocer exactamente uno sin revelar su elección. Suponga que el protocolo se basa en codificación tipo BB84 y el teorema de no-clonación para evitar que Bob extraiga ambos bits. Sin embargo, un adversario con recursos suficientes puede comprometer estas garantías. ¿Qué metodología de ataque avanzada puede comprometer la seguridad de este protocolo?",
    "A": "Limitaciones de almacenamiento cuántico ruidoso que causan degradación de la fidelidad de los qubits",
    "B": "Control de canal que rompe el entrelazamiento mediante ataques de retransmisión de medición y preparación que interceptan qubits transmitidos, realizan mediciones de base, luego reenvían estados recién preparados a los destinatarios, reemplazando así las correlaciones cuánticas por clásicas y evadiendo las protecciones de no-clonación.",
    "C": "Computación cuántica no-local facilitada por entrelazamiento previamente compartido entre nodos adversariales distribuidos",
    "D": "Explotación de almacenamiento cuántico acotado, donde el atacante almacena temporalmente estados cuánticos más allá del límite de memoria asumido por el protocolo, luego los mide después de que se revela la información clásica, rompiendo efectivamente la seguridad teórica de la información que depende de memoria cuántica limitada. Este enfoque explota la brecha entre los límites teóricos de almacenamiento y las restricciones de implementación práctica, permitiendo a adversarios con capacidades de almacenamiento incluso modestamente mejoradas extraer ambos bits al retrasar las mediciones hasta que los datos de desambiguación estén disponibles.",
    "solution": "D"
  },
  {
    "id": 122,
    "question": "En una red cuántica multiusuario que soporta tanto teletransportación cuántica como aplicaciones de detección cuántica distribuida, considere el siguiente escenario: tres usuarios (Alice, Bob y Charlie) solicitan simultáneamente pares entrelazados, donde Alice necesita estados GHZ para un protocolo de detección, Bob necesita pares de Bell para teletransportación con 99% de fidelidad, y Charlie necesita estados W para un esquema de comunicación. La red tiene capacidad limitada de generación de entrelazamiento en sus nodos intermedios, y algo del entrelazamiento previamente compartido ha comenzado a degradarse. ¿Qué funcionalidad clave proporciona un planificador de red cuántica que no tiene equivalente directo clásico en la gestión de este problema de asignación de recursos?",
    "A": "Coordinar la generación y asignación de recursos de entrelazamiento entre nodos mientras se considera la naturaleza monógama de las correlaciones cuánticas que impide compartir entrelazamiento más allá de cortes bipartitos, equilibrando solicitudes heterogéneas de usuarios para diferentes tipos de estados entrelazados contra capacidad finita de generación y requisitos variables de fidelidad, priorizando asignaciones basadas en medidas de negatividad solicitadas y accesibilidad LOCC de estados objetivo, realizando optimización en tiempo real de rutas de intercambio de entrelazamiento que preservan restricciones de subaditividad fuerte en la entropía de von Neumann para cumplir umbrales de pureza específicos de aplicación, todo mientras se gestiona un recurso cuya distribución está fundamentalmente limitada por teoremas de no-difusión—restricciones completamente ausentes en la planificación clásica de paquetes donde la transmisión multicast permite replicación arbitraria a múltiples destinatarios sin degradar el contenido de información transmitido o violar límites fundamentales teóricos de información sobre distribución de correlación entre particiones de red",
    "B": "Coordinar la generación y asignación de recursos de entrelazamiento entre nodos mientras se considera la naturaleza no-almacenable y sensible al tiempo de los estados cuánticos que se degradan por decoherencia, equilibrando solicitudes heterogéneas de usuarios para diferentes tipos de estados entrelazados contra capacidad finita de generación y requisitos variables de fidelidad, priorizando asignaciones basadas tanto en la complejidad del estado solicitado como en el tiempo de coherencia restante, realizando optimización en tiempo real de rutas de intercambio de entrelazamiento y protocolos de purificación para cumplir umbrales de fidelidad específicos de aplicación, todo mientras se gestiona un recurso que no puede ser copiado ni almacenado indefinidamente—restricciones completamente ausentes en la planificación clásica de paquetes donde los datos pueden ser almacenados en búfer, duplicados y retransmitidos sin limitaciones físicas fundamentales sobre duración de almacenamiento o replicación",
    "C": "Coordinar la generación y asignación de recursos de entrelazamiento entre nodos mientras se explota el entrelazamiento tiempo-energía para realizar multiplexación temporal de canales cuánticos, equilibrando solicitudes heterogéneas de usuarios para diferentes tipos de estados entrelazados contra capacidad finita de generación y requisitos variables de fidelidad, priorizando asignaciones basadas en el rango de Schmidt solicitado y la visibilidad del interferómetro de Franson necesaria para cada aplicación, realizando optimización en tiempo real de secuencias de intercambio de entrelazamiento que explotan post-selección en tiempos de llegada de fotones anunciados para impulsar tasas efectivas de generación, todo mientras se gestiona un recurso cuya distribución obedece restricciones de causalidad relativista que impiden señalización superlumínica—limitaciones completamente ausentes en la planificación clásica de paquetes donde la propagación de información está restringida solo por dispersión de fibra y ancho de banda del amplificador en lugar de estructura de cono de luz",
    "D": "Coordinar la generación y asignación de recursos de entrelazamiento entre nodos mientras se consideran límites fundamentales impuestos por la cota de Holevo sobre extracción de información clásica, equilibrando solicitudes heterogéneas de usuarios para diferentes tipos de estados entrelazados contra capacidad finita de generación y requisitos variables de fidelidad, priorizando asignaciones basadas en entropía de entrelazamiento solicitada y contenido de información accesible computable vía información mutua I(X:Y) entre resultados de medición, realizando optimización en tiempo real de protocolos de codificación densa y esquemas de teletransportación superdensa para maximizar utilización de capacidad de canal aproximándose a 2 bits por ebit, todo mientras se gestiona un recurso cuya utilidad se satura en la cantidad χ de Holevo—restricciones completamente ausentes en redes clásicas donde la capacidad de Shannon escala linealmente con el ancho de banda sin límites superiores cuántico-mecánicos sobre distinguibilidad de símbolos o densidad de información por partícula transmitida",
    "solution": "B"
  },
  {
    "id": 123,
    "question": "¿En qué se espera que se base teóricamente la \"ventaja cuántica\" en tareas de optimización como QAOA?",
    "A": "Correlaciones no-locales que permiten escapar de cuencas de mínimos locales",
    "B": "Muestreo no-clásico de subespacios de baja energía mediante amplitudes de tunelización",
    "C": "Dinámica no-perturbativa que genera paisajes favorables de función de coste",
    "D": "Interferencia no-trivial sobre grandes espacios de superposición",
    "solution": "D"
  },
  {
    "id": 124,
    "question": "¿Cómo afecta el intercambio de entrelazamiento multi-salto a la fidelidad de un par de Bell distribuido?",
    "A": "Cada intercambio acumula ruido de mediciones imperfectas de estados de Bell e imperfecciones del enlace inicial, pero la degradación sigue F_final ≈ F_link - (n-1)·ε_BSM donde n es el número de saltos y ε_BSM es la infidelidad por intercambio, produciendo una decaída aproximadamente lineal en lugar de multiplicativa. Esto ocurre porque el ruido despolarizante de operaciones de intercambio independientes se suma incoherentemente, y para regímenes de alta fidelidad (F > 0.95), la fórmula multiplicativa F_total = ∏F_i puede expandirse en serie de Taylor a primer orden, mostrando que la pérdida de fidelidad es casi aditiva. Sin embargo, una vez que F cae por debajo de ~0.90, los términos cruzados de segundo orden se vuelven significativos y la decaída se acelera, eventualmente transicionando al régimen completamente multiplicativo descrito para enlaces de baja fidelidad.",
    "B": "Cada medición intermedia de estado de Bell proyecta los pares consumidos sobre una base máximamente entrelazada, implementando una forma de filtrado débil de errores que suprime parcialmente errores de fase heredados de enlaces anteriores mientras preserva errores de amplitud. Esta supresión selectiva de ruido causa que la degradación de fidelidad escale sublinealmente con el número de saltos, siguiendo F_total ≈ F_link^(0.7n) en lugar del F_link^n multiplicativo ingenuo, porque los errores de inversión de fase de la generación inicial se eliminan probabilísticamente cuando se anti-correlacionan entre los dos pares que se intercambian. El efecto se vuelve más pronunciado para cadenas más largas, y aunque la fidelidad general aún decae, el exponente mejorado significa que la distribución de diez saltos logra fidelidades que exceden lo que la multiplicación simple predeciría por varios puntos porcentuales.",
    "C": "Cada intercambio multiplica las fidelidades de los enlaces constituyentes, reduciendo la fidelidad general. Como las mediciones imperfectas de estados de Bell y el ruido residual en los pares iniciales de corta distancia se acumulan multiplicativamente en cada nodo repetidor, la fidelidad punto-a-punto F_total es igual al producto F₁ × F₂ × ... × Fₙ de las fidelidades de enlaces individuales, haciendo que el entrelazamiento distribuido sea inherentemente más frágil que la generación directa a medida que aumenta el número de saltos intermedios.",
    "D": "Las operaciones de intercambio preservan la fidelidad total cuando los enlaces constituyentes tienen características de ruido coincidentes (mismo F_i para todos los segmentos), porque la medición de estado de Bell en cada nodo realiza una verificación de paridad que detecta y corrige errores de inversión de bit heredados del enlace anterior. Esta propiedad de corrección de errores significa que F_total = F_link para n arbitrario, siempre que todos los segmentos se generen con protocolos idénticos y experimenten ruido despolarizante estadísticamente independiente. Sin embargo, si los enlaces tienen ruido asimétrico—digamos, un segmento con errores predominantemente de fase y otro con amortiguamiento de amplitud—entonces el mecanismo de corrección inducido por medición falla y la fidelidad se degrada como F_total ~ min(F_i)·√n, con el peor enlace dominando pero parcialmente compensado por la propiedad de detección de errores del intercambio de entrelazamiento entre múltiples segmentos balanceados.",
    "solution": "C"
  },
  {
    "id": 125,
    "question": "En el contexto de aprendizaje por refuerzo cuántico, considere un agente navegando un entorno tipo laberinto donde ciertas transiciones de estado son clásicamente prohibidas debido a barreras de energía, pero cuánticamente accesibles mediante efectos de tunelización. El agente usa un circuito cuántico variacional para representar su política, con codificación de amplitud del espacio de estados y puertas de rotación parametrizadas que determinan probabilidades de acción. ¿Cómo altera fundamentalmente la superposición cuántica la capacidad de exploración del agente en comparación con estrategias de exploración clásicas epsilon-greedy o softmax?",
    "A": "La superposición permite evaluación simultánea de múltiples acciones en un estado dado, pero esta ventaja es en gran medida teórica—en la práctica, el colapso de medición obliga al agente a comprometerse con una sola trayectoria, y la verdadera aceleración proviene de usar el algoritmo de Grover para buscar en el búfer de reproducción experiencias de alto valor durante la fase de aprendizaje. El circuito cuántico prepara una superposición sobre todas las transiciones almacenadas, aplica amplificación de amplitud para impulsar los coeficientes de muestras con alto error-TD, y mide para seleccionar experiencias para actualizaciones de gradiente, proporcionando una aceleración de raíz cuadrada sobre muestreo aleatorio uniforme.",
    "B": "La tunelización cuántica a través de barreras de función de valor es el mecanismo principal—el agente puede atravesar regiones energéticamente desfavorables del espacio de estados sin acumular recompensa negativa, similar a cómo los electrones tunelizan a través de barreras de potencial en física de estado sólido, lo cual es fundamentalmente imposible para agentes de RL clásicos restringidos por estadística de Boltzmann.",
    "C": "La ventaja principal es el entrelazamiento cuántico entre diferentes ramas del espacio de estados, que correlaciona señales de recompensa a través de regiones distantes del MDP de maneras que violan desigualdades de Bell, permitiendo al agente aprender estructura global en la función de valor exponencialmente más rápido que métodos basados en actualizaciones TD locales. Específicamente, cuando el agente visita el estado s_i y recibe recompensa r_i, el entrelazamiento propaga esta información instantáneamente a representaciones de estados s_j que pueden estar arbitrariamente lejos en el grafo de transición.",
    "D": "La superposición permite al agente evaluar efectivamente una combinación coherente de múltiples pares estado-acción en un solo paso hacia adelante a través del circuito cuántico, creando patrones de interferencia que pueden guiar el gradiente de política hacia regiones del espacio de acciones que requerirían muchas ejecuciones clásicas secuenciales para descubrir, particularmente cuando se combina con técnicas de amplificación de amplitud que mejoran la probabilidad de muestrear trayectorias de alta recompensa. Esto representa una desviación genuina de la exploración estocástica clásica porque el circuito cuántico puede interferir constructivamente rutas hacia estados de alto valor.",
    "solution": "D"
  },
  {
    "id": 126,
    "question": "¿Qué vulnerabilidad específica permite la filtración de información entre programas en cálculos cuánticos secuenciales?",
    "A": "Los sustratos de qubits superconductores contienen concentraciones diluidas de defectos de sistemas de dos niveles (TLS) a escala atómica—típicamente vacantes de oxígeno o enlaces colgantes en capas de interfaz amorfas—que se acoplan coherentemente a las transiciones de qubits con intensidades que varían de 1-100 MHz dependiendo de la proximidad espacial y la orientación del momento dipolar. Cuando el cálculo del usuario anterior acciona qubits particulares a través de operaciones de puertas repetidas, los defectos TLS cercanos experimentan saturación e inversión de población que persiste durante tiempos anómalamente largos (10-1000 segundos) debido al débil acoplamiento térmico con el baño de fonones del refrigerador de dilución a 10-20 mK.",
    "B": "La entrega de señales de control a través de líneas de transmisión coaxiales crea histéresis electromagnética en películas metálicas y dieléctricos, induciendo patrones de magnetización persistentes que desplazan las frecuencias de resonancia en 10-50 kHz basándose en las secuencias de pulsos previas. Los atacantes pueden medir estos desplazamientos de frecuencia mediante interferometría de Ramsey para reconstruir secuencias de puertas anteriores e información de temporización a partir de la memoria magnética almacenada en la infraestructura de control clásica.",
    "C": "Los pulsos de lectura de microondas inyectados en resonadores superconductores acoplados a qubits inducen campos electromagnéticos oscilantes que persisten durante múltiples tiempos de vida de la cavidad (Q/2πf ≈ 200-500 ns para resonadores típicos de 7 GHz con factores de calidad Q ~ 10^4-10^5) después de que concluyen las operaciones de medición. Estas excitaciones residuales del resonador—fotones atrapados en modos de cavidad cuasi-confinados que experimentan decaimiento exponencial—permanecen almacenados coherentemente cuando el programa del usuario siguiente comienza a ejecutarse en el mismo hardware, creando un canal lateral fotónico que codifica los resultados de medición del trabajo anterior. Implementando esquemas de detección heterodina o desplazamientos de frecuencia dependientes del estado del qubit durante sus puertas iniciales, el siguiente usuario puede efectivamente 'escuchar' el campo del resonador en decaimiento y extraer estadísticas de medición del cálculo previo, aunque los qubits mismos hayan experimentado relajación T1 hacia el estado fundamental.",
    "D": "La reinicialización incompleta de qubits entre ejecuciones sucesivas de programas permite que persista información residual del estado cuántico—incluyendo poblaciones de estados excitados, coherencias de fase y correlaciones de entrelazamiento de cálculos previos—y se vuelva accesible para usuarios subsecuentes mediante circuitos de sondeo cuidadosamente diseñados. Esta falla en restaurar completamente los qubits a su estado fundamental crea un canal lateral cuántico donde la estructura algorítmica y los resultados de medición de trabajos anteriores se filtran a través de los límites entre programas.",
    "solution": "D"
  },
  {
    "id": 127,
    "question": "¿En qué difiere fundamentalmente el concepto de entrelazamiento extremo a extremo de la conectividad clásica extremo a extremo?",
    "A": "El entrelazamiento permite comunicación basada en teletransportación que consume el par entrelazado durante la transmisión de información cuántica entre puntos extremos, requiriendo regeneración continua a diferencia de los canales clásicos—sin embargo, el protocolo de teletransportación permite la transmisión de estados cuánticos arbitrarios con fidelidad perfecta independiente de la distancia, lo que otorga a las redes cuánticas una ventaja en aplicaciones sensibles a la latencia ya que el paso de comunicación clásica en la teletransportación puede pre-computarse y transmitirse durante períodos de inactividad antes de que el estado cuántico a teletransportar siquiera se prepare.",
    "B": "Los estados entrelazados se degradan bajo medición y no pueden clonarse ni amplificarse según el teorema de no-clonación, forzando a las redes cuánticas a regenerar continuamente el entrelazamiento entre nodos para mantener la conectividad—a diferencia de las señales clásicas que toleran amplificación—pero la decoherencia inducida por medición procede determinísticamente según la ecuación maestra de Lindblad, permitiendo predicción precisa de cuándo debe refrescarse el entrelazamiento basándose en el tiempo acumulado de interacción ambiental en lugar de umbrales probabilísticos de fidelidad.",
    "C": "El entrelazamiento proporciona correlaciones no-locales que se consumen tras la medición u operaciones cuánticas debido al colapso de la función de onda, y el teorema de no-clonación impide copiar o amplificar estados entrelazados—así las redes cuánticas necesitan regeneración constante de entrelazamiento a diferencia de enlaces clásicos con amplificación indefinida de señal—sin embargo, la tasa de consumo sigue una ley de decaimiento universal independiente del método de generación de entrelazamiento, con pares de Bell degradándose a 1/√t por medición independientemente de si se originaron de conversión paramétrica descendente espontánea o almacenamiento en ensambles atómicos.",
    "D": "El entrelazamiento se agota cuando se mide o cuando se realiza una operación cuántica sobre él—fundamentalmente no se pueden amplificar estados entrelazados ni clonarlos debido al teorema de no-clonación, lo que significa que las redes cuánticas requieren regeneración constante de pares entrelazados entre nodos para mantener la conectividad, a diferencia de enlaces clásicos donde las señales pueden amplificarse indefinidamente.",
    "solution": "D"
  },
  {
    "id": 128,
    "question": "La probabilidad de colisión en el muestreo bosónico crece cuando los fotones se agrupan porque los eventos agrupados:",
    "A": "Corresponden a permanentes de matrices cuyos argumentos tienen filas repetidas, que pueden calcularse eficientemente usando el principio de inclusión-exclusión aplicado a la descomposición de cosetas del grupo simétrico. Específicamente, cuando k fotones ocupan el mismo modo de salida, el permanente se factoriza en un producto de k! términos idénticos, cada uno evaluando una submatriz de (n-k+1)×(n-k+1), reduciendo el cálculo #P-completo a k! cálculos de determinantes en tiempo polinomial cuyos resultados se multiplican para dar la magnitud al cuadrado de la amplitud de transición.",
    "B": "Satisfacen la regla de selección bosónica que requiere que todas las distribuciones de número de fotones de entrada y salida tengan paridad coincidente a través de cada par de modos acoplados por el hamiltoniano del interferómetro. Cuando los fotones se agrupan, necesariamente crean configuraciones de paridad par que se encuentran en el autoespacio +1 del operador de intercambio espacial de la red, y los permanentes de matrices correspondientes a tales configuraciones se descomponen en forma de bloques diagonales donde cada bloque corresponde a un subespacio de dos fotones, reduciendo la dimensión efectiva de la matriz de n a aproximadamente n/2 para eventos k-agrupados.",
    "C": "Dependen de permanentes de matrices más pequeñas que el conteo de fotones. Cuando k fotones se agrupan en un único modo de salida, la amplitud de transición involucra calcular el permanente de una submatriz de (n-k)×(n-k) en lugar de la matriz n×n completa, reduciendo la complejidad dimensional del cálculo #P-completo requerido para determinar la probabilidad de esa configuración.",
    "D": "Exhiben interferencia constructiva que concentra masa de probabilidad en un subconjunto de tamaño polinomial del espacio de salida exponencialmente grande, específicamente las O(n^k) configuraciones donde al menos k fotones comparten un modo. Los algoritmos clásicos explotan esta estructura mediante muestreo por importancia del sector agrupado usando muestreo por rechazo ponderado por la magnitud del permanente, logrando ε-aproximación con O(n^k/ε) muestras en lugar de las O(2^n/ε²) requeridas para muestreo uniforme sobre todas las configuraciones de salida, aunque resultados recientes muestran que esta ventaja desaparece para k > n^(1/3).",
    "solution": "C"
  },
  {
    "id": 129,
    "question": "¿Por qué los algoritmos eficientes en hardware evitan la inversión de matrices?",
    "A": "La inversión de matrices en la métrica de información cuántica de Fisher—frecuentemente requerida para optimización por gradiente natural—demanda estimar O(p²) elementos fuera de la diagonal donde p es el conteo de parámetros, y cada elemento requiere exponencialmente muchas repeticiones de circuito para resolverse a alta precisión debido a la supresión exponencial de superposiciones entre autoestados cuasi-degenerados, causando que el procedimiento de inversión consuma presupuestos de disparos prohibitivos que escalan como exp(p) incluso cuando la matriz está bien condicionada.",
    "B": "La inversión de matrices se vuelve numéricamente inestable cuando se aplica a tensores métricos mal condicionados que surgen comúnmente en la optimización de parámetros variacionales, donde valores propios pequeños conducen a ruido amplificado en los elementos de la matriz invertida, causando que las estimaciones de gradiente diverjan e impidiendo la convergencia confiable del paisaje de optimización.",
    "C": "Los ansatz eficientes en hardware típicamente generan jacobianos de parámetros con números de condición que crecen exponencialmente con la profundidad del circuito debido a mesetas áridas, e invertir estas matrices mal condicionadas amplifica el ruido de muestreo por el recíproco del valor propio más pequeño—dado que la estimación de gradiente ya requiere O(1/ε²) disparos para precisión ε, el paso de inversión multiplica esto por κ² donde κ es el número de condición, haciendo que el costo total escale como O(exp(profundidad)/ε²), lo que rápidamente agota los presupuestos de disparos disponibles.",
    "D": "Los algoritmos cuánticos inherentemente producen transformaciones unitarias que preservan la norma del espacio de Hilbert, lo que significa que todas las operaciones directamente implementables deben corresponder a inmersiones isométricas con vectores columna ortogonales—la inversión de matrices, particularmente de matrices no normales que surgen en tensores de covarianza de gradiente, produce transformaciones que expanden o contraen normas de vectores no-unitariamente, requiriendo dilatación asistida por ancilla en un espacio mayor donde la inversa se incrusta como un bloque unitario, lo que duplica los requisitos de qubits e introduce sobrecarga de medición de ancilla que degrada la fidelidad de actualización de parámetros.",
    "solution": "B"
  },
  {
    "id": 130,
    "question": "¿Qué mecanismo específico proporciona privacidad diferencial cuántica en canales cuánticos ruidosos?",
    "A": "La aleatorización de fase introduce incertidumbre aplicando puertas de fase aleatorias muestreadas de una distribución continua, típicamente uniforme sobre [0, 2π), a cada qubit antes de la transmisión a través del canal cuántico. Este mecanismo oscurece la información de fase relativa que de otro modo distinguiría entre diferentes estados de entrada pertenecientes a conjuntos de datos vecinos. Dado que las patadas de fase se acumulan incoherentemente a través del conjunto de posibles elecciones aleatorias, cualquier adversario que intente extraer información a nivel individual mediante tomografía cuántica debe enfrentarse a un proceso efectivo de desfasamiento.",
    "B": "El desfasamiento sirve como mecanismo fundamental de ruido para la privacidad diferencial cuántica porque destruye selectivamente los elementos fuera de la diagonal de la matriz de densidad en la base computacional, borrando así las correlaciones cuánticas que de otro modo filtrarían información sobre puntos de datos individuales. Cuando un estado cuántico pasa a través de un canal de desfasamiento con tasa de decoherencia γ cuidadosamente calibrada, los términos de coherencia decaen exponencialmente como exp(-γt), creando un compromiso ajustable entre privacidad y utilidad donde un desfasamiento más fuerte proporciona límites de privacidad más estrictos a costa de fidelidad de medición reducida.",
    "C": "La despolarización sirve como mecanismo fundamental de ruido al aplicar una mezcla uniforme de errores de Pauli a cada qubit con probabilidad cuidadosamente calibrada p. Esto crea un canal que mapea cualquier estado de entrada hacia el estado maximalmente mezclado a una tasa controlada, enmascarando efectivamente las contribuciones de datos individuales mientras preserva propiedades estadísticas agregadas. El canal despolarizante satisface los requisitos de composición para privacidad diferencial porque proporciona ruido simétrico a través de todos los estados base.",
    "D": "La amortiguación de amplitud introduce un proceso disipativo controlado que transfiere asimétricamente estados cuánticos hacia el estado fundamental |0⟩, implementando efectivamente una forma de ruido cuántico que enmascara las contribuciones de entradas individuales en el conjunto de datos compilado. Ingeniando la tasa de amortiguación κ para escalar apropiadamente con el tamaño del conjunto de datos y los parámetros de sensibilidad, el mecanismo asegura que cualquier consulta aplicada al estado cuántico ruidoso revele información sobre propiedades estadísticas agregadas mientras proporciona negación plausible para registros individuales. El superoperador de amortiguación de amplitud crea una evolución no unitaria que limita fundamentalmente la distinguibilidad entre bases de datos cuánticas vecinas.",
    "solution": "C"
  },
  {
    "id": 131,
    "question": "¿Cuál es la diferencia entre un estado de Bell y un estado GHZ?",
    "A": "Los estados de Bell se definen exclusivamente para partículas de espín-1/2 y requieren descomposición en la base singlete-triplete mediante coeficientes de Clebsch-Gordan que suman los momentos angulares individuales a números cuánticos de espín total, mientras que los estados GHZ se generalizan a qudits de dimensión arbitraria a través de operadores de Weyl multipartitos que generan estados de grafo maximalmente entrelazados en topologías bipartitas completas. La distinción estructural clave reside en la simetría: los estados de Bell se transforman irreduciblemente bajo operaciones locales SU(2)⊗SU(2) que preservan el espín total, mientras que los estados GHZ exhiben simetría de permutación bajo reetiquetado cíclico de qubits, convirtiéndolos en autoestados naturales de operadores de espín colectivo J².",
    "B": "Los estados de Bell demuestran violación máxima de la desigualdad CHSH con cota de Tsirelson 2√2 mediante mediciones de correlación en bases complementarias separadas por ángulos de 22.5 grados, mientras que los estados GHZ logran correlaciones no clásicas más fuertes al violar las desigualdades de Mermin-Klyshko con márgenes de violación exponencialmente crecientes a medida que aumenta el número de qubits, alcanzando cotas clásicas que escalan como 2^(n-1) versus predicciones cuánticas de 2^(n/2) para sistemas de n partes. Ambas clases de estados son estados estabilizadores, pero los estados de Bell habitan espacios de Hilbert de dos qubits que admiten cuatro bases maximalmente entrelazadas ortogonales, mientras que los estados GHZ requieren al menos tres qubits para exhibir entrelazamiento multipartito genuino que no puede crearse mediante operaciones por pares y comunicación clásica.",
    "C": "Los estados de Bell mantienen coherencia bajo ruido local despolarizante con fidelidad que decae exponencialmente como F(t)=¼(1+3e^(-4γt)) donde γ es la tasa de decoherencia de un solo qubit, mientras que los estados GHZ exhiben fragilidad superexponencial bajo decoherencia que afecta a cualquier qubit individual, con fidelidad que colapsa como F(t)≈e^(-nγt²) debido a la naturaleza todo-o-nada de la coherencia de fase de n qubits requerida para mantener la superposición (|000⟩+|111⟩)/√2. Esta robustez diferencial proviene de la estructura simétrica de dos partes de los estados de Bell que permite corrección de errores mediante operaciones locales de filtrado, mientras que la susceptibilidad de los estados GHZ surge de su hipersensibilidad a errores de fase: un solo evento de decoherencia de qubit proyecta todo el estado en una mezcla separable, destruyendo las correlaciones de n vías esenciales para la ventaja cuántica en tareas como compartición de secretos y protocolos de acuerdo bizantino.",
    "D": "Los estados de Bell involucran exactamente dos qubits en una configuración maximalmente entrelazada descrita por los cuatro pares EPR canónicos (|Φ±⟩ y |Ψ±⟩), formando la base de los protocolos de teletransportación cuántica y codificación superdensa, mientras que los estados GHZ involucran tres o más qubits con propiedades específicas de entrelazamiento multipartito que no pueden descomponerse en subsistemas entrelazados por pares, exhibiendo estructuras de correlación fundamentalmente diferentes que revelan violaciones más fuertes del realismo local mediante desigualdades de Mermin en lugar de las pruebas CHSH estándar.",
    "solution": "D"
  },
  {
    "id": 132,
    "question": "¿Cómo se llama el proceso de adaptar un circuito cuántico a hardware específico?",
    "A": "Pipelining es el proceso de descomponer un circuito cuántico en etapas secuenciales que pueden ejecutarse en sucesión temporal respetando las restricciones de hardware, análogo al pipelining de instrucciones en procesadores clásicos. Este enfoque programa operaciones de puertas para maximizar el rendimiento superponiendo la ejecución de operaciones independientes en diferentes subconjuntos de qubits, transformando efectivamente un circuito lógico en un programa de ejecución optimizado para hardware que considera la conectividad limitada, las fidelidades de las puertas y las restricciones temporales específicas de la arquitectura de control del procesador cuántico objetivo.",
    "B": "Compilación abarca el pipeline completo de transformación desde algoritmos cuánticos de alto nivel hasta instrucciones ejecutables por hardware, incluyendo descomposición de puertas en operaciones nativas, optimización de circuitos mediante reglas de reescritura algebraica, asignación de qubits a qubits físicos respetando restricciones de conectividad, y la inserción de puertas SWAP para enrutar operaciones multi-qubit a través de topologías limitadas. Este proceso integral transforma circuitos cuánticos abstractos en secuencias concretas de pulsos o microcódigo que controla directamente el hardware, convirtiéndolo en el término correcto para adaptar circuitos a procesadores cuánticos específicos.",
    "C": "Linking se refiere al proceso abstracto de conectar descripciones de puertas de alto nivel al conjunto de operaciones nativas de hardware disponibles en el procesador cuántico objetivo.",
    "D": "Mapping o transpilación es el término estándar para adaptar circuitos cuánticos a restricciones específicas de hardware, incluyendo enrutamiento, descomposición de puertas y optimización para la topología del dispositivo objetivo.",
    "solution": "D"
  },
  {
    "id": 133,
    "question": "¿Por qué las técnicas de red clásicas no logran abordar los desafíos de la Internet Cuántica?",
    "A": "Las limitaciones de ancho de banda y la falta de soporte para enrutamiento basado en superposición impiden que los protocolos clásicos manejen eficientemente el tráfico cuántico, ya que los canales cuánticos requieren un rendimiento exponencialmente mayor para preservar la coherencia a través de segmentos de red. Los enrutadores clásicos procesan paquetes secuencialmente y no pueden explotar el paralelismo inherente en estados cuánticos superpuestos que viajan por múltiples rutas simultáneamente. Además, los algoritmos de control de congestión TCP/IP asumen capacidades de enlace deterministas, mientras que los enlaces cuánticos experimentan fidelidades de transmisión dependientes del estado que varían con las tasas de distribución de entrelazamiento.",
    "B": "Asumen entrega determinista de paquetes con mecanismos fiables de retransmisión y confirmación, a diferencia de la naturaleza probabilística de la llegada de qubits que depende de la fidelidad del canal cuántico y los resultados de medición. Los códigos clásicos de corrección de errores como CRC y checksums se basan en copiar el contenido de los paquetes para detectar corrupción, pero la información cuántica no puede clonarse, por lo que estas técnicas destruyen el estado cuántico al inspeccionarlo.",
    "C": "El teorema de no clonación y el colapso inducido por medición impiden la aplicación directa de técnicas clásicas como copia de paquetes, almacenamiento en búfer y detección de errores, que fundamentalmente dependen de duplicar información sin perturbar el estado original.",
    "D": "Las redes cuánticas requieren hardware de conmutación más rápido de lo que los sistemas clásicos pueden proporcionar, particularmente para mantener la coherencia durante decisiones de enrutamiento a través de múltiples saltos de red. Los enrutadores clásicos introducen latencia del orden de microsegundos por salto debido a retrasos de conmutación electrónica, pero los qubits decoheran en nanosegundos en las implementaciones actuales, lo que significa que incluso velocidades de conmutación clásicas optimizadas causan pérdida inaceptable de fidelidad del estado. El procesamiento electrónico requerido para inspección de encabezados, búsquedas en tablas de enrutamiento y decisiones de reenvío opera inherentemente en escalas temporales incompatibles con preservar la superposición cuántica.",
    "solution": "C"
  },
  {
    "id": 134,
    "question": "Las técnicas de mitigación de errores de medición como las matrices de error de lectura se aplican típicamente:",
    "A": "Durante la etapa de transpilación donde convierten puertas no nativas en primitivas específicas del dispositivo al descomponer cada operación de puerta en secuencias que inherentemente compensan patrones conocidos de error de lectura, corrigiendo efectivamente la estructura del circuito mismo. El transpilador identifica operaciones de medición e inserta rotaciones correctivas inmediatamente antes de cada medición basándose en la matriz de confusión caracterizada, de modo que cuando ocurre la medición física sesgada, produce estadísticas que se aproximan a lo que se obtendría de una medición proyectiva ideal sobre el verdadero estado cuántico.",
    "B": "Antes de la ejecución del circuito, ajustando formas y duraciones de pulsos para minimizar errores de lectura durante la compilación, incrustando efectivamente la corrección en la capa de hardware misma. Este enfoque preventivo calibra los operadores de medición usando matrices de caracterización inversa derivadas de tomografía preliminar del sistema, modificando los parámetros de pulso de lectura para que los resultados de medición física ya estén corregidos cuando emergen del procesador cuántico, eliminando la necesidad de cualquier post-procesamiento de distribuciones de probabilidad.",
    "C": "Dentro del bucle optimizador clásico donde reescalan las magnitudes de gradiente por el inverso de los valores propios de la matriz de confusión de lectura, asegurando que las actualizaciones de parámetros consideren el sesgo de medición sistemático durante el entrenamiento de algoritmos variacionales. Las matrices de mitigación se multiplican elemento por elemento con los gradientes calculados antes de que se ejecute el paso del optimizador.",
    "D": "Después de las mediciones, para corregir las distribuciones de probabilidad de salida basándose en matrices de confusión calibradas",
    "solution": "D"
  },
  {
    "id": 135,
    "question": "¿Cuál es la principal ventaja de los códigos expansores cuánticos en términos de escalado de recursos?",
    "A": "Logran complejidad logarítmica de extracción de síndrome mediante la alta conectividad del grafo expansor, que permite medir cada estabilizador usando solo O(log n) qubits ancilla mediante comprobaciones de paridad paralelas recursivas. Esta estructura de árbol de medición jerárquico, habilitada por las propiedades espectrales del grafo, reduce la sobrecarga de medición de síndrome de lineal a logarítmica manteniendo la misma distancia de código que los códigos estabilizadores convencionales.",
    "B": "Logran buena distancia de código manteniendo una tasa de codificación constante, lo que significa que la proporción de qubits lógicos a físicos permanece favorable a medida que escala el tamaño del sistema. Además, su conectividad de grafo estructurada permite algoritmos de decodificación clásica eficientes que se ejecutan en tiempo casi lineal, haciéndolos prácticos para corrección de errores en tiempo real en procesadores cuánticos de gran escala.",
    "C": "Reducen el ruido de medición de síndrome utilizando la propiedad de expansión de aristas del grafo expansor para distribuir información de síndrome a través de múltiples operadores de verificación redundantes. Cada error físico produce síndromes en Θ(log n) estabilizadores vecinos en lugar de solo vecinos más cercanos, permitiendo votación mayoritaria para suprimir errores de medición sin requerir rondas repetidas de extracción de síndrome.",
    "D": "Permiten extracción de síndrome de profundidad constante mediante la brecha espectral del expansor, que garantiza que los generadores estabilizadores puedan medirse en O(1) capas paralelas independientemente del tamaño del código. La propiedad de mezcla rápida del grafo asegura que la información de síndrome se propague a todos los operadores de verificación en un número fijo de pasos, eliminando el cuello de botella de escalado de profundidad presente en códigos de superficie donde los circuitos de síndrome crecen con el diámetro de la red.",
    "solution": "B"
  },
  {
    "id": 136,
    "question": "¿Qué propiedad de los sistemas cuánticos es más relevante para la posible aceleración cuántica en el agrupamiento k-means?",
    "A": "Calcular distancias en superposición permite que los algoritmos cuánticos evalúen la distancia euclidiana entre un punto de datos y todos los k centroides simultáneamente dentro de una única consulta a un oráculo cuántico de distancias, reduciendo la complejidad de asignación por punto de O(k) cálculos clásicos de distancia a O(1) operaciones cuánticas. Este cálculo de distancia basado en superposición aprovecha la codificación de amplitud de vectores característicos y la lectura basada en interferencia para colapsar el paso de comparación de centroides en un resultado de medición que revela directamente la asignación de grupo más cercana.",
    "B": "La amplificación de amplitud basada en Grover sobre asignaciones de grupos permite una aceleración cuadrática en la fase de actualización de centroides al tratar cada posible k-partición como un estado marcado en el espacio de búsqueda. El algoritmo codifica todos los n puntos de datos en superposición y aplica un operador de difusión que amplifica configuraciones donde la varianza intragrupo se minimiza, reduciendo la complejidad de iteración de O(√n) pasos clásicos de Lloyd a O(n^(1/4)) iteraciones cuánticas. Esta mejora basada en amplitud aprovecha la interferencia destructiva para suprimir asignaciones de alta varianza mientras refuerza constructivamente las ubicaciones óptimas de centroides mediante operaciones repetidas de inversión sobre el promedio.",
    "C": "La estimación de fase cuántica aplicada a la matriz de covarianza del grupo permite la descomposición de valores propios en profundidad O(log d) para vectores característicos de d dimensiones, en comparación con O(d³) para la descomposición en valores singulares clásica, al codificar el operador de covarianza como un unitario cuyo retroceso de fase revela directamente las direcciones de componentes principales. Este análisis espectral permite al algoritmo identificar orientaciones naturales de grupos en el espacio de características mediante la lectura de vectores propios, reduciendo la dimensionalidad mientras preserva la separabilidad de grupos. Los valores propios codificados en fase colapsan el refinamiento de centroides de minimización iterativa de distancias a una única medición de la dirección propia dominante para cada grupo.",
    "D": "El recocido cuántico aprovecha las fluctuaciones térmicas en el campo transversal para escapar de mínimos locales, pero la ventaja crítica proviene de mantener la superposición coherente sobre configuraciones de grupos durante el cronograma de recocido, no del salto térmico clásico. El algoritmo codifica variables de pertenencia como qubits lógicos en un Hamiltoniano de Ising frustrado donde el estado fundamental corresponde a la distancia intragrupo mínima, y la tasa de barrido adiabático se ajusta para mantener el sistema en el estado propio instantáneo mientras la brecha de energía permanece polinomialmente inversa en n. Esta evolución coherente evita la desaceleración exponencial del recocido simulado al utilizar interferencia cuántica en lugar de amplitud de tunelaje.",
    "solution": "A"
  },
  {
    "id": 137,
    "question": "En arquitecturas de computación cuántica distribuida, ¿qué propiedad fundamental determina si un algoritmo cuántico puede particionarse eficientemente a través de múltiples procesadores cuánticos conectados por canales cuánticos de ancho de banda limitado? Considere tanto la sobrecarga de profundidad del circuito como los requisitos de comunicación clásica para mantener la fidelidad del entrelazamiento a través del sistema distribuido.",
    "A": "La partición eficiente requiere que el algoritmo se descomponga en bloques computacionales que exhiban lo que los teóricos de sistemas distribuidos llaman 'separabilidad cuántica' — la propiedad de que el estado de salida de cada bloque puede expresarse como un producto tensorial con interfaces clásicas bien definidas. Cuando el circuito cuántico satisface esta descomposición, con el número de qubits cruzando límites de partición escalando como máximo logarítmicamente con el tamaño total del sistema, entonces la corrección de errores clásica en los enlaces entre procesadores es suficiente para mantener la coherencia. La ausencia de entrelazamiento de largo alcance en la base computacional significa que la sobrecarga de teletransportación permanece sublineal.",
    "B": "La ejecución distribuida se vuelve práctica cuando la representación del vector de estado del algoritmo mantiene un rango de Schmidt bajo a través de cualquier bipartición que separe procesadores, lo que significa que la mayor parte de la información cuántica permanece localizada en lugar de globalmente entrelazada. Si el algoritmo puede reformularse de modo que los qubits de cada procesador interactúen principalmente a través de propagación clásica desde resultados de medición en lugar de puertas cuánticas directas, entonces la costosa fase de distribución de entrelazamiento colapsa a un costo de configuración único amortizado sobre muchas ejecuciones del circuito. La observación crítica es que los algoritmos expresables en el formalismo de computación cuántica basada en medición naturalmente satisfacen este criterio porque la conectividad del estado cluster puede diseñarse para coincidir con la topología de la red.",
    "C": "El factor determinante es si la clase de complejidad del algoritmo cuántico permanece sin cambios bajo la restricción de que solo se permitan puertas de vecinos más cercanos en el modelo de circuito subyacente. Los algoritmos naturalmente adecuados para arquitecturas distribuidas son precisamente aquellos que evitan la transformada cuántica rápida de Fourier como operación primitiva, ya que QFT requiere conectividad de todos con todos en su descomposición estándar. Cuando un algoritmo puede reescribirse usando solo Hamiltonianos locales e interacciones de vecinos más cercanos, se mapea naturalmente a procesadores distribuidos asignando regiones de qubits espacialmente contiguas a cada nodo.",
    "D": "El factor clave es la localidad en el grafo de interacción — los algoritmos donde la mayoría de las puertas actúan sobre qubits cercanos en alguna topología lógica pueden particionarse asignando regiones contiguas a cada procesador, minimizando el costoso intercambio de entrelazamiento entre nodos distantes. La métrica crítica es el tiempo de mezcla espacial del circuito en relación con las escalas de tiempo de decoherencia de los canales cuánticos entre procesadores. Cuando esta relación permanece acotada por un factor polinómico, la implementación distribuida mantiene la ventaja cuántica a pesar de la sobrecarga de teletransportar estados cuánticos a través de límites de procesador para puertas no locales.",
    "solution": "D"
  },
  {
    "id": 138,
    "question": "En el contexto de la computación cuántica topológica, considere un sistema donde los anyones se trenzan para implementar puertas lógicas, y el espacio computacional está protegido por la brecha de energía hacia estados excitados. La protección depende de mantener el sistema a temperaturas muy por debajo de esta brecha. Si realizamos mediciones para extraer información de síndrome sobre errores, ¿cuál es la función principal de estas mediciones de síndrome en un código estabilizador como el surface code, que comparte algunas características conceptuales con la protección topológica pero utiliza corrección activa de errores?",
    "A": "Las mediciones de síndrome en códigos estabilizadores sirven múltiples propósitos: primero verifican que el qubit lógico haya sido preparado en el espacio de código correcto comprobando valores propios de estabilizadores, luego monitorean continuamente errores durante el cálculo detectando violaciones de estabilizadores, y finalmente recodifican la información cuántica después de cada puerta lógica para asegurar que los qubits permanezcan en el subespacio protegido a lo largo del cálculo. Esta triple función es esencial porque cada operación lógica puede potencialmente expulsar al sistema del espacio de código, requiriendo reproyección inmediata mediante extracción de síndrome y operaciones de recuperación subsecuentes que restauran las condiciones de estabilizador.",
    "B": "En códigos estabilizadores las mediciones de síndrome principalmente reducen la diafonía entre modos bosónicos que codifican la información lógica proyectando correlaciones de error de alta frecuencia que acoplan parches de código vecinos, imponiendo así las restricciones de paridad local que definen los límites del espacio de código. Estas mediciones realizan monitoreo débil continuo de los números de ocupación de modos, extrayendo bits de síndrome que indican cuándo las excitaciones se han filtrado entre cavidades bosónicas adyacentes.",
    "C": "Las mediciones identifican qué errores han ocurrido sin colapsar el estado cuántico lógico codificado en el subespacio protegido",
    "D": "Las mediciones de síndrome verifican la fidelidad de preparación del qubit lógico comprobando que todos los operadores estabilizadores iniciales produzcan los valores propios esperados (+1 o -1) inmediatamente después de la codificación, confirmando que los qubits físicos han sido correctamente entrelazados en la variedad del espacio de código antes de que comience cualquier cálculo. Si algún estabilizador devuelve un valor propio inesperado, la secuencia de preparación debe repetirse, ya que esto indica que el circuito de codificación no logró distribuir adecuadamente la información cuántica a través del bloque de código.",
    "solution": "C"
  },
  {
    "id": 139,
    "question": "En el surface code, ¿cuál es el papel principal de los operadores de plaqueta y estrella?",
    "A": "Actúan como generadores estabilizadores que permiten la extracción de síndrome: los operadores de plaqueta detectan errores tipo X mediante mediciones de paridad Z⊗Z⊗Z⊗Z en límites de caras, mientras que los operadores de estrella detectan errores tipo Z mediante paridad X⊗X⊗X⊗X en vecindarios de vértices, pero estos operadores deben anticonmutar con operadores lógicos para permitir la medición de síndrome sin colapsar el estado lógico codificado, asegurando que la detección de errores preserve la ocupación del espacio de código.",
    "B": "Sirven como generadores estabilizadores cuyos resultados de medición proporcionan síndromes de error, permitiendo al decodificador inferir qué errores físicos han ocurrido y determinar operaciones de corrección apropiadas sin medir directamente el estado del qubit lógico en sí.",
    "C": "Estos operadores definen el espacio de código como el espacio propio +1 simultáneo de todos los estabilizadores, y su medición proyecta el sistema sobre este espacio mientras revela qué qubits físicos experimentaron errores mediante desviaciones de valores propios de +1, pero crucialmente conmutan con todos los operadores lógicos, por lo que la extracción de síndrome deja intacta la información codificada, distinguiéndolos de las mediciones directas en base computacional que destruirían la superposición.",
    "D": "Los operadores de plaqueta imponen restricciones locales de paridad tipo Z mientras que los operadores de estrella imponen restricciones tipo X, juntos definen un conjunto conmutante de observables cuyos estados propios conjuntos abarcan el espacio de código, y la extracción de síndrome mide si el sistema permanece en este espacio después de que ocurren errores, con restricciones violadas identificando ubicaciones de error mediante emparejamiento perfecto de peso mínimo en el grafo de síndrome, aunque los operadores deben medirse cuidadosamente usando qubits ancilla para evitar proyectar el estado lógico.",
    "solution": "B"
  },
  {
    "id": 140,
    "question": "¿Cuál es una razón por la que el hardware FPGA es favorecido en sistemas avanzados de Distribución de Clave Cuántica (QKD)?",
    "A": "Operaciones paralelas de alta velocidad para procesamiento de claves, que permiten cribado en tiempo real, corrección de errores y amplificación de privacidad a las tasas de múltiples gigabits requeridas por redes de fibra metropolitanas y de larga distancia. El tejido lógico reconfigurable permite segmentación personalizada de algoritmos de reconciliación de base, decodificación adaptativa de síndrome para códigos LDPC y hash Toeplitz concurrente para extracción de aleatoriedad, todo mientras mantiene perfiles de latencia determinista críticos para sincronizar fuentes de entrelazamiento distribuidas a través de nodos geográficamente separados en redes cuánticas.",
    "B": "Lógica personalizada de alto rendimiento para postprocesamiento de números aleatorios cuánticos, que permite estimación de min-entropía acelerada por hardware, evaluación de extractor Toeplitz en tiempo real y monitoreo continuo de salud a las tasas de gigabit requeridas por protocolos de preparar-y-medir. El tejido reconfigurable permite segmentación personalizada de decorrelación de von Neumann, compensación adaptativa de sesgo para detectores de fotón único e implementación paralela de funciones de acondicionamiento de fuerza criptográfica, todo mientras mantiene tiempos de respuesta submicrosegundo críticos para ajustar dinámicamente patrones de modulación de fuente cuando las tasas de conteo oscuro del detector derivan durante operación continua a través de despliegues de fibra metropolitana con variación de temperatura.",
    "C": "Control de temporización de alta precisión para sincronización de detectores, que permite ventanas de coincidencia subnanosegundo, lógica de compuerta adaptativa para supresión de pulsación posterior y correlación de etiquetas de tiempo en tiempo real a las tasas de multimegaconteo requeridas por protocolos basados en entrelazamiento. La arquitectura reconfigurable permite implementación personalizada de tuberías de conversión tiempo-a-digital, compensación de retardo programable para dispersión cromática en fibra desplegada y acumulación de histograma paralelo para estimación de visibilidad, todo mientras mantiene especificaciones de fluctuación de escala de femtosegundo críticas para mantener contraste de interferencia de dos fotones al conectar múltiples longitudes de onda de fuente a través de infraestructuras de red cuántica metropolitana multiplexadas por división de longitud de onda.",
    "D": "Interfaz de canal clásico de alto ancho de banda para intercambio de mensajes autenticados, que permite motores de protocolo dedicados para intercambios desafío-respuesta, verificación MAC segmentada y derivación paralela de clave de sesión a las tasas de múltiples sesiones requeridas por topologías de red hub-spoke. La lógica reconfigurable permite máquinas de estado personalizadas para negociación de variantes BB84, validación de cadena de certificados acelerada por hardware durante autenticación inicial y procesamiento concurrente de múltiples flujos de usuario a través de infraestructura QKD compartida, todo mientras mantiene garantías de temporización de protocolo críticas para cumplir acuerdos de nivel de servicio cuando clientes empresariales establecen túneles VPN asegurados cuánticamente bajo demanda a través de redes cuánticas de acceso metropolitano operadas por operadores.",
    "solution": "A"
  },
  {
    "id": 141,
    "question": "¿Cuáles son las ventajas y limitaciones del Aprendizaje por Refuerzo Cuántico (QRL)?",
    "A": "El QRL logra una aceleración exponencial en la búsqueda de políticas mediante la codificación de amplitud cuántica de pares estado-acción combinada con amplificación de amplitud tipo Grover que concentra la masa de probabilidad en trayectorias de alta recompensa, permitiendo una convergencia más rápida que la exploración epsilon-greedy clásica. Sin embargo, la ventaja se degrada en la práctica porque la retroacción de la medición colapsa la representación superpuesta de la política después de cada episodio, forzando la re-preparación completa del estado que introduce una sobrecarga que escala linealmente con el tamaño del espacio de estados y anula la aceleración cuántica para problemas que requieren refinamiento iterativo de la política a lo largo de muchos episodios.",
    "B": "El QRL reduce la complejidad de muestreo al codificar funciones de valor como amplitudes cuánticas y explotar efectos de interferencia para suprimir secuencias de acción de baja recompensa mediante superposición destructiva, permitiendo que los agentes identifiquen políticas casi óptimas con polinomialmente menos interacciones con el entorno que el Q-learning clásico. Sin embargo, el hardware cuántico actual permanece inmaduro, con altas tasas de ruido despolarizante por implementaciones imperfectas de puertas y topologías de conectividad limitada que impiden la codificación eficiente de las matrices de transición dispersas típicas en procesos de decisión de Markov realistas, restringiendo el despliegue práctico a problemas de juguete con menos de 10 estados.",
    "C": "El QRL permite la evaluación paralela de exponencialmente muchos candidatos de política mediante superposición cuántica de trayectorias, combinada con técnicas de estimación de fase que extraen retornos esperados sin la sobrecarga del muestreo Monte Carlo clásico. Sin embargo, permanece impráctico porque la ventaja cuántica requiere evaluación coherente a través de pasos temporales más largos de lo que los tiempos T2 actuales permiten, y porque extraer parámetros de política clásicos de los resultados de medición cuántica introduce un cuello de botella de reconstrucción tomográfica que escala exponencialmente con el número de qubits necesarios para representar la estrategia del agente, negando la aceleración para problemas de escala significativa.",
    "D": "El QRL acelera el aprendizaje mediante paralelismo cuántico y ofrece exploración mejorada a través de superposición, permitiendo que los agentes muestreen múltiples trayectorias simultáneamente y descubran políticas óptimas de manera más eficiente que los métodos clásicos. Sin embargo, el hardware cuántico actual permanece inmaduro, con cantidades insuficientes de qubits, altas tasas de error y tiempos de coherencia cortos que impiden el despliegue práctico de algoritmos QRL en problemas del mundo real de escala significativa.",
    "solution": "D"
  },
  {
    "id": 142,
    "question": "¿Por qué es beneficiosa la paralelización de la ejecución de subcircuitos en esquemas de corte?",
    "A": "La paralelización reduce significativamente el tiempo real de ejecución al superponer ejecuciones cuánticas independientes a través de múltiples unidades de procesamiento, permitiendo la finalización más rápida de las exponencialmente muchas evaluaciones de subcircuitos requeridas para el corte de circuitos. Esta ganancia en eficiencia temporal es crucial para implementaciones prácticas donde el tiempo total de ejecución, más que la complejidad de muestreo únicamente, determina la viabilidad.",
    "B": "La ejecución paralela a través de múltiples procesadores cuánticos permite estrategias de muestreo adaptativo que concentran mediciones en términos de cuasiprobabilidad de alto peso, reduciendo el conteo total de disparos requerido para la reconstrucción. Al ejecutar subcircuitos simultáneamente y compartir estadísticas de medición intermedias entre dispositivos, el postprocesamiento clásico puede identificar qué contribuciones de cuasiprobabilidad dominan el valor esperado y reasignar dinámicamente recursos de muestreo en consecuencia, logrando reducción de varianza comparable al muestreo por importancia en métodos Monte Carlo sin la sobrecarga secuencial del repeso iterativo.",
    "C": "La paralelización mitiga la sobrecarga de muestreo exponencial inherente en descomposiciones de cuasiprobabilidad al distribuir la carga de medición a través de unidades de procesamiento cuántico independientes, permitiendo que el número total de evaluaciones de circuito se complete dentro de una ventana de tiempo fija. Dado que el paso de postprocesamiento clásico para el corte de circuitos requiere combinar resultados de todos los fragmentos de subcircuito con coeficientes de cuasiprobabilidad específicos, la ejecución paralela en N dispositivos reduce el número de rondas secuenciales necesarias por un factor de N, traduciendo la complejidad de muestreo exponencial de un cuello de botella temporal a un requerimiento de recurso espacial que escala más favorablemente con el hardware disponible.",
    "D": "La ejecución paralela de subcircuitos permite que técnicas de mitigación de errores cuánticos se apliquen independientemente a cada fragmento antes de la reconstrucción clásica, mejorando la fidelidad de la estimación del valor esperado final. Dado que los protocolos de corte descomponen el circuito original en subcircuitos más pequeños que caben dentro de los límites de coherencia de los dispositivos disponibles, ejecutar estos fragmentos simultáneamente en procesadores separados previene la acumulación de errores por ejecución secuencial mientras permite extrapolación de ruido cero o cancelación de error probabilística por fragmento que sería inviable para el circuito completo sin cortar, reduciendo así la tasa de error efectiva en el observable reconstruido sin aumentar el conteo total de disparos.",
    "solution": "A"
  },
  {
    "id": 143,
    "question": "Si un qubit comienza en el estado |ψ⟩ = α|0⟩ + β|1⟩, ¿cómo afecta un error combinado de inversión de bit (X) e inversión de fase (Z) al estado?",
    "A": "El estado se convierte en α|1⟩ + β|0⟩, porque X primero intercambia los estados de base produciendo β|0⟩ + α|1⟩, luego Z aplica una fase solo a componentes |1⟩, pero como X ya ha movido el α original a la posición |1⟩, la fase afecta a α no a β.",
    "B": "El estado se convierte en α|1⟩ - β|0⟩.",
    "C": "El estado se convierte en -α|1⟩ + β|0⟩, porque ZX aplica Z primero (produciendo α|0⟩ - β|1⟩), luego X intercambia para dar -β|0⟩ + α|1⟩, pero la fase global hace esto equivalente a β|0⟩ - α|1⟩ salvo normalización.",
    "D": "El estado se convierte en β|1⟩ - α|0⟩, porque cuando X y Z se componen como XZ, la fase de la puerta Z se aplica en la base computacional antes de que X reordene los estados, así que el signo menos se adjunta a cualquier amplitud que estuviera originalmente en |1⟩.",
    "solution": "B"
  },
  {
    "id": 144,
    "question": "¿Qué factor se ha demostrado que no mejora consistentemente el rendimiento de los clasificadores cuánticos?",
    "A": "Reducir la dimensionalidad de entrada mediante métodos agresivos de compresión de características o proyección aleatoria tiende a eliminar las correlaciones sutiles y componentes de alta frecuencia que los circuitos cuánticos están teóricamente mejor adaptados para capturar, neutralizando así la potencial ventaja cuántica. Cuando los vectores de entrada se comprimen desde sus dimensiones nativas (a menudo 100+ características en conjuntos de datos del mundo real) hasta coincidir con el conteo de qubits (típicamente 4-10 qubits en hardware actual), el rendimiento de clasificación frecuentemente se degrada en un 10-20% comparado con usar el conjunto completo de características. Este efecto es especialmente pronunciado en problemas donde se espera que los kernels cuánticos superen a los métodos clásicos, porque la reducción de dimensionalidad efectivamente fuerza los datos a un régimen donde los algoritmos clásicos ya funcionan casi óptimamente, haciendo redundante el enfoque cuántico.",
    "B": "Optimizar el preprocesamiento clásico de datos de entrada mediante técnicas de reducción de dimensionalidad como análisis de componentes principales o selección de características ha demostrado empíricamente degradar la precisión del clasificador cuántico en muchos casos, particularmente cuando las características descartadas contienen correlaciones no lineales que el circuito cuántico podría haber explotado. Estudios en dispositivos NISQ revelan que reducir la dimensionalidad de entrada de, digamos, 64 características a 16 características a menudo causa que la precisión de clasificación caiga en 8-12 puntos porcentuales porque la capacidad del kernel cuántico para mapear datos en un espacio de Hilbert de alta dimensión es efectivamente desperdiciada cuando el preprocesamiento clásico ya ha colapsado el espacio de características. Este hallazgo contraintuitivo sugiere que la ventaja cuántica depende críticamente de exponer el circuito cuántico a los vectores de características crudos y de alta dimensión en lugar de resúmenes preprocesados.",
    "C": "Aumentar el número de qubits asignados a un clasificador cuántico generalmente falla en mejorar el rendimiento una vez que el conteo de qubits excede la dimensionalidad intrínseca del problema de clasificación, y a menudo causa degradación debido a la dilución de la densidad de información a través del espacio de Hilbert expandido. Por ejemplo, escalar de 10 a 20 qubits para una tarea de clasificación binaria en conjuntos de datos con solo 8-10 características relevantes típicamente resulta en sobreajuste y mayor susceptibilidad a mesetas áridas durante el entrenamiento, ya que el espacio de parámetros exponencialmente mayor se vuelve disperso relativo a los datos de entrenamiento disponibles. Evaluaciones empíricas en conjuntos de datos como MNIST muestran que las mejoras de precisión se estancan o incluso disminuyen más allá de 12-15 qubits, sugiriendo que simplemente agregar qubits sin un diseño arquitectónico cuidadoso desperdicia recursos cuánticos y tiempo de entrenamiento.",
    "D": "Introducir entrelazamiento más allá de los niveles mínimos requeridos para la tarea de clasificación a menudo falla en mejorar el rendimiento del clasificador cuántico y puede realmente degradar la precisión cuando las puertas de entrelazamiento introducen ruido adicional sin proporcionar ventaja computacional. Estudios en dispositivos de corto plazo muestran que circuitos altamente entrelazados con profundidad que excede 15-20 capas típicamente tienen un rendimiento inferior a alternativas menos entrelazadas debido a la decoherencia acumulada.",
    "solution": "D"
  },
  {
    "id": 145,
    "question": "¿Qué sucede si el estado inicial en el algoritmo de Grover no es la superposición uniforme?",
    "A": "El número de iteraciones requeridas para encontrar el estado marcado escala exponencialmente con el tamaño de la base de datos en lugar de cuadráticamente, porque la distribución inicial no uniforme rompe el mecanismo de amplificación de amplitud que depende de la reflexión simétrica sobre la amplitud media. Esto efectivamente reduce el algoritmo de Grover al rendimiento del muestreo aleatorio clásico, requiriendo O(N) consultas en lugar de O(√N), ya que el ángulo de rotación por iteración se vuelve insignificantemente pequeño al comenzar desde un estado de base computacional arbitrario.",
    "B": "El algoritmo proyecta automáticamente el estado inicial sobre el subespacio correcto mediante un paso de ortogonalización implícito que ocurre durante la primera llamada al oráculo, realizando esencialmente un proceso cuántico de Gram-Schmidt que restaura la superposición uniforme sobre el espacio de búsqueda. Este mecanismo de autocorrección está integrado en la estructura del operador de difusión, que mide la superposición con el estado de superposición igual y reescala las amplitudes en consecuencia, asegurando que las iteraciones subsiguientes procedan exactamente como si el algoritmo hubiera comenzado correctamente.",
    "C": "El algoritmo aún funciona y encuentra el estado marcado, pero con probabilidad de éxito reducida que depende de la superposición entre el estado inicial y la superposición uniforme sobre el espacio de búsqueda.",
    "D": "Ocurre un fallo completo porque la estructura de reflexión del oráculo depende críticamente de comenzar desde la superposición igual sobre todos los estados de base computacional.",
    "solution": "C"
  },
  {
    "id": 146,
    "question": "En el contexto de la teoría de complejidad cuántica y modelos físicos exóticos, imagine un marco computacional donde los sistemas cuánticos pueden acceder a curvas temporales cerradas (CTCs) pero solo mediante postselección — es decir, seleccionamos resultados después del hecho en lugar de garantizarlos causalmente. Esto plantea preguntas profundas sobre la relación entre paradojas temporales y poder computacional. ¿Cuál es la significancia de los modelos de postselección con curvas temporales cerradas en la teoría de complejidad cuántica?",
    "A": "Demuestran que PostBQP es igual a PP a través del resultado de Aaronson, mostrando que la computación cuántica postseleccionada captura precisamente el poder de la computación clásica probabilística con error no acotado, donde la postselección sobre resultados de medición permite resolver problemas en la jerarquía de conteo explotando interferencia cuántica para amplificar resultados deseados, aunque sin CTCs esto requiere solo postselección estándar sobre bases de medición.",
    "B": "Colapsan la jerarquía polinomial, revelando poder computacional extremo bajo supuestos de física exótica donde las CTCs postseleccionadas permiten soluciones a problemas más allá de las clases de complejidad cuántica convencionales explotando paradojas temporales para satisfacer retroactivamente condiciones de consistencia.",
    "C": "Prueban que BQP/qpoly es igual a PSPACE a través de la condición de consistencia de Deutsch, donde circuitos cuánticos con acceso a CTC postseleccionadas pueden resolver fórmulas booleanas cuantificadas preparando estados autoconsistentes protegidos por cronología que codifican todas las trayectorias de ramificación simultáneamente, requiriendo solo cadenas de consejo cuántico de tamaño polinomial para especificar la estructura causal válida que coincide con el resultado computacional deseado.",
    "D": "Establecen PP-completitud para modelos CTC postseleccionados mostrando que la retroalimentación temporal permite el conteo exacto de asignaciones satisfactorias, donde circuitos cuánticos que postseleccionan sobre restricciones de consistencia logran computación #P-dura mediante el mecanismo de protección cronológica de Lloyd que convierte testigos NP en bucles causales verificables en tiempo polinomial, resolviendo efectivamente problemas más allá de la jerarquía polinomial estándar.",
    "solution": "B"
  },
  {
    "id": 147,
    "question": "Un circuito cuántico parametrizado alcanza alta expresividad cubriendo densamente el espacio de Hilbert. Sin embargo, el entrenamiento no logra converger. ¿Cuál es la explicación más probable?",
    "A": "El circuito alcanza un umbral de aproximación universal y se vuelve inestable una vez que gana suficiente expresividad para representar transformaciones unitarias arbitrarias, momento en el cual el espacio de parámetros transita a un régimen caótico donde pequeñas perturbaciones en ángulos de puertas conducen a saltos discontinuos en el estado de salida.",
    "B": "Entrelazamiento insuficiente en la estructura del ansatz, porque la alta expresividad del circuito se logra principalmente mediante capas profundas de rotaciones de un qubit en lugar de puertas de entrelazamiento multi-qubit. Sin entrelazamiento adecuado entre qubits, la dimensionalidad efectiva del espacio de Hilbert accesible permanece polinomial en lugar de exponencial en el número de qubits, creando un cuello de botella representacional que impide al circuito codificar las correlaciones necesarias para aproximar la función objetivo.",
    "C": "Mesetas áridas (barren plateaus) — el paisaje de gradientes se vuelve exponencialmente plano a medida que aumenta la profundidad del circuito, haciendo la optimización basada en gradientes esencialmente imposible. Esta es una maldición bien conocida de espacios de parámetros de alta dimensionalidad en circuitos cuánticos, donde la varianza de los gradientes desaparece exponencialmente con el tamaño del sistema. A medida que el circuito se vuelve más expresivo mediante capas adicionales, la probabilidad de que una inicialización aleatoria se encuentre en una región con magnitud de gradiente apreciable decrece exponencialmente, dejando al optimizador incapaz de encontrar direcciones de descenso significativas independientemente de la tasa de aprendizaje o el tamaño de lote empleado.",
    "D": "La expresividad causa colapso de modos en la variedad de la función de costo, lo que impide al optimizador explorar diferentes regiones del espacio de soluciones efectivamente porque circuitos altamente expresivos generan paisajes de costo con un número exponencialmente proliferante de óptimos locales que son casi degenerados en energía. El optimizador queda atrapado en un modo particular correspondiente a una familia de soluciones, incapaz de transitar entre modos debido a la probabilidad de tunelamiento desvaneciente a través de las barreras de alta dimensionalidad que los separan.",
    "solution": "C"
  },
  {
    "id": 148,
    "question": "¿Por qué puede cuantificarse la \"propagación de operadores\" mediante correladores fuera del orden temporal (OTOCs)?",
    "A": "Los OTOCs miden el crecimiento de operadores cuantificando el anticonmutador al cuadrado ⟨{W(t), V(0)}†{W(t), V(0)}⟩, que comienza cerca de cero para operadores espacialmente separados y crece a medida que la evolución temporal hace que operadores inicialmente locales desarrollen soporte en regiones expansivas del sistema. Este crecimiento señala la propagación de información a través del espacio de Hilbert de muchos cuerpos—cuando el anticonmutador se vuelve grande, indica que los operadores W y V han desarrollado regiones de soporte superpuestas y que la información cuántica se ha propagado a través del sistema. El OTOC proporciona un diagnóstico para el mezclado (scrambling) porque los anticonmutadores de operadores cuantifican el grado en que W(t) ha crecido de un operador local a un operador complejo de muchos cuerpos cuyo soporte se superpone con la región de V, con la función de correlación de cuatro puntos midiendo directamente cómo la estructura del operador evolucionado temporalmente ha cambiado de su forma localizada inicial, permitiendo la identificación de la escala temporal de mezclado y la velocidad de Lieb-Robinson que gobierna la propagación de información en el sistema cuántico.",
    "B": "Los OTOCs miden cómo operadores inicialmente conmutantes fallan en conmutar bajo evolución de Heisenberg, proporcionando un diagnóstico para el crecimiento de operadores y el mezclado de información en sistemas cuánticos de muchos cuerpos. Específicamente, el OTOC cuantifica el conmutador al cuadrado ⟨[W(t), V(0)]†[W(t), V(0)]⟩, que comienza cerca de cero cuando los operadores W y V están espacialmente separados y crece a medida que la evolución temporal hace que operadores inicialmente locales desarrollen soporte en una región expansiva del sistema. Este crecimiento señala que la información del operador se está propagando a través del espacio de Hilbert de muchos cuerpos—cuando el conmutador se vuelve grande, indica que realizar la operación V seguida de W(t) produce un resultado diferente al orden inverso, demostrando que el operador W ha crecido para superponerse con la región de soporte de V y que la información cuántica se ha propagado a través del sistema.",
    "C": "Los OTOCs cuantifican la propagación de operadores midiendo la función de correlación de cuatro puntos ⟨W(t)V(0)W(t)V(0)⟩, que detecta cómo operadores inicialmente localizados desarrollan soporte no local mediante evolución temporal en sistemas de muchos cuerpos. El OTOC comienza cerca de la unidad cuando los operadores W y V están espacialmente separados y decae a medida que la dinámica de mezclado hace que W(t) crezca a un operador complejo con soporte superpuesto con la región de V, señalando la propagación de información a través del sistema cuántico. Esta decaída mide directamente el crecimiento del operador porque la función de cuatro puntos se vuelve pequeña cuando W(t) y V fallan en conmutar, indicando que W se ha propagado de su forma local inicial a un operador de muchos cuerpos cuya acción ya no conmuta con operadores distantes, proporcionando un diagnóstico cuantitativo para el tiempo de mezclado y la velocidad mariposa que caracteriza qué tan rápido el soporte del operador se expande a través del sistema durante la evolución caótica.",
    "D": "Los OTOCs proporcionan una medida de propagación de operadores evaluando el crecimiento del conmutador evolucionado temporalmente mediante el valor esperado ⟨[W(t), V(0)]²⟩, que caracteriza cómo operadores inicialmente locales adquieren soporte en grados de libertad distantes bajo evolución hamiltoniana en sistemas fuertemente interactuantes. El OTOC captura el crecimiento del operador porque el conmutador al cuadrado cuantifica directamente la medida en que W se ha propagado desde su soporte localizado inicial para desarrollar superposición no nula con el operador espacialmente separado V, con la función de correlación comenzando cerca de cero y creciendo exponencialmente durante el régimen de mezclado antes de saturarse en tiempos tardíos. Esta medición distingue dinámica integrable de caótica porque la propagación de operadores en sistemas caóticos sigue un efecto mariposa donde el conmutador crece exponencialmente con tasa λL (el exponente de Lyapunov cuántico), mientras que los sistemas integrables exhiben solo crecimiento polinomial o balístico, haciendo del OTOC un diagnóstico poderoso para la propagación de información y el caos cuántico de muchos cuerpos mediante su sensibilidad a la redistribución del peso del operador a través del sistema.",
    "solution": "B"
  },
  {
    "id": 149,
    "question": "¿Cómo facilita el uso de tramas o ranuras temporales el enrutamiento en redes fotónicas?",
    "A": "La división en ranuras temporales sincroniza la red para que cada nodo sepa cuándo esperar fotones que intentan distribución de entrelazamiento, previniendo conflictos de enrutamiento donde múltiples fuentes apuntan simultáneamente al mismo puerto de salida del conmutador o canal de longitud de onda. Al discretizar la transmisión en ventanas programadas, el controlador de red puede pre-asignar rutas a través del tejido de conmutación que garantizan operación sin bloqueo incluso cuando múltiples pares de fotones atraviesan enlaces físicos superpuestos.",
    "B": "Los protocolos basados en tramas asignan a cada canal cuántico una ventana de transmisión periódica durante la cual la fuente emite pares de fotones heraldados, con enrutadores usando el índice de ranura temporal para determinar decisiones de reenvío sin requerir información de encabezado por fotón. Esta multiplexación temporal permite que múltiples intentos de distribución de entrelazamiento compartan la misma infraestructura de fibra intercalando sus tramas de transmisión, aunque el esquema requiere sincronización de reloj a nivel de red dentro del tiempo de coherencia del fotón para mantener indistinguibilidad en nodos de fusión.",
    "C": "Alinear los intentos de entrelazamiento con ventanas programadas evita colisiones, permitiendo que fuentes multiplexadas compartan recursos de red sin interferencia destructiva. Al discretizar el tiempo en ranuras, los conmutadores de enrutamiento fotónico saben exactamente cuándo esperar qubits y pueden coordinar reservaciones de ruta que previenen que múltiples fotones contengan simultáneamente por el mismo puerto de salida o canal de longitud de onda.",
    "D": "Las arquitecturas de ranuras temporales permiten enrutamiento determinista asignando a cada fuente un desplazamiento de trama fijo que codifica su dirección de red en el dominio temporal, permitiendo a conmutadores intermedios decodificar información de enrutamiento a partir de tiempos de llegada de fotones relativos al pulso de sincronización global. Esto elimina la necesidad de mensajes de control clásicos para configurar estados de conmutador porque la estructura de trama periódica implícitamente porta información de ruta, aunque el esquema requiere que todos los fotones dentro de una ranura sean temporalmente indistinguibles para preservar interferencia cuántica en divisores de haz usados para mediciones de estado de Bell.",
    "solution": "C"
  },
  {
    "id": 150,
    "question": "¿Por qué una puerta con menor expresividad aún podría superar a una puerta altamente expresiva en algunas implementaciones de algoritmos?",
    "A": "Las puertas más simples con menor expresividad pueden ser más eficientes cuando la estructura del algoritmo no demanda patrones complejos de entrelazamiento o preparaciones intrincadas de estados. Típicamente requieren menos recursos físicos para implementarse, tienen tiempos de puerta más cortos que reducen la exposición a la decoherencia, y a menudo están mejor caracterizadas y calibradas en hardware. Cuando la tarea computacional puede lograrse con expresividad limitada, usar puertas más complejas innecesariamente aumenta la profundidad del circuito y la acumulación de errores sin proporcionar beneficio algorítmico adicional.",
    "B": "Las puertas altamente expresivas típicamente requieren descomposición en secuencias más largas de operaciones nativas durante la compilación, lo que aumenta tanto la profundidad del circuito como el conteo total de puertas. Para algoritmos donde las transformaciones unitarias requeridas yacen dentro de un subespacio de baja dimensionalidad de SU(2^n), las puertas más simples pueden apuntar directamente a este subespacio sin invocar grados de libertad innecesarios. Además, las técnicas de caracterización de hardware y mitigación de errores a menudo están optimizadas para puertas de baja expresividad comúnmente usadas, produciendo mejor fidelidad efectiva. Cuando la tarea algorítmica no explota la expresividad completa, el costo adicional de puertas complejas—requisitos más largos de tiempo de coherencia y errores de fase acumulados—supera sus ventajas teóricas.",
    "C": "Las puertas de menor expresividad generan representaciones más dispersas en el formalismo de matriz de transferencia de Pauli, lo que significa que sus canales de error se acoplan a menos elementos fuera de diagonal en la matriz de proceso. Este acoplamiento reducido se traduce en características más favorables de propagación de errores cuando se componen en circuitos profundos. Para algoritmos dominados por operaciones diagonales o mediciones de base computacional, las puertas altamente expresivas introducen rotaciones innecesarias en bases conjugadas que amplifican la decoherencia de fase. Dado que los algoritmos variacionales a menudo convergen a soluciones dentro de subespacios restringidos del espacio de Hilbert completo, las puertas más simples que operan nativamente en esos subespacios evitan los desafíos de optimización de parámetros y el costo adicional de estimación de gradientes asociados con puertas expresivas sobre-parametrizadas.",
    "D": "Las puertas expresivas a menudo exhiben decaimiento de fidelidad no monotónico como función del tiempo de implementación debido a errores de control coherentes que se acumulan cuadráticamente con la complejidad del pulso, mientras que las puertas más simples se benefician de escalamiento lineal de errores en la expansión de Magnus del operador de evolución temporal. Para algoritmos donde el estado de solución tiene baja entropía de entrelazamiento (como ciertos estados fundamentales en VQE), las puertas altamente expresivas generan correlaciones bipartitas excesivas que deben posteriormente deshacerse mediante capas de circuito adicionales. Esto crea rutas computacionales redundantes que aumentan la susceptibilidad a errores correlacionados. Además, la fuga espectral inherente en puertas expresivas multi-parámetro conduce a población no intencionada de niveles fuera del subespacio computacional en implementaciones de hardware reales.",
    "solution": "A"
  },
  {
    "id": 151,
    "question": "¿Qué estructura matemática fundamental se utiliza para describir los estados cuánticos en computación cuántica?",
    "A": "El Espacio de Minkowski, la variedad pseudo-riemanniana de cuatro dimensiones que combina tres dimensiones espaciales con una dimensión temporal, proporciona el marco geométrico para la evolución del estado cuántico porque las transformaciones unitarias sobre qubits deben preservar el intervalo espacio-temporal entre vectores de estado para mantener la causalidad y asegurar que las probabilidades de medición permanezcan invariantes bajo boosts de Lorentz aplicados a marcos de referencia de dispositivos cuánticos separados",
    "B": "El Espacio Euclídeo sirve como base ya que las amplitudes cuánticas deben satisfacer el producto interno estándar derivado del teorema de Pitágoras, y el requisito de que los resultados de medición correspondan a probabilidades de valores reales restringe los estados cuánticos a espacios vectoriales de dimensión finita con norma euclídea convencional",
    "C": "El Espacio de Hilbert, el espacio con producto interno completo de vectores complejos donde residen los estados cuánticos como rayos, equipado con la estructura necesaria para representar superposición, calcular amplitudes de probabilidad mediante productos internos y describir evolución unitaria.",
    "D": "El Plano Cartesiano representa con precisión los estados cuánticos porque los sistemas de un solo qubit se caracterizan completamente por dos parámetros reales correspondientes a componentes de polarización horizontal y vertical, y los sistemas de múltiples qubits se construyen tomando productos directos de sistemas de coordenadas reales bidimensionales",
    "solution": "C"
  },
  {
    "id": 152,
    "question": "En algoritmos cuánticos para aprendizaje automático, el Análisis de Componentes Principales Cuántico (QPCA) se ha propuesto como un método para lograr una aceleración exponencial sobre el PCA clásico bajo ciertas condiciones. La ventaja teórica surge de la capacidad de procesar datos de alta dimensionalidad codificados en estados cuánticos. Sin embargo, esta aceleración depende críticamente de componentes algorítmicos específicos y suposiciones sobre el acceso a datos. ¿Cuál es el recurso cuántico principal que proporciona a QPCA su ventaja potencial sobre enfoques clásicos al analizar conjuntos de datos con espacios de características exponencialmente grandes?",
    "A": "La estimación de fase cuántica, que permite la extracción de autovalores y autovectores de la matriz de densidad que codifica la estructura de covarianza de los datos en profundidad logarítmica, siempre que los datos puedan cargarse eficientemente en estados cuánticos y la brecha entre autovalores principales sea suficientemente grande para resolverlos dentro de los requisitos de precisión de la aplicación. La ventaja exponencial emerge porque la estimación de fase en un sistema de n qubits puede distinguir autovalores con precisión polinomial usando solo O(poly(n)) puertas, mientras que los algoritmos clásicos de descomposición de autovalores requieren tiempo al menos lineal en la dimensión de la matriz 2ⁿ. Esta ventaja cuántica se aplica específicamente a la tarea de preparar estados cuánticos proporcionales a los autovectores principales y estimar sus autovalores correspondientes, permitiendo que los algoritmos de aprendizaje automático cuántico posteriores operen en el subespacio de componentes principales sin construir explícitamente la matriz de covarianza completa o realizar diagonalización clásica en estructuras de datos exponencialmente grandes.",
    "B": "La amplificación de amplitud cuántica aplicada iterativamente para aumentar el solapamiento entre estados de prueba y los autovectores principales de la matriz de covarianza, permitiendo la extracción de subespacios propios dominantes en tiempo logarítmico en el número de condición en lugar de polinomial como requieren los métodos clásicos de iteración de potencia. La ventaja exponencial emerge porque la amplificación de amplitud en un sistema de n qubits puede mejorar la amplitud de los componentes del autovector objetivo por un factor de √(2ⁿ) usando solo O(√(2ⁿ)) iteraciones, mientras que los enfoques clásicos requieren Ω(2ⁿ) operaciones para lograr precisión comparable al trabajar con matrices de covarianza exponencialmente grandes. Este recurso cuántico permite a QPCA preparar estados aproximados de componentes principales y estimar sus autovalores con precisión ε usando O(poly(n, 1/ε)) operaciones, siempre que esté disponible el acceso cuántico eficiente a los datos y la brecha de autovalores entre componentes principales y no principales exceda el umbral de amplificación requerido para distinguir componentes mediante efectos de interferencia en la distribución de amplitud del estado cuántico preparado.",
    "C": "La transformación de valores singulares cuántica, que permite la evaluación de funciones polinomiales sobre el espectro de autovalores de la matriz de densidad mediante aplicaciones controladas de operadores de codificación de bloques, permitiendo la extracción de componentes principales en tiempo logarítmico en la dimensión de la matriz cuando se combina con oráculos de preparación de estado eficientes. La ventaja exponencial emerge porque la transformación de valores singulares sobre una matriz codificada en n qubits puede aplicar funciones umbral que proyectan sobre el subespacio propio principal usando solo O(poly(n)) puertas, mientras que la descomposición de autovalores clásica requiere tiempo al menos Ω(2ⁿ) para análisis espectral explícito de estructuras de covarianza exponencialmente grandes. Este recurso cuántico permite a QPCA implementar funciones de corte suaves que aíslan autovalores por encima de un umbral especificado, preparando estados cuánticos soportados principalmente en el subespacio de autovectores dominantes sin requerir diagonalización completa, siempre que los datos de entrada admitan preparación de estado cuántico eficiente y la brecha de autovalores exceda la precisión de transformación necesaria para distinguir componentes principales de no principales mediante filtrado polinomial del espectro de la matriz.",
    "D": "La transformada de Fourier cuántica aplicada a la estructura de correlación temporal de muestras de datos secuenciales, permitiendo la extracción eficiente de componentes principales en el dominio de frecuencia mediante mecanismos de retroceso de fase que codifican información de autovalores en fases de qubits auxiliares. La ventaja exponencial emerge porque la QFT sobre un registro de n qubits transforma entre representaciones temporal y frecuencial usando solo O(n²) puertas, mientras que el análisis de covarianza clásico basado en FFT requiere Ω(2ⁿ log 2ⁿ) operaciones al procesar espacios de características exponencialmente grandes codificados en amplitudes cuánticas. Este recurso cuántico permite a QPCA identificar componentes de frecuencia dominantes correspondientes a autovectores principales midiendo fases auxiliares después de aplicaciones controladas del operador de covarianza de datos, con el protocolo de estimación de fase resolviendo autovalores con precisión ε usando O(1/ε) repeticiones siempre que la brecha espectral entre autovalores principales exceda el límite de resolución de fase determinado por el número de qubits auxiliares asignados para el análisis de frecuencia de la matriz de correlación codificada cuánticamente.",
    "solution": "A"
  },
  {
    "id": 153,
    "question": "¿Cuál es la ventaja principal de usar qubits colgantes en compilación cuántica distribuida basada en LNN?",
    "A": "Flexibilidad de enrutamiento—menos SWAPs para interacciones no locales, ya que los qubits colgantes en los límites del módulo pueden servir como áreas de preparación temporal para información cuántica que se transfiere entre qubits distantes sin consumir recursos de conectividad interior.",
    "B": "Reutilización de ancillas durante protocolos de distribución de entrelazamiento, donde los qubits colgantes sirven como recursos reiniciables para generar pares de Bell entre módulos sin requerir sobrecarga completa de reinicialización. Dado que los qubits de frontera se conectan a un solo vecino computacional, pueden participar en intentos repetidos de generación de entrelazamiento con módulos externos mientras la cadena LNN interior continúa ejecutando operaciones lógicas, efectivamente canalizando el establecimiento de entrelazamiento con el cómputo.",
    "C": "Reducción de interferencia cruzada de módulos adyacentes al posicionar qubits sensibles en sitios colgantes donde experimentan menor densidad de conectividad, minimizando canales de acoplamiento no deseados que degradan fidelidades de puertas. La topología de vecino único de los qubits colgantes los aísla naturalmente de mecanismos de propagación de error de múltiples rutas que afectan a los qubits interiores con conectividad bidireccional LNN, mejorando los tiempos de coherencia generales del módulo mediante supresión topológica de errores.",
    "D": "Paralelismo mejorado para redes SWAP distribuidas donde los qubits colgantes permiten la ejecución concurrente de operaciones de enrutamiento en múltiples módulos sin cuellos de botella de serialización. Al dedicar qubits de frontera exclusivamente a comunicación entre módulos mientras los qubits interiores manejan puertas locales, la compilación puede superponer secuencias SWAP remotas con cómputo en curso, efectivamente ocultando la latencia de operaciones no locales detrás de trabajo productivo en qubits interiores que de otro modo permanecerían inactivos durante fases de enrutamiento.",
    "solution": "A"
  },
  {
    "id": 154,
    "question": "¿En qué escenario superaría QkNN al kNN clásico? Esta es una pregunta que ha sido debatida extensamente en la comunidad de aprendizaje automático cuántico, y la respuesta depende críticamente de cómo modelamos tanto los datos como las suposiciones de hardware que estamos dispuestos a hacer.",
    "A": "Cuando los cálculos de distancia dominan el tiempo de ejecución y la codificación de amplitud cuántica permite la estimación de distancia entre estados cuánticos en tiempo O(log N) por consulta usando circuitos de test SWAP, comparado con cálculos de distancia clásicos O(N), siempre que esté disponible el acceso coherente a RAM cuántica.",
    "B": "Cuando los datos residen en espacios de características exponencialmente de alta dimensión donde la codificación de amplitud cuántica proporciona compresión logarítmica, y los cálculos de distancia pueden explotar la interferencia cuántica para lograr aceleración polinomial sobre la búsqueda clásica de vecino más cercano.",
    "C": "Cuando los datos de entrenamiento llegan como estados cuánticos de sensores o simuladores cuánticos, evitando el costo exponencial de reconstrucción tomográfica clásica, y la estimación de distancia cuántica puede realizarse directamente en el dominio cuántico usando métricas basadas en fidelidad.",
    "D": "Cuando el espacio de características exhibe una estructura de producto tensorial que el kNN clásico no puede explotar eficientemente, pero QkNN puede aprovechar mediante métricas de distancia basadas en entrelazamiento que capturan correlaciones inaccesibles a representaciones clásicas separables, como se demuestra en conjuntos de datos estructurados con simetrías jerárquicas.",
    "solution": "B"
  },
  {
    "id": 155,
    "question": "¿Por qué la cancelación de cadenas de Jordan-Wigner mejora la eficiencia en la simulación de interacciones fermiónicas?",
    "A": "Los términos de salto adyacentes en el Hamiltoniano fermiónico comparten cadenas de Jordan-Wigner superpuestas cuyo producto tensorial se simplifica cuando las puertas se aplican consecutivamente, ya que los operadores Z en sitios intermedios aparecen en ambas cadenas con orientaciones opuestas. Esta cancelación algebraica reduce operaciones controladas de múltiples qubits a puertas de dos qubits para interacciones de vecino más cercano, reduciendo directamente la profundidad del circuito proporcional a la conectividad de la red.",
    "B": "Las operaciones de paridad sucesivas golpean regiones orbitales compartidas y cancelan las colas de la cadena Z cuando los términos de salto fermiónicos se aplican secuencialmente, reduciendo la profundidad efectiva de la puerta de O(n) a O(1) por término de interacción.",
    "C": "Las rotaciones de base adaptadas a simetría alinean la transformación de Jordan-Wigner con números cuánticos conservados como el espín total o el número de partículas, causando que las cadenas que codifican estas simetrías se factoricen en representaciones bloque-diagonales. Dentro de cada sector de simetría, las cadenas de paridad colapsan a factores de fase que pueden rastrearse clásicamente en lugar de implementarse como puertas cuánticas, reduciendo la sobrecarga para simulaciones restringidas a variedades específicas de carga o espín.",
    "D": "Las relaciones de conmutador entre operadores fermiónicos se mapean a álgebra de Pauli simplificada cuando múltiples términos de salto involucran los mismos índices orbitales, ya que la estructura de anticonmutación fuerza a ciertas cadenas de Jordan-Wigner a telescoparse durante la construcción del circuito. Este efecto telescópico significa que la aplicación secuencial de puertas fermiónicas produce un unitario neto cuya longitud de cadena crece sublinealmente con el número de términos, particularmente en descomposiciones de Trotter donde el ordenamiento sistemático explota patrones de adyacencia orbital.",
    "solution": "B"
  },
  {
    "id": 156,
    "question": "En algoritmos cuánticos para simulación a temperatura finita, ¿cuál es la motivación detrás del uso del ansatz de espectro producto (PSA)?",
    "A": "El ansatz de espectro producto parametriza estados térmicos como operadores de densidad de producto matricial con dimensión de enlace que escala logarítmicamente con la temperatura inversa, permitiendo una representación eficiente de correlaciones a temperatura finita mediante contracciones de redes tensoriales que evitan el costo exponencial del almacenamiento completo de la matriz de densidad. Al optimizar variacionalmenete los elementos tensoriales para minimizar la energía libre de Helmholtz usando barridos tipo TEBD en tiempo imaginario adaptados para estados mixtos, PSA captura fluctuaciones térmicas esenciales con profundidad de circuito polinomial en el tamaño del sistema. Esto elimina la necesidad de purificación basada en ancillas o tomografía completa del estado mientras se recuperan con precisión los valores esperados térmicos, haciéndolo práctico para hardware de corto plazo donde los tiempos de coherencia restringen la profundidad del circuito.",
    "B": "PSA construye estados térmicos aproximados preparando estados mixtos separables sobre subsistemas locales y refinando iterativamente las matrices de densidad de un solo sitio mediante actualizaciones autoconsistentes de campo medio que minimizan la divergencia de Kullback-Leibler del verdadero ensamble de Gibbs. Este procedimiento variacional converge a un ansatz de estado producto cuyas marginales coinciden con la distribución térmica exacta en el límite de correlaciones débiles entre qubits, evitando completamente circuitos costosos de propagación en tiempo imaginario. El enfoque se vuelve práctico en hardware de corto plazo porque solo requiere rotaciones locales de un qubit y optimización clásica de O(N) parámetros reales, evitando el escalamiento exponencial de la tomografía completa del estado y las demandas de tiempo de coherencia de circuitos cuánticos profundos.",
    "C": "El ansatz de espectro producto aprovecha la observación de que las matrices de densidad térmicas pueden diagonalizarse mediante un cambio de base que mapea el hamiltoniano del sistema en una forma no interactuante, permitiendo la extracción de valores propios mediante deflación cuántica variacional sin requerir circuitos de estimación de fase o qubits ancilla para amplificación de amplitud, reduciendo así la profundidad del circuito a escalas alcanzables en dispositivos NISQ.",
    "D": "PSA permite la preparación de estados térmicos de Gibbs aproximados usando circuitos cuánticos de profundidad limitada construyendo funciones de onda variacionales que minimizan la energía libre del sistema comenzando desde estados producto simples sobre qubits individuales. Este enfoque evita la evolución costosa en tiempo imaginario o la tomografía completa del estado, haciéndolo práctico para hardware cuántico de corto plazo donde la profundidad del circuito y los tiempos de coherencia están severamente restringidos, mientras aún captura correlaciones térmicas esenciales necesarias para propiedades a temperatura finita.",
    "solution": "D"
  },
  {
    "id": 157,
    "question": "¿En qué se diferencian los algoritmos de enrutamiento probabilístico de los determinísticos?",
    "A": "Construyen un grafo ponderado donde los costos de las aristas representan la fidelidad del entrelazamiento, luego aplican el algoritmo de Dijkstra para identificar la ruta globalmente óptima para cada solicitud de comunicación. Una vez calculada, esta ruta de mayor fidelidad se almacena en caché y se reutiliza para solicitudes subsiguientes entre el mismo par de nodos, asegurando tasas consistentes de éxito de teletransportación. La selección determinística explota la localidad temporal en las condiciones de la red, evitando el costo de reevaluar rutas cuando las calidades de los enlaces permanecen estables a través de múltiples rondas.",
    "B": "Muestrean de una distribución de probabilidad sobre múltiples rutas de red posibles, donde cada ruta se pondera según su fidelidad de entrelazamiento de extremo a extremo y tasa de éxito esperada. En lugar de comprometerse con una única ruta predeterminada, estos algoritmos seleccionan rutas dinámicamente por solicitud extrayendo de esta distribución, permitiendo la exploración de canales alternativos cuando los enlaces primarios experimentan degradación transitoria o congestión.",
    "C": "Estos algoritmos mantienen una distribución de Boltzmann sobre todas las rutas factibles, con temperatura inversa β ajustada para equilibrar exploración versus explotación. Al muestrear rutas proporcionalmente a exp(−β·costo), donde el costo incorpora tanto pérdida de fidelidad como latencia, el sistema gravita naturalmente hacia rutas de alta calidad mientras ocasionalmente prueba alternativas. Sin embargo, el muestreo rechaza rutas basándose en congestión en tiempo real en lugar de fidelidad, por lo que la calidad del enlace no influye directamente en las probabilidades de ruta—solo la disponibilidad lo hace.",
    "D": "Los métodos probabilísticos aplican inferencia bayesiana para estimar distribuciones posteriores sobre fidelidades de enlaces dada la retroalimentación de medición ruidosa de intentos previos de entrelazamiento. Las rutas se seleccionan entonces maximizando la utilidad esperada bajo estas creencias actualizadas, considerando la incertidumbre en la calidad del canal. Las decisiones de enrutamiento clásicas incorporan resultados de medición para refinar la selección de rutas, pero los estados cuánticos mismos se enrutan determinísticamente una vez que el controlador clásico se compromete con un canal específico de extremo a extremo basado en la distribución de fidelidad inferida.",
    "solution": "B"
  },
  {
    "id": 158,
    "question": "En el contexto de la computación cuántica de variables continuas, la codificación de Gottesman–Kitaev–Preskill (GKP) ha ganado considerable atención para proteger información cuántica almacenada en modos bosónicos como cavidades de microondas o modos ópticos. La estrategia de codificación difiere fundamentalmente de los códigos de variables discretas al explotar el espacio de Hilbert de dimensión infinita de los osciladores armónicos. ¿Por qué los códigos GKP son particularmente atractivos para modos bosónicos?",
    "A": "Las superposiciones tipo rejilla de estados propios de posición permiten corrección de errores contra pequeños desplazamientos usando solo operaciones gaussianas, que son experimentalmente accesibles en la mayoría de las plataformas bosónicas y preservan la naturaleza de variable continua del sistema mientras proporcionan información lógica discreta. La estructura de rejilla posición-momento permite extracción de síndrome mediante mediciones homodinas sin requerir recursos no gaussianos para las operaciones de corrección mismas",
    "B": "La codificación de rejilla GKP naturalmente cuantiza errores de desplazamiento en síndromes discretos que se mapean directamente en canales de error tipo qubit, permitiendo que la maquinaria del formalismo estabilizador estándar—originalmente desarrollado para sistemas discretos—se aplique casi sin modificación. Las mediciones homodinas de posición y momento módulo el espaciamiento de la rejilla producen síndromes de valores enteros correspondientes a operadores de desplazamiento sobre el qubit lógico, y unitarios gaussianos como compresores y desplazamientos son suficientes para implementar todo el grupo de Clifford sobre la información codificada, creando un marco híbrido donde mediciones continuas impulsan corrección de errores discreta",
    "C": "Las aproximaciones GKP de energía finita con funciones de envolvente gaussiana ψ(x) ∝ Σₙ exp(-(x-n√π)²/2Δ²) logran distancia de código que aumenta exponencialmente con el parámetro de compresión Δ⁻², alcanzando distancias de d>100 con la tecnología actual de compresión de 15dB. Este escalamiento surge porque el solapamiento entre palabras código adyacentes disminuye como exp(-π/2Δ²), haciendo que los errores lógicos sean exponencialmente suprimidos. Combinado con el hecho de que la pérdida de fotones—el error dominante en cavidades de microondas—causa principalmente desplazamientos de posición/momento en lugar de borrados, los códigos GKP naturalmente coinciden con la estructura de error del hardware bosónico",
    "D": "Los códigos GKP explotan el espectro continuo del modo bosónico para implementar una forma de corrección temporal de errores donde la información cuántica se codifica redundantemente a través de la escalera infinita de estados propios de energía del oscilador, distribuyendo amplitud de estado lógico entre infinitos estados de Fock |n⟩. Cuando ocurre pérdida de fotones—eliminando amplitud de estados de n alto—la estructura periódica asegura que los componentes restantes de n bajo retengan información lógica completa. La extracción de síndrome mediante mediciones que resuelven número identifica qué subespacio de estado de Fock contiene errores, y operaciones de compresión gaussiana restauran la distribución correcta de amplitud, proporcionando protección contra pérdida sin requerir mediciones estabilizadoras de cuadraturas posición-momento",
    "solution": "A"
  },
  {
    "id": 159,
    "question": "¿Cuál es una ventaja clave de usar métodos basados en IA sobre enfoques convencionales en corrección de errores cuánticos?",
    "A": "Eliminan la necesidad de cualquier qubit físico simulando datos cuánticos completamente dentro de arquitecturas de redes neuronales clásicas que pueden aprender a emular propiedades de superposición cuántica y entrelazamiento, permitiendo así que algoritmos cuánticos se ejecuten en clusters de GPU convencionales sin requerir infraestructura criogénica o lidiar con decoherencia en absoluto.",
    "B": "Garantizan computación tolerante a fallos sin mejoras de hardware, evitando completamente los requisitos de umbral mediante estrategias de decodificador aprendidas que pueden corregir errores más allá de los límites teóricos impuestos por el teorema de umbral de corrección de errores cuánticos.",
    "C": "Eficiencia y precisión superiores a lo largo de toda la cadena de QEC, incluyendo decodificación de síndromes, optimización de puertas lógicas y estrategias de mitigación de errores, donde las redes neuronales pueden aprender patrones complejos en correlaciones de errores y adaptarse a modelos de ruido no estándar, superando a los decodificadores tradicionales de emparejamiento perfecto de peso mínimo tanto en velocidad como en supresión de errores para ruido realista de hardware.",
    "D": "Eliminan la necesidad de comprender modelos de ruido cuántico subyacentes porque las redes neuronales descubren automáticamente estrategias óptimas de corrección mediante entrenamiento en datos de síndrome sin procesar, haciendo posible desplegar corrección de errores cuánticos en plataformas de qubit novedosas.",
    "solution": "C"
  },
  {
    "id": 160,
    "question": "¿Qué hace que ciertos códigos de corrección de errores cuánticos (por ejemplo, códigos Bivariate Bicycle) sean más adecuados para implementaciones de hardware de corto plazo?",
    "A": "Estos códigos exhiben matrices de verificación de paridad dispersas donde cada qubit físico participa en solo un pequeño número constante de mediciones estabilizadoras, traduciéndose directamente en requisitos de conectividad bajos en el grafo de hardware. Esta estructura local permite implementaciones en arquitecturas con restricciones de acoplamiento a vecinos más cercanos, evitando las interacciones de largo alcance que afectan a los códigos de superficie en redes planares.",
    "B": "Estos códigos logran tasas de codificación favorables k/n aproximándose al límite de Hamming cuántico mientras mantienen verificaciones de paridad de peso constante donde cada estabilizador involucra exactamente w qubits físicos (típicamente w=6 para códigos Bivariate Bicycle). Este peso de estabilizador acotado se traduce en extracción de síndrome paralelizable usando solo acoplamientos a vecinos más cercanos y segundos vecinos más cercanos en geometrías de rejilla teseladas apropiadas. Sin embargo, a diferencia de los códigos de superficie, la decodificación óptima requiere contracción de red tensorial sobre el grafo de Tanner en lugar de emparejamiento de peso mínimo, aumentando el costo clásico a pesar de la conectividad reducida.",
    "C": "Al construir códigos padre LDPC clásicos con grafos de Tanner de circunferencia 8 y aplicar la construcción CSS mediante subcódigos auto-ortogonales, estos códigos generan verificaciones de paridad cuánticas donde los circuitos de medición de síndrome requieren profundidad logarítmica en la distancia del código en lugar de lineal. Esta reducción surge porque las restricciones de circunferencia alta eliminan ciclos cortos que de otro modo serializarían las mediciones estabilizadoras, permitiendo extracción de profundidad constante mediante mediciones de Pauli apropiadamente programadas a través de subconjuntos de qubits no superpuestos, aunque el costo de enrutamiento entre rondas de medición escala cuadráticamente con la distancia.",
    "D": "Estos códigos explotan construcciones de producto hipergrafo que producen grupos estabilizadores conmutantes con parámetro de dispersión s satisfaciendo s ≪ √n donde n es la longitud del bloque, habilitando directamente la extracción de síndrome mediante circuitos cuánticos de profundidad constante en grafos de grado limitado. La ventaja clave emerge de la estructura algebraica: los estabilizadores se factorizan en productos tensoriales de Paulis pequeños, evitando los generadores estabilizadores densos que necesitan escaleras CNOT secuenciales en códigos concatenados. Sin embargo, esta descomposición requiere costo de ancillas que escala como Θ(d² log d) donde d es la distancia del código.",
    "solution": "A"
  },
  {
    "id": 161,
    "question": "¿Cuál es la importancia de las clases de complejidad cuántica como BQP en la computación cuántica teórica?",
    "A": "Caracterizan qué problemas computacionales pueden resolver eficientemente las computadoras cuánticas en tiempo polinomial, estableciendo límites fundamentales de la ventaja computacional cuántica sobre los modelos clásicos e identificando dónde es teóricamente posible la aceleración cuántica.",
    "B": "Las clases de complejidad cuántica como BQP establecen límites estrictos sobre los recursos de medición requeridos para protocolos de verificación cuántica, mostrando que la computación cuántica en tiempo polinomial es exactamente equivalente a la computación clásica aumentada con un número polinomial de violaciones de desigualdades de Bell. Esta caracterización revela que la ventaja cuántica emerge precisamente de las correlaciones no locales en lugar de únicamente de la superposición.",
    "C": "Formalizan la relación entre la profundidad de circuitos cuánticos y el tiempo de computación paralela clásica, demostrando que BQP es igual a NC (Clase de Nick) bajo restricciones de profundidad polilogarítmica. Esto establece que la aceleración cuántica deriva fundamentalmente de la paralelización eficiente de puertas cuánticas en lugar del entrelazamiento, aunque las separaciones de oráculos sugieren que BQP puede extenderse más allá de P en ciertas instancias de problemas estructurados.",
    "D": "Las clases BQP caracterizan la complejidad de muestreo cuántico y la generación de aleatoriedad certificable, definiendo qué distribuciones de probabilidad pueden muestrearse eficientemente con circuitos cuánticos mientras permanecen computacionalmente difíciles de simular clásicamente. Esto captura la ventaja cuántica en aplicaciones a corto plazo donde los problemas de decisión pueden ser intratables pero el muestreo es suficiente para utilidad práctica.",
    "solution": "A"
  },
  {
    "id": 162,
    "question": "¿Qué técnica avanzada proporciona seguridad contra la fuga de información en el post-procesamiento clásico de la distribución cuántica de claves?",
    "A": "Los protocolos de amplificación de privacidad mejorados con funciones hash criptográficas resistentes a computación cuántica como SHA-3 o BLAKE3 eliminan la fuga de información comprimiendo el material de clave bruta mediante operaciones de hashing computacionalmente seguras.",
    "B": "El cifrado autenticado con seguridad teórica de la información previene la fuga por canales laterales durante la fase de post-procesamiento clásico al garantizar que no se requiere ninguna suposición computacional para acotar la ganancia de información del adversario, incluso cuando el adversario tiene recursos computacionales ilimitados. Este enfoque incorpora etiquetas de autenticación derivadas del material de clave cuántica bruta en cada mensaje clásico intercambiado durante la corrección de errores y estimación de parámetros, garantizando que cualquier intento de ataque de intermediario o medición de emanaciones electromagnéticas del hardware de procesamiento revela provadamente cero bits de la clave final, ya que la autenticación es incondicionalmente segura contra todos los ataques, incluidos los cuánticos.",
    "C": "Los extractores cuántico-resistentes diseñados específicamente para la etapa de post-procesamiento aplican extractores de aleatoriedad fuertes con pruebas de seguridad que permanecen válidas contra adversarios cuánticos, convirtiendo los bits de clave bruta parcialmente correlacionados en una cadena uniformemente aleatoria.",
    "D": "Los marcos de seguridad universalmente componibles establecen garantías rigurosas de que los protocolos QKD permanecen seguros cuando se componen con otros protocolos criptográficos en sistemas más grandes, asegurando que las propiedades de seguridad se preservan incluso cuando las claves se usan en aplicaciones arbitrarias. Estos marcos proporcionan pruebas formales de que la fuga de información durante el post-procesamiento está acotada independientemente de cómo se despliegue posteriormente la clave final.",
    "solution": "D"
  },
  {
    "id": 163,
    "question": "¿Cómo superan los emparejadores de plantillas conscientes de la conmutación a los optimizadores peephole ingenuos?",
    "A": "Los optimizadores peephole tradicionalmente operan sobre ventanas deslizantes fijas de puertas consecutivas, perdiendo oportunidades de optimización cuando los patrones emparejables están separados por puertas intermedias que conmutan. Los emparejadores de plantillas conscientes de la conmutación superan esta limitación analizando las relaciones de conmutación para reordenar efectivamente las puertas, acercando puertas no adyacentes pero conmutantes donde las plantillas pueden emparejar. Este alcance extendido sobre ordenamientos lógicos de puertas expone sustancialmente más oportunidades de simplificación que los enfoques de ventana fija, logrando una reducción superior del conteo de puertas y optimización de la profundidad del circuito.",
    "B": "Los emparejadores de plantillas conscientes de la conmutación explotan las relaciones de conmutación de puertas cuánticas para extender la ventana de búsqueda más allá de puertas inmediatamente adyacentes, permitiendo al optimizador identificar y aplicar patrones de reescritura que abarcan puertas no adyacentes que de otro modo serían invisibles para los optimizadores peephole de ventana fija. Esta capacidad de búsqueda expandida expone oportunidades de optimización en regiones de circuito más grandes, conduciendo a una simplificación de circuito más agresiva y mejor reducción general del conteo de puertas.",
    "C": "Los optimizadores peephole ingenuos examinan solo puertas sintácticamente adyacentes en la representación del circuito, mientras que los emparejadores conscientes de la conmutación construyen dinámicamente clases de equivalencia de ordenamientos de puertas aplicando reglas de conmutación para permutar puertas en formas canónicas. Este enfoque de clases de equivalencia permite que las plantillas emparejen patrones distribuidos en segmentos de circuito no contiguos que satisfacen restricciones de conmutación, expandiendo efectivamente la visibilidad del optimizador de vecindarios locales de tamaño k a regiones de tamaño O(k²) donde k acota cadenas de puertas conmutantes, descubriendo así oportunidades de optimización invisibles para métodos peephole dependientes de posición.",
    "D": "Los optimizadores peephole tradicionales aplican reglas de reescritura solo cuando las puertas aparecen en orden secuencial estricto dentro de una ventana fija, perdiendo optimizaciones cuando existen subsecuencias funcionalmente equivalentes pero están intercaladas con operaciones conmutantes. Los emparejadores de plantillas conscientes de la conmutación resuelven esto construyendo grafos de dependencias que identifican límites de conmutación, luego reordenando virtualmente las puertas para maximizar el emparejamiento de plantillas sin alterar la semántica del circuito. Este reordenamiento basado en grafos expone patrones de optimización en regiones extendidas determinadas por la estructura de conmutación en lugar de la proximidad posicional, logrando mejor reducción que enfoques restringidos por ventanas.",
    "solution": "B"
  },
  {
    "id": 164,
    "question": "¿Cómo funcionan los Autocodificadores Cuánticos?",
    "A": "Las transformadas cuánticas de Fourier convierten el estado cuántico de entrada en una representación en el dominio de frecuencia distribuida entre todos los qubits, después de lo cual truncamos sistemáticamente los componentes de Fourier de alta frecuencia descartando qubits correspondientes a oscilaciones rápidas en el espectro de amplitud.",
    "B": "Mediciones reversibles que colapsan selectivamente grados de libertad innecesarios mientras preservan perfectamente las amplitudes cuánticas significativas en un subconjunto más pequeño de qubits, realizando efectivamente reducción de dimensionalidad mediante colapso parcial de la función de onda. La arquitectura del autocodificador implementa mediciones débiles con fuerzas de medición cuidadosamente ajustadas, extrayendo suficiente información clásica para descartar subespacios de baja varianza mientras deja los componentes de alta información en superposición, logrando así compresión con pérdida sin destrucción completa del estado.",
    "C": "Codifican y decodifican información cuántica mediante circuitos cuánticos parametrizados entrenados para comprimir datos de alta dimensión en estados cuánticos en menos qubits mientras preservan características esenciales. El codificador mapea estados de entrada a un espacio latente de menor dimensión, y el decodificador intenta la reconstrucción, con entrenamiento que optimiza el circuito para minimizar la pérdida de información durante este proceso de reducción de dimensionalidad.",
    "D": "Operadores unitarios predeterminados que aíslan los autoestados correspondientes a los valores singulares más grandes de la matriz de densidad de entrada, descartando automáticamente el resto mediante interferencia destructiva sin requerir ningún entrenamiento u optimización. La compresión ocurre porque aplicar este unitario fijo causa que los eigenmodos de baja varianza interfieran destructivamente y desaparezcan del estado reducido, concentrando toda la información cuántica en los top-k eigenvectores que sobreviven la transformación de codificación.",
    "solution": "C"
  },
  {
    "id": 165,
    "question": "Al entrenar máquinas de Born de circuitos cuánticos (QCBMs) para aprender distribuciones de probabilidad, típicamente estás minimizando ¿qué medida de divergencia? El objetivo es hacer que la distribución del modelo coincida lo más posible con la distribución de datos objetivo, y la elección de divergencia afecta directamente tanto las estimaciones de gradiente como las propiedades de convergencia del procedimiento de optimización.",
    "A": "Pérdida hinge clásica computada sobre etiquetas binarias derivadas de resultados de medición, tomada directamente de la teoría de máquinas de vectores de soporte.",
    "B": "Distancia de variación total, que requiere computar la distribución de probabilidad completa sobre todos los estados de la base computacional realizando tomografía sobre la matriz de densidad de salida, luego tomando la norma L1 entre la distribución del modelo reconstruida y la distribución objetivo empírica.",
    "C": "Error cuadrático medio entre los vectores de parámetros del circuito de épocas de entrenamiento sucesivas, tratando efectivamente el algoritmo cuántico variacional como un problema de aprendizaje supervisado clásico donde el objetivo de optimización en cada paso está definido por la configuración de parámetros de la iteración anterior.",
    "D": "Divergencia KL estimada a partir de probabilidades muestrales — el enfoque estándar es extraer muestras de ambas distribuciones y computar la entropía relativa, lo que proporciona gradientes que pueden estimarse mediante reglas de desplazamiento de parámetros en el circuito cuántico. Específicamente, se minimiza D_KL(p_data || p_model) donde p_data es la distribución objetivo empírica y p_model es la distribución de la regla de Born del circuito cuántico. Esta elección es natural porque la divergencia KL es asimétrica de manera que prioriza ajustar el soporte de la distribución de datos, los gradientes se descomponen elegantemente para circuitos cuánticos variacionales usando la regla de desplazamiento de parámetros para valores esperados, y el objetivo puede estimarse eficientemente a partir de números polinomiales de muestras de medición sin requerir tomografía de estado completa, haciéndolo computacionalmente tratable incluso para circuitos con muchos qubits.",
    "solution": "D"
  },
  {
    "id": 166,
    "question": "¿Qué ataque específico se dirige a la generación de portadora de microondas en sistemas de control cuántico?",
    "A": "Manipular el bucle de enganche de fase (PLL) para desestabilizar la síntesis de portadora implica inyectar interferencia electromagnética calibrada a frecuencias cercanas al rango de enganche natural del PLL, provocando que el mecanismo de retroalimentación oscile entre objetivos de frecuencia competidores y produzca deriva de fase variable en el tiempo.",
    "B": "La contaminación de síntesis de frecuencia mediante la introducción deliberada de armónicos espurios corrompe la pureza de la señal de control al explotar productos de mezcla no lineales en la cadena de conversión ascendente, donde señales de interferencia cuidadosamente diseñadas a frecuencias subarmónicas generan distorsiones de intermodulación que se superponen directamente sobre la frecuencia de transición del qubit objetivo.",
    "C": "Los ataques de inyección al oscilador de referencia comprometen la señal de reloj maestro introduciendo interferencia electromagnética en la entrada de frecuencia de referencia, explotando el hecho de que todas las portadoras de microondas posteriores derivan su coherencia de fase de esta única fuente, corrompiendo así la temporización de pulsos de control en todo el procesador cuántico simultáneamente.",
    "D": "La interferencia en la distribución de reloj apunta a la coherencia de fase entre osciladores locales distribuidos espacialmente induciendo jitter de temporización en la red del árbol de reloj mediante acoplamiento electromagnético estratégico a líneas de distribución de alta impedancia, explotando el hecho de que las puertas cuánticas dependen de la sincronización temporal precisa de pulsos de control a través de múltiples canales.",
    "solution": "C"
  },
  {
    "id": 167,
    "question": "¿Qué característica de las plataformas de iones atrapados las hace particularmente adecuadas para experimentos cuánticos distribuidos?",
    "A": "Los largos tiempos de coherencia y las puertas de alta fidelidad permiten teletransportación confiable entre módulos, proporcionando los estados cuánticos estables y operaciones precisas necesarias para distribuir entrelazamiento entre sistemas de trampas de iones físicamente separados. Los tiempos de coherencia superiores a segundos permiten protocolos de intercambio de entrelazamiento de múltiples pasos, mientras que fidelidades de puertas de dos qubits superiores al 99.9% aseguran que los pares de Bell generados para enlaces remotos mantengan calidad suficientemente alta para soportar computación distribuida tolerante a fallos.",
    "B": "La combinación de transiciones ópticas estrechas (anchuras de línea sub-kHz para transiciones cuadrupolares) y acoplamiento eficiente ión-fotón mediante emisión espontánea mejorada por cavidad permite distribución determinista de entrelazamiento a través de canales fotónicos. Al recolectar fotones de fluorescencia de cada trampa de iones en fibras monomodo y realizar interferencia Hong-Ou-Mandel en un divisor de haz, se puede generar entrelazamiento remoto heraldado a tasas superiores a 1 kHz con fidelidades superiores al 95%, suficiente para protocolos cuánticos distribuidos. Los tiempos de coherencia superiores a segundos aseguran que los pares de Bell generados permanezcan utilizables durante los pasos de intercambio y purificación de entrelazamiento requeridos para enlaces de larga distancia.",
    "C": "Los iones atrapados se acoplan naturalmente a modos ópticos propagantes mediante dispersión Raman espontánea cuando son impulsados por campos láser fuera de resonancia, creando entrelazamiento ión-fotón que puede enrutarse a través de redes de fibra óptica hacia módulos de trampa remotos. El proceso Raman genera fotones entrelazados con el estado interno del ión en una codificación de bin temporal o polarización, y al realizar mediciones de estado de Bell sobre fotones de diferentes trampas, se puede establecer entrelazamiento ión-ión remoto. Los largos tiempos de coherencia (superiores a segundos) y puertas locales de alta fidelidad (>99.9%) aseguran que los estados entrelazados distribuidos sobrevivan la latencia de comunicación clásica requerida para operaciones de avance en circuitos distribuidos basados en teletransportación.",
    "D": "Los estados de qubit codificados en subniveles hiperfinos o Zeeman de iones atrapados exhiben tiempos de coherencia excepcionalmente largos (T₂ > 10 segundos) y pueden interfazar directamente con fotones ópticos viajeros mediante transiciones de dipolo eléctrico cuando los iones están embebidos en circuitos integrados fotónicos en chip. El confinamiento de modo ajustado en guías de onda de nitruro de silicio mejora la fuerza de acoplamiento ión-fotón por factores superiores a 100 comparado con la recolección en espacio libre, permitiendo emisión de fotón único casi determinista en el modo de guía de onda. Combinado con fidelidades de puertas de dos qubits superiores al 99.9%, esta interfaz fotónica permite generación eficiente de entrelazamiento remoto mediante interferencia de fotones en divisores de haz integrados, lo cual es esencial para distribuir estados cuánticos a través de módulos de trampa espacialmente separados sin pérdidas ópticas en espacio libre.",
    "solution": "A"
  },
  {
    "id": 168,
    "question": "En escenarios de computación cuántica distribuida, los protocolos de enrutamiento para estados entrelazados multipartitos como estados GHZ deben enfrentar varios desafíos únicos. A diferencia del enrutamiento clásico donde la duplicación de paquetes es trivial, la información cuántica no puede clonarse debido a principios fundamentales. Sin embargo, el desafío principal de enrutamiento al establecer estados como |GHZ⟩ = (|000⟩ + |111⟩)/√2 a través de nodos geográficamente separados es asegurar que cada participante reciba entrelazamiento de alta calidad simultáneamente. ¿Qué restricción específica impone este requisito de simultaneidad sobre la arquitectura de red?",
    "A": "El enrutamiento cuántico requiere que todos los nodos intermedios realicen mediciones conjuntas sobre pares entrelazados que llegan asincrónicamente de diferentes fuentes, pero la regla de Born garantiza que los resultados de medición son independientes del orden de llegada siempre que cada par se mida antes de la decoherencia. Sin embargo, los estados GHZ demandan que todos los pares constituyentes se originen de un evento de heraldo común en la fuente, forzando a la red a implementar canales de difusión síncronos donde cada pulso de fotón alcanza su destino dentro de la misma ventana de coherencia, eliminando completamente el almacenamiento intermedio de store-and-forward.",
    "B": "Debido a que los estados GHZ exhiben correlaciones perfectas a través de todas las mediciones de base computacional, cualquier asimetría en el ruido del canal entre participantes rompe la simetría de permutación del estado y causa que la matriz de densidad reducida en cada nodo se vuelva distinguible. El protocolo de enrutamiento debe por lo tanto garantizar que todos los canales cuánticos entre la fuente y cada participante tengan parámetros de fidelidad idénticos—no solo fidelidad aceptable, sino tasas de error coincidentes—de lo contrario la distinguibilidad viola la monogamia del entrelazamiento y el estado distribuido no puede usarse para protocolos multipartitos como compartición de secretos o acuerdo bizantino.",
    "C": "Todos los canales cuánticos que conectan el nodo de distribución central a cada participante deben mantener fidelidad suficientemente alta al mismo tiempo—si un enlace de comunicación se degrada mientras se preparan los otros, todo el estado entrelazado multipartito se ve comprometido y el protocolo falla.",
    "D": "La distribución de estados GHZ requiere que la red satisfaga una restricción de ordenamiento temporal donde las operaciones de intercambio de entrelazamiento en enrutadores intermedios deben completarse en una secuencia específica determinada por los generadores de estabilizador del estado. Si los eventos de intercambio ocurren fuera de orden relativo a la topología lógica codificada en los estabilizadores, el estado resultante se convierte en un estado entrelazado multipartito diferente—a menudo un estado W o estado cluster—en lugar del estado GHZ pretendido. La arquitectura de red debe hacer cumplir el ordenamiento causal de todas las mediciones de Bell, típicamente implementado mediante mensajes de sincronización clásicos que aseguran que cada enrutador espere confirmación de predecesores antes de ejecutar su intercambio.",
    "solution": "C"
  },
  {
    "id": 169,
    "question": "¿Cuál es la motivación para calibrar pulsos parametrizados usando optimización bayesiana de bucle cerrado?",
    "A": "La optimización bayesiana proporciona un marco fundamentado para incorporar conocimiento previo sobre el paisaje de parámetros de pulso en el procedimiento de calibración mediante funciones kernel cuidadosamente elegidas en el modelo sustituto de proceso gaussiano. El método destaca cuando el hamiltoniano de control exhibe múltiples óptimos locales debido a diafonía entre canales de control o respuesta no lineal en el régimen de anarmonicidad del transmon. Sin embargo, la ventaja principal emerge en sistemas puramente coherentes donde los tiempos T₁ y T₂ exceden la duración total del experimento de calibración—en tales casos, la optimización puede ejecutarse en bucle abierto sin retroalimentación de medición, usando la varianza posterior del GP para guiar la exploración de parámetros. La arquitectura de bucle cerrado se vuelve esencial principalmente para validar que los parámetros de pulso óptimos descubiertos en simulación se transfieran confiablemente al hardware físico sin requerir refinamiento iterativo.",
    "B": "Los métodos bayesianos de bucle cerrado abordan el desafío fundamental de que la estimación directa de gradiente mediante reglas de cambio de parámetro o métodos de diferencias finitas escala pobremente con el número de parámetros de pulso debido al ruido de disparo en mediciones cuánticas. La función de adquisición del proceso gaussiano (típicamente mejora esperada o límite de confianza superior) balancea inteligentemente la exploración de regiones de parámetros submuestreadas contra la explotación de áreas actualmente prometedoras, requiriendo muchos menos experimentos costosos de hardware que optimizadores basados en derivadas. El enfoque demuestra ser particularmente efectivo al tratar con errores sistemáticos dependientes del tiempo o cuando el paisaje de función objetivo contiene características agudas que causarían que los estimadores de gradiente sufran de alta varianza. Sin embargo, las garantías de convergencia solo se mantienen cuando la fidelidad del pulso es una función suave de parámetros con derivadas acotadas, lo cual se rompe para pulsos cerca de resonancias de desacoplamiento dinámico.",
    "C": "La motivación clave es que la optimización bayesiana maneja naturalmente las funciones objetivo estocásticas que surgen de mediciones cuánticas de disparo finito modelando el paisaje de fidelidad como un proceso gaussiano con ruido de observación. Cada experimento de calibración proporciona una muestra ruidosa de la verdadera fidelidad de puerta en parámetros de pulso específicos, y la distribución posterior del GP captura tanto la fidelidad media estimada como la incertidumbre en valores de parámetros no explorados. La función de adquisición luego guía la selección de qué parámetros probar a continuación maximizando la ganancia de información esperada sobre la ubicación del óptimo global. Este enfoque eficiente en muestras supera al descenso de gradiente cuando las evaluaciones de función son costosas—cada experimento de hardware consume tiempo de reloj y contribuye a la decoherencia del qubit por operaciones repetidas. La arquitectura de bucle cerrado también se adapta a la deriva del hardware refinando continuamente el modelo GP a medida que llegan nuevas mediciones, aunque la convergencia requiere que la escala temporal de deriva exceda sustancialmente la duración de experimentos de calibración individuales.",
    "D": "La retroalimentación de medición del hardware cuántico permite que la optimización bayesiana actualice un modelo probabilístico del paisaje de control, incorporando información de cada prueba experimental para refinar las estimaciones de parámetros iterativamente. Este enfoque adaptativo converge a parámetros de pulso de alta fidelidad mucho más eficientemente que búsqueda exhaustiva en cuadrícula o muestreo aleatorio, porque el prior del proceso gaussiano captura estructura suave en cómo la fidelidad de puerta varía con parámetros de pulso, permitiendo al algoritmo seleccionar inteligentemente el siguiente punto de medición que maximalmente reduce la incertidumbre. La arquitectura de bucle cerrado es particularmente valiosa al tratar con deriva de hardware, paisajes de optimización no convexos, o evaluaciones de función costosas donde minimizar el número de experimentos de calibración es crítico.",
    "solution": "D"
  },
  {
    "id": 170,
    "question": "En el contexto de computación cuántica tolerante a fallos, ¿qué limitación fundamental abordan los teoremas de umbral? Estos teoremas son centrales para entender si las computadoras cuánticas a gran escala pueden funcionar en la práctica, dado que todos los componentes físicos introducen cierta cantidad de ruido y error.",
    "A": "Establecen condiciones bajo las cuales la computación cuántica permanece confiable incluso con componentes imperfectos, siempre que las tasas de error permanezcan por debajo de cierto valor umbral. Este umbral depende del código específico de corrección de errores, modelo de error y decodificador siendo usado. Por debajo del umbral, concatenar códigos o aumentar la distancia del código permite computaciones arbitrariamente largas con tasas de error lógico arbitrariamente bajas.",
    "B": "Establecen límites superiores sobre la razón de sobrecarga asintótica entre tiempos de puerta lógicos y físicos requeridos para computación tolerante a fallos. Los teoremas de umbral prueban que siempre que las tasas de error físico permanezcan por debajo de un valor crítico (típicamente 10⁻³ a 10⁻⁴), la penalidad de tiempo de puerta lógica por corrección de errores se satura en un factor multiplicativo constante independiente de la longitud de computación. Este umbral de sobrecarga de tiempo asegura que computaciones cuánticas arbitrariamente largas permanezcan como procesos de tiempo polinomial en lugar de tiempo exponencial.",
    "C": "Establecen condiciones necesarias sobre las propiedades de coherencia de códigos de corrección de errores cuánticos, específicamente probando que las mediciones de síndrome deben superar la decoherencia por un margen umbral. Los teoremas de umbral demuestran que cuando el tiempo de extracción de síndrome excede una fracción crítica del tiempo de coherencia de memoria (típicamente alrededor de 1/7 para códigos de superficie), la propagación de errores supera la capacidad de corrección independientemente de la distancia del código. Por debajo de esta razón umbral, aumentar la distancia del código permite computación confiable incluso con mediciones de síndrome imperfectas.",
    "D": "Establecen la distancia mínima de código requerida para una tasa de error físico dada, probando que la corrección de errores cuánticos se vuelve efectiva solo cuando la distancia de código d excede un valor umbral proporcional a 1/√p, donde p es la tasa de error físico. Los teoremas de umbral muestran que por debajo de este umbral de distancia, incluso el procesamiento ideal de síndrome no puede suprimir errores lógicos, pero por encima del umbral, los códigos concatenados permiten supresión exponencial de tasas de error lógico con cada nivel de concatenación.",
    "solution": "A"
  },
  {
    "id": 171,
    "question": "¿Qué compromiso aborda DisMap en su proceso de particionamiento y mapeo?",
    "A": "El algoritmo equilibra la minimización de la profundidad del circuito dentro de cada partición frente a la sobrecarga introducida al distribuir el entrelazamiento entre particiones: al mantener juntos los subgrafos fuertemente conectados, reduces el número de operaciones SWAP internas, pero esto crea módulos más grandes que requieren más pares de Bell para establecer conectividad entre particiones, aumentando la latencia.",
    "B": "El algoritmo equilibra la localidad de las puertas dentro de las particiones frente a la sobrecarga de comunicación entre módulos: intentas mantener alta la fidelidad de las puertas de dos qubits minimizando las operaciones que abarcan múltiples módulos de hardware, pero esta restricción fuerza operaciones SWAP adicionales o costes de distribución de entrelazamiento.",
    "C": "DisMap optimiza el compromiso entre la eficiencia de utilización de qubits y la acumulación de errores de puertas al particionar el circuito en módulos balanceados que minimizan la sobrecarga de qubits inactivos, pero esta estrategia de balanceo de carga aumenta inadvertidamente la longitud del camino crítico porque las puertas que podrían ejecutarse en paralelo se serializan para mantener la simetría de las particiones.",
    "D": "El proceso de particionamiento intercambia la decoherencia inducida por medición frente al paralelismo de puertas: concentrar las mediciones en menos particiones reduce el número de eventos de medición en medio del circuito y sus penalizaciones de decoherencia asociadas, pero fuerza la ejecución secuencial de capas de puertas que de otro modo podrían ejecutarse simultáneamente en módulos separados, aumentando la duración total del circuito.",
    "solution": "B"
  },
  {
    "id": 172,
    "question": "¿Cuál es una ventaja significativa de entrenar HQNNs en simuladores cuánticos antes de desplegarlos en hardware cuántico real?",
    "A": "El entrenamiento en simuladores proporciona una inicialización robusta de parámetros mediante optimización a nivel de puertas en el régimen sin ruido, donde los métodos basados en gradientes convergen a configuraciones que permanecen casi óptimas cuando se introduce ruido durante el despliegue en hardware. Los parámetros variacionales aprendidos codifican soluciones aproximadas al paisaje de optimización que se transfieren eficazmente entre plataformas debido a la universalidad subyacente de los conjuntos de puertas cuánticas, aunque la convergencia final típicamente requiere un ajuste fino modesto (5-10 épocas adicionales) en el hardware objetivo para compensar las diferencias en tiempos de coherencia y restricciones de conectividad. Esta estrategia de inicialización reduce el tiempo total de la unidad de procesamiento cuántico al permitir un despliegue con arranque en caliente en lugar de una inicialización aleatoria de parámetros, y las estructuras de circuitos aprendidas a menudo exhiben propiedades de robustez inherentes donde la optimización descubre naturalmente regiones de parámetros con paisajes de pérdida planos que toleran imperfecciones del hardware dentro de rangos operativos típicos.",
    "B": "Los modelos entrenados en simuladores se transfieren a dispositivos reales con rendimiento comparable, permitiendo a los investigadores iterar rápidamente a través de la optimización de hiperparámetros, selección de arquitectura y refinamiento de protocolos de entrenamiento en el entorno de simulación antes de comprometer tiempo costoso de la unidad de procesamiento cuántico en ejecuciones de validación finales. Este ciclo de desarrollo acelerado reduce el coste por experimento en órdenes de magnitud mientras permite la exploración sistemática del espacio de diseño de HQNN, incluyendo estructuras de ansatz variacionales, estrategias de medición y protocolos de interfaz clásica-cuántica. El simulador proporciona un banco de pruebas controlado donde las hipótesis sobre la ventaja cuántica pueden evaluarse eficientemente, y las configuraciones exitosas pueden desplegarse en hardware con la confianza de que los principios algorítmicos centrales han sido validados, aunque el ajuste de rendimiento final puede seguir siendo necesario para tener en cuenta las características específicas del dispositivo.",
    "C": "Los simuladores permiten una caracterización integral del ruido mediante la inyección controlada de modelos de ruido parametrizados calibrados a partir de datos de caracterización del hardware, permitiendo el estudio sistemático de cómo los HQNNs responden a mecanismos de error específicos como la amortiguación de amplitud, el defasaje y los errores de puertas correlacionados. Al entrenar bajo estas condiciones de ruido que se aproximan pero no replican perfectamente el comportamiento del hardware, los modelos desarrollan robustez parcial a patrones de error antes del despliegue en hardware, reduciendo pero no eliminando la necesidad de entrenamiento en el dispositivo. Sin embargo, los simuladores no pueden capturar todos los efectos sutiles específicos del dispositivo, como la diafonía dependiente de la frecuencia, la deriva de calibración variable en el tiempo y el acoplamiento ambiental no markoviano, lo que significa que los modelos aún deben someterse a validación en hardware donde emergen brechas de rendimiento residuales de estos fenómenos no modelados que requieren ajustes de optimización finales que típicamente involucran el 15-25% de las iteraciones de entrenamiento originales.",
    "D": "Los HQNNs entrenados en simuladores explotan primitivas algorítmicas independientes del hardware basadas en representaciones abstractas de circuitos cuánticos que desacoplan las operaciones lógicas de las implementaciones físicas, permitiendo que el mismo modelo entrenado se ejecute en cualquier procesador cuántico que soporte el conjunto de puertas requerido. Aunque la topología de conectividad influye en la sobrecarga de compilación mediante inserciones adicionales de puertas SWAP, los parámetros aprendidos codifican transformaciones a nivel lógico que permanecen funcionalmente equivalentes entre plataformas, requiriendo solo transpilación automática del circuito en lugar de reentrenamiento. Esta abstracción arquitectónica significa que los modelos entrenados en simulación alcanzan el 85-95% de su rendimiento simulado inmediatamente después del despliegue en hardware en diferentes familias de procesadores (superconductor, trampa de iones, fotónico) sin modificación, con las brechas de rendimiento restantes atribuibles principalmente a la decoherencia dependiente de la profundidad en lugar de incompatibilidad algorítmica fundamental entre la ejecución en simulación y hardware.",
    "solution": "B"
  },
  {
    "id": 173,
    "question": "¿Qué técnica sofisticada proporciona la amplificación de privacidad más eficiente en la distribución cuántica de claves?",
    "A": "Las funciones hash dos-universales con seguridad cuántica proporcionan amplificación de privacidad óptima al garantizar que para cualquier par de claves de entrada distintas, la probabilidad de colisión está acotada por 1/2^n, donde n es la longitud de salida. Esta familia de funciones hash satisface el lema del hash residual bajo información lateral cuántica, garantizando que la salida sea exponencialmente cercana a uniforme incluso cuando un adversario mantiene correlaciones cuánticas con la entrada, logrando así seguridad teórica de la información con consumo mínimo de claves en comparación con extractores clásicos.",
    "B": "Los extractores resistentes a computación cuántica aprovechan primitivas criptográficas post-cuánticas como construcciones basadas en retículos o en códigos para garantizar que incluso adversarios con computadoras cuánticas no puedan extraer información del material de claves comprimido. Estos extractores incorporan funciones pseudoaleatorias cuántico-seguras en la fase de compresión, proporcionando garantías de seguridad computacional que permanecen válidas incluso después del advenimiento de computadoras cuánticas a gran escala.",
    "C": "La multiplicación de matrices de Toeplitz logra una amplificación de privacidad óptima mediante mapeos lineales estructurados que comprimen la clave en bruto en una clave segura más corta, con seguridad demostrable contra adversarios cuánticos que poseen información lateral.",
    "D": "La extracción de aleatoriedad teórica de la información logra amplificación de privacidad mediante la aplicación de funciones deterministas que comprimen cadenas parcialmente aleatorias en salidas más cortas, casi uniformes, con el límite de seguridad derivado de consideraciones de min-entropía. En el contexto de QKD, este enfoque usa extractores con semilla donde la semilla se comparte públicamente, y se demuestra que la clave extraída está ε-cercana a la distribución uniforme independientemente de cualquier información lateral clásica o cuántica que posea el espía.",
    "solution": "C"
  },
  {
    "id": 174,
    "question": "¿Por qué es crítico reducir el número de puertas SWAP en circuitos cuánticos distribuidos basados en LNN?",
    "A": "Las puertas SWAP fundamentalmente no pueden implementarse en hardware superconductor sin descomponerse en tres puertas CNOT, lo que viola las restricciones de vecinos más cercanos porque cada CNOT en sí misma requiere acoplamiento capacitivo directo entre qubits, creando un problema de arranque donde implementar la operación de enrutamiento en sí misma requiere enrutamiento.",
    "B": "Cada puerta SWAP aumenta sustancialmente la profundidad del circuito y contribuye múltiples operaciones de dos qubits que tienen tasas de error significativamente más altas que las puertas de un qubit, lo que significa que los SWAPs excesivos acumulan errores que degradan la fidelidad de los estados cuánticos que se enrutan a través de la topología de vecinos más cercanos, haciendo que la minimización sea esencial para mantener la precisión computacional dentro de las restricciones de tiempo de coherencia del hardware cuántico actual donde los errores de puertas típicamente exceden el 0,1% por operación de dos qubits.",
    "C": "Las operaciones SWAP interfieren con los protocolos de destilación de estados mágicos necesarios para la computación universal tolerante a fallos porque no pueden implementarse de manera transversal en códigos de superficie u otros códigos de corrección de errores topológicos, requiriendo operaciones lógicas que consumen estados ancilla costosos preparados mediante múltiples rondas de destilación.",
    "D": "Borran el entrelazamiento al permutar el etiquetado de qubits de una manera que rompe los patrones de correlación cuidadosamente construidos establecidos por capas anteriores del circuito, aleatorizando efectivamente qué qubits están entrelazados con cuáles y destruyendo las correlaciones cuánticas de largo alcance necesarias para la ventaja cuántica.",
    "solution": "B"
  },
  {
    "id": 175,
    "question": "¿Cuál es la importancia de la información de Fisher cuántica en algoritmos cuánticos variacionales?",
    "A": "Caracteriza la geometría del espacio de parámetros al codificar el tensor métrico riemanniano que define las geodésicas a través de la variedad variacional, permitiendo a los optimizadores seguir trayectorias de descenso de gradiente natural que tienen en cuenta la curvatura inducida por el solapamiento de estados cuánticos.",
    "B": "Proporciona un marco geométrico y estadístico integral que cuantifica simultáneamente la sensibilidad de medición a variaciones de parámetros, revela la estructura riemanniana local de la variedad de estados cuánticos para fines de optimización, y sirve como diagnóstico para la producción de entrelazamiento a lo largo del circuito variacional, convirtiéndola en una métrica unificada que captura tanto los aspectos teóricos de la información como los geométricos esenciales para comprender la entrenabilidad y expresividad en algoritmos variacionales.",
    "C": "Rastrea el entrelazamiento generado por el circuito al calcular la información mutua entre subsistemas como función de los parámetros variacionales, proporcionando así una medida directa de cuánto entrelazamiento bipartito o multipartito emerge durante la evolución del ansatz, con valores más altos de información de Fisher cuántica indicando que el circuito está produciendo estados más fuertemente correlacionados a través del registro de qubits.",
    "D": "Cuantifica con qué sensibilidad los resultados de medición responden a ajustes infinitesimales de parámetros, esencialmente calculando la varianza en los valores esperados de observables bajo pequeños cambios en los parámetros del circuito: las direcciones con alta información de Fisher cuántica son aquellas donde cambios minúsculos producen resultados estadísticamente distinguibles, guiando la asignación de recursos durante el entrenamiento variacional.",
    "solution": "B"
  },
  {
    "id": 176,
    "question": "¿Por qué la conectividad limitada de qubits presenta un desafío?",
    "A": "Las puertas de dos qubits solo pueden ejecutarse directamente en qubits físicamente adyacentes que comparten un elemento de acoplamiento en el grafo de conectividad del hardware. Cuando un algoritmo requiere una operación de dos qubits entre qubits no adyacentes, el compilador debe insertar secuencias de puertas SWAP para mover físicamente la información cuántica a lo largo de rutas a través de la topología de conectividad hasta que los qubits sean vecinos, luego realizar la puerta deseada y potencialmente intercambiarlos de vuelta. Esta sobrecarga de SWAP aumenta sustancialmente la profundidad del circuito—a veces por factores de 10× o más—introduciendo oportunidades adicionales de decoherencia y extendiendo el tiempo de ejecución, lo cual degrada directamente la fidelidad del estado cuántico final y limita la complejidad de los algoritmos que pueden ejecutarse exitosamente antes de que se pierda la coherencia.",
    "B": "Las puertas de dos qubits requieren acoplamiento directo entre qubits mediante hamiltonianos de interacción física que solo existen para pares adyacentes en la topología de conectividad del hardware. Cuando los algoritmos especifican operaciones entre qubits distantes, los compiladores deben descomponer estas puertas en secuencias de interacciones entre vecinos más cercanos usando aproximaciones de Trotter-Suzuki, donde los unitarios no locales de dos qubits U = exp(-iH₁₂t) se sintetizan mediante múltiples capas de puertas adyacentes. Esta sobrecarga de trotterización aumenta la profundidad del circuito por factores que escalan con la distancia de separación de qubits, introduciendo errores sistemáticos de la aproximación que se acumulan como ε ∝ (Δt)²||[H₁,H₂]|| por paso de Trotter. Estos errores de aproximación se componen con la decoherencia, degradando la fidelidad del estado final y limitando la complejidad algorítmica alcanzable antes de que las tasas de error excedan los umbrales de tolerancia a fallos.",
    "C": "Las operaciones de entrelazamiento de dos qubits solo pueden realizarse entre qubits físicamente vecinos que comparten canales de acoplamiento directo en la arquitectura de conectividad del dispositivo. Cuando las especificaciones del circuito requieren puertas entre qubits no adyacentes, los algoritmos de enrutamiento deben insertar secuencias de puertas BRIDGE que crean cadenas de entrelazamiento temporales a través de qubits intermedios, teletransportando efectivamente información cuántica a través del grafo de conectividad hasta que los qubits objetivo se vuelvan lógicamente adyacentes. Este protocolo de puente aumenta significativamente la profundidad del circuito—a veces por 5-8× dependiendo del diámetro del grafo—pero críticamente, cada operación de puente consume un ebit de la capacidad de entrelazamiento del qubit intermedio, creando contención de recursos que limita la ejecución paralela de puertas y extiende el tiempo total de ejecución, permitiendo que la decoherencia degrade la fidelidad computacional más allá de umbrales recuperables para circuitos profundos.",
    "D": "Las puertas de dos qubits dependen del acoplamiento físico directo entre qubits mediante resonadores compartidos o enlaces capacitivos presentes solo para pares adyacentes en el grafo de conectividad. Cuando los circuitos compilados requieren operaciones entre qubits separados, la etapa de mapeo debe insertar operaciones MOVE que transportan físicamente excitaciones de qubits a través de la red de acoplamiento intercambiando secuencialmente estados cuánticos a lo largo de rutas más cortas. Esto introduce sobrecarga que escala linealmente con el diámetro de la red—típicamente 4-12 saltos para arquitecturas planares—donde cada MOVE añade una capa de puerta. Sin embargo, la limitación principal surge del crosstalk: mover físicamente excitaciones más allá de qubits intermedios induce errores de acoplamiento ZZ no deseados proporcionales a ξ·t_move que se acumulan coherentemente, creando errores de fase sistemáticos que la corrección de errores no puede abordar ya que conmutan con las mediciones de estabilizadores, limitando fundamentalmente la fidelidad del circuito.",
    "solution": "A"
  },
  {
    "id": 177,
    "question": "¿Qué dos métricas se utilizan para evaluar la calidad de un circuito cuántico sintetizado?",
    "A": "La calidad del circuito se evalúa principalmente usando el recuento de puertas de entrelazamiento de dos qubits, que dominan las tasas de error debido a sus fidelidades significativamente más bajas en comparación con las operaciones de un solo qubit, junto con la métrica de fidelidad matemática que cuantifica cuán estrechamente la transformación unitaria implementada coincide con la operación objetivo mediante medidas como la distancia de traza o la fidelidad promedio de puerta",
    "B": "La calidad de síntesis se evalúa contando la profundidad de las capas CNOT (profundidad de puertas de dos qubits) en el circuito compilado, que determina la acumulación temporal de errores de decoherencia que dominan sobre las rotaciones de un solo qubit en implementaciones tolerantes a fallos, combinada con la métrica de fidelidad de proceso que cuantifica cuán precisamente el canal cuántico preserva la estructura del estado de entrada mediante medidas como la distancia de norma de diamante o la degradación de la capacidad del canal",
    "C": "Las métricas primarias se enfocan en el número de puertas no-Clifford (típicamente puertas T) requeridas en la compilación tolerante a fallos, que determina la sobrecarga de recursos mediante protocolos de destilación de estados mágicos que dominan los costos de ejecución, junto con una medida cuantitativa de la fidelidad de implementación del circuito evaluada mediante benchmarking aleatorizado o tomografía de conjuntos de puertas para capturar errores de control sistemáticos a lo largo de la secuencia de puertas descompuesta",
    "D": "La evaluación de calidad del circuito se basa en medir la profundidad total del circuito—calculada como el número máximo de capas secuenciales de puertas cuando la paralelización se explota óptimamente a través de subsistemas de qubits independientes—que determina la acumulación de decoherencia durante la ejecución, junto con la métrica de fidelidad promedio de puerta que cuantifica la tasa de error por operación mediante reconstrucción unitaria directa o benchmarking de entropía cruzada contra la transformación objetivo ideal",
    "solution": "A"
  },
  {
    "id": 178,
    "question": "¿Qué desafío surge en la evolución cuántica en tiempo imaginario (QITE) cuando el dominio de mediciones locales se expande?",
    "A": "A medida que los operadores de medición se extienden más allá de regiones locales, el generador anti-hermitiano de la evolución en tiempo imaginario adquiere términos no locales que violan el supuesto de descomposición subyacente a la implementación eficiente de QITE. Mientras que las mediciones locales son suficientes para hamiltonianos de vecinos más cercanos, los dominios de medición extendidos crean elementos de matriz fuera de la diagonal en el propagador de tiempo imaginario que acoplan qubits distantes, requiriendo secuencias de puertas cuya profundidad escala exponencialmente con el rango de medición para aproximar fielmente la evolución.",
    "B": "Expandir el dominio de medición introduce correlaciones entre subsistemas distantes que causan que el principio variacional de McLachlan se vuelva mal condicionado, ya que la matriz de solapamiento entre estados de base variacionales desarrolla autovalores cercanos a cero que crecen exponencialmente con la extensión de la medición. Esta inestabilidad numérica hace imposible invertir de manera confiable el sistema lineal que determina el paso óptimo de tiempo imaginario, aunque la evolución física permanece bien definida y los operadores unitarios requeridos tienen representaciones matriciales de tamaño polinomial.",
    "C": "Generar los operadores unitarios necesarios se vuelve exponencialmente complejo a medida que aumenta el dominio de medición, porque las puertas cuánticas requeridas deben implementar operaciones no locales que no pueden descomponerse eficientemente en secuencias de puertas de dos qubits entre vecinos más cercanos. Esta escalabilidad exponencial en profundidad del circuito surge de la necesidad de propagar información cuántica a través de qubits cada vez más distantes mientras se mantienen las relaciones de fase precisas necesarias para una propagación precisa en tiempo imaginario.",
    "D": "Cuando los dominios de medición se extienden más allá de los vecinos más cercanos, el número de mediciones de cadenas de Pauli requeridas crece exponencialmente porque cada operador de medición de n qubits puede descomponerse en 4^n mediciones de Pauli de un solo qubit en el peor caso. Aunque las técnicas de agrupación de Pauli reducen esta sobrecarga para observables conmutantes, las mediciones no locales generalmente producen términos no conmutantes cuyos valores esperados deben estimarse independientemente, creando una complejidad de medición que escala exponencialmente con el tamaño del soporte del operador de medición.",
    "solution": "C"
  },
  {
    "id": 179,
    "question": "¿Qué tipo de puertas se consideran primero para fusión en la estrategia propuesta?",
    "A": "Puertas SWAP que operan sobre qubits adyacentes en el grafo de conectividad, que se priorizan para fusión porque las operaciones SWAP consecutivas a menudo surgen de algoritmos de enrutamiento y pueden simplificarse mediante cancelación algebraica—específicamente, SWAP(i,j) seguida de SWAP(i,j) es igual a la identidad, y ciertas secuencias SWAP pueden reescribirse como rutas más cortas a través del mapa de acoplamiento.",
    "B": "Puertas de medición que proyectan qubits sobre la base computacional, que se examinan primero para oportunidades de fusión porque las mediciones consecutivas sobre el mismo qubit son redundantes—la primera medición colapsa el estado, haciendo que las mediciones subsiguientes sean determinísticas. Además, ciertos patrones de medición pueden consolidarse cuando ocurren en paralelo a través de múltiples qubits o cuando las operaciones intermedias conmutan con la base de medición, reduciendo tanto la profundidad del circuito como el número de operaciones de lectura clásicas requeridas, lo cual es crítico para minimizar el tiempo total de ejecución en hardware con ciclos de medición y reinicio lentos.",
    "C": "Puertas de 1 qubit, incluyendo rotaciones y operaciones de Pauli, que se examinan primero para fusión porque exhiben las tasas de error más bajas y los tiempos de ejecución más rápidos, haciéndolas candidatas ideales para optimización agresiva. Las puertas secuenciales de un solo qubit sobre el mismo qubit a menudo pueden componerse en una sola rotación equivalente usando representaciones de eje-ángulo, reduciendo la profundidad del circuito mientras se mantiene la equivalencia funcional perfecta.",
    "D": "Puertas de 2 qubits como CNOT o CZ, que se dirigen primero porque dominan tanto las tasas de error como el tiempo de ejecución en dispositivos NISQ—típicamente exhibiendo tasas de error 10-100× más altas que las puertas de un solo qubit. La estrategia de fusión busca puertas de 2 qubits adyacentes que actúan sobre pares de qubits solapados que pueden consolidarse mediante identidades de puertas (por ejemplo, CNOT(a,b) seguida de CNOT(b,a) seguida de CNOT(a,b) es igual a SWAP(a,b)), o fusionarse en operaciones de dos qubits nativas más eficientes soportadas por el hardware, reduciendo así el cuello de botella principal para la fidelidad del circuito.",
    "solution": "C"
  },
  {
    "id": 180,
    "question": "La falta de interferencia entre bosones distinguibles en un interferómetro lineal simplifica la simulación porque sus probabilidades de salida se factorizan en:",
    "A": "Probabilidades de transición de partículas individuales independientes, ya que la distinguibilidad elimina los efectos de interferencia cuántica y permite que la trayectoria de cada partícula a través del interferómetro se calcule por separado.",
    "B": "Productos de permanentes de columna calculados sobre submatrices disjuntas indexadas por las etiquetas de partículas distinguibles, donde cada permanente captura la simetría bosónica dentro de una sola especie de partícula pero la interferencia entre especies está suprimida. La probabilidad general permanece como un producto de evaluaciones de permanentes #P-difíciles, preservando la intratabilidad computacional a pesar de la distinguibilidad.",
    "C": "Productos de determinantes |det(U₁)|²|det(U₂)|²... donde cada Uₖ corresponde a la submatriz del interferómetro que conecta los modos de entrada ocupados por la partícula k con los modos de detección de salida, y la distinguibilidad previene el agrupamiento bosónico que de otro modo requeriría evaluación de permanentes. Si bien los determinantes son computables en tiempo polinomial, la necesidad de rastrear qué partícula ocupa qué modo reintroduce complejidad de muestreo del espacio de configuración exponencial.",
    "D": "Convoluciones de distribuciones de probabilidad de partículas individuales ponderadas por los coeficientes multinomiales que cuentan el número de permutaciones distinguibles que mapean partículas de entrada a modos de salida, y la estructura multinomial elimina la necesidad de cálculo de permanentes al reemplazar probabilidades de agrupamiento bosónico con combinatoria clásica. Sin embargo, la integral de convolución sobre etiquetas de grados de libertad continuos aún requiere tiempo de integración exponencial para evaluación exacta.",
    "solution": "A"
  },
  {
    "id": 181,
    "question": "¿Cómo se comparan los métodos de Descomposición en Valores Singulares Cuántica (QSVD) con la SVD clásica?",
    "A": "La estimación de fase cuántica sobre matrices de Gram produce una aceleración exponencial para cualquier matriz independientemente del número de condición o estructura, extrayendo todos los valores singulares en tiempo logarítmico sin supuestos de oráculo.",
    "B": "La codificación de amplitud revela automáticamente el espectro de valores singulares a través de estadísticas de medición, evitando el cálculo de autovalores mediante patrones de interferencia cuántica que proyectan sobre la base de vectores singulares con mediciones de una sola ronda.",
    "C": "Los métodos de descomposición en valores singulares cuántica ofrecen posibles aceleraciones exponenciales sobre los algoritmos SVD clásicos para ciertas matrices bien condicionadas de bajo rango cuando se dispone de acceso cuántico a la entrada, pero las implementaciones prácticas enfrentan restricciones significativas que limitan su aplicabilidad inmediata. La profundidad de circuito requerida escala con la precisión deseada y el número de condición, necesitando corrección de errores para mantener la exactitud durante todo el cálculo. Además, preparar el estado cuántico inicial que codifica la matriz y extraer los resultados finales implican sobrecarga computacional que puede disminuir las ventajas teóricas. Estos factores—requisitos de profundidad de circuito, desafíos de mantenimiento de exactitud y sobrecarga de corrección de errores—restringen colectivamente la utilidad práctica de los métodos QSVD en hardware cuántico a corto plazo a pesar de su promesa asintótica.",
    "D": "La evolución adiabática con hamiltonianos codificados en matriz se estabiliza en estados fundamentales cuyo espectro energético corresponde directamente a valores singulares ordenados, con mediciones de base produciendo inmediatamente vectores singulares.",
    "solution": "C"
  },
  {
    "id": 182,
    "question": "¿Qué es la Transformación de Valores Singulares Cuántica (QSVT) y por qué es importante?",
    "A": "QSVT permite transformaciones polinomiales de valores singulares mediante una secuencia de operadores de procesamiento de señales y reflexión, pero su utilidad principal radica en descomponer unitarios arbitrarios en su forma canónica SVD en lugar de implementar primitivas algorítmicas. Al aplicar sistemáticamente rotaciones controladas condicionadas a umbrales de valores singulares, reconstruye la descomposición espectral explícitamente, haciéndola particularmente valiosa para tomografía de estados cuánticos y reconstrucción de matrices de densidad donde el conocimiento completo del espectro de valores singulares es esencial para caracterizar la pureza de estados mixtos y cuantificar medidas de entropía de entrelazamiento.",
    "B": "QSVT es una primitiva universal para implementar esencialmente cualquier algoritmo cuántico que pueda expresarse como una transformación polinomial de los valores singulares de una matriz. Al intercalar operadores de procesamiento de señales con operadores de reflexión en una secuencia cuidadosamente diseñada, QSVT proporciona un marco sistemático que unifica y generaliza muchas técnicas algorítmicas cuánticas fundamentales incluyendo amplificación de amplitud, métodos de paseo cuántico y protocolos de simulación hamiltoniana bajo una única estructura matemática coherente.",
    "C": "QSVT proporciona un marco sistemático para implementar funciones polinomiales de valores singulares mediante procesamiento de señales cuántico intercalado, pero logra esto explotando la estructura de autovalores generalizada de operadores codificados en bloques en lugar de la descomposición en valores singulares misma. La técnica funciona convirtiendo el polinomio objetivo en una secuencia de rotaciones de fase controladas que actúan sobre los subespacios propios de una codificación en bloques no hermítica, donde la transformación efectiva aparece como manipulación de valores singulares solo en el subespacio proyectado, haciéndola fundamentalmente un método espectral en lugar de una verdadera transformación de valores singulares a pesar de la convención de nomenclatura.",
    "D": "QSVT implementa transformaciones polinomiales de valores singulares de matrices mediante secuencias cuidadosamente diseñadas de operadores de señal cuántica y reflexiones, proporcionando un marco poderoso que subsume amplificación de amplitud y paseos cuánticos. Sin embargo, su ventaja computacional depende críticamente del supuesto de que la matriz de entrada ya está codificada en bloques con límites de normalización conocidos, lo que limita aplicaciones prácticas porque construir esta codificación en bloques para matrices generales típicamente requiere complejidad de preparación de estado cuántico que escala polinomialmente con el número de condición, negando así la aceleración para sistemas mal condicionados donde QSVT proporcionaría las mejoras más dramáticas sobre métodos clásicos.",
    "solution": "B"
  },
  {
    "id": 183,
    "question": "¿Por qué se agregan a menudo marcadores de posición como puertas identidad a circuitos cuánticos de longitud fija?",
    "A": "Los marcadores de posición de identidad sirven para sincronizar tiempos de inactividad de qubits a través de ramas de ejecución paralela en circuitos con operaciones condicionales, asegurando que todos los caminos computacionales a través del DAG del circuito consuman igual tiempo de reloj independientemente de qué resultados de medición desencadenen qué subcircuitos. Sin rellenar caminos más cortos con identidades, los qubits que terminan sus puertas asignadas temprano permanecerían inactivos mientras otros completan ramas más profundas, acumulando cantidades desiguales de defasamiento en reposo y causando que la fidelidad global del circuito dependa del historial específico de mediciones. Al insertar identidades para igualar longitudes de camino, el compilador garantiza decoherencia uniforme a través de todos los qubits, haciendo las tasas de error predecibles y permitiendo modelado preciso de ruido.",
    "B": "Mantener estructuras de capa consistentes a través de circuitos con diferentes profundidades lógicas para que las pasadas de compilación y optimización puedan operar uniformemente, asegurando alineación de programación apropiada y asignación de recursos independientemente del conteo de puertas algorítmicas.",
    "C": "Las puertas identidad imponen la separación temporal requerida para que secuencias de desacoplamiento dinámico supriman ruido de baja frecuencia, ya que los pulsos DD deben insertarse a intervalos regulares determinados por la densidad espectral de ruido, e insertar identidades proporciona el espaciado necesario entre puertas algorítmicas para acomodar estos pulsos de supresión de errores. El compilador calcula el retardo mínimo entre puertas necesario para ajustar una secuencia completa XY-4 o CPMG basándose en la frecuencia de esquina de ruido 1/f medida, luego rellena el circuito con identidades que expanden la programación para coincidir con el período DD, convirtiendo efectivamente tiempo inactivo en corrección activa de errores sin cambiar la secuencia lógica de puertas o requerir inserción explícita de pulsos DD en la representación de circuito de alto nivel.",
    "D": "Las identidades marcadoras de posición permiten huella digital eficiente de circuitos para almacenar en caché resultados de compilación, ya que circuitos de longitud fija con identidades en posiciones predecibles producen representaciones canónicas que pueden ser hasheadas y emparejadas contra una base de datos de secuencias de puertas previamente optimizadas. Cuando el compilador encuentra un nuevo circuito, primero rellena a la longitud estándar con identidades, calcula un hash sobre la estructura rellenada, y consulta la caché de compilación—si existe una coincidencia, se recupera la descomposición pre-optimizada sin volver a ejecutar síntesis, reduciendo tiempo de compilación de segundos a microsegundos. Esta estrategia de almacenamiento en caché explota el hecho de que las identidades no afectan la tasa de colisión de hash ya que conmutan con todas las puertas, haciendo del hash del circuito rellenado una huella digital confiable para equivalencia estructural.",
    "solution": "B"
  },
  {
    "id": 184,
    "question": "En el contexto de una computadora cuántica de iones atrapados que implementa el algoritmo de Shor para factorizar un módulo RSA de 2048 bits, se te encarga caracterizar el presupuesto completo de errores incluyendo fidelidades de puertas, canales de decoherencia y errores de medición. La arquitectura usa una trampa de Paul lineal con iones 171Yb+, donde las puertas de dos qubits se implementan mediante interacciones Mølmer-Sørensen a través de modos de movimiento compartidos. Tu evaluación preliminar muestra fidelidades de puertas de un qubit del 99.97%, pero las fidelidades de puertas de dos qubits rondan el 99.3% con errores dominantes por calentamiento del movimiento a una tasa de 10 quanta/s. Dado que el algoritmo de Shor para este tamaño de problema requiere aproximadamente 10^10 puertas de dos qubits antes de la corrección de errores, y estás usando un código Steane [[7,1,3]] para tolerancia a fallos, ¿cuál es el cuello de botella más crítico que impide la ejecución exitosa?",
    "A": "Las implementaciones tolerantes a fallos requieren extracción de síndrome después de cada pocas puertas lógicas para detectar y corregir errores antes de la propagación, y con el código Steane [[7,1,3]], cada ciclo de medición de síndrome involucra 6 mediciones de qubits ancilla. Para las 10^10 puertas de dos qubits requeridas, esto se traduce en aproximadamente 10^8 rondas de extracción de síndrome asumiendo que los síndromes se miden cada 100 puertas lógicas. Los sistemas de iones atrapados típicamente exhiben errores de medición alrededor del 0.3% debido a discriminación imperfecta de estados y emisión espontánea durante detección por fluorescencia. A lo largo de 10^8 ciclos de medición, estos errores del 0.3% se acumulan a una tasa efectiva de fallo de medición de 1 - (0.997)^(10^8) ≈ 1, garantizando fallo del algoritmo incluso si todas las operaciones de puerta fueran perfectamente libres de errores.",
    "B": "Aunque la fidelidad reportada de puertas de un qubit del 99.97% parece aceptable, la compilación detallada del circuito revela que la exponenciación modular de Shor requiere aproximadamente 3×10^10 rotaciones de un qubit—aproximadamente tres veces el número de puertas de dos qubits—debido a descomposiciones de puertas Toffoli y correcciones de fase en subrutinas de transformada de Fourier cuántica. Con esta relación 3:1, los errores de un qubit contribuyen una probabilidad acumulativa de fallo de 1 - (0.9997)^(3×10^10) ≈ 0.9999, significando fallo virtualmente cierto del algoritmo incluso antes de contabilizar errores de dos qubits o de medición. Para factorización RSA-2048 específicamente, el volumen absoluto de operaciones de un qubit invierte la sabiduría convencional, haciendo de la fidelidad de un qubit la restricción primaria a pesar de su superficialmente impresionante tasa de éxito del 99.97%.",
    "C": "El mecanismo de puerta Mølmer-Sørensen depende de que todos los iones se acoplen a un modo de movimiento de centro de masa compartido, creando una restricción fundamental: solo una puerta de dos qubits puede ejecutarse en cualquier momento dado a través de toda la cadena, ya que puertas simultáneas interferirían destructivamente mediante modulaciones competitivas del movimiento colectivo. Este cuello de botella de serialización significa que incluso con corrección de errores perfecta, las 10^10 puertas de dos qubits del algoritmo deben ejecutarse secuencialmente en lugar de en paralelo a través de múltiples bloques de qubits lógicos. Esta falta de paralelización extiende el tiempo total de ejecución a aproximadamente 10^6 segundos (≈11 días), durante los cuales los iones atrapados experimentarían decoherencia catastrófica por perturbaciones ambientales.",
    "D": "La tasa de calentamiento del movimiento introduce errores correlacionados a través de la cadena de iones que no son adecuadamente abordados por la capacidad de corrección de errores de distancia-3 del código [[7,1,3]], ya que el ruido espacialmente correlacionado requiere códigos con propiedades geométricas específicamente diseñadas o distancia sustancialmente mayor para mantener la tasa de error umbral por debajo de 10^-4 por puerta necesaria para la profundidad de este algoritmo.",
    "solution": "D"
  },
  {
    "id": 185,
    "question": "¿Cómo ayuda el entrelazamiento cuántico a abordar los desafíos de la comunicación cuántica en la Internet Cuántica?",
    "A": "El entrelazamiento permite la teletransportación, dejando que los qubits se transmitan sin movimiento físico, evitando así pérdida y decoherencia durante el tránsito. Al consumir un par entrelazado pre-compartido y enviar solo bits clásicos para comunicar el resultado de la medición de teletransportación, la información cuántica es efectivamente transportada a través de distancias arbitrarias sin que el estado cuántico mismo atraviese el canal ruidoso, eludiendo la atenuación exponencial en fibra óptica.",
    "B": "El entrelazamiento permite codificación superdensa de estados cuánticos, duplicando la capacidad del canal al codificar información equivalente a dos qubits en cada fotón entrelazado transmitido. Al pre-compartir pares maximalmente entrelazados entre emisor y receptor, los canales cuánticos pueden transmitir información cuántica al doble de la tasa de protocolos no entrelazados. Esto compensa efectivamente la pérdida de fotones en fibra al permitir que cada fotón detectado exitosamente transporte el doble de carga útil cuántica, reduciendo a la mitad la tasa de transmisión requerida para un ancho de banda de comunicación dado.",
    "C": "El entrelazamiento permite protocolos de corrección de errores cuánticos que purifican activamente estados cuánticos degradados durante la transmisión al explotar correlaciones no locales. Cuando pares entrelazados atraviesan canales ruidosos, los receptores pueden realizar mediciones de Bell sobre múltiples pares degradados para destilar entrelazamiento de mayor fidelidad mediante concentración de entrelazamiento. Este proceso suprime exponencialmente efectos de decoherencia con cada ronda de purificación, permitiendo que la información cuántica se propague arbitrariamente lejos al destilar repetidamente ruido del canal en pares ancilla descartados mientras se preserva coherencia cuántica en pares retenidos.",
    "D": "El entrelazamiento permite protocolos de repetidores cuánticos que extienden el rango de comunicación al dividir canales en segmentos más cortos con tasas de error independientes. Al generar entrelazamiento sobre enlaces elementales y realizar intercambio de entrelazamiento en nodos intermedios, las redes cuánticas logran escalamiento polinomial de fidelidad con distancia en lugar de decaimiento exponencial. Cada segmento repetidor opera por debajo de la longitud de pérdida de fibra óptica, con purificación de entrelazamiento en nodos restaurando fidelidad antes del intercambio, permitiendo comunicación cuántica sobre distancias continentales a pesar de la absorción de fotones.",
    "solution": "A"
  },
  {
    "id": 186,
    "question": "La extracción simultánea de observables que conmutan es beneficiosa en la inferencia de QML porque:",
    "A": "Te permite reutilizar disparos de medición para múltiples términos de costo de manera eficiente",
    "B": "Permite la tomografía conjunta de valores esperados en una única base de medición",
    "C": "Reduce la sobrecarga de muestreo al medir todos los términos en una sola base de autoestados diagonalizada",
    "D": "Permite la lectura paralela de múltiples operadores que comparten autoestados simultáneos",
    "solution": "A"
  },
  {
    "id": 187,
    "question": "¿Por qué el cálculo ZX no logra proporcionar un verificador de equivalencia completo para circuitos universales?",
    "A": "Porque los ángulos de fase de las puertas de rotación crean clases de equivalencia infinitas",
    "B": "Carece de reglas de reescritura necesarias para ciertas transformaciones no Clifford",
    "C": "Porque no puede representar rotaciones arbitrarias de un qubit con ángulos irracionales",
    "D": "Porque la puerta Hadamard crea estructuras de grafos no bipartitas durante la reducción",
    "solution": "B"
  },
  {
    "id": 188,
    "question": "Considera un escenario en el que estás implementando el código Steane [[7,1,3]] en un procesador cuántico ruidoso y comparando las operaciones lógicas reales con las operaciones lógicas ideales esperadas de una implementación perfecta del código. Quieres cuantificar qué tan bien se están desempeñando tus puertas corregidas de errores en todos los estados de entrada posibles y todas las mediciones posibles que podrían realizarse en la salida. En el contexto de la norma diamante en corrección de errores cuánticos, ¿qué indica una menor distancia entre los canales cuánticos ideal y real?",
    "A": "Una menor distancia de norma diamante indica fundamentalmente que la implementación ruidosa real de tu canal cuántico proporciona una mejor aproximación de la operación ideal libre de errores. Esta métrica es particularmente valiosa porque acota el peor escenario posible: te indica la máxima distinguibilidad entre los dos canales sobre todos los estados de entrada posibles, incluyendo estados entrelazados con un sistema auxiliar. Cuando esta distancia es pequeña, puedes estar seguro de que para cualquier algoritmo o protocolo cuántico que use este canal, la desviación del comportamiento ideal estará acotada por esta cantidad, convirtiéndola en una medida operacional robusta de la fidelidad de puertas en sistemas corregidos de errores.",
    "B": "Una menor distancia de norma diamante indica que tus operaciones lógicas implementadas del código Steane aproximan más estrechamente las proyecciones ideales del espacio de código, lo que significa específicamente que la extracción de síndromes está identificando y localizando exitosamente los errores antes de que se propaguen más allá de la capacidad de corrección de distancia 3. La norma diamante captura esto de manera única al medir la distancia de traza máxima entre los canales real e ideal cuando ambos se extienden para actuar sobre un sistema de referencia entrelazado, lo que en el contexto de QEC corresponde a garantizar que los pares de Bell codificados permanezcan máximamente entrelazados después de las operaciones lógicas. Dado que el código [[7,1,3]] puede corregir cualquier error de un solo qubit, lograr distancias de norma diamante por debajo de 1/7 garantiza que los errores residuales permanezcan dentro del conjunto corregible con alta probabilidad en cualquier base computacional.",
    "C": "La distancia de norma diamante cuantifica qué tan bien tu implementación física preserva la estructura del espacio de código durante las operaciones lógicas, con valores menores indicando que las puertas transversales se están ejecutando con mayor fidelidad relativa a los generadores del grupo estabilizador. Para el código Steane [[7,1,3]] específicamente, la norma diamante mide la desviación máxima en la matriz de transferencia de Pauli lógica: cuando esta distancia es pequeña, significa que los operadores X̄ y Z̄ lógicos se están implementando con una fuga mínima fuera del subespacio de código definido por los seis generadores estabilizadores S₁ a S₆. Esto es particularmente importante porque el CNOT transversal del código Steane requiere mantener relaciones de fase entre los siete qubits físicos simultáneamente, e incluso pequeñas violaciones de la norma diamante pueden causar errores coherentes sutiles que se acumulan a través de múltiples capas de puertas.",
    "D": "Una menor distancia de norma diamante en tu implementación del código Steane indica que el modelo de ruido efectivo de tu canal lógico está convergiendo hacia un canal de Pauli, que es el escenario ideal para corrección de errores concatenada porque los errores de Pauli son exactamente los errores que los códigos estabilizadores están diseñados para corregir. La norma diamante mide específicamente la norma de operador de la diferencia ℰ_real - ℰ_ideal cuando ambos canales se extienden mediante el isomorfismo de Choi-Jamiołkowski, lo que en términos prácticos significa que cuantifica si tus errores residuales después de la medición de síndromes y corrección son predominantemente errores de inversión de bits y de fase en lugar de procesos coherentes más complejos. Esta interpretación es crucial para el código [[7,1,3]] porque su capacidad de distancia 3 asume que los errores siguen una aproximación de Pauli-twirl.",
    "solution": "A"
  },
  {
    "id": 189,
    "question": "En un algoritmo cuántico variacional diseñado para resolver un problema de optimización combinatoria con 50 variables binarias, notas que al aumentar la profundidad del ansatz de p=1 a p=8 capas, el proceso de entrenamiento se vuelve cada vez más difícil y el paisaje de costo se aplana significativamente. Mientras tanto, una línea base clásica usando recocido simulado continúa encontrando soluciones aproximadas razonables. Tu estudiante de posgrado sugiere que esto podría ser un problema fundamental en lugar de solo un problema de ajuste de hiperparámetros. ¿En qué se diferencia la transformada cuántica de Fourier de la transformada clásica de Fourier?",
    "A": "La versión cuántica opera sobre amplitudes de probabilidad en superposición y puede implementarse con O(n²) puertas para n qubits, mientras que la FFT clásica requiere O(n log n) operaciones sobre n = 2^n bits clásicos de datos — esta separación exponencial es la diferencia estructural clave",
    "B": "La versión cuántica opera sobre estados base superpuestos con O(n²) puertas para n qubits, mientras que la DFT clásica requiere O(N²) operaciones sobre N = 2^n puntos de datos — aunque QFT produce solo una amplitud por medición a diferencia de la salida de vector completo de la clásica",
    "C": "La versión cuántica codifica coeficientes como amplitudes que requieren medición para su extracción con O(n²) puertas para n qubits, mientras que la FFT clásica calcula todos los N = 2^n coeficientes explícitamente en tiempo O(N log N) — intercambiando aceleración exponencial por lectura probabilística",
    "D": "La versión cuántica transforma estados base de n qubits usando O(n²) rotaciones controladas, mientras que la FFT clásica procesa N = 2^n muestras en tiempo O(N log N) — pero extraer todas las amplitudes de QFT requiere exponencialmente muchas mediciones anulando la ventaja de puertas",
    "solution": "A"
  },
  {
    "id": 190,
    "question": "¿Cuál de las siguientes opciones explica mejor por qué las funciones de costo locales mejoran la entrenabilidad de las redes neuronales cuánticas?",
    "A": "Las funciones de costo locales evitan las mesetas estériles al restringir las contribuciones del gradiente a mediciones de subsistemas, lo que previene la supresión exponencial que ocurre cuando los observables globales promedian sobre el espacio de Hilbert completo. Sin embargo, esto introduce un sesgo hacia estados producto en la trayectoria de optimización, ya que los observables locales no pueden distinguir estados máximamente entrelazados de estados separables, potencialmente causando que el optimizador converja a soluciones subóptimas que carecen de las correlaciones cuánticas de largo alcance necesarias para la ventaja cuántica en tareas de aprendizaje.",
    "B": "Las funciones de costo locales descomponen el paisaje variacional en problemas de optimización de subsistemas independientes, permitiendo que los gradientes se calculen mediante contracciones de redes tensoriales clásicas sin sobrecarga exponencial. Esto funciona porque medir observables k-locales en un sistema de n qubits requiere calcular valores esperados sobre subespacios de dimensión 2^k en lugar del espacio completo 2^n, reduciendo el costo de estimación del gradiente de exponencial a polinomial. La señal del gradiente persiste incluso a grandes profundidades de circuito ya que las mediciones de subsistemas se acoplan solo a parámetros de puertas cercanas a través del límite de Lieb-Robinson sobre la propagación de operadores.",
    "C": "Los observables locales concentran la información del gradiente en componentes de Fourier de baja frecuencia del paisaje de costo, previniendo la dilución exponencial de varianza que afecta a las mediciones globales. Dado que los operadores de Pauli k-locales tienen norma de operador acotada independientemente del tamaño del sistema, sus valores esperados escalan independientemente del conteo total de qubits, preservando la magnitud del gradiente. Sin embargo, esto también restringe la clase de funciones accesibles a aquellas aprendibles por circuitos de profundidad constante con entrelazamiento limitado, ya que las funciones de costo locales no pueden recompensar correlaciones exponencialmente de largo alcance que requieren unitarios parametrizados profundos para establecerse.",
    "D": "Las funciones de costo locales evitan el promediado exponencial sobre todo el estado cuántico, lo que preserva la relación señal-ruido del gradiente al medir solo observables de subsistemas. Esto previene el fenómeno de meseta estéril donde los observables globales diluyen los gradientes exponencialmente con el tamaño del sistema.",
    "solution": "D"
  },
  {
    "id": 191,
    "question": "¿Cómo afecta la latencia de ejecución de las puertas condicionales a los protocolos de retroalimentación en medio del circuito, como el reinicio activo?",
    "A": "Las puertas condicionales aprovechan el colapso instantáneo del estado cuántico tras la medición, permitiendo que el controlador clásico aplique correcciones de Pauli basadas en los resultados de medición sin esperar los retrasos de propagación de señal, porque el estado actualizado ya está determinado por la regla de Born en el instante de la medición. Dado que la actualización del estado cuántico es físicamente inmediata una vez que el registro de medición está finalizado, la única latencia restante proviene de la etapa de discriminación de la medición en sí, no de ninguna lógica condicional posterior, lo que hace posible lograr retroalimentación por debajo del microsegundo en todas las plataformas contemporáneas.",
    "B": "Si la etapa de procesamiento clásico entre la lectura de la medición y la rotación condicional subsiguiente consume demasiados microsegundos en relación con el tiempo de decoherencia del qubit, el estado cuántico pierde el beneficio de coherencia que el protocolo de reinicio fue diseñado para preservar. Por lo tanto, el hardware de propagación rápida con latencia de decisión por debajo del microsegundo es crítico para mantener la fidelidad del protocolo.",
    "C": "La latencia de las puertas condicionales afecta principalmente a protocolos que requieren operaciones paralelas síncronas a través de múltiples qubits, porque el controlador clásico debe serializar las decisiones de retroalimentación para evitar condiciones de carrera en la lógica de control. Sin embargo, el reinicio activo opera sobre un qubit a la vez sin dependencias entre qubits durante la ventana de retroalimentación, por lo que la latencia de ejecución afecta la programación del tiempo de inactividad pero no limita fundamentalmente la fidelidad del protocolo, siempre que el resultado de la medición se registre antes de que comience la siguiente operación.",
    "D": "La restricción de latencia se aplica predominantemente a arquitecturas superconductoras donde las señales de control deben propagarse a través de líneas coaxiales a temperatura ambiente hasta la etapa del refrigerador de dilución, introduciendo retrasos de ida y vuelta de varios microsegundos. Las plataformas de iones atrapados y átomos neutros evitan este cuello de botella al colocar el hardware de decisión clásico junto con el procesador cuántico en la misma cámara de vacío, permitiendo latencias de retroalimentación muy por debajo del tiempo de decoherencia. En consecuencia, los protocolos de reinicio activo muestran comportamientos de escalado cualitativamente diferentes entre modalidades de hardware, con sistemas superconductores que requieren mitigación de errores agresiva mientras que los sistemas atómicos logran fidelidad de reinicio casi ideal independientemente de la profundidad del circuito.",
    "solution": "B"
  },
  {
    "id": 192,
    "question": "¿Qué es la información mutua cuántica y cuál es su importancia?",
    "A": "Cuantifica las correlaciones totales —tanto clásicas como cuánticas— entre dos sistemas A y B como I(A:B) = S(ρ_A) + S(ρ_B) - S(ρ_AB), donde S denota la entropía de von Neumann. Esta cantidad captura todas las dependencias estadísticas: correlaciones clásicas (medibles mediante observaciones locales), correlaciones cuánticas como el entrelazamiento (que requieren mediciones conjuntas), e incluso la discordia (correlaciones inaccesibles a mediciones proyectivas locales). A diferencia de las medidas puramente clásicas, permanece no negativa incluso para estados entrelazados donde la entropía condicional puede ser negativa.",
    "B": "Cuantifica las correlaciones totales entre los sistemas A y B como I(A:B) = S(ρ_A) + S(ρ_B) - S(ρ_AB), capturando dependencias tanto clásicas como cuánticas, pero sirve principalmente como límite superior sobre la información accesible: excede la cantidad χ de Holevo exactamente por la discordia cuántica, representando la brecha entre las correlaciones totales y aquellas extraíbles mediante mediciones locales. Aunque I(A:B) permanece no negativa por la subaditividad de la entropía de von Neumann, este límite se satura solo para estados clásico-cuánticos, convirtiéndola en una medida de correlación potencial en lugar de operacionalmente accesible en presencia de superposición.",
    "C": "Mide la distinguibilidad entre el estado conjunto ρ_AB y el estado producto ρ_A ⊗ ρ_B mediante la entropía relativa I(A:B) = S(ρ_AB || ρ_A ⊗ ρ_B), cuantificando cuánto se desvía el sistema de la independencia estadística. Esta divergencia de Kullback-Leibler captura todas las correlaciones—clásicas y cuánticas—como la ganancia de información al conocer las estadísticas conjuntas versus asumir independencia. Se reduce a S(ρ_A) + S(ρ_B) - S(ρ_AB) por la definición de entropía relativa cuántica, permanece no negativa por la desigualdad de Klein, y gobierna la tasa asintótica de prueba de hipótesis entre estados correlacionados versus no correlacionados.",
    "D": "Cuantifica la máxima fidelidad de entrelazamiento alcanzable al transmitir estados cuánticos de A a B a través de un canal ruidoso, definida como I(A:B) = max_ρ [S(ρ_A) + S(ρ_B) - S(ρ_AB)], donde la maximización corre sobre todos los posibles ensambles de entrada. Esta definición operacional se conecta con la capacidad del canal mediante la desigualdad cuántica de procesamiento de datos: la información mutua limita superiormente la información coherente I(A⟩B) = S(ρ_B) - S(ρ_AB), que a su vez determina el umbral de corrección de errores cuánticos. A diferencia de la información mutua clásica, la versión cuántica puede exceder log(d) debido a la codificación superdensa, capturando tanto la eficiencia de distribución del entrelazamiento como los efectos de retroacción de la medición.",
    "solution": "A"
  },
  {
    "id": 193,
    "question": "Considere la implementación práctica del algoritmo de Grover para invertir funciones hash criptográficas como SHA-256. El circuito cuántico debe implementar tanto la función hash como su inversa como parte del oráculo. Dado que las computadoras cuánticas reales tienen tiempos de coherencia limitados y fidelidades de puerta que se degradan con la profundidad del circuito, ¿qué factor fundamental hace que esta aplicación sea particularmente desafiante en comparación con buscar en una base de datos no estructurada donde el oráculo es simplemente un cambio de fase?",
    "A": "La implementación reversible de la función hash requiere un circuito extremadamente profundo con miles de puertas por cada llamada al oráculo, y esta profundidad se compone a través de todas las √N iteraciones del proceso de amplificación de amplitud, haciendo que el requisito de tiempo de coherencia total sea astronómico incluso para espacios de preimagen modestos. La complejidad del circuito domina sobre cualquier otra consideración.",
    "B": "La implementación reversible de SHA-256 exige una gestión extensa de ancillas para preservar la unitariedad durante operaciones no lineales como la suma modular y las rotaciones bit a bit, requiriendo entrelazamiento persistente a través de miles de qubits físicos durante cada evaluación del oráculo. Esta sobrecarga de ancillas escala tanto con el tamaño del estado hash como con el número de rondas de compresión, y dado que estas ancillas deben mantener coherencia a través de todas las √N iteraciones de Grover mientras acumulan errores de las escaleras CNOT repetidas en los subcircuitos aritméticos modulares, el umbral de fidelidad se vuelve inalcanzable incluso con tasas de error de puerta optimistas.",
    "C": "Las funciones hash criptográficas emplean capas de mezcla dependientes de avalancha donde cada bit de salida depende no linealmente de la mayoría de los bits de entrada a través de árboles de lógica combinacional profundos, forzando al circuito cuántico a implementar estas dependencias usando puertas Toffoli en cascada con expansión de ancillas que crece cuadráticamente con el tamaño de entrada. Esto crea un cuello de botella crítico porque los qubits ancilla requeridos deben permanecer coherentes no solo durante una sola llamada al oráculo sino a través de todas las √N rondas de amplificación de amplitud, y la decoherencia acumulada en estas ancillas persistentes corrompe las relaciones de fase necesarias para la interferencia constructiva sobre la preimagen objetivo.",
    "D": "El desafío fundamental surge del hecho de que el cálculo hash reversible requiere descomputar todos los valores intermedios para restaurar los qubits ancilla a sus estados iniciales, pero esta descomputación debe ocurrir después del cambio de fase objetivo y antes de la siguiente iteración de Grover. La dependencia secuencial entre la evaluación hacia adelante, el marcado de fase y la descomputación hacia atrás crea una ruta crítica a través de cada llamada al oráculo donde cualquier error de puerta en la etapa de descomputación causa fuga de ancillas que se propaga en iteraciones subsiguientes, aleatorizando gradualmente las amplitudes amplificadas y destruyendo la ventaja cuántica después de solo O(√N / fidelidad²) iteraciones en lugar de las √N completas requeridas.",
    "solution": "A"
  },
  {
    "id": 194,
    "question": "¿Cómo afecta la dispersión de datos a los modelos de IA en la corrección de errores cuánticos?",
    "A": "Los datos de entrenamiento dispersos conducen al sobreajuste y a una generalización pobre porque el modelo aprende a memorizar patrones de síndrome raros sin capturar la estructura estadística subyacente de los errores cuánticos. Cuando la mayoría de los ejemplos de entrenamiento representan casos extremos infrecuentes en lugar de distribuciones de error típicas, la red neuronal desarrolla límites de decisión demasiado ajustados al conjunto de entrenamiento, fallando en generalizar a nuevas secuencias de síndrome encontradas durante operaciones reales de corrección de errores en hardware cuántico.",
    "B": "Entrenar predominantemente con síndromes raros fuerza a la red a asignar peso desproporcionado a configuraciones de baja probabilidad, lo que paradójicamente mejora la generalización para estos patrones exactos pero a costa de tasas aumentadas de falsos positivos en síndromes comunes. El modelo aprende a reconocer cadenas de error infrecuentes con alta precisión al dedicar capacidad de red a sus firmas específicas, pero esta especialización desplaza los límites de decisión lejos de las regiones de alta densidad del espacio de síndrome donde ocurren la mayoría de los errores operacionales, reduciendo la precisión general de decodificación durante el tiempo de ejecución a pesar de las ganancias aparentes en eventos raros retenidos en la validación.",
    "C": "Los conjuntos de datos dispersos concentran la capacidad del modelo en distinguir eventos de error verdaderos del ruido de medición al excluir los casos abrumadores de síndrome nulo que dominan la recolección de datos crudos, pero este filtrado introduce un sesgo crítico: el modelo nunca aprende la distribución de síndrome base bajo operación normal. En consecuencia, durante el despliegue el decodificador sobreestima sistemáticamente las tasas de error porque interpreta cualquier fluctuación de síndrome como significativa, habiendo sido entrenado exclusivamente en ejemplos donde los errores realmente ocurrieron. Esto resulta en correcciones excesivas que introducen más errores de los que corrigen, degradando las tasas de error lógico por debajo del umbral físico.",
    "D": "Cuando los datos de síndrome exhiben dispersión extrema, la dimensionalidad efectiva del colector de entrada colapsa porque la mayoría de los ejemplos de entrenamiento se agrupan cerca de un subespacio de baja dimensión definido por la estructura estabilizadora del código. Aunque esto parece simplificar el aprendizaje al reducir la complejidad de características, en realidad previene que el modelo estime la distribución de probabilidad completa sobre síndromes—la red aprende solo distribuciones condicionales P(corrección|síndrome ≠ 0) mientras permanece ignorante de P(síndrome), que es esencial para la decodificación bayesiana. Este conocimiento parcial conduce a correcciones subóptimas que no logran tener en cuenta las probabilidades previas de diferentes mecanismos de error.",
    "solution": "A"
  },
  {
    "id": 195,
    "question": "¿Qué método de codificación de datos utiliza estados cuánticos analógicos para representar valores clásicos?",
    "A": "La codificación de amplitud mapea datos clásicos en las amplitudes de probabilidad de estados cuánticos, almacenando N valores clásicos en log(N) qubits a través de la estructura de amplitud de la función de onda. Aunque esto logra compresión exponencial, es fundamentalmente una codificación discreta ya que las amplitudes son coeficientes normalizados de estados base.",
    "B": "La codificación one-hot dedica un qubit separado a cada valor clásico posible, estableciendo exactamente un qubit en |1⟩ mientras todos los demás permanecen en |0⟩, reflejando el esquema de codificación categórica clásica. Este enfoque preserva la estructura clásica pero escala linealmente en lugar de aprovechar la superposición cuántica.",
    "C": "La codificación analógica emplea sistemas cuánticos de variable continua como las amplitudes de cuadratura de modos de campo electromagnético o los observables de posición y momento de osciladores armónicos para representar directamente datos clásicos de valor real. A diferencia de las codificaciones discretas de qubit que mapean datos a ángulos de rotación o estados de base computacional, la codificación analógica utiliza el espacio de Hilbert de dimensión infinita de modos bosónicos donde los valores clásicos corresponden a desplazamientos analógicos en el espacio de fase. Este enfoque maneja naturalmente datos continuos sin artefactos de discretización y se implementa en procesadores cuánticos fotónicos usando estados comprimidos, estados coherentes y mediciones homodinas que extraen señales de voltaje analógicas proporcionales a los valores de entrada.",
    "D": "La codificación de fase utiliza ángulos de rotación continuos para codificar datos de valor real directamente en estados de qubit a través de puertas de fase parametrizadas como Rz(θ), donde θ es proporcional al valor de entrada. Aunque esto parece analógico ya que θ puede tomar cualquier valor real, la discretización eventual ocurre en la etapa de medición.",
    "solution": "C"
  },
  {
    "id": 196,
    "question": "En el contexto de frameworks de computación cuántica como Qiskit, tienes un circuito abstracto escrito usando puertas estándar (H, CNOT, RZ, etc.). Tu hardware objetivo solo admite un conjunto de puertas nativas {√X, X, CZ} y tiene un grafo de conectividad de qubits específico donde solo ciertos pares pueden interactuar directamente. Además, el dispositivo tiene tasas de error de puerta calibradas que varían entre qubits, y deseas que el circuito final minimice el error total. ¿Cuál es el término para el proceso de compilación que toma tu circuito abstracto y produce un circuito equivalente optimizado para este dispositivo específico, y qué hace realmente este proceso?",
    "A": "La traducción de base realiza la conversión del conjunto de puertas mediante secuencias de descomposición exactas derivadas de pruebas de universalidad de teoría de grupos, mapeando cada puerta abstracta a productos de puertas nativas mediante descomposición de Cartan del unitario objetivo en componentes su(2)⊗su(2), luego aplica relaciones de conmutación para reducir el conteo de puertas, pero no incorpora restricciones de topología de hardware ni enrutamiento consciente de errores, manteniendo la optimalidad de profundidad del circuito solo para arquitecturas completamente conectadas.",
    "B": "La programación de instrucciones cuánticas aplica compilación consciente de recursos que descompone puertas abstractas en primitivas de hardware, realiza asignación de qubits basada en satisfacción de restricciones respetando la topología de conectividad, inserta cadenas SWAP para interacciones no adyacentes, luego aplica optimizaciones peephole usando métricas de calibración del dispositivo, pero optimiza principalmente para profundidad del circuito en lugar de error acumulativo, produciendo potencialmente circuitos más cortos con mayor error en perfiles de ruido heterogéneos.",
    "C": "La transpilación realiza una transformación integral que descompone puertas abstractas en el conjunto de instrucciones nativas, inserta operaciones SWAP para enrutar interacciones de qubits lógicos a través del grafo de conectividad física, y aplica pasadas de optimización aprovechando datos de calibración para minimizar el conteo de puertas y el error esperado basándose en las características de ruido específicas del dispositivo.",
    "D": "La síntesis consciente del layout descompone puertas en la base nativa usando descomposiciones óptimas de ángulos de Euler, construye mapeos de qubits mediante algoritmos de colocación inicial adaptados al ruido que minimizan la fidelidad de ruta esperada, inserta redes SWAP para cumplimiento de topología, luego aplica reglas de reescritura basadas en cálculo ZX para cancelar puertas de fase redundantes, pero no itera las fases de mapeo/enrutamiento conjuntamente, produciendo potencialmente soluciones subóptimas cuando los costos de enrutamiento dominan.",
    "solution": "C"
  },
  {
    "id": 197,
    "question": "¿Cuál es el desafío principal al implementar versiones cuánticas de regularización por dropout?",
    "A": "Eliminar operaciones aleatoriamente destruye la unitariedad, porque el dropout introduce inherentemente brechas no determinísticas en el grafo computacional—cuando omites puertas probabilísticamente, diferentes ejecuciones del circuito siguen diferentes trayectorias de evolución a través del espacio de Hilbert, impidiendo que la transformación general sea representada por una única matriz unitaria.",
    "B": "El colapso por medición previene el promediado estocástico—en el dropout clásico, entrenas con eliminación aleatoria de neuronas pero promedias sobre todas las posibles máscaras de dropout en tiempo de inferencia, lo cual funciona porque las probabilidades clásicas se suman linealmente. Sin embargo, las amplitudes cuánticas siguen reglas de superposición cuadrática, por lo que no puedes simplemente promediar resultados de medición de diferentes configuraciones de dropout y esperar recuperar la predicción de la red completa.",
    "C": "No puedes desactivar selectivamente parte de una superposición—o está toda ahí o la has medido y colapsado el estado. Además, eliminar puertas aleatorias rompe la estructura unitaria que requieren los circuitos cuánticos, y pierdes completamente la ventaja cuántica. El dropout clásico funciona porque las redes neuronales son inherentemente redundantes con representaciones distribuidas; los circuitos cuánticos son máquinas de interferencia cuidadosamente coreografiadas donde cada puerta contribuye a la distribución de amplitud final, haciendo que la eliminación aleatoria sea catastrófica en lugar de regularizadora.",
    "D": "La superposición dificulta la desactivación selectiva, porque el dropout requiere controlar independientemente si cada ruta computacional permanece activa o se enmascara, pero la superposición cuántica significa que todas las rutas existen simultáneamente en un único vector de estado ponderado por amplitudes.",
    "solution": "C"
  },
  {
    "id": 198,
    "question": "En la práctica, ¿qué limita el número máximo de qubits por subcircuito?",
    "A": "El conteo de qubits del dispositivo y las restricciones de profundidad impuestas por los tiempos de coherencia determinan el tamaño máximo del subcircuito, ya que subcircuitos más grandes requieren más qubits y secuencias de puertas más profundas que deben completarse antes de que la decoherencia corrompa el cálculo.",
    "B": "El techo de tamaño del subcircuito está establecido por la dimensión máxima de enlace de red tensorial que el software de control clásico puede contraer en tiempo real para retroalimentación de medición a mitad de circuito, ya que cada qubit adicional duplica la dimensión del espacio de Hilbert que requiere simulación. Las bibliotecas modernas de redes tensoriales ejecutándose en coprocesadores FPGA pueden manejar hasta dimensión de enlace χ=2^14 con latencia submicrosegundo usando ordenamientos de contracción optimizados, lo cual se traduce en aproximadamente 14 qubits de entrelazamiento máximo por subcircuito antes de que la sobrecarga de simulación clásica exceda el presupuesto de tiempo inactivo del qubit y obligue al procesador cuántico a esperar que el sistema de control termine de calcular los parámetros de puerta condicionales.",
    "C": "La partición de subcircuitos está limitada por la RAM cuántica disponible para almacenar estados computacionales intermedios durante protocolos de corte de circuito, ya que cada límite de partición requiere log(d) qubits ancilla para codificar el índice de corte de dimensión d mediante codificación de amplitud. Para subcircuitos que exceden aproximadamente 30 qubits, la sobrecarga de ancilla para representar el espacio de corte exponencialmente grande crece hasta consumir más de la mitad de los qubits físicos del dispositivo, dejando recursos insuficientes para los registros computacionales reales. Esto obliga al compilador a reducir el tamaño del subcircuito o aceptar costos de posprocesamiento clásico exponencialmente crecientes de la descomposición de cuasi-probabilidad, creando un límite práctico donde los requisitos combinados de recursos cuánticos y clásicos exceden la capacidad de hardware disponible.",
    "D": "El tamaño máximo del subcircuito está gobernado por la capacidad del compilador para enrutar puertas de dos qubits dentro del grafo de conectividad del dispositivo mientras respeta la sobrecarga de inserción de swap, ya que subcircuitos más grandes requieren más comunicación entre pares de qubits distantes, y cada puerta SWAP añade tres capas CNOT de profundidad. Para topologías típicas de red heavy-hex con grado promedio ~3, subcircuitos que exceden 40 qubits crean congestión de enrutamiento donde la profundidad de inserción SWAP crece cuadráticamente con el diámetro del subcircuito, consumiendo todo el presupuesto de coherencia en comunicación en lugar de puertas lógicas. Este cuello de botella de enrutamiento efectivamente limita el tamaño del subcircuito a aproximadamente √N qubits para un dispositivo de N qubits, independiente del conteo bruto de qubits disponibles.",
    "solution": "A"
  },
  {
    "id": 199,
    "question": "¿Cuál es el desafío principal que plantean los errores de fuga para la corrección de errores cuánticos?",
    "A": "Los errores de fuga corrompen la extracción de síndrome al hacer que las mediciones de estabilizador devuelvan resultados que parecen válidos dentro del espacio de código pero codifican información de error incorrecta, ya que un qubit en |2⟩ aún puede producir valores propios determinísticos ±1 para operadores de Pauli a pesar de no residir en el subespacio computacional. Esta corrupción de síndrome se propaga sin detectar a través del decodificador porque las estadísticas de medición permanecen consistentes con los patrones de correlación diseñados del código, llevando a cadenas de error mal diagnosticadas que aplican operadores de recuperación incorrectos e inadvertidamente introducen errores lógicos mientras aparentan completar exitosamente el ciclo de corrección de errores.",
    "B": "Las mediciones de síndrome estándar asumen solo el subespacio computacional de |0⟩ y |1⟩, pero la fuga a |2⟩ o niveles de energía superiores rompe esta suposición fundamental. Los códigos estabilizadores no pueden detectar ni corregir errores fuera de su espacio de código diseñado ya que los estados filtrados producen resultados de medición impredecibles.",
    "C": "Los qubits filtrados comprometen el ciclo de corrección de errores al reducir la distancia de código efectiva, ya que cada qubit físico en el estado |2⟩ actúa como un borrado permanente que no puede participar en mediciones de estabilizador hasta ser activamente reiniciado mediante transiciones de banda lateral o ingeniería de reservorio. Mientras que el código puede tolerar borrados hasta distancia d-1, la fuga se acumula con el tiempo ya que las puertas de dos qubits entre qubits computacionales y filtrados transfieren probabilísticamente población a niveles superiores, saturando eventualmente la capacidad de corrección de borrado y causando fallo lógico una vez que el número de qubits simultáneamente filtrados excede el parámetro de umbral de borrado del código.",
    "D": "La fuga crea un problema de retroacción de medición donde la extracción de síndrome perturba los qubits filtrados de manera diferente a los qubits de base computacional, causando que el acto de medir estabilizadores inyecte errores de fase correlacionados a través del bloque lógico. Como el desplazamiento dispersivo para |2⟩ difiere de |0⟩ y |1⟩, cada lectura de síndrome aplica una rotación condicional no deseada proporcional a la población filtrada, y estas fases inducidas por medición se acumulan coherentemente a través de rondas de síndrome, convirtiendo efectivamente el protocolo de corrección de errores en una fuente de ruido que degrada la fidelidad lógica más rápido que dejar los qubits inactivos sin aplicar ninguna medición de síndrome.",
    "solution": "B"
  },
  {
    "id": 200,
    "question": "¿Cuál es la ventaja principal de usar códigos de corrección de errores cuánticos asimétricos en entornos de ruido sesgado?",
    "A": "Al explotar la asimetría direccional en canales de ruido sesgado, estos códigos permiten cronogramas de medición de síndrome adaptativos donde los tipos de error de alto sesgo activan ciclos de corrección más rápidos mientras que los errores de baja probabilidad usan retroalimentación retardada, reduciendo así la latencia de corrección promedio. Esta optimización temporal mantiene tasas de error lógico por debajo del umbral mientras disminuye la sobrecarga de ancilla promediada en el tiempo por factores que se aproximan a la razón de sesgo misma, mejorando fundamentalmente el compromiso rendimiento-fidelidad para hardware con asimetría de ruido nativa.",
    "B": "Asignan recursos de corrección eficientemente al proporcionar protección más fuerte contra los tipos de error que ocurren con mayor frecuencia en el modelo de ruido mientras dedican menos qubits y puertas a corregir los canales de error más raros, optimizando así el compromiso sobrecarga-rendimiento para las firmas de ruido reales del hardware.",
    "C": "Los códigos asimétricos logran codificación óptima al adaptar la distribución de peso del estabilizador para coincidir con la razón de sesgo de ruido, de modo que los tipos de error que ocurren frecuentemente requieren estabilizadores de menor peso para detección mientras que los errores raros usan mediciones de mayor peso. Esta asimetría de peso reduce la profundidad promedio del circuito de medición del estabilizador por un factor proporcional a la raíz cuadrada del parámetro de sesgo, disminuyendo así la propagación de errores durante la extracción de síndrome mientras se mantiene la distancia de código necesaria para tolerancia a fallos por debajo del umbral.",
    "D": "La estructura asimétrica explota la no conmutatividad entre canales de error dominantes y raros para crear un espacio de síndrome donde los errores de alta probabilidad se proyectan sobre subespacios propios de bajo peso del grupo estabilizador, permitiendo la extracción de síndrome usando menos qubits ancilla que los códigos simétricos. Esta estratificación de subespacios propios permite que los errores de alto sesgo sean detectados mediante mediciones de estabilizadores de orden log(n) en lugar de O(n), reduciendo fundamentalmente la sobrecarga de extracción de síndrome mientras se preserva la tasa de supresión de error lógico.",
    "solution": "B"
  },
  {
    "id": 201,
    "question": "¿Cuál es un beneficio único de los códigos de superficie en el contexto de la resiliencia a la pérdida de átomos?",
    "A": "Su estructura de estabilizadores 2D de vecinos más cercanos requiere solo mediciones de síndrome locales que naturalmente aíslan los qubits perdidos a subgrafos pequeños de estabilizadores, evitando la propagación del síndrome a través de acoplamientos de largo alcance que distribuirían fallos de medición inducidos por pérdidas por todo el arreglo. Al restringir cada generador de estabilizador a términos de cuatro cuerpos en sitios adyacentes, el código asegura que una sola vacante atómica afecte como máximo cuatro verificaciones de tipo X y Z, permitiendo que decodificadores estándar de emparejamiento perfecto de peso mínimo señalen estas contribuciones de síndrome faltantes y continúen la corrección de errores con distancia de código reducida, manteniendo la supresión de errores lógicos incluso cuando los arreglos de atrapamiento de átomos neutros desarrollan patrones de vacantes dispersas durante secuencias de puertas extendidas.",
    "B": "Su disposición planar permite la reconfiguración localizada de estabilizadores para evitar sitios perdidos, permitiendo que el decodificador reencamine dinámicamente la extracción de síndrome alrededor de las vacantes sin recompilación global del circuito. Al tratar la pérdida de átomos como errores de borrado con ubicaciones conocidas, el código puede adaptar sus definiciones de operadores lógicos y calendarios de medición de estabilizadores en tiempo real, manteniendo la capacidad de corrección de errores incluso cuando el arreglo de qubits físicos desarrolla patrones irregulares de vacantes durante cálculos extendidos en plataformas como sistemas de pinzas ópticas.",
    "C": "Su distribución de peso de estabilizadores permite calendarios de medición de síndrome adaptativos que omiten generadores que involucran átomos perdidos, permitiendo que el decodificador reconstruya verificaciones de paridad faltantes a través de propagación redundante de restricciones de estabilizadores intactos vecinos sin ambigüedad de síndrome. Al tratar la pérdida de átomos como borrados detectables en lugar de errores de Pauli desconocidos, el código puede invocar propagación de creencias modificada que explota la redundancia de estabilizadores inherente en la estructura homológica, manteniendo umbrales de corrección de errores dependientes de la distancia incluso cuando el arreglo de qubits físicos desarrolla hasta (d−1)/2 vacantes dispersas durante extracción de síndrome de múltiples rondas en plataformas de red óptica con tasas realistas de pérdida de átomos.",
    "D": "Su estructura de degeneración topológica permite la reubicación de operadores lógicos tolerante a fallos mediante deformación de ciclos homológicamente equivalentes alejándose de sitios perdidos, permitiendo que el decodificador redefina estados de base computacional usando operadores de cadena alternativos que eviten vacantes sin romper la conmutatividad de estabilizadores. Al tratar la pérdida de átomos como ubicaciones de borrado conocidas que restringen las clases de homología disponibles, el código puede realizar migración de qubits lógicos en tiempo real a subregiones sin defectos mientras preserva la información codificada mediante deformación continua de cadenas lógicas tanto X como Z, manteniendo la integridad computacional incluso cuando los arreglos de átomos neutros experimentan patrones de vacantes dinámicamente evolutivos a través de cientos de sitios de atrapamiento durante simulaciones cuánticas extendidas.",
    "solution": "B"
  },
  {
    "id": 202,
    "question": "En teorías de marcos de referencia cuánticos, ¿qué prohíbe la \"superselección\"?",
    "A": "Superposiciones coherentes a través de sectores de carga distintos cuando esos sectores corresponden a subespacios propios de un generador globalmente conservado, porque la ley de conservación restringe los estados físicamente preparables a aquellos que respetan la estructura de simetría. Sin embargo, una vez que se introduce un marco de referencia relacional mediante un sistema auxiliar que porta una distribución de carga complementaria, la fase relativa entre sectores adquiere significado operacional a través de protocolos interferométricos que miden la diferencia de carga, restaurando así la realizabilidad física de la superposición dentro del espacio de Hilbert extendido que incluye tanto los grados de libertad del sistema como del marco de referencia.",
    "B": "Superposiciones coherentes de estados que difieren por una cantidad conservada cuando no existe un marco de referencia compartido para dar significado operacional a la fase relativa entre diferentes sectores de carga. Sin tal marco, la superposición carece de realizabilidad física porque los observadores no pueden distinguir la fase relativa a través de ningún protocolo de medición local, forzando al estado a comportarse efectivamente como una mezcla clásica de los sectores de valores propios distintos a pesar de estar formalmente descrito por una superposición en el formalismo del espacio de Hilbert.",
    "C": "Superposiciones coherentes entre estados en diferentes sectores de carga cuando la simetría global es descrita por un grupo de Lie compacto, porque los grupos compactos imponen reglas de superselección discretas a través de su teoría de representaciones. La distinción clave es que los grupos no compactos como el grupo de Poincaré permiten superposiciones continuas a través de estados propios de momento, mientras que los grupos compactos como U(1) o SU(2) imponen condiciones de cuantización estrictas que prohíben cualquier combinación lineal de estados con diferentes valores propios de los generadores conservados, independientemente de si los observadores poseen marcos de referencia adecuados para comparación de fase.",
    "D": "Superposiciones coherentes de estados con diferente carga conservada cuando se consideran desde la perspectiva de un solo observador localizado que carece de acceso a un sistema de referencia deslocalizado, porque la regla de superselección emerge dinámicamente a través de decoherencia inducida por la incapacidad del observador de rastrear relaciones de fase globales. Una vez que el observador construye u obtiene acceso a un marco de referencia cuántico suficientemente deslocalizado—tal como un estado coherente de un oscilador armónico con número medio grande de fotones—la decoherencia efectiva se suprime, y las superposiciones entre sectores de carga recuperan su coherencia cuántica en escalas de tiempo experimentalmente accesibles.",
    "solution": "B"
  },
  {
    "id": 203,
    "question": "¿Qué propiedad del modelo se ha encontrado que correlaciona menos con el rendimiento en conjuntos de datos del mundo real?",
    "A": "La profundidad total del circuito—medida como el número de capas de puertas secuenciales desde la entrada hasta la medición—exhibe una correlación sorprendentemente débil con la precisión empírica en tareas de clasificación y regresión del mundo real. Mientras que la sabiduría convencional sugiere que circuitos más profundos deberían permitir aproximaciones de funciones más expresivas, estudios experimentales en múltiples plataformas de hardware revelan que la profundidad excesiva principalmente amplifica la decoherencia y errores de puertas sin aumentar proporcionalmente la capacidad del modelo. De hecho, ansätze superficiales con patrones de entrelazamiento cuidadosamente diseñados a menudo superan a sus contrapartes profundas cuando se entrenan en dispositivos de escala intermedia ruidosos, sugiriendo que la profundidad por sí sola es un pobre predictor del rendimiento de generalización en el aprendizaje automático cuántico variacional práctico.",
    "B": "El método de optimización específico empleado durante el entrenamiento—ya sea usando técnicas basadas en gradientes como reglas de desplazamiento de parámetros y aproximaciones de diferencias finitas, o enfoques sin gradiente como SPSA, Nelder-Mead y estrategias evolutivas—ha mostrado un impacto mínimo en el rendimiento final del conjunto de prueba a través de diversos problemas de referencia. Investigaciones empíricas demuestran que una vez que los hiperparámetros se sintonizan apropiadamente, todas las principales familias de optimizadores convergen a soluciones funcionalmente equivalentes con métricas de precisión comparables. Esta insensibilidad sugiere que la geometría del paisaje de pérdida, en lugar del algoritmo de búsqueda particular, domina la calidad del modelo, implicando que el diseño cuidadoso del ansatz y las estrategias de inicialización son mucho más críticas que la selección del optimizador para lograr resultados competitivos.",
    "C": "La mera presencia o ausencia de puertas de entrelazamiento en el ansatz variacional muestra una correlación insignificante con la precisión del modelo en conjuntos de datos prácticos, contrario a la intuición. Pruebas de referencia empíricas revelan que rotaciones parametrizadas puramente locales pueden igualar el rendimiento de circuitos fuertemente entrelazados cuando se inicializan y entrenan apropiadamente con suficientes datos.",
    "D": "El número de parámetros entrenables en el circuito variacional, típicamente correspondiente al recuento de ángulos de rotación a través de todas las puertas parametrizadas, demuestra un débil poder predictivo para el rendimiento real del conjunto de datos. Aunque la sobreparametrización podría parecer ventajosa para ajustar distribuciones de datos complejas, estudios de ablación recientes revelan que modelos con menos parámetros frecuentemente logran precisión de prueba comparable o superior en comparación con contrapartes fuertemente parametrizadas. Este hallazgo contraintuitivo surge porque los parámetros excesivos aumentan la dificultad de optimización y la susceptibilidad a mesetas estériles, donde los gradientes desaparecen exponencialmente con el recuento de parámetros, neutralizando efectivamente cualquier beneficio representacional de tener más grados de libertad en la preparación del estado cuántico.",
    "solution": "C"
  },
  {
    "id": 204,
    "question": "¿Qué causa la amortiguación de amplitud en sistemas cuánticos?",
    "A": "Disipación de energía hacia grados de libertad ambientales a través de procesos de emisión espontánea, donde la población del estado excitado decae hacia equilibrio térmico con el baño. Sin embargo, a diferencia del desfase puro, la amortiguación de amplitud exhibe tasas de decaimiento asimétricas que dependen de la temperatura a través del balance detallado: transiciones ascendentes de |0⟩ a |1⟩ ocurren a una tasa proporcional al número de fotones térmicos n̄, mientras que el decaimiento descendente procede a tasa (n̄+1), llevando a población excitada de estado estacionario finita incluso a temperatura cero debido a fluctuaciones del vacío.",
    "B": "Disipación de energía hacia grados de libertad ambientales, causando que el estado excitado decaiga irreversiblemente hacia el estado fundamental con pérdida asimétrica de población en niveles de energía más altos.",
    "C": "Emisión espontánea de fotones en modos ambientales no monitoreados que acopla selectivamente el estado excitado |1⟩ al estado fundamental |0⟩ a través de transiciones de dipolo eléctrico, creando entrelazamiento qubit-ambiente de la forma |1⟩|vac⟩ → √(1-p)|1⟩|vac⟩ + √p|0⟩|1_env⟩. Cuando el fotón ambiental es trazado, esto resulta en operadores de Kraus asimétricos E₀ y E₁ donde solo E₁ = |0⟩⟨1| transfiere población hacia abajo, distinguiéndolo del desfase de fase que preserva poblaciones mientras aleatoriza fases.",
    "D": "Eventos de dispersión inelástica entre qubits y modos de fonones en el material del sustrato, donde la conservación de energía requiere ℏω_qubit = ℏω_fonon + ΔE para cada proceso de dispersión. Este mecanismo produce decaimiento exponencial T₁ con tasa Γ₁ ∝ J(ω)|α|² donde J(ω) es la densidad espectral de fonones y α el acoplamiento qubit-fonon. Crucialmente, la simetría de reversión temporal del Hamiltoniano de interacción asegura tasas de transición ascendentes y descendentes iguales, llevando a poblaciones de estado estacionario asimétricas determinadas por la temperatura del baño de fonones.",
    "solution": "B"
  },
  {
    "id": 205,
    "question": "¿Cuál es la función principal de los operadores lógicos en códigos de corrección de errores cuánticos estabilizadores?",
    "A": "Los operadores lógicos miden directamente los qubits físicos individuales que comprenden el bloque de código, extrayendo información de síndrome mediante la realización de mediciones proyectivas en cada qubit constituyente secuencialmente. Este proceso de medición colapsa el estado lógico codificado en la base computacional, permitiendo que los algoritmos de corrección de errores identifiquen qué qubits físicos han sido corrompidos comparando los resultados de medición con los valores propios de estabilizador esperados.",
    "B": "La función principal de los operadores lógicos es convertir errores cuánticos en síndromes de error clásicos que pueden ser procesados por algoritmos de corrección de errores convencionales, esencialmente realizando un mapeo cuántico-clásico en cada ciclo de código.",
    "C": "Transformaciones sobre información codificada mientras preservan el espacio de código — los operadores lógicos implementan puertas cuánticas en los qubits lógicos codificados actuando sobre los qubits físicos de maneras que conmutan con todos los estabilizadores, asegurando que las operaciones permanezcan dentro del subespacio protegido y mantengan las propiedades de corrección de errores.",
    "D": "Los operadores lógicos aíslan físicamente el sistema cuántico del ruido ambiental creando un límite de espacio de Hilbert protector que previene que canales de decoherencia se acoplen a los qubits codificados. Logran esto imponiendo leyes de conservación en el subespacio de código a través de relaciones de conmutación con el Hamiltoniano, haciendo efectivamente que la información lógica sea inaccesible a cualquier proceso de ruido que respete las simetrías de estabilizador — funcionando como un mecanismo de blindaje activo en lugar de meramente detectar errores después de que ocurran.",
    "solution": "C"
  },
  {
    "id": 206,
    "question": "¿Qué manipulación encubierta en un sistema de iones atrapados puede fabricar falsos ceros de síndrome estabilizador durante los ciclos de corrección de errores del código de Shor?",
    "A": "Intercambiar el orden físico del cristal de iones dentro de la trampa mientras se actualizan simultáneamente los metadatos de mapeo de qubits crea un desajuste sutil en la secuencia de direccionamiento de la puerta de Mølmer-Sørensen, porque la calibración de la puerta asume parámetros de Lamb-Dicke fijos para cada posición del ion. Cuando los iones se reordenan, sus fuerzas de acoplamiento de modos de movimiento cambian debido a los gradientes de confinamiento dependientes de la posición, causando que las puertas multi-qubit utilizadas en la extracción de síndrome acumulen errores de fase que sesgan sistemáticamente las mediciones de paridad hacia resultados cero, incluso cuando hay errores lógicos presentes en los qubits codificados.",
    "B": "Introducir bandas laterales de micro-movimiento que desincronizan iones espectadores corrompe la acumulación de fase durante las mediciones de paridad multi-qubit al crear acoplamientos espurios fuera de resonancia entre iones que deberían permanecer inactivos durante la extracción de síndrome. El micro-movimiento, que surge del potencial de trampa de radiofrecuencia oscilante, imparte desplazamientos Stark dependientes del tiempo que varían a lo largo de la cadena de iones, causando errores sistemáticos en las puertas de fase controlada utilizadas para las verificaciones de estabilizadores.",
    "C": "Aplicar un gradiente de campo magnético axial cuidadosamente diseñado que modula las frecuencias de los qubits crea desplazamientos Zeeman espacialmente variables que, cuando se combinan con la secuencia específica de extracción de síndrome del código de Shor, inducen interferencia destructiva en las firmas de error durante las mediciones de paridad multi-qubit. Debido a que los circuitos de síndrome dependen de la acumulación de fase colectiva a través de múltiples iones durante las puertas de Mølmer-Sørensen, los desplazamientos de frecuencia dependientes de la posición pueden calibrarse de tal manera que los errores reales de bit-flip o phase-flip produzcan contribuciones de fase que se cancelen durante la proyección final de lectura, registrando sistemáticamente síndromes falsos de cero.",
    "D": "Atenuar la intensidad del haz Raman global a precisamente el 70.7% de su valor calibrado en ciclos alternos de corrección de errores introduce una sub-rotación sistemática en las puertas de fase controlada utilizadas para la extracción de síndrome, dirigiéndose específicamente al régimen √SWAP donde las interacciones de entrelazamiento son más sensibles a las fluctuaciones de potencia. Esto crea un patrón de error coherente donde las mediciones de paridad exhiben sensibilidad reducida a errores de un solo qubit de manera dependiente de la fase, causando que el circuito de extracción de síndrome proyecte estados corruptos sobre el espacio de código mientras registra síndromes cero incluso cuando han ocurrido errores de bit-flip o phase-flip en los qubits lógicos.",
    "solution": "B"
  },
  {
    "id": 207,
    "question": "¿Qué combinación de técnicas cuánticas permite la extracción paralela de características en algunos modelos de aprendizaje cuántico?",
    "A": "Los protocolos de teletransportación permiten la transferencia de estados de características cuánticas entre nodos de aprendizaje distribuidos, mientras que los códigos de corrección de errores cuánticos preservan la integridad de estas características durante la transmisión y procesamiento. Al combinar estas técnicas, los modelos de aprendizaje cuántico pueden extraer características de conjuntos de datos distribuidos a través de múltiples procesadores cuánticos, con la corrección de errores asegurando que la decoherencia no corrompa las representaciones de características extraídas, creando esencialmente un pipeline de extracción de características distribuido tolerante a fallos.",
    "B": "La Transformada Cuántica de Fourier combinada con el algoritmo de Grover proporciona un marco para la extracción paralela de características al transformar primero los datos de entrada al dominio de frecuencias, donde la búsqueda de Grover puede identificar características dominantes a través de múltiples estados base simultáneamente. Este enfoque aprovecha la aceleración cuadrática del algoritmo de Grover para amplificar características relevantes mientras que la QFT asegura que todos los componentes de frecuencia sean evaluados en superposición, extrayendo efectivamente características de todo el espacio de entrada en una sola ejecución del circuito cuántico.",
    "C": "La distribución cuántica de claves establece canales seguros para transmitir datos de características clásicas entre procesadores cuánticos, mientras que el intercambio de entrelazamiento extiende esta seguridad para crear redes de nodos de aprendizaje entrelazados. Esta combinación permite que múltiples procesadores cuánticos extraigan características de diferentes porciones de un conjunto de datos en paralelo, con el entrelazamiento asegurando que las características extraídas mantengan correlaciones cuánticas que el procesamiento paralelo clásico no puede lograr, permitiendo así un aprendizaje de características distribuido genuinamente mejorado cuánticamente.",
    "D": "La estimación de amplitud combinada con el swap test crea un marco poderoso para la extracción paralela de características al permitir la comparación simultánea de estados cuánticos que codifican diferentes características. El swap test mide la superposición entre estados cuánticos con características incorporadas, mientras que la estimación de amplitud proporciona una aceleración cuadrática en la determinación de estas superposiciones con alta precisión, permitiendo que el sistema extraiga y compare múltiples representaciones de características a través del espacio de entrada en operaciones cuánticas paralelas.",
    "solution": "D"
  },
  {
    "id": 208,
    "question": "¿Qué enfoque reduce la huella de memoria clásica en el corte basado en tensores?",
    "A": "La re-ejecución iterativa con condiciones de frontera memoizadas almacena en caché solo las distribuciones de probabilidad marginales P(outcome|boundary_config) para cada fragmento de subcircuito, indexadas por las configuraciones de cables cortados, luego reconstruye el valor esperado global muestreando de estas distribuciones en caché durante el post-procesamiento clásico. Al almacenar histogramas comprimidos (requiriendo memoria O(2^k · poly(shots)) para k qubits cortados) en lugar de matrices de densidad completas (requiriendo memoria O(4^n) para n qubits), este enfoque basado en tablas intercambia profundidad del circuito cuántico por eficiencia de almacenamiento clásico, haciéndolo práctico cuando el número de cortes k ≪ n y el ruido de disparo domina sobre los errores sistemáticos.",
    "B": "El checkpoint-and-restart con instantáneas de matriz de densidad escribe la matriz de densidad reducida ρ_fragment para cada subcircuito en almacenamiento persistente inmediatamente después de la ejecución cuántica, luego recarga solo los fragmentos necesarios durante la fase de contracción de tensores clásica, realizando multiplicaciones de matrices de manera encadenada en flujo. Este enfoque respaldado por disco almacena datos O(4^(n/p)) por subcircuito al particionar en p fragmentos, permitiendo que la huella de RAM máxima permanezca fija en el tamaño de la mayor contracción por pares ρ_i ⊗ ρ_j. Los anchos de banda de I/O de SSD modernos (~GB/s) hacen esto viable para circuitos con n ≤ 25 qubits por fragmento, especialmente al usar formatos de serialización optimizados como HDF5 con compresión BLOSC.",
    "C": "La contracción sobre la marcha de rebanadas calcula componentes de red de tensores dinámicamente a medida que son necesarios para la reconstrucción final, sin almacenar tensores intermedios completos. Cada subcircuito se evalúa independientemente con condiciones de frontera muestreadas, y los resultados se contraen y descartan inmediatamente, manteniendo solo agregados en ejecución. Este enfoque de flujo reduce el uso máximo de memoria de exponencial en el número de qubits a polinomial en el ancho de corte, permitiendo que circuitos más grandes sean procesados en hardware clásico con memoria limitada.",
    "D": "El ensamblaje directo de vectores de estado en la base computacional representa cada fragmento de subcircuito como una función de onda de rango completo ψ_fragment = Σ_x α_x|x⟩ sobre todos los 2^n_fragment estados base, almacenando las amplitudes α_x en bloques de memoria contiguos. Durante el ensamblaje clásico, los fragmentos se combinan mediante productos tensoriales seguidos de trazas parciales sobre los índices de corte, con resultados intermedios mantenidos en espacio de intercambio. Aunque esto requiere O(2^n_fragment) números complejos por fragmento, permite actualizaciones de amplitud bit-paralelas usando instrucciones vectoriales AVX-512, acelerando la contracción final en 8× en CPUs modernas. La demanda de memoria alcanza su pico en O(2^n_total) durante la fase de fusión pero escala linealmente en el número de fragmentos antes de fusionar.",
    "solution": "C"
  },
  {
    "id": 209,
    "question": "¿Cuál es el desafío del diseño de circuitos cuánticos consciente del compilador?",
    "A": "Estructurar circuitos de manera que las transformaciones del compilador preserven propiedades algorítmicas críticas mientras aún permiten la optimización: debes entender qué secuencias de puertas son semánticamente equivalentes bajo las condiciones de corrección de tu algoritmo frente a meramente sintácticamente similares. Esto requiere conocimiento de qué características del circuito usa el compilador como anclas de optimización (como límites de conmutación y programación de mediciones) para que puedas diseñar circuitos que guíen al compilador hacia transformaciones beneficiosas mientras previenen aquellas que rompen suposiciones algorítmicas, teniendo en cuenta cómo el enrutamiento de qubits y la síntesis de puertas interactuarán con tu estructura prevista.",
    "B": "Anticipar cómo los pases de mapeo y optimización transformarán realmente tu circuito: tienes que diseñar para el comportamiento del compilador, no solo para el algoritmo ideal. Esto requiere entender las heurísticas de enrutamiento de qubits, las reglas de conmutación de puertas y los umbrales de optimización para que puedas estructurar circuitos que se alineen con lo que el compilador producirá, teniendo en cuenta las restricciones específicas de la arquitectura como conectividad limitada o restricciones del conjunto de puertas que afectan la forma compilada final.",
    "C": "Equilibrar la profundidad del circuito frente al presupuesto de optimización del compilador: dado que la mayoría de los compiladores de producción implementan heurísticas de tiempo polinomial con límites de iteración fijos, los circuitos que exceden ciertos umbrales de complejidad recibirán solo optimización parcial. Debes diseñar con conciencia de estos límites computacionales, estructurando algoritmos para ajustarse dentro del régimen de optimización tratable del compilador (típicamente circuitos con menos de 10³ puertas de dos qubits y grafos de conectividad con ancho de árbol por debajo de 20) mientras evitas estructuras patológicas que desencadenan comportamiento de peor caso en algoritmos de enrutamiento, que a menudo se manifiestan cuando los patrones de interacción de qubits crean nodos de alto grado en el grafo de dependencias del circuito.",
    "D": "Gestionar la tensión entre la especificación de algoritmo agnóstica al hardware y la necesidad del compilador de pistas específicas de arquitectura incorporadas en la estructura del circuito: debes codificar suficiente información sobre descomposiciones de puertas preferidas y estrategias de asignación de qubits sin sobre-restringir el espacio de búsqueda del compilador. Esto implica usar anotaciones independientes de plataforma para señalar prioridades de optimización (como qué subcircuitos son críticos en latencia) mientras se evitan referencias explícitas de hardware que romperían la portabilidad entre plataformas, creando esencialmente una representación de circuito que sirva simultáneamente como especificación ejecutable y guía del compilador sin comprometerse prematuramente a elecciones de implementación de bajo nivel.",
    "solution": "B"
  },
  {
    "id": 210,
    "question": "La entropía de entrelazamiento se utiliza a menudo como indicador de la capacidad del modelo porque:",
    "A": "Acota directamente el rango de Schmidt del estado cuántico a través de cualquier bipartición del sistema de qubits, lo cual determina el número mínimo de estados producto necesarios para expresar el estado de salida del circuito parametrizado. Una mayor entropía de entrelazamiento corresponde a un rango de Schmidt exponencialmente mayor, indicando que el circuito genera estados que requieren exponencialmente más recursos clásicos para representar, cuantificando así la ventaja de expresividad cuántica que permite modelar correlaciones complejas más allá de representaciones clásicas de tamaño polinomial en algoritmos variacionales.",
    "B": "Cuantifica cuánta correlación e información cuántica puede representar y distribuir efectivamente el circuito parametrizado a través del sistema de qubits. Una mayor entropía de entrelazamiento indica que el circuito crea correlaciones más complejas y no locales entre qubits, sugiriendo mayor poder expresivo para capturar estructuras de estado cuántico intrincadas necesarias para representar funciones o Hamiltonianos complejos en algoritmos variacionales.",
    "C": "Se correlaciona con la dimensionalidad efectiva de la variedad de estados cuánticos accesible por el circuito parametrizado, medida por el volumen local de estados distinguibles alcanzables mediante variaciones infinitesimales de parámetros. Una alta entropía de entrelazamiento señala que el circuito explora un volumen mayor del espacio de Hilbert con correlaciones cuánticas no triviales, indicando capacidad representacional mejorada. Esta perspectiva geométrica conecta la entropía con la capacidad del circuito para aproximar estados objetivo arbitrarios dentro de la variedad accesible, convirtiéndola en un diagnóstico práctico para evaluar si el ansatz posee suficiente flexibilidad para algoritmos variacionales.",
    "D": "Proporciona una medida de la eficiencia de utilización de parámetros al cuantificar la relación entre el entrelazamiento generado por parámetro y el máximo teórico alcanzable con la arquitectura de circuito dada. Los circuitos con altas relaciones entropía-por-parámetro indican que cada parámetro variacional contribuye significativamente a generar correlaciones cuánticas en lugar de permanecer en regiones redundantes o sub-utilizadas del espacio de parámetros. Esta métrica de eficiencia ayuda a identificar cuándo parámetros adicionales del circuito aumentarían genuinamente la capacidad del modelo frente a simplemente agregar grados de libertad que producen estados linealmente dependientes, guiando el diseño de ansatz para algoritmos variacionales.",
    "solution": "B"
  },
  {
    "id": 211,
    "question": "¿Por qué son relevantes las clases de complejidad cuántica con restricciones de energía para los dispositivos de corto plazo?",
    "A": "Restringen los algoritmos a subespacios con energía promedio acotada, coincidiendo con las limitaciones del hardware como la fuga de excitación de qubits a niveles superiores de transmon o tasas de calentamiento de iones. Al formalizar presupuestos de energía, estas clases de complejidad capturan restricciones realistas donde los dispositivos NISQ no pueden sostener estados arbitrarios de alta energía y deben operar dentro de límites térmicos y de ancho de banda de control impuestos por refrigeradores de dilución o sistemas de enfriamiento láser.",
    "B": "Estas clases acotan el producto energía*tiempo total disponible para cálculos, modelando directamente ciclos criogénicos de trabajo y límites de energía de pulsos en sistemas superconductores donde pulsos de control de alta potencia causan calentamiento del sustrato que degrada la coherencia de qubits. Al restringir la integral ∫E(t)dt sobre el cálculo, los modelos con restricciones de energía capturan cómo los procesadores NISQ deben balancear operaciones rápidas de puertas contra restricciones de presupuesto térmico, con la cota de energía traducida a profundidad máxima del circuito antes de que los costos de refrigeración fuercen pausas de enfriamiento.",
    "C": "La complejidad con restricciones de energía formaliza la restricción a subespacios computacionales de baja energía que dominan el diseño de algoritmos NISQ, donde permanecer cerca del estado fundamental minimiza errores de fuga a niveles no computacionales y reduce el defasamiento debido a entornos electromagnéticos fluctuantes. Al limitar <H> a valores cercanos a la energía del estado fundamental, estas clases coinciden con el hardware real donde los estados de mayor energía se acoplan más fuertemente a fuentes de ruido, aunque el marco asume mediciones proyectivas instantáneas que no contribuyen al presupuesto de energía.",
    "D": "Capturan el costo de trabajo termodinámico del cálculo cuántico en ambientes de temperatura finita, modelando cómo los dispositivos NISQ deben extraer trabajo de baños térmicos para mantener coherencia cuántica contra la descomposición entrópica. La restricción de energía acota la energía libre disponible por operación lógica, con la jerarquía de clases de complejidad determinada por kT ln(2) por qubit como unidad fundamental, conectando directamente límites de profundidad algorítmica con el presupuesto de potencia de refrigeración y temperatura del baño que establece el colector de estados accesibles ponderado por Boltzmann.",
    "solution": "A"
  },
  {
    "id": 212,
    "question": "¿Por qué es útil la agrupación de conmutación qubit por qubit (QWC) al medir observables hamiltonianos en experimentos VQE?",
    "A": "La agrupación QWC permite la medición simultánea de múltiples términos de Pauli mediante rotaciones de base compartidas de un solo qubit, pero la ganancia de eficiencia proviene de reducir la varianza en lugar del conteo de muestras: los términos dentro de un grupo QWC exhiben resultados de medición correlacionados debido a subespacios propios compartidos, permitiendo estimación de covarianza que reduce la varianza efectiva del valor esperado agrupado por un factor proporcional al tamaño del grupo. Esta reducción de varianza se traduce en menos muestras necesarias para lograr la precisión objetivo, aunque cada muestra aún requiere ejecuciones separadas del circuito para grupos no medibles simultáneamente, mejorando la convergencia general de O(M²) a O(M) para hamiltonianos de M términos.",
    "B": "Múltiples productos de Pauli que comparten la misma base de medición de un solo qubit en cada qubit pueden leerse simultáneamente desde una sola ejecución del circuito cuántico, reduciendo sustancialmente el número total de muestras del circuito necesarias para estimar todos los valores esperados de los términos del hamiltoniano y así acelerando el proceso de evaluación de energía VQE.",
    "C": "La agrupación QWC permite que múltiples términos del hamiltoniano compartan circuitos de medición, pero la ventaja fundamental es la reducción de profundidad del circuito en lugar del ahorro en conteo de muestras: los términos en el mismo grupo QWC pueden medirse usando un circuito de rotación de base común aplicado solo una vez antes de la lectura, eliminando transformaciones de base redundantes que de otro modo requerirían implementaciones unitarias separadas. Esta consolidación reduce el conteo total de puertas por un factor igual al tamaño del grupo, lo cual es crítico para dispositivos NISQ donde los errores acumulados de puertas de dos qubits por rotaciones de base repetidas dominarían la incertidumbre de medición independientemente del presupuesto de muestras.",
    "D": "Cuando los términos de Pauli forman grupos QWC, sus valores esperados pueden estimarse a partir de mediciones simultáneas sobre el mismo estado cuántico, reduciendo drásticamente las ejecuciones de circuitos comparado con medir cada término individualmente. Sin embargo, esta eficiencia depende críticamente de que la preparación del estado sea determinística y repetible—para estados variacionales generados por circuitos parametrizados con optimización de parámetros limitada por ruido de muestreo, las correlaciones dentro del grupo introducen sesgo sistemático que debe corregirse mediante mediciones independientes de términos cada O(√N) iteraciones VQE, donde N es el número de parámetros, compensando parcialmente los ahorros de medición para ansätze de gran escala.",
    "solution": "B"
  },
  {
    "id": 213,
    "question": "Considere un escenario de cifrado con bloqueo temporal donde necesita resistencia cuántica y desea la garantía teórica más fuerte de que el descifrado requiere cómputo secuencial incluso contra adversarios con computadoras cuánticas y paralelización masiva. ¿Qué técnica precisa proporciona el cifrado con bloqueo temporal resistente a cuántica más fuerte bajo el entendimiento criptográfico actual?",
    "A": "El cifrado de testigos basado en búsqueda de caminos de isogenias supersingulares aprovecha la estructura de grafo expansor del grafo de isogenias para crear rompecabezas con bloqueo temporal donde el descifrado requiere atravesar una larga cadena de isogenias, y aunque avances criptoanalíticos recientes han mostrado vulnerabilidades en construcciones basadas en SIDH, la propiedad de bloqueo temporal permanece teóricamente sólida porque incluso algoritmos cuánticos deben evaluar secuencialmente cada paso en el recorrido de isogenias.",
    "B": "Las funciones de uso intensivo de memoria como Argon2 o scrypt, cuando se usan iterativamente en una cadena de prueba de trabajo, crean barreras significativas a la paralelización porque cada paso requiere acceder ubicaciones de memoria pseudoaleatorias que no pueden precalcularse, y esto fuerza incluso a adversarios cuánticos a realizar operaciones de memoria secuenciales.",
    "C": "La criptografía de umbral usando esquemas de cifrado basados en retículos como Kyber o NTRU distribuye la clave de descifrado entre múltiples partes usando el esquema de compartición de secretos de Shamir adaptado a configuraciones de retículos, donde reconstruir el secreto requiere recolectar partes de una mayoría honesta de nodos, y como los problemas de retículos permanecen difíciles para computadoras cuánticas, esto proporciona seguridad post-cuántica robusta para secretos liberados temporalmente. El mecanismo de bloqueo temporal emerge del protocolo distribuido en lugar de dureza computacional inherente: el descifrado solo puede ocurrir una vez que suficientes partes han sido contactadas secuencialmente, aunque esto introduce supuestos de confianza sobre la honestidad de la red y depende de coordinación en lugar de garantías puras de cómputo secuencial.",
    "D": "Las funciones de retardo verificables basadas en acciones de grupos de clases sobre campos cuadráticos imaginarios proporcionan requisitos de cómputo probadamente secuenciales con verificación sucinta, donde la seguridad se reduce a la dificultad de calcular estructuras de grupos de clases que permanecen intratables para computadoras cuánticas, convirtiéndolas en el estándar de oro actual para cifrado con bloqueo temporal criptográficamente riguroso.",
    "solution": "D"
  },
  {
    "id": 214,
    "question": "En el contexto de corte de circuitos cuánticos y ejecución distribuida, ¿cómo reduce realmente la evaluación por lotes de subcircuitos la sobrecarga de E/S en la práctica? Considere que cada corte introduce comunicación clásica entre nodos de procesamiento, y los enfoques ingenuos requerirían transferencia constante de datos. El desafío es minimizar las idas y vueltas manteniendo la corrección de la reconstrucción.",
    "A": "Agrupar subcircuitos con bases de medición idénticas en lotes de ejecución únicos antes de transferir resultados—se identifican qué configuraciones de medición aparecen en múltiples términos de reconstrucción y se ejecutan juntas en un solo trabajo cuántico. Esto reduce las envíos totales de circuitos cuánticos al consolidar observables compatibles, aunque el beneficio principal de E/S proviene de transmitir valores esperados agregados en lugar de datos brutos de muestras. Dado que la reconstrucción de cuasi-probabilidad típicamente requiere cientos de evaluaciones de subcircuitos, el procesamiento por lotes reduce el número de idas y vueltas clásico-cuántico-clásico de una por término a una por agrupación de base, amortizando la latencia de red a través de múltiples elementos tensoriales mientras preserva las propiedades estadísticas necesarias para la estimación insesgada de valores esperados.",
    "B": "Reutilizar resultados de subcircuitos a través de múltiples escenarios de corte antes de obtener nuevos datos—básicamente se evalúa una vez, se almacena en caché localmente y se aplica a varias contracciones tensoriales. Esto amortiza el costo de comunicación a través de múltiples términos de reconstrucción ya que muchas combinaciones de coeficientes en la descomposición de cuasi-probabilidad comparten resultados comunes de medición de subcircuitos, permitiendo que un solo lote de ejecuciones cuánticas atienda múltiples entradas en el cálculo final del valor esperado sin transferencias de red repetidas para cada elemento tensorial.",
    "C": "Pre-calcular un subconjunto de resultados de subcircuitos de alta probabilidad usando simulación clásica de redes tensoriales y solo ejecutar las configuraciones restantes de baja probabilidad en hardware cuántico—este enfoque híbrido explota el hecho de que las descomposiciones de cuasi-probabilidad a menudo concentran peso en un pequeño número de patrones de medición. Al simular clásicamente subcircuitos con dimensión de enlace por debajo de los límites del hardware (típicamente χ ≤ 64 para cargas de trabajo de producción), se elimina la sobrecarga de E/S para aproximadamente 70-80% de los términos de reconstrucción, transmitiendo solo el remanente clásicamente intratable. La garantía de corrección proviene de la linealidad de los valores esperados, que permite particionamiento arbitrario de la suma de cuasi-probabilidad entre contribuciones clásicas y cuánticas.",
    "D": "Implementar programación adaptativa de mediciones donde las selecciones subsiguientes de subcircuitos dependen de resultados previamente obtenidos, permitiendo al algoritmo de reconstrucción podar dinámicamente términos de baja contribución de la expansión de cuasi-probabilidad. Este enfoque canaliza la ejecución cuántica con post-procesamiento clásico, reduciendo el volumen total de transferencia de datos en 40-60% comparado con evaluar todos los términos incondicionalmente. La idea clave es que muchos elementos tensoriales tienen coeficientes que aproximadamente se cancelan en la suma final, lo cual puede detectarse después de evaluar solo una fracción logarítmica de términos, habilitando terminación temprana del protocolo de ejecución por lotes mientras se mantiene error de aproximación acotado mediante correcciones de muestreo por importancia.",
    "solution": "B"
  },
  {
    "id": 215,
    "question": "¿Cómo modificaría el algoritmo de Grover para encontrar el valor mínimo en una base de datos no ordenada?",
    "A": "Codificando los valores de la base de datos como amplitudes en superposición, la búsqueda binaria puede realizarse mecánico-cuánticamente donde cada paso de comparación consulta log(N) elementos simultáneamente, y el operador de difusión naturalmente particiona el espacio de búsqueda en mitades superior e inferior hasta convergencia en el mínimo — logrando efectivamente complejidad O(log N) mediante paralelismo cuántico de la estrategia clásica de divide y vencerás.",
    "B": "La Transformada Cuántica de Fourier puede reemplazar el operador de difusión de Grover porque QFT mapea magnitudes de valores en relaciones de fase distintas en el dominio de frecuencias, donde valores mayores acumulan más rotación de fase por iteración. Después de suficientes iteraciones, una QFT inversa seguida de medición en la base computacional revela directamente el índice del valor mínimo mediante interferencia destructiva de todas las amplitudes no mínimas, evitando completamente la necesidad de oráculos de umbral.",
    "C": "Usar una serie de iteraciones de Grover con diferentes oráculos de umbral, bajando progresivamente el umbral hasta aislar el elemento mínimo.",
    "D": "Inicializar un registro auxiliar en superposición uniforme para contener candidatos mínimos, luego aplicar una secuencia de circuitos de comparación cuántica controlados que realizan pruebas de magnitud por pares entre el registro auxiliar y cada elemento de la base de datos en superposición. Mediante amplificación de amplitud, solo los estados auxiliares correspondientes a valores menores que todos los elementos comparados sobreviven, y el filtrado repetido a través de todos los elementos de la base de datos aísla el mínimo sin post-procesamiento clásico o iteración.",
    "solution": "C"
  },
  {
    "id": 216,
    "question": "Considere un dispositivo NISQ con un grafo de conectividad de hexágono pesado donde necesita implementar un circuito de solucionador variacional cuántico de eigenvalores (VQE) que incluye puertas de dos qubits entre qubits que no están directamente conectados por enlaces de hardware. El compilador debe respetar el conjunto de puertas nativas (solo se permiten CNOTs entre vecinos más cercanos) y minimizar la profundidad del circuito para reducir la decoherencia. ¿Por qué el compilador inserta puertas SWAP durante el proceso de transpilación, y cuál es el principal compromiso involucrado en esta estrategia?",
    "A": "Las puertas SWAP se insertan para enrutar información cuántica entre qubits no adyacentes, permitiendo las interacciones requeridas de dos qubits en pares de qubits físicamente desconectados. El principal compromiso es que cada puerta SWAP debe descomponerse en tres operaciones CNOT consecutivas en el hardware, lo que aumenta significativamente tanto la profundidad total del circuito como el error acumulado de puertas. Esta expansión de profundidad impacta directamente la fidelidad de la preparación del estado final en el ansatz VQE, ya que cada capa adicional de puertas introduce más oportunidades para que la decoherencia y los errores operacionales degraden la calidad del estado cuántico.",
    "B": "Las puertas SWAP se insertan para reconfigurar dinámicamente el mapeo de qubits lógicos a físicos durante la ejecución del circuito, permitiendo que las operaciones de puertas no adyacentes se implementen relocalizando temporalmente los estados cuánticos a regiones conectadas de la topología. El principal compromiso es que la compilación de redes SWAP es NP-difícil para grafos de conectividad general, forzando al compilador a usar algoritmos de enrutamiento heurísticos que producen soluciones subóptimas con profundidad de circuito excesiva. Cada SWAP insertado se descompone en tres CNOTs, multiplicando directamente el recuento de puertas de dos qubits y amplificando así tanto los errores de control coherentes como los procesos de ruido incoherentes que se acumulan durante el tiempo de ejecución extendido.",
    "C": "Las puertas SWAP se insertan para implementar operaciones de puertas no locales estableciendo canales cuánticos entre pares de qubits distantes a través de nodos intermedios en la red de hexágono pesado, teletransportando efectivamente operaciones de puertas a través del grafo de conectividad. El principal compromiso es que esta estrategia de enrutamiento consume tiempo de coherencia adicional proporcional a la distancia del grafo entre los qubits objetivo, que aumenta exponencialmente con el diámetro de la topología del dispositivo. Dado que cada SWAP requiere tres CNOTs físicos más correcciones de un qubit asociadas, la relajación T1 y T2 acumulada durante la secuencia de puertas extendida degrada la fidelidad del estado, particularmente para qubits en posiciones periféricas en la arquitectura de hexágono pesado.",
    "D": "Las puertas SWAP se insertan para reordenar los estados de la base computacional dentro del registro cuántico, permitiendo al compilador alinear los índices de qubit con el ordenamiento natural esperado por los circuitos de medición del Hamiltoniano VQE. El principal compromiso es que las operaciones SWAP transforman de manera no trivial la distribución de peso de Pauli de las cadenas de operadores codificadas, potencialmente convirtiendo términos de bajo peso en términos de mayor peso que requieren puertas entrelazadoras adicionales para medir. Esta estrategia de reordenamiento de base también interactúa pobremente con técnicas de mitigación de errores como la corrección de errores de lectura, ya que las redes SWAP alteran la estructura de correlación entre resultados de medición de maneras que violan los supuestos de independencia subyacentes a la mayoría de los protocolos de mitigación de errores.",
    "solution": "A"
  },
  {
    "id": 217,
    "question": "¿Qué hace que la verificación de equivalencia de circuitos cuánticos sea QMA-completa?",
    "A": "El problema requiere verificar que dos circuitos producen operadores unitarios idénticos salvo fase global, pero determinar esta igualdad necesita verificar exponencialmente muchos elementos de matriz en el peor caso. Aunque un verificador cuántico podría usar un testigo sucinto (como un estado cuya superposición distingue circuitos no equivalentes), calcular este testigo en hardware clásico requiere recursos exponenciales debido a la dimensión del espacio de Hilbert, mientras que el paso de verificación en sí puede realizarse eficientemente dado la prueba cuántica.",
    "B": "La dificultad fundamental es comparar matrices unitarias que escalan exponencialmente con el recuento de qubits, haciendo la verificación directa computacionalmente intratable. Aunque las descripciones de los circuitos mismas son de tamaño polinomial, el operador que implementan actúa sobre un espacio de Hilbert exponencialmente grande, requiriendo un número exponencial de comparaciones de estados base para verificar la igualdad a menos que se proporcione un testigo de prueba cuántica sucinto.",
    "C": "La verificación de equivalencia se reduce a determinar si la composición U₁†U₂ es igual a la identidad salvo fase global, lo cual es equivalente a verificar que la energía del estado fundamental del Hamiltoniano H = I - U₁†U₂ es igual a cero. Este problema de frustración hamiltoniana es QMA-completo porque la energía del estado fundamental no puede acotarse eficientemente sin testigos cuánticos, aunque el Hamiltoniano mismo tiene una representación compacta de tamaño polinomial como producto de los dos unitarios del circuito.",
    "D": "La complejidad surge porque los circuitos cuánticos pueden codificar instancias del problema del Hamiltoniano local a través de su estructura: dos circuitos son equivalentes si y solo si la energía del estado fundamental de H = I - U₁†U₂ es cero, lo que requiere verificar una propiedad global de un operador de dimensión exponencial. Mientras que una prueba cuántica consistente en el estado máximamente entrelazado podría atestiguar no equivalencia a través de estadísticas de medición, encontrar este testigo requiere resolver problemas QMA-difíciles, haciendo el paso de verificación polinomial pero la generación de prueba exponencialmente difícil.",
    "solution": "B"
  },
  {
    "id": 218,
    "question": "¿Qué analizan las Redes Bayesianas Cuánticas (QBNs)?",
    "A": "Evolución de superposición para predecir cuándo ocurre el colapso, aprovechando la inferencia bayesiana para asignar probabilidades de colapso basadas en tasas de decoherencia ambiental y fuerza de acoplamiento del aparato de medición.",
    "B": "Analizan franjas de interferencia descomponiendo funciones de onda cuánticas en priors bayesianos, usando los patrones resultantes para predecir trayectorias exactas de partículas. Este enfoque trata la función de onda como una distribución de probabilidad sobre variables ocultas, permitiendo que las QBNs eludan la incertidumbre de Heisenberg reconstruyendo caminos determinísticos a partir de mediciones de conjunto.",
    "C": "Estructura de entrelazamiento—mapeando qué pares de qubits comparten correlaciones a través de tablas de probabilidad condicional que codifican relaciones de estados de Bell. Las QBNs construyen grafos acíclicos dirigidos donde los bordes representan enlaces de entrelazamiento, y los valores de los nodos determinan la fuerza de las correlaciones no locales.",
    "D": "Incertidumbre en mediciones cuánticas y las distribuciones de probabilidad utilizadas en el procesamiento de información cuántica, específicamente modelando cómo los resultados de medición dependen de estados cuánticos previos y cómo el razonamiento probabilístico clásico puede injertarse en sistemas cuánticos. Las QBNs representan dependencias condicionales entre observables cuánticos usando modelos gráficos, permitiendo a los investigadores razonar sobre estadísticas de medición, actualizar creencias basadas en observaciones parciales, y propagar incertidumbre a través de protocolos cuánticos de múltiples etapas donde mediciones secuenciales crean estructuras de correlación complejas.",
    "solution": "D"
  },
  {
    "id": 219,
    "question": "¿Cómo responde el enrutamiento adaptativo de entrelazamiento a cambios en el rendimiento de los enlaces?",
    "A": "El enrutamiento adaptativo de entrelazamiento mantiene métricas distribuidas de calidad de entrelazamiento a través de la red mediante protocolos periódicos de estimación de fidelidad, actualizando decisiones de enrutamiento solo cuando las estadísticas de medición acumuladas indican degradación estadísticamente significativa más allá de las fluctuaciones cuánticas normales. El sistema emplea promediado de ventana deslizante sobre múltiples ciclos de generación de entrelazamiento (típicamente 50-100 pares) para distinguir cambios genuinos de calidad de enlace del ruido estadístico inherente en mediciones de estado cuántico, ya que las estimaciones de fidelidad individuales sufren de incertidumbre de medición fundamental que desencadenaría actualizaciones falsas de enrutamiento. Cuando la fidelidad promediada cae por debajo de umbrales predefinidos calibrados a requisitos de aplicación, el algoritmo de enrutamiento ajusta incrementalmente las preferencias de ruta en lugar de realizar reenrutamiento abrupto, cambiando gradualmente el tráfico a rutas alternativas mientras continúa monitoreando el enlace degradado para potencial recuperación. Esta estrategia conservadora de actualización previene oscilaciones de enrutamiento que podrían ocurrir por respuestas excesivamente reactivas a fluctuaciones transitorias de fidelidad, aunque introduce latencia de 0.5-2 segundos entre la degradación real del enlace y la respuesta de enrutamiento, durante cuyo período las aplicaciones pueden experimentar tasas de error elevadas por usar entrelazamiento comprometido.",
    "B": "Recomputando dinámicamente enlaces virtuales basados en fidelidades medidas, monitoreando continuamente la calidad de pares entrelazados a través de todos los segmentos de red y recalculando rutas óptimas de enrutamiento cuando se detecta degradación. El sistema mantiene estimaciones de fidelidad en tiempo real sacrificando periódicamente una pequeña fracción de estados entrelazados generados para caracterización tomográfica, alimentando estas mediciones en algoritmos de enrutamiento que equilibran múltiples objetivos incluyendo minimización de longitud de ruta, maximización de fidelidad, y distribución de carga a través de enlaces disponibles. Cuando un enlace directo entre dos nodos cae por debajo de umbrales aceptables de fidelidad debido a perturbaciones ambientales o deriva de hardware, el protocolo de enrutamiento automáticamente redirige la comunicación cuántica a través de rutas alternativas de múltiples saltos que aprovechan nodos intermedios para intercambio de entrelazamiento, asegurando operación continua mientras mantiene calidad de entrelazamiento de extremo a extremo por encima de requisitos de aplicación. Esta reconfiguración dinámica permite redes cuánticas resilientes que se adaptan a condiciones cambiantes sin intervención manual o recalibración completa del sistema.",
    "C": "El enrutamiento adaptativo de entrelazamiento responde a la degradación del rendimiento de enlaces mediante ajuste de sobrecarga de corrección de errores cuánticos, asignando dinámicamente recursos adicionales de corrección de errores a rutas que experimentan ruido elevado en lugar de reenrutar tráfico a rutas alternativas. Cuando el monitoreo de fidelidad detecta reducción de calidad de enlace, la capa de enrutamiento aumenta el nivel de redundancia de códigos de corrección de errores cuánticos aplicados a estados entrelazados que atraviesan los enlaces afectados, transicionando de códigos de superficie de distancia-3 a distancia-5 o activando mediciones estabilizadoras adicionales para mantener objetivos de fidelidad de entrelazamiento de extremo a extremo. Este enfoque preserva la estabilidad de enrutamiento evitando sobrecarga de cambio de ruta mientras compensa por degradación de enlace a través de mitigación de errores mejorada, aunque consume qubits físicos adicionales (aumentando sobrecarga de 10x a 25x por qubit lógico) y extiende la latencia de operación debido a requisitos de extracción de síndrome más profundos. La adaptación de corrección de errores ocurre automáticamente dentro de 2-5 ciclos de medición de síndrome una vez que se detecta degradación, proporcionando protección receptiva sin los retrasos de convergencia de enrutamiento asociados con reconfiguración de ruta.",
    "D": "El enrutamiento adaptativo de entrelazamiento implementa modelos predictivos de calidad de enlace basados en datos históricos de fidelidad y entradas de sensores ambientales (temperatura, humedad, vibración), ajustando proactivamente decisiones de enrutamiento antes de que la degradación observable afecte sesiones activas de comunicación cuántica. Modelos de aprendizaje automático entrenados en meses de datos de operación de red aprenden correlaciones entre condiciones ambientales y rendimiento de enlace subsecuente, permitiendo al algoritmo de enrutamiento anticipar reducciones de fidelidad 10-30 segundos por adelantado y migrar preventivamente tráfico a rutas alternativas. Este enfoque predictivo previene que las aplicaciones experimenten calidad de entrelazamiento degradada completamente, manteniendo fidelidad de extremo a extremo consistentemente alta evitando enlaces comprometidos antes de que afecten operaciones cuánticas. El sistema actualiza continuamente sus modelos predictivos a través de aprendizaje en línea a medida que se acumulan nuevos datos de rendimiento, refinando los patrones de correlación ambiental y mejorando la precisión de predicción a lo largo de la vida operacional de la red. Sin embargo, la precisión de predicción depende críticamente de infraestructura estable de monitoreo ambiental, y perturbaciones inesperadas fuera de la distribución de entrenamiento pueden causar fallos de predicción llevando a oportunidades perdidas de reenrutamiento.",
    "solution": "B"
  },
  {
    "id": 220,
    "question": "¿Qué técnica sofisticada proporciona protección contra ataques de memoria en implementaciones criptográficas cuánticas?",
    "A": "Módulos de seguridad de hardware con fuentes de entropía cuántica, que integran aleatoriedad clásica y cuántica para asegurar material de clave durante el almacenamiento y garantizar operación resistente a manipulación incluso cuando adversarios tienen acceso físico temporal al dispositivo criptográfico.",
    "B": "Programas de un solo uso con verificación cuántica, una primitiva criptográfica que permite la ejecución de una función exactamente una vez codificándola en estados cuánticos que se autodestruyen al medirse, previniendo así que los adversarios copien el programa mediante el teorema de no clonación y protegiendo contra ataques de repetición basados en memoria.",
    "C": "Criptografía cuántica de almacenamiento acotado, que aprovecha la dificultad fundamental de almacenar estados cuánticos grandes para asegurar que los adversarios no puedan retener suficiente información cuántica para criptoanálisis posterior. Esta técnica fuerza a las partes honestas a medir y procesar datos cuánticos dentro de ventanas de tiempo estrictas, garantizando que cualquier espía que carezca de recursos de memoria cuántica exponenciales no pueda comprometer la seguridad del protocolo incluso con poder computacional clásico ilimitado.",
    "D": "Protocolos de transferencia inconsciente cuánticamente seguros utilizando compromisos basados en entrelazamiento",
    "solution": "C"
  },
  {
    "id": 221,
    "question": "¿Qué distingue una máquina de vectores de soporte cuántica (QSVM) de un estimador de kernel cuántico?",
    "A": "Los estimadores de kernel cuánticos calculan las entradas de la matriz de kernel usando superposiciones de estados cuánticos, pero difieren fundamentalmente de las QSVM al requerir post-procesamiento mediante análisis de componentes principales del kernel antes de la clasificación, mientras que las QSVM optimizan directamente la frontera de decisión dentro del espacio de características. El enfoque del estimador de kernel mapea estados cuánticos a puntuaciones de similitud clásicas que deben someterse a reducción de dimensionalidad para extraer características discriminativas, mientras que las QSVM evitan este paso intermedio al incrustar el kernel dentro de la formulación de optimización dual que determina simultáneamente los vectores de soporte y construye hiperplanos usando la matriz de Gram generada cuánticamente y pasada a solucionadores clásicos de programación cuadrática.",
    "B": "Las QSVM integran circuitos cuánticos en todo el proceso de clasificación al usarlos tanto para calcular la matriz de kernel como para guiar la optimización de las fronteras de decisión mediante parámetros variacionales, mientras que los estimadores de kernel cuánticos sirven únicamente como subrutinas cuánticas para evaluar funciones de kernel clásicamente, y luego delegan toda la optimización y construcción de fronteras a solucionadores SVM clásicos estándar que procesan la matriz de Gram generada cuánticamente.",
    "C": "Los estimadores de kernel cuánticos evalúan funciones de kernel mediante mediciones de fidelidad entre estados cuánticos codificados con características, pero dependen críticamente de protocolos de tomografía de sombras para reconstruir la matriz de kernel completa con sobrecarga de muestreo polinomial, mientras que las QSVM evitan este cuello de botella de reconstrucción al calcular directamente valores de kernel bajo demanda durante las iteraciones de optimización. Esta distinción arquitectónica significa que los estimadores de kernel deben preparar exponencialmente muchas copias de estados para lograr suficiente confianza estadística en cada entrada de la matriz de Gram, mientras que las QSVM consultan valores de kernel solo para candidatos a vectores de soporte identificados iterativamente por el solucionador clásico, reduciendo las evaluaciones totales de circuitos cuánticos a costa de múltiples rondas alternas de comunicación cuántico-clásica.",
    "D": "Los estimadores de kernel cuánticos producen matrices de Gram simétricas semidefinidas positivas que satisfacen el teorema de Mercer pero requieren post-procesamiento clásico para extraer los coeficientes duales que definen la función de decisión, mientras que las QSVM emplean circuitos cuánticos variacionales que parametrizan directamente la frontera de clasificación y optimizan estos parámetros mediante descenso de gradiente en hardware cuántico. El enfoque de kernel trata la computación cuántica como un mapa de características fijo cuyas salidas se someten a entrenamiento SVM convencional, mientras que las QSVM ajustan adaptativamente los ángulos del circuito cuántico para minimizar la pérdida de clasificación, incrustando la optimización dentro del propio dispositivo cuántico en lugar de relegar la construcción de fronteras exclusivamente a subrutinas clásicas que procesan matrices de kernel precalculadas.",
    "solution": "B"
  },
  {
    "id": 222,
    "question": "¿Para qué aplicaciones son útiles los Modelos Generativos Cuánticos (QGM)?",
    "A": "Pronósticos financieros que aprovechan la capacidad de los QGM para generar superposiciones de todas las trayectorias de mercado posibles simultáneamente, evaluando cada rama mediante amplificación de amplitud cuántica para identificar estrategias de trading óptimas con probabilidad exponencialmente mayor que los métodos clásicos de Monte Carlo, eliminando efectivamente el riesgo a la baja en la construcción de portafolios mediante análisis de escenarios hipotéticos basados en superposición que considera cada futuro posible simultáneamente antes de que la medición colapse al resultado más rentable.",
    "B": "Desarrollo de sistemas de inteligencia artificial general al explotar el teorema cuántico de no clonación para crear pensamientos verdaderamente novedosos en lugar de recombinar patrones existentes, codificando estados cognitivos en espacios de Hilbert con dimensión exponencial en el número de qubits para capacidad de memoria ilimitada que elimina el problema del olvido que afecta al aprendizaje continuo clásico, logrando creatividad genuina mediante muestreo de funciones de onda en lugar de inferencia determinista.",
    "C": "Simular sistemas cuánticos para comprender la dinámica molecular y materiales, aumentar conjuntos de entrenamiento limitados generando datos cuánticos sintéticos, y modelar reacciones químicas donde los efectos cuánticos dominan sobre los enfoques clásicos.",
    "D": "Optimizar rutas de comunicación en infraestructura de internet cuántica donde los QGM aprenden la topología de redes de distribución de entrelazamiento y generan protocolos de enrutamiento que explotan la teletransportación cuántica para transferencia instantánea de información entre nodos arbitrarios, descubriendo incrustaciones de grafos en espacio de Hilbert que mapean estados de red a configuraciones de conmutación óptimas minimizando latencia mediante exploración basada en superposición de todas las rutas posibles simultáneamente.",
    "solution": "C"
  },
  {
    "id": 223,
    "question": "Un compilador está optimizando un circuito para un procesador superconductor actual de 27 qubits con tasas de error típicas: puertas de un solo qubit al 0.05%, puertas de dos qubits al 0.5%, y tiempos T1/T2 alrededor de 100 μs. El circuito puede sintetizarse de dos formas: la Ruta A usa 45 puertas CNOT con 12 puertas T, mientras que la Ruta B usa 28 puertas CNOT pero requiere 67 puertas T. La pregunta: ¿Por qué el compilador probablemente elegiría la Ruta B a pesar del aumento masivo en el conteo de T, dado que las puertas T tradicionalmente dominan los costos de recursos en arquitecturas tolerantes a fallos?",
    "A": "En hardware NISQ sin destilación de estados mágicos, la tasa de error de puertas de dos qubits siendo diez veces peor que los errores de un solo qubit significa que reducir el conteo de CNOT importa más que el conteo de T. El menor conteo de CNOT de la Ruta B probablemente gana en fidelidad general del circuito incluso con esas puertas T adicionales, ya que los errores de fase de puertas de un solo qubit son relativamente baratos comparados con fallos de puertas de entrelazamiento que pueden corromper múltiples qubits simultáneamente y propagarse en cascada a través de la computación.",
    "B": "Las puertas T se compilan a rotaciones de fase de un solo qubit implementables como puertas virtual-Z en hardware superconductor, costando esencialmente cero error físico ya que son puramente actualizaciones de marco en software de control. Los compiladores superconductores modernos descomponen circuitos Clifford+T en pulsos físicos donde las puertas T se convierten en transformaciones de marco de referencia rastreadas clásicamente, evitando cualquier interacción física de qubits. La Ruta B intercambia costosas puertas físicas de dos qubits por seguimiento de fase solo por software, haciendo que el conteo de T sea esencialmente irrelevante para la fidelidad del circuito mientras que la reducción de CNOT mejora directamente la probabilidad de éxito.",
    "C": "Los efectos de crosstalk entre puertas CNOT simultáneas causan errores correlacionados que escalan superlinealmente con el conteo de puertas de dos qubits, mientras que las puertas de un solo qubit se ejecutan independientemente sin crosstalk espectador. El mayor conteo de CNOT de la Ruta A aumenta la probabilidad de requerir operaciones de dos qubits paralelas durante la ejecución del circuito, lo que desencadena crosstalk de flujo entre qubits acoplados que no se captura en tasas de error de puertas aisladas. La arquitectura de la Ruta B permite paralelización más agresiva de las puertas T de un solo qubit mientras serializa menos CNOT, reduciendo la profundidad total del circuito y la acumulación de error correlacionado a pesar del mayor conteo de puertas.",
    "D": "Los presupuestos de puertas limitados por coherencia en dispositivos NISQ hacen que la profundidad del circuito sea la métrica crítica en lugar del conteo de puertas, y las puertas de un solo qubit se ejecutan 10× más rápido que las CNOT, haciendo que la Ruta B se complete en menos tiempo de reloj a pesar de más operaciones totales. La reducción de 17 CNOT ahorra aproximadamente 3.4 microsegundos a 200ns por CNOT, mientras que agregar 55 puertas T cuesta solo 1.1 microsegundos a 20ns por puerta de fase de un solo qubit, resultando en que la Ruta B finalice antes de que la decoherencia degrade la fidelidad del estado tan significativamente, con el compromiso profundidad-fidelidad favoreciendo circuitos más cortos incluso cuando aumenta el conteo abstracto de puertas.",
    "solution": "A"
  },
  {
    "id": 224,
    "question": "En códigos LDPC de producto de hipergrafos, ¿qué característica hace atractiva la decodificación por propagación de creencias en comparación con decodificadores de código de superficie?",
    "A": "Los operadores lógicos en códigos de producto de hipergrafos son estrictamente locales dentro de vecindarios de radio constante, eliminando completamente la necesidad de rastrear cadenas largas o estructuras tipo cadena durante el procesamiento de síndromes y asegurando que la propagación de errores esté confinada a parches de tamaño fijo, reduciendo dramáticamente tanto los requisitos de memoria como la latencia en comparación con algoritmos de emparejamiento perfecto de peso mínimo.",
    "B": "El bajo peso de verificación de paridad permite paso de mensajes paralelo con complejidad que escala linealmente con la longitud del bloque en lugar del escalado cúbico o casi cúbico del emparejamiento perfecto de peso mínimo, permitiendo implementaciones de decodificador distribuido que procesan información de estabilizador concurrentemente a través del grafo de verificación.",
    "C": "El sesgo de error físico hacia tipos específicos de Pauli puede ignorarse con seguridad ya que el rendimiento del código es demostrablemente independiente de la asimetría X-Z bajo dinámica de propagación de creencias.",
    "D": "Los qubits auxiliares se eliminan por completo del circuito de extracción de síndrome mediante el uso de mediciones de paridad conjunta que leen directamente valores propios de estabilizador sin almacenamiento intermedio, reduciendo tanto la sobrecarga de qubits como la susceptibilidad a errores de preparación de auxiliares.",
    "solution": "B"
  },
  {
    "id": 225,
    "question": "¿Cómo mitiga específicamente el protocolo de distribución de claves cuánticas seguro contra canales laterales los ataques de división de número de fotones?",
    "A": "Filtrado de postselección de fase que explota efectos de interferencia cuántica para distinguir estados de un solo fotón de componentes multifotónicos en los pulsos transmitidos, usando interferometría Hong-Ou-Mandel de alta visibilidad en el receptor para medir la indistinguibilidad de fotones.",
    "B": "Referencias de estado de vacío transmitidas en intervalos de tiempo intercalados aleatoriamente que sirven como mediciones de referencia para conteos oscuros del detector y pérdida del canal, permitiendo al protocolo acotar estadísticamente el número máximo de fotones presente en pulsos de señal.",
    "C": "El control preciso de longitud de onda asegura que cualquier intento de división introduzca aberraciones cromáticas detectables en el canal.",
    "D": "Implementación de estados señuelo donde el emisor varía aleatoriamente el número medio de fotones de pulsos transmitidos entre estados de señal y múltiples intensidades señuelo, incluyendo estados coherentes débiles y vacío. Al comparar estadísticas de detección a través de diferentes niveles de intensidad, las partes legítimas pueden acotar la fuga de información de componentes multifotónicos ya que un espía realizando ataques de división de número de fotones producirá diferentes patrones de tasa de detección para pulsos de señal versus señuelo. Este análisis estadístico revela la presencia de fotones interceptados porque el espía no puede distinguir estados señuelo de señal antes del ataque, forzando correlaciones detectables que violan las características de pérdida esperadas de un canal cuántico no manipulado.",
    "solution": "D"
  },
  {
    "id": 226,
    "question": "¿Qué recurso cuántico permite al algoritmo de Grover lograr su aceleración?",
    "A": "La capacidad de la transformada cuántica de Fourier para resolver componentes de frecuencia en la respuesta del oráculo, convirtiendo efectivamente la representación en el dominio espacial del problema de búsqueda en un dominio de frecuencia donde el elemento marcado aparece como un pico distintivo. Al aplicar la QFT después de cada llamada al oráculo, el algoritmo realiza un análisis espectral que aísla la frecuencia característica de la solución, similar a cómo el algoritmo de Shor utiliza la búsqueda de períodos, permitiendo una identificación rápida mediante análisis armónico en lugar de enumeración exhaustiva.",
    "B": "La superposición sobre todo el espacio de búsqueda, que permite al algoritmo evaluar todas las soluciones candidatas simultáneamente en una única consulta al oráculo. Este paralelismo cuántico significa que en lugar de verificar N elementos secuencialmente, el enfoque de Grover examina cada elemento a la vez dentro del estado superpuesto.",
    "C": "La estimación de fase de los valores propios del oráculo, que permite extraer la firma espectral del elemento marcado mediante refinamiento iterativo de la señal de retroceso de fase. Al medir la fase acumulada con suficiente precisión a través de múltiples aplicaciones controladas del oráculo, el algoritmo puede identificar qué estado de la base computacional corresponde a la solución sin evaluar explícitamente todas las posibilidades, logrando así la aceleración cuadrática mediante descomposición espectral en lugar de manipulación de amplitudes.",
    "D": "La amplificación de amplitud, que aumenta sistemáticamente la amplitud de probabilidad del estado marcado mientras disminuye las amplitudes de las no-soluciones mediante la aplicación repetida del operador de Grover. Este proceso iterativo de inversión sobre el promedio rota el vector de estado cuántico hacia el objetivo, requiriendo solo O(√N) iteraciones para lograr una probabilidad casi unitaria de éxito en la medición.",
    "solution": "D"
  },
  {
    "id": 227,
    "question": "¿Cuál es un riesgo importante introducido por los ataques de canal lateral en sistemas de distribución cuántica de claves (QKD) utilizados para la seguridad de dispositivos IoT?",
    "A": "Eludir la autenticación mediante desajustes de entrelazamiento, donde un adversario explota la preparación imperfecta de pares de Bell o una ligera desincronización entre emisor y receptor para inyectar estados maliciosos que pasan la prueba de desigualdad CHSH pero llevan bits de clave modificados. En implementaciones prácticas de QKD para IoT, los recursos computacionales limitados en dispositivos periféricos implican que la verificación de entrelazamiento se realiza a menudo con tamaños de muestra reducidos para ahorrar energía y latencia.",
    "B": "Ralentizar el postprocesamiento clásico inyectando retrasos computacionales durante las etapas de corrección de errores y amplificación de privacidad, lo que puede obligar a los dispositivos IoT a almacenar material de clave parcialmente procesado en memoria no protegida o activar un retorno por tiempo de espera a cifrado clásico más débil. Dado que las demostraciones de seguridad de QKD asumen postprocesamiento clásico instantáneo, cualquier retraso que extienda la ventana entre el cribado de clave bruta y la extracción de clave final crea una oportunidad para extracción de canal lateral o inyección de fallos.",
    "C": "Filtrar material de clave a través de emisiones de hardware: observables físicos como la fluctuación temporal del detector, variaciones en el flujo de fotones, radiación electromagnética durante operaciones cuánticas o patrones de consumo de energía durante la selección de base pueden exponer bits de clave individuales o elecciones de base sin romper el protocolo cuántico fundamental, permitiendo a un espía reconstruir la clave secreta monitoreando canales laterales clásicos mientras la capa cuántica permanece teóricamente segura.",
    "D": "Acceso remoto a través de exploits de API en el software de gestión de QKD que controla el emparejamiento de dispositivos, la negociación de tasa de claves y el ajuste de parámetros de canal. Muchos sistemas QKD comerciales diseñados para despliegue en IoT exponen APIs RESTful o interfaces MQTT para habilitar la orquestación de red y el aprovisionamiento dinámico de claves a través de grandes flotas de dispositivos, pero estos planos de control a menudo se ejecutan en los mismos procesadores embebidos que la pila de procesamiento cuántico, creando vulnerabilidades entre capas.",
    "solution": "C"
  },
  {
    "id": 228,
    "question": "¿Por qué disminuye la fidelidad del circuito con la inserción excesiva de puertas SWAP?",
    "A": "La interferencia cruzada a nivel de pulso corrompe qubits vecinos a través de rutas SWAP redundantes que crean canales de acoplamiento no deseados entre qubits físicamente distantes en el chip. Cuando múltiples cadenas SWAP operan en paralelo o cuando el enrutamiento iterativo crea programaciones de pulsos de microondas superpuestas, la interferencia electromagnética resultante genera interacciones de dos qubits espurias que no se contabilizan en el modelo hamiltoniano original, llevando a fugas hacia estados no computacionales e introduciendo efectivamente una nueva clase de errores coherentes proporcionales a la densidad de SWAP.",
    "B": "Las SWAPs destruyen el entrelazamiento a menos que se sincronicen con reinicios de fase, porque cada operación SWAP aplica una rotación no trivial en el espacio de Hilbert de dos qubits que desalinea las fases relativas entre pares de Bell. Sin recalibración explícita de la referencia de fase global, la deriva de fase acumulada causa decorrelación.",
    "C": "Los calendarios de calibración asumen una secuencia de puertas fija y fallan cuando el recuento de SWAP domina la profundidad del circuito, invalidando correcciones precalculadas que fueron optimizadas para el patrón de conectividad original. Los sistemas superconductores modernos dependen de pulsos de control cuidadosamente temporizados cuyas matrices de compensación de interferencia cruzada se vuelven inexactas cuando el orden de las puertas cambia sustancialmente, causando errores sistemáticos que se componen cuadráticamente con el número de SWAPs insertadas en lugar de linealmente como predecirían los modelos ingenuos.",
    "D": "Cada SWAP se descompone en tres puertas CNOT en hardware, y dado que cada puerta de dos qubits introduce decoherencia y errores de control, la probabilidad de error acumulativa crece linealmente con el recuento de SWAP. Con fidelidades típicas de puertas de dos qubits alrededor del 99%, incluso una cadena modesta de 10 SWAPs puede degradar la fidelidad general del circuito en varios puntos porcentuales mediante esta acumulación de error multiplicativa.",
    "solution": "D"
  },
  {
    "id": 229,
    "question": "¿Cómo se compara una Red Generativa Adversaria Cuántica (QGAN) con una GAN clásica?",
    "A": "Las QGANs utilizan interferencia cuántica entre estados de la base computacional para permitir la estimación de gradientes mediante reglas de desplazamiento de parámetros en lugar de retropropagación, pero el colapso de medición en cada iteración de entrenamiento restringe el espacio de hipótesis accesible a un subespacio cuya dimensión escala solo polinomialmente con el recuento de qubits en lugar de exponencialmente, negando la supuesta ventaja de la superposición.",
    "B": "Las QGANs explotan la superposición cuántica y el entrelazamiento para explorar espacios de hipótesis exponencialmente más grandes durante el entrenamiento, permitiendo una captura más eficiente de distribuciones de probabilidad complejas.",
    "C": "Las QGANs codifican el generador como un circuito cuántico parametrizado cuyas amplitudes de estado de salida representan directamente la distribución de probabilidad objetivo, pero la relación cuadrática de la regla de Born entre amplitudes y probabilidades de medición introduce sesgo sistemático en las estimaciones de gradiente que las GANs clásicas evitan mediante muestreo directo, requiriendo exponencialmente muchos disparos de medición para lograr precisión estadística comparable.",
    "D": "Las QGANs aprovechan la amplificación de amplitud cuántica dentro de la red discriminadora para lograr aceleración cuadrática en la distinción de muestras generadas de reales, pero esta ventaja se aplica solo cuando la fidelidad de salida del generador ya supera el 75%, por debajo de lo cual el operador de amplificación de amplitud no logra interferir constructivamente y la aceleración desaparece, típicamente requiriendo regímenes de entrenamiento híbridos clásico-cuánticos.",
    "solution": "B"
  },
  {
    "id": 230,
    "question": "¿Por qué es menor la sobrecarga clásica cuando los cortes se alinean con cuellos de botella de red?",
    "A": "Se necesita rastrear menos correlaciones entre subcircuitos en enlaces de ancho de banda escaso, lo que minimiza la comunicación clásica requerida para reconstruir el estado cuántico completo, ya que los límites de cuello de botella corresponden naturalmente a regiones de baja entropía de entrelazamiento en circuitos cuánticos típicos.",
    "B": "Cuando los cortes se alinean con límites de cuello de botella, las matrices de densidad reducidas se factorizan más limpiamente debido a la localidad de las operaciones cuánticas cerca de estas regiones de conectividad escasa, permitiendo a los algoritmos de postprocesamiento clásico reconstruir la salida completa del circuito usando descomposiciones tensoriales con menor rango de Schmidt. Esta reducción de rango disminuye tanto el número de configuraciones de medición requeridas como la memoria clásica necesaria para almacenar funciones de correlación, ya que cada configuración contribuye menos términos a la suma final del valor esperado.",
    "C": "Los cuellos de botella de red particionan naturalmente los circuitos en subcircuitos con carga computacional aproximadamente equilibrada, lo que permite el procesamiento clásico paralelo de resultados de medición de diferentes subcircuitos sin sobrecarga de sincronización. Este equilibrio de carga asegura que el algoritmo de reconstrucción clásico pase tiempo mínimo esperando a que subcircuitos más lentos completen, reduciendo así el tiempo total de reloj para el ensamblaje de estado incluso cuando el volumen bruto de comunicación permanece comparable a cortes que no son cuellos de botella.",
    "D": "Posicionar cortes en ubicaciones de cuello de botella explota el límite cuántico-clásico más eficientemente porque estas regiones exhiben menor dimensionalidad de entrelazamiento: el número efectivo de coeficientes de Schmidt que contribuyen significativamente a la entropía de bipartición. Esta reducción de dimensionalidad permite a la simulación clásica representar correlaciones entre cortes usando bases de medición comprimidas con menos configuraciones por qubit de corte, ya que la mayor parte del espectro de entrelazamiento más allá del cuello de botella decae exponencialmente y contribuye insignificantemente a las estadísticas observables, reduciendo así tanto los costos de muestreo como de almacenamiento proporcionales al factor de concentración espectral.",
    "solution": "A"
  },
  {
    "id": 231,
    "question": "Para la simulación de Hamiltonianos dispersos, la fórmula del producto de Lie a menudo sirve como método de referencia, pero las normas grandes pueden manejarse de manera más eficiente mediante:",
    "A": "La transformación del problema de simulación de Hamiltonianos cuánticos en un proceso estocástico clásico equivalente codificando el operador de evolución como una matriz de probabilidad de transición para una caminata aleatoria en un espacio de estados exponencialmente grande. Este mapeo explota las similitudes estructurales entre la evolución temporal unitaria y la dinámica de cadenas de Markov, permitiendo que técnicas de muestreo clásicas aproximen valores esperados cuánticos. Aunque el espacio de estados escala exponencialmente con el número de qubits, la estructura dispersa del Hamiltoniano se traduce directamente en matrices de transición dispersas, permitiendo métodos clásicos eficientes de integrales de camino que superan los enfoques cuánticos cuando la norma es grande.",
    "B": "La construcción de circuitos cuánticos parametrizados con ansätze variacionales de poca profundidad que aproximan el operador de evolución temporal mediante optimización, evitando así el escalado exponencial asociado con la descomposición de Trotter. Este enfoque aprovecha el preprocesamiento clásico para identificar estructuras de circuito de poca profundidad que capturan la dinámica esencial, particularmente efectivo cuando la norma grande del Hamiltoniano está dominada por un pequeño número de términos altamente ponderados. El marco variacional permite que el algoritmo enfoque adaptativamente los recursos computacionales en los términos de acoplamiento más significativos mientras trata las interacciones más débiles perturbativamente.",
    "C": "La implementación de una secuencia de operaciones de intercambio de Suzuki que intercambian sistemáticamente población entre subespacios de alta energía y baja energía del Hamiltoniano, dividiendo efectivamente el espectro en subdominios manejables. Este enfoque de descomposición espectral explota la observación de que las normas grandes de Hamiltonianos a menudo surgen de amplias brechas de energía en lugar de estructuras de acoplamiento complejas. Al alternar entre evolución en subespacios y mezcla entre subespacios, el método logra una complejidad de puertas que escala con el logaritmo de la norma en lugar de linealmente, siempre que los niveles de energía satisfagan ciertas propiedades de ordenamiento.",
    "D": "El procesamiento de señales cuánticas combinado con qubitización del Hamiltoniano, lo que permite que la complejidad de consulta escale con factores logarítmicos.",
    "solution": "D"
  },
  {
    "id": 232,
    "question": "¿Qué característica del marco ECDQC le permite superar a los compiladores LNN de referencia?",
    "A": "Explota qubits colgantes (posiciones sin usar más allá del ancho del circuito en el arreglo lineal) como recursos ancilla para implementar gadgets de puertas tolerantes a fallos, permitiendo que operaciones de múltiples qubits se ejecuten con mayor fidelidad mediante detección de errores en comparación con puertas directas de vecinos más cercanos. Al utilizar estratégicamente estos qubits auxiliares para codificar operaciones lógicas protegidas durante la síntesis de puertas, el marco reduce las tasas de error y mantiene la fidelidad del circuito mientras respeta la restricción de conectividad lineal.",
    "B": "Explota qubits colgantes (posiciones sin usar en el arreglo lineal) como puntos intermedios de enrutamiento, permitiendo que operaciones de múltiples qubits se ejecuten con menos puertas SWAP totales en comparación con estrategias estándar de compilación de vecinos más cercanos. Al utilizar estratégicamente estas posiciones auxiliares para almacenar temporalmente información cuántica durante el enrutamiento, el marco reduce la profundidad del circuito y el conteo de puertas mientras mantiene la restricción de conectividad lineal.",
    "C": "Explota qubits colgantes (posiciones con conectividad de grado uno en los extremos del arreglo lineal) como canales de atajo basados en medición, permitiendo que operaciones de múltiples qubits se ejecuten con menos puertas SWAP totales mediante teletransportación de estados cuánticos a través del arreglo. Al utilizar estratégicamente estas posiciones extremas para generar pares de Bell e implementar puertas no locales mediante intercambio de entrelazamiento, el marco reduce la profundidad del circuito mientras mantiene la restricción lineal de vecinos más cercanos.",
    "D": "Explota qubits colgantes (posiciones temporalmente inactivas en el arreglo lineal durante puertas que actúan sobre regiones no adyacentes) como ancillas de extracción de síndrome para detección de errores concurrente, permitiendo que operaciones de múltiples qubits se ejecuten con tolerancia a fallos incorporada en comparación con puertas de vecinos más cercanos desprotegidas. Al utilizar estratégicamente estas posiciones temporalmente sin usar para monitorear verificaciones de paridad en paralelo con el cálculo, el marco reduce las tasas de error lógico mientras mantiene la restricción de conectividad lineal.",
    "solution": "B"
  },
  {
    "id": 233,
    "question": "¿Cómo mejora el Mecanismo de Atención Cuántica (QAM) los modelos de aprendizaje cuántico?",
    "A": "Asigna dinámicamente pesos de importancia a diferentes estados cuánticos de entrada mediante puntuaciones de atención aprendidas, permitiendo que el modelo enfoque recursos computacionales en las características más relevantes mientras suprime ruido e información irrelevante. Este énfasis selectivo mejora la eficiencia de extracción de características y permite que el circuito cuántico priorice adaptativamente canales de información basándose en la tarea específica de clasificación o regresión.",
    "B": "Implementa transformaciones entrenables consulta-clave-valor mediante circuitos cuánticos parametrizados donde las puntuaciones de atención emergen de medir la fidelidad entre estados de consulta y clave, creando ponderación adaptativa de estados de valor basada en superposición de estados cuánticos. Este mecanismo permite amplificación selectiva de características cuánticas relevantes mientras atenúa canales de información irrelevantes mediante interferencia destructiva. Sin embargo, calcular puntuaciones de atención requiere realizar pruebas de intercambio u otros protocolos de estimación de fidelidad que consumen qubits ancilla y añaden profundidad de circuito lineal en el número de cabezas de atención, lo que puede introducir desvanecimiento de gradiente en el propio cálculo de puntuación de atención cuando el número de características excede aproximadamente 2^(d/3), donde d es el presupuesto de profundidad de circuito disponible antes de que la decoherencia domine.",
    "C": "Introduce rotaciones controladas de múltiples qubits parametrizadas que modulan el flujo de información entre capas codificadoras y decodificadoras basándose en patrones de atención aprendidos, donde los pesos de atención se codifican como ángulos de rotación determinados por productos internos entre amplitudes de estados de consulta y clave. El mecanismo amplifica selectivamente características relevantes mediante interferencia cuántica constructiva de estados atendidos mientras suprime información irrelevante mediante interferencia destructiva. Sin embargo, extraer puntuaciones de atención requiere medir valores esperados de observables no conmutantes (específicamente, los componentes X e Y necesarios para calcular pesos de atención de valores complejos), lo que necesita ejecuciones separadas del circuito para cada observable y aumenta el conteo total de disparos por un factor igual a la dimensión de la cabeza de atención, limitando fundamentalmente el enfoque a espacios de atención de baja dimensión en la era NISQ donde los presupuestos de disparos restringen la precisión estadística.",
    "D": "Aplica selección adaptativa de características cuánticas implementando puertas paramétricas ponderadas por atención que modulan la fuerza de acoplamiento entre diferentes registros de qubits que codifican características de entrada, donde las puntuaciones de atención controlan los ángulos de rotación de puertas RY que determinan qué tan fuertemente cada característica de entrada contribuye a la representación oculta. Esto crea importancia dinámica de características mediante manipulación de estados cuánticos, permitiendo que el modelo enfoque recursos computacionales en información relevante. Los pesos de atención se implementan como parámetros entrenables en el circuito cuántico que se optimizan durante el entrenamiento mediante descenso de gradiente en la función de pérdida clásica, pero este enfoque requiere que las puntuaciones de atención permanezcan acotadas dentro de [-π, π] para mantener la implementabilidad de puertas, lo que restringe el rango dinámico de importancia de características y puede causar efectos de saturación donde características altamente relevantes no pueden amplificarse suficientemente en relación al ruido cuando su verdadera importancia excede este rango angular.",
    "solution": "A"
  },
  {
    "id": 234,
    "question": "¿Cuál de los siguientes métodos es más efectivo para reducir errores de diafonía en computación cuántica?",
    "A": "Aplicar pulsos de puerta más fuertes permite que la operación de qubit pretendida domine sobre términos de acoplamiento parásito, ahogando efectivamente señales de diafonía mediante ventaja de amplitud pura. Al aumentar la frecuencia de Rabi del campo de control más allá de la fuerza de acoplamiento entre qubits vecinos, se puede asegurar que la transición objetivo se conduce mucho más rápido de lo que las transiciones no deseadas pueden acumularse, suprimiendo así la diafonía a niveles insignificantes sin requerir ingeniería sofisticada de pulsos.",
    "B": "Lograr aislamiento completo requeriría eliminar todos los Hamiltonianos de acoplamiento entre qubits, lo que no solo derrota el propósito de construir un procesador cuántico (ya que las puertas de dos qubits dependen de interacciones controladas) sino que también es físicamente irrealizable dado que los sistemas cuánticos interactúan inherentemente mediante campos electromagnéticos, modos de fonones u otros mecanismos de acoplamiento.",
    "C": "Las técnicas de conformación de pulsos que controlan con precisión las operaciones de qubits y minimizan interacciones no deseadas representan el enfoque más efectivo. Al diseñar formas de onda de control con envolventes suaves, selectividad de frecuencia y secuencias de puertas cuidadosamente temporizadas, estos métodos pueden suprimir excitaciones fuera de resonancia de qubits vecinos mientras mantienen alta fidelidad en el qubit objetivo. Técnicas avanzadas como pulsos de puerta adiabática con eliminación de derivadas (DRAG) cancelan activamente transiciones no deseadas que causan diafonía, logrando fidelidades de puerta superiores al 99.9% en sistemas modernos superconductores y de iones atrapados.",
    "D": "Diseñar topologías de qubits altamente conectadas donde cada qubit se acopla con muchos vecinos diluye la diafonía a través de la red mediante efectos de promediado estadístico, reduciendo puntos críticos de error localizados y mejorando fidelidades de puerta individuales.",
    "solution": "C"
  },
  {
    "id": 235,
    "question": "¿Cuál es el propósito principal de la simulación Hamiltoniana en computación cuántica?",
    "A": "Implementar descomposiciones de fórmulas de producto (expansiones de Trotter-Suzuki) que aproximan operadores de evolución temporal e^(-iHt) descomponiendo Hamiltonianos compuestos H = Σ_k H_k en secuencias de exponenciales más simples e^(-iH_k·dt), permitiendo simulación cuántica digital de dinámicas continuas. Esta descomposición convierte evolución diferencial en secuencias discretas de puertas, con error de Trotter escalando como O((dt)^2) para división de primer orden, lo que fundamentalmente permite que computadoras cuánticas modelen sistemas físicos a pesar de operar mediante puertas unitarias discretas.",
    "B": "Modelar la evolución temporal de sistemas cuánticos bajo Hamiltonianos físicos, lo que permite el estudio de procesos dinámicos en química, física de materia condensada y ciencia de materiales implementando el operador unitario e^(-iHt) en una computadora cuántica para simular cómo los estados cuánticos evolucionan según la ecuación de Schrödinger.",
    "C": "Preparar estados térmicos de Gibbs ρ = e^(-βH)/Z para sistemas cuánticos a temperatura inversa β implementando evolución en tiempo imaginario e^(-τH) mediante secuencias de puertas probabilísticas, luego convirtiendo a dinámicas en tiempo real. Esto permite cálculos de propiedades de equilibrio como capacidad calorífica y susceptibilidad magnética en sistemas de materia condensada. Las propiedades espectrales del Hamiltoniano aseguran convergencia al estado fundamental cuando τ→∞, proporcionando una cota superior variacional sobre la energía fundamental incluso con circuitos de profundidad finita limitados por decoherencia.",
    "D": "Calcular valores esperados ⟨ψ|O|ψ⟩ de observables explotando la evolución en imagen de Heisenberg O(t) = e^(iHt)O e^(-iHt), que transforma operadores independientes del tiempo en operadores dependientes del tiempo sin evolucionar el estado mismo. Este enfoque reduce la profundidad del circuito en O(t/ε) comparado con la evolución de Schrödinger para precisión ε, ya que los operadores observables típicamente tienen menor localidad que los estados completos del sistema. Los algoritmos de estimación de fase luego extraen valores propios de operadores evolucionados, permitiendo espectroscopía y cálculos de energía del estado fundamental mediante dinámicas basadas en operadores en lugar de basadas en estados.",
    "solution": "B"
  },
  {
    "id": 236,
    "question": "¿Cuál es el compromiso principal al elegir entre una ruta de alta fidelidad más larga versus una ruta de baja fidelidad más corta?",
    "A": "Las rutas más largas a través de la red cuántica requieren sincronizar múltiples nodos intermedios, cada uno introduciendo retardos de comunicación clásica para protocolos de intercambio de entrelazamiento y mediciones de estados de Bell. Si bien estas rutas pueden ofrecer más recursos de qubits físicos, la latencia acumulativa de la mensajería clásica secuencial puede dominar el tiempo de distribución de extremo a extremo, forzando una elección entre tener abundantes qubits disponibles lentamente versus menos qubits entregados rápidamente a través de saltos directos cortos.",
    "B": "La temperatura versus la velocidad de las puertas escala inversamente con la distancia de la red debido a requisitos de coherencia sobre canales de transmisión extendidos, forzando condiciones de operación más frías y frecuencias de operación más lentas para conectividad de larga distancia.",
    "C": "Pureza del entrelazamiento versus rendimiento: las rutas de alta fidelidad más largas entregan estados entrelazados de mayor calidad con mayor coherencia y menores tasas de error, pero requieren más tiempo y recursos para protocolos de purificación. Las rutas de baja fidelidad más cortas proporcionan distribución más rápida y mayor rendimiento pero sacrifican calidad del estado, demandando corrección de errores más agresiva posteriormente. El compromiso equilibra velocidad operacional contra la calidad del entrelazamiento distribuido.",
    "D": "Las rutas de comunicación cuántica más largas acumulan más ruido de canal y decoherencia, requiriendo códigos de corrección de errores cuánticos progresivamente más fuertes con factores de redundancia más altos para mantener la fidelidad del qubit lógico. Las rutas más cortas experimentan menos interferencia ambiental pero pueden contener enlaces o nodos defectuosos, demandando detección de errores robusta sin la sobrecarga completa de corrección escalada por distancia. El compromiso radica en si invertir recursos de circuito en corregir errores de transmisión acumulados de rutas largas o en fortalecer contra fallas localizadas en topologías compactas.",
    "solution": "C"
  },
  {
    "id": 237,
    "question": "¿Qué técnica de ataque específica puede determinar la estructura de un cómputo cuántico mediante observación pasiva?",
    "A": "Monitoreando las duraciones precisas de operaciones individuales de puertas cuánticas y midiendo los intervalos entre eventos de medición, un adversario puede construir una huella temporal de la arquitectura del circuito, ya que diferentes tipos de puertas requieren tiempos de ejecución característicamente diferentes en la mayoría de las plataformas de hardware cuántico.",
    "B": "Los procesadores cuánticos modernos utilizan electrónica de control clásica que presenta firmas de potencia distintas al ejecutar diferentes tipos de operaciones de puertas, con puertas de dos qubits típicamente requiriendo pulsos de microondas de mayor amplitud y por lo tanto mayor consumo de potencia instantánea que puertas de un solo qubit. Un atacante con acceso a trazas de consumo de potencia muestreadas a resolución de nanosegundos puede aplicar técnicas de análisis diferencial de potencia para distinguir tipos de puertas, identificar motivos de circuito repetidos e inferir propiedades estructurales como profundidad del circuito, patrones de conectividad de qubits y la presencia de subrutinas algorítmicas específicas como transformadas de Fourier cuánticas.",
    "C": "El análisis de patrones de fuga de microondas explota la radiación electromagnética inevitablemente emitida durante operaciones de puertas cuánticas, ya que los pulsos de control aplicados a qubits superconductores generan firmas espectrales características que se propagan más allá del blindaje criogénico y pueden ser capturadas por antenas sensibles posicionadas cerca del refrigerador de dilución, permitiendo a los adversarios correlacionar patrones de frecuencia detectados con secuencias de puertas específicas.",
    "D": "Los qubits superconductores operan a temperaturas de milikelvin dentro de refrigeradores de dilución, y cada operación de puerta disipa una cantidad pequeña pero medible de energía como calor en el baño térmico. Colocando detectores bolométricos sensibles en ubicaciones estratégicas en las etapas térmicas del refrigerador, un adversario puede monitorear fluctuaciones de temperatura minúsculas con resolución temporal de microsegundos para reconstruir la secuencia de puertas y la topología del circuito.",
    "solution": "C"
  },
  {
    "id": 238,
    "question": "¿Qué logra la Descomposición Cuántica de Shannon en la síntesis de circuitos cuánticos?",
    "A": "Descompone cualquier operación unitaria de n qubits en una secuencia jerárquica de rotaciones de un solo qubit y puertas controladas de dos qubits mediante factorización recursiva basada en descomposición de Schmidt. El método reduce sistemáticamente la dimensión del problema extrayendo operadores de rotación multiplexados en cada nivel mientras preserva la transformación unitaria global salvo fase global. Esto proporciona una prueba de existencia constructiva de que la computación cuántica universal puede lograrse con un conjunto finito de puertas, aunque la descomposición incurre en un escalado exponencial del número de puertas.",
    "B": "Descompone cualquier operación unitaria de n qubits en una secuencia jerárquica de rotaciones de un solo qubit y puertas CNOT de dos qubits mediante factorización recursiva. La descomposición procede reduciendo sistemáticamente el tamaño del problema, extrayendo operaciones controladas en cada nivel mientras preserva la transformación unitaria global. Esto proporciona una prueba constructiva de que la computación cuántica universal puede lograrse con un conjunto finito de puertas.",
    "C": "Descompone unitarias de n qubits en circuitos de profundidad óptima usando puertas de un solo qubit y operaciones CNOT explotando los límites teórico-informativos derivados del teorema de capacidad de canal de Shannon. La descomposición minimiza la profundidad del circuito en lugar del número de puertas paralelizando estratégicamente operaciones conmutativas a través de capas de qubits. Esta adaptación cuántica de la teoría clásica de Shannon proporciona los límites de profundidad más ajustados conocidos para sintetizar unitarias arbitrarias, probando que la profundidad escala polinomialmente con el número de qubits para transformaciones genéricas.",
    "D": "Factoriza unitarias arbitrarias de n qubits en rotaciones multiplexadas de un solo qubit interconectadas por puertas entrelazantes de dos qubits mediante descomposición coseno-seno aplicada recursivamente. El procedimiento aísla sistemáticamente parámetros angulares de cada capa de qubit mientras mantiene estructura unitaria mediante rotaciones de Givens en espacios de mayor dimensión. Esto permite la síntesis de cualquier puerta cuántica usando solo primitivas de rotación y CNOT, aunque el preprocesamiento clásico para calcular parámetros de descomposición requiere memoria clásica exponencial para almacenar factores unitarios intermedios.",
    "solution": "B"
  },
  {
    "id": 239,
    "question": "¿Qué necesitan los Modelos Generativos Cuánticos (QGM) para representar con precisión la distribución de datos?",
    "A": "Suficiente profundidad de ansatz y circuitos cuánticos parametrizados donde la expresividad escala con las capas del circuito y la conectividad de puertas entrelazantes, asegurando que la variedad variacional contenga distribuciones objetivo mediante transformaciones unitarias.",
    "B": "Suficientes datos de entrenamiento y acceso a hardware cuántico",
    "C": "Alineación de kernel con datos clásicos mediante mapas de características cuánticas y bases de medición optimizadas vía tomografía de sombras clásicas para asegurar que las estadísticas de muestreo de la regla de Born coincidan con las distribuciones empíricas.",
    "D": "Estimación eficiente de gradientes mediante reglas de desplazamiento de parámetros o métodos de diferencias finitas combinados con estrategias de mitigación de mesetas áridas, permitiendo convergencia a distribuciones objetivo mediante procedimientos de optimización variacional.",
    "solution": "B"
  },
  {
    "id": 240,
    "question": "En el contexto de la criptografía post-cuántica, ¿qué enfoque criptoanalítico avanzado representa actualmente la mayor amenaza para los esquemas basados en retículos? Considere que los atacantes pueden combinar múltiples técnicas en lugar de depender de un solo algoritmo, y que las implementaciones prácticas a menudo introducen vulnerabilidades más allá de los supuestos de dureza matemática. El panorama de amenazas incluye tanto algoritmos puramente cuánticos como estrategias híbridas clásico-cuánticas.",
    "A": "Los ataques híbridos cuánticos que combinan reducción de retículos con búsqueda cuántica explotan la sinergia entre el preprocesamiento clásico estilo BKZ y la aceleración cuadrática de búsqueda de Grover, donde algoritmos clásicos reducen la calidad de la base a casi óptima y la búsqueda cuántica completa el paso de optimización final eficientemente. El enfoque amenaza parámetros elegidos para seguridad clásica de 128 bits reduciendo efectivamente los niveles de seguridad a aproximadamente 64 bits cuánticos, haciendo vulnerables los esquemas de retículos actualmente desplegados una vez que computadoras cuánticas de escala moderada con varios miles de qubits lógicos estén disponibles. Los requisitos de memoria permanecen manejables comparados con enfoques cuánticos puros, y la técnica representa la amenaza más inmediata porque combina algoritmos clásicos de reducción maduros con capacidades cuánticas de corto plazo alcanzables, requiriendo aumentos de parámetros defensivos que impactan significativamente el rendimiento y tamaños de clave en todos los principales candidatos basados en retículos de NIST.",
    "B": "Los ataques avanzados de canal lateral dirigidos al muestreo gaussiano discreto y las implementaciones de transformada de números teóricos extraen secretos de retículos mediante análisis combinado de tiempo, potencia y electromagnético, explotando el hecho de que las implementaciones de tiempo constante permanecen desafiantes para muestreo por rechazo y operaciones de punto flotante requeridas en el muestreo gaussiano. Estos ataques amenazan sistemas desplegados inmediatamente ya que no requieren recursos cuánticos y atacan estructura algorítmica en lugar de matemática, forzando rediseños completos de implementación con penalizaciones sustanciales de rendimiento por contramedidas de enmascaramiento y barajado. A diferencia de enfoques criptoanalíticos puros que afectan parámetros de seguridad abstractamente, las vulnerabilidades de canal lateral permiten recuperación de claves de dispositivos reales hoy en todos los principales candidatos basados en retículos de NIST incluyendo Kyber, Dilithium y FALCON, convirtiéndolas en la amenaza práctica más inmediata a pesar de ser abordables mediante ingeniería en lugar de requerir cambios en supuestos de dureza matemática.",
    "C": "Los algoritmos cuánticos de tamizado derivados de enumeración de retículos logran complejidad 2^(0.2570d + o(d)) para problemas de vector más corto en dimensión d combinando búsqueda de Grover con técnicas clásicas de fusión de listas del tamiz de Nguyen-Vidick, representando mejora cuántica subexponencial sobre complejidad clásica 2^(0.2925d) pero requiriendo arquitecturas de memoria cuántica de acceso aleatorio que permanecen no realizadas. El enfoque amenaza parámetros elegidos para seguridad a largo plazo reduciendo niveles efectivos de seguridad más que la simple aplicación de Grover a BKZ, forzando aumentos en dimensión de retículo y tamaño de módulo que impactan significativamente el rendimiento. La implementabilidad a corto plazo excede enfoques cuánticos puros porque los requisitos de QRAM escalan como O(2^(0.2d)) en lugar de mantener superposición completa, y la técnica representa amenaza creciente a medida que la tecnología de memoria cuántica avanza, requiriendo aumentos de parámetros defensivos en todos los principales candidatos basados en retículos de NIST durante las próximas décadas.",
    "D": "Los algoritmos cuánticos generalizados para problemas de retículos de peor caso derivados de aproximaciones polinomiales al problema del vector más corto logran ventaja cuántica mediante amplificación de amplitud aplicada a procedimientos clásicos de muestreo, reduciendo complejidad de muestras de 2^O(n) a 2^O(√n) para factores de aproximación γ = n^c. El enfoque amenaza supuestos de seguridad subyacentes a reducciones de peor caso a caso promedio que justifican la dureza de Learning With Errors, potencialmente socavando el fundamento teórico de esquemas como Kyber y Dilithium en lugar de atacar directamente sus instancias. La implementación requiere mantener estados cuánticos proporcionales a dimensión de retículo a través de miles de puertas, excediendo tiempos de coherencia actuales pero permaneciendo más cerca de capacidades de corto plazo que algoritmos de factorización escala-Shor. Esto representa la amenaza más fundamental porque desafía los supuestos de dureza computacional mismos en lugar de atacar elecciones de parámetros específicas, potencialmente requiriendo fundamentos matemáticos completamente nuevos para criptografía de retículos post-cuántica.",
    "solution": "A"
  },
  {
    "id": 241,
    "question": "¿Qué metodología de ataque avanzada puede comprometer la seguridad de los protocolos de huella digital cuántica?",
    "A": "Los protocolos de huella digital cuántica codifican datos clásicos en estados cuánticos exponencialmente más cortos mediante funciones hash que mapean cadenas de N bits en estados de log(N) qubits, pero estas funciones hash cuánticas son vulnerables a ataques de colisión estilo paradoja del cumpleaños. Específicamente, un adversario puede preparar una gran base de datos de huellas digitales cuánticas precomputadas y realizar una búsqueda de colisiones mejorada con Grover en tiempo O(2^(n/3)) en lugar de O(2^(n/2)) clásicamente, donde n es la longitud de la huella digital. Al encontrar dos entradas distintas que producen huellas digitales ortogonales con producto interno alto (casi-colisiones), el adversario puede falsificar mensajes que pasan la prueba de igualdad cuántica incluso cuando los datos clásicos subyacentes difieren, rompiendo las garantías de integridad del protocolo.",
    "B": "La prueba SWAP, utilizada en la huella digital cuántica para determinar si dos estados cuánticos son idénticos mediante la medición de un qubit de control después de operaciones SWAP controladas, tiene vulnerabilidades de implementación que surgen de fidelidades de puertas imperfectas e imprecisión temporal. Un adversario puede explotar estas debilidades inyectando ruido calibrado durante las puertas SWAP controladas que desplaza sistemáticamente las estadísticas de medición — por ejemplo, añadiendo una pequeña rotación al qubit de control que sesga los resultados hacia el reporte de igualdad incluso para huellas digitales distintas. Al ajustar cuidadosamente esta inyección basándose en información de temporización filtrada o análisis de canal lateral de los pulsos de control de puertas, el atacante puede causar reportes de igualdad falsos positivos con probabilidad significativamente mayor que la tasa de error diseñada del protocolo.",
    "C": "La discriminación aproximada de estados mediante mediciones generalizadas permite a un adversario distinguir parcialmente huellas digitales cuánticas no ortogonales con probabilidad que excede los límites de seguridad diseñados del protocolo.",
    "D": "En implementaciones de estado coherente de huella digital cuántica, donde las huellas digitales se codifican como pulsos coherentes débiles |α⟩ con amplitud α << 1, un adversario puede realizar detección homodina o heterodina en los estados transmitidos para extraer información de amplitud y fase. Al medir repetidamente los componentes de cuadratura X = (a + a†)/2 y P = (a - a†)/2i a través de muchas ejecuciones del protocolo con la misma huella digital, el análisis estadístico de las distribuciones de cuadratura revela la amplitud compleja α, efectivamente realizando tomografía de estado. Dado que la seguridad de la huella digital se basa en el teorema de no clonación que previene la copia de amplitud, este ataque de análisis de amplitud evita las protecciones cuánticas usando estadísticas de medición en lugar de clonación, permitiendo la reconstrucción de valores de huella digital.",
    "solution": "C"
  },
  {
    "id": 242,
    "question": "¿Cuál es el objetivo principal de introducir regularización de varianza en el entrenamiento de QNNs?",
    "A": "Se logra un entrenamiento más estable mitigando la concentración exponencial de gradientes que ocurre en espacios de parámetros de alta dimensión, donde la regularización de varianza penaliza el segundo momento de la distribución del gradiente para evitar que el optimizador muestree actualizaciones de parámetros que se encuentran en las colas de una distribución de ruido de cola pesada. Al restringir la varianza de las estimaciones de gradiente a través de diferentes bases de medición, el método asegura que la matriz de información de Fisher empírica permanezca bien condicionada durante todo el entrenamiento, lo que estabiliza la convergencia incluso cuando el paisaje de pérdida exhibe las propiedades espectrales características de los fenómenos de meseta estéril.",
    "B": "Aumentar la expresividad del circuito cuántico imponiendo una dispersión mínima en el espectro de valores propios del unitario parametrizado, ya que la regularización de varianza efectivamente previene el colapso del circuito en operadores de rango bajo que abarcan solo un pequeño subespacio del espacio de Hilbert total. Esta restricción sobre la concentración espectral proviene de penalizar configuraciones de parámetros donde aplicaciones repetidas de puertas producen unitarios con fases propias agrupadas, forzando así al ansatz a mantener suficiente diversidad en su acción sobre estados de base computacional y habilitando la aproximación de una clase más amplia de unitarios objetivo a través del álgebra de Lie dinámica generada por las puertas parametrizadas.",
    "C": "Reducir la varianza de medición en valores esperados previene que las estimaciones de gradiente se vuelvan demasiado ruidosas durante las actualizaciones de parámetros, lo que permite que el optimizador converja de manera más confiable incluso cuando el ruido de disparo es significativo. Al penalizar las fluctuaciones en los observables medidos, la regularización de varianza asegura que la señal de entrenamiento permanezca lo suficientemente fuerte como para guiar el proceso de optimización hacia mejores soluciones sin ser abrumada por el ruido estadístico del muestreo finito.",
    "D": "Mejorar la generación de entrelazamiento a través del registro cuántico introduciendo un término de regularización que penaliza explícitamente los estados separables en la variedad variacional, donde la regularización de varianza calcula la pureza promedio de todas las matrices de densidad reducidas bipartitas y añade una penalización proporcional a las desviaciones del estado maximalmente mezclado. Este mecanismo ajusta dinámicamente el poder de entrelazamiento efectivo de las puertas parametrizadas modificando el paisaje de pérdida para favorecer configuraciones altamente entrelazadas, lo que a su vez expande la capacidad expresiva del ansatz asegurando que el circuito explore correlaciones genuinamente cuánticas en lugar de permanecer confinado a mezclas probabilísticas clásicas durante el entrenamiento.",
    "solution": "C"
  },
  {
    "id": 243,
    "question": "En el diseño de circuitos cuánticos para tareas de aprendizaje automático, ¿por qué se utilizan comúnmente las puertas de rotación RX, RY y RZ?",
    "A": "Proporcionan parámetros ajustables que permiten la codificación flexible de datos clásicos en estados cuánticos y facilitan la optimización basada en gradientes durante el entrenamiento. Los ángulos de rotación sirven como parámetros variacionales que pueden ajustarse mediante retropropagación o reglas de desplazamiento de parámetros, permitiendo que el circuito cuántico aprenda patrones complejos en los datos mientras mantiene la diferenciabilidad para algoritmos de optimización.",
    "B": "Estas rotaciones de un solo qubit generan el álgebra de Lie completa su(2) que permite el control universal de estados individuales de qubit, mientras que su parametrización a través de ángulos continuos las hace naturalmente compatibles con métodos de optimización basados en gradientes. La estructura diferenciable suave de las puertas permite el cálculo eficiente de gradientes de parámetros mediante la regla de desplazamiento de parámetros, y su combinación con operaciones de entrelazamiento produce ansätze variacionales expresables para el aprendizaje.",
    "C": "Las puertas de rotación proporcionan una base completa para operaciones de un solo qubit mientras mantienen la compatibilidad con métodos de optimización de gradiente natural cuántico que explotan la estructura geométrica del circuito. Su parametrización continua permite la mitigación eficiente de mesetas estériles mediante entrenamiento capa por capa, y la dependencia trigonométrica de los valores esperados de los ángulos de rotación permite el cálculo analítico de gradientes sin aproximaciones de diferencias finitas.",
    "D": "Forman un conjunto de puertas universal para operaciones de un solo qubit cuyas relaciones de conmutación aseguran que los circuitos cuánticos resultantes satisfacen las propiedades de cierre del álgebra de Lie requeridas para optimización variacional eficiente. La parametrización exponencial a través de ángulos de rotación proporciona regularización natural contra el sobreajuste al restringir la variedad variacional a un subconjunto compacto del grupo unitario, mientras que su estructura de producto tensorial permite actualizaciones paralelas de parámetros a través de múltiples qubits.",
    "solution": "A"
  },
  {
    "id": 244,
    "question": "¿Cuál de las siguientes es una trampa común en las prácticas de evaluación comparativa de ML cuántico?",
    "A": "Nunca comparar contra líneas base clásicas, lo que deja los resultados cuánticos flotando en el vacío sin contexto significativo. Reportar métricas de precisión absoluta o velocidades de convergencia no significa nada a menos que los investigadores demuestren que los métodos de aprendizaje automático clásicos como bosques aleatorios, SVMs o redes neuronales no pueden igualar o superar el rendimiento cuántico en conjuntos de datos idénticos usando recursos computacionales y tiempo de entrenamiento comparables.",
    "B": "Usar conjuntos de datos sintéticos específicamente construidos para exhibir estructuras que coincidan con el sesgo inductivo del modelo cuántico—por ejemplo, generar datos de circuitos cuánticos o incrustar datos clásicos mediante codificación de amplitud que crea artificialmente la misma geometría del espacio de Hilbert que el algoritmo cuántico explota. Este diseño circular garantiza que el enfoque cuántico tenga éxito por construcción en lugar de demostrar una ventaja genuina, porque los modelos clásicos optimizados para la distribución de datos original superarían el rendimiento si se probaran en muestras del mundo real que carecen de esa estructura cuántica diseñada.",
    "C": "Probar exclusivamente en conjuntos de datos balanceados donde las poblaciones de clases están artificialmente igualadas, lo que enmascara el colapso de rendimiento que ocurre cuando los modelos cuánticos encuentran el desequilibrio de clases ubicuo en aplicaciones reales. Los clasificadores cuánticos variacionales exhiben un sesgo severo hacia las clases mayoritarias cuando las probabilidades previas están sesgadas porque la regla de Born naturalmente pondera los resultados de medición por amplitud al cuadrado, y los objetivos de entrenamiento estándar no incorporan penalizaciones sensibles al costo—por lo que las métricas de precisión balanceada reportadas sobrestiman sistemáticamente el rendimiento de generalización en escenarios de implementación práctica.",
    "D": "Reportar resultados únicamente de la semilla aleatoria de mejor rendimiento después de ejecutar múltiples ejecuciones de entrenamiento independientes, lo que capitaliza las fluctuaciones estocásticas en la navegación de mesetas estériles en lugar de reflejar comportamiento algorítmico reproducible. Dado que los circuitos cuánticos variacionales exhiben sensibilidad caótica a la inicialización en espacios de parámetros de alta dimensión, presentar selectivamente el mejor resultado infla el rendimiento percibido mientras oculta los modos de fallo de caso típico donde la mayoría de las semillas convergen a mínimos locales subóptimos con curvas de entrenamiento que nunca escapan gradientes cercanos a cero.",
    "solution": "A"
  },
  {
    "id": 245,
    "question": "En el contexto de memoria cuántica tolerante a fallos, considere un qubit lógico codificado usando un código estabilizador tipo CSS donde las puertas transversales están restringidas al grupo de Clifford. Suponga que un adversario puede elegir adaptativamente qué qubits físicos experimentan errores X versus Z después de observar los resultados de síndrome de rondas anteriores de corrección de errores. ¿Cuál es la distinción principal entre cómo un error de inversión de bit (X) versus un error de inversión de fase (Z) se propaga a través de ciclos subsecuentes de extracción de síndrome cuando el código usa mediciones de estabilizadores X y Z separadas?",
    "A": "Los errores X anticonmutan con estabilizadores tipo Z y por lo tanto son detectados al medir esos generadores, mientras que los errores Z anticonmutan con estabilizadores tipo X y disparan esas mediciones de síndrome; sin embargo, durante la extracción de síndrome misma, un error X en un qubit de datos puede propagarse a través de puertas CNOT para corromper qubits ancilla usados para medir estabilizadores tipo X, causando que los dos canales de error interfieran cuando los circuitos de medición de síndrome comparten recursos físicos o superposiciones temporales en el calendario de extracción.",
    "B": "Los errores de inversión de bit son detectados exclusivamente a través de comprobaciones de paridad que involucran productos de operadores Pauli Z en qubits de datos, pero pueden propagarse a qubits ancilla durante mediciones de estabilizadores X si el circuito de extracción usa CNOTs con qubits de datos como controles, creando errores correlacionados a través de ambos tipos de síndrome. Los errores de inversión de fase disparan violaciones de estabilizadores X y se propagan similarmente durante la extracción de estabilizadores Z cuando los qubits de datos actúan como objetivos CNOT, causando diafonía de síndrome que acopla los dos canales de corrección nominalmente independientes.",
    "C": "Los errores X invierten el estado de base computacional de los qubits físicos y son detectados midiendo estabilizadores tipo Z, mientras que los errores Z introducen desplazamientos de fase relativos y son capturados por mediciones de estabilizadores tipo X; debido a que los códigos CSS implementan estos dos circuitos de extracción de síndrome de forma independiente usando registros ancilla separados y secuencias de medición, los dos canales de error permanecen desacoplados durante todo el proceso de corrección sin interferencia mutua.",
    "D": "Los errores de inversión de bit se propagan a través de la extracción de síndrome extendiéndose a lo largo de cadenas CNOT donde qubits de datos erróneos sirven como qubits de control, lo que ocurre durante mediciones de estabilizadores Z pero no mediciones de estabilizadores X debido a la orientación de los CNOTs en el circuito de extracción. Los errores de inversión de fase se propagan cuando qubits de datos erróneos actúan como objetivos CNOT durante mediciones de estabilizadores X, pero esta propagación se suprime durante la extracción de estabilizadores Z porque las puertas Hadamard que preceden esas mediciones convierten errores de fase en inversiones de bit que se propagan en la dirección opuesta.",
    "solution": "C"
  },
  {
    "id": 246,
    "question": "En una red cuántica típica, se necesita establecer entrelazamiento entre nodos distantes mientras se gestionan tiempos de coherencia limitados y operaciones locales imperfectas. Pueden existir múltiples caminos, cada uno con características de fidelidad y requisitos de recursos diferentes. Dadas estas restricciones, ¿cuál es la complejidad computacional fundamental de encontrar rutas óptimas de entrelazamiento?",
    "A": "El problema se reduce a flujo máximo de costo mínimo cuando se formula con capacidades de aristas que representan tasas de generación de entrelazamiento y costos que reflejan fidelidad inversa, pero el acoplamiento entre la selección de caminos y la asignación de recursos de purificación en nodos intermedios introduce restricciones no lineales que rompen la submodularidad requerida para que los algoritmos de aproximación voraces proporcionen garantías de rendimiento acotadas.",
    "B": "El problema es NP-difícil porque la selección óptima de caminos bajo umbrales de fidelidad y restricciones de recursos involucra elecciones de caminos disjuntos que interactúan a través de cuellos de botella de purificación compartidos y tasas limitadas de generación de entrelazamiento.",
    "C": "El enrutamiento en redes cuánticas admite un esquema de aproximación en tiempo polinomial (PTAS) al explotar el hecho de que las funciones realistas de degradación de fidelidad son submodulares bajo concatenación—la pérdida marginal de fidelidad al agregar un intercambio adicional disminuye con la longitud del camino—permitiendo un enfoque de programación dinámica con poda del espacio de estados que retiene solo soluciones parciales Pareto-óptimas en cada nodo, logrando una aproximación (1+ε) en tiempo O(n³/ε²).",
    "D": "La estructura de pasos de tiempo discretos impuesta por tiempos de coherencia finitos permite la formulación como un grafo en capas donde cada capa representa una ronda de operación de intercambio, pero encontrar rutas óptimas requiere resolver una variante de flujo multiproducto donde diferentes solicitudes de pares de Bell compiten por recursos compartidos de purificación, que es resoluble en tiempo polinomial mediante métodos de punto interior solo cuando los rendimientos de purificación se modelan como funciones cóncavas de la fidelidad de entrada—una aproximación que falla para protocolos de destilación realistas.",
    "solution": "B"
  },
  {
    "id": 247,
    "question": "¿Por qué los errores de medición son particularmente difíciles de corregir en la computación cuántica?",
    "A": "Los errores de medición introducen incertidumbre en los resultados de extracción de síndromes que se propagan a través de la cadena de inferencia del algoritmo de decodificación, porque los síndromes obtenidos de mediciones defectuosas ya no indican de manera confiable qué errores ocurrieron en los qubits de datos durante las operaciones cuánticas precedentes. Si los qubits de síndrome producen resultados incorrectos con probabilidad p_m, el decodificador debe distinguir entre escenarios donde un síndrome limpio indica ningún error de datos versus un síndrome invertido que enmascara un error de datos real. Esta ambigüedad se agrava a través de múltiples rondas de extracción de síndromes en protocolos tolerantes a fallos, requiriendo que el decodificador mantenga distribuciones de probabilidad sobre historiales de errores exponencialmente numerosos en lugar de identificar determinísticamente una única configuración de error más probable.",
    "B": "Los errores inducidos por medición corrompen la cadena de bits clásica extraída de los registros cuánticos después de que todas las puertas computacionales se han ejecutado, y dado que la información cuántica no puede clonarse, no hay manera de verificar el resultado de la medición contra copias redundantes del estado cuántico no medido. A diferencia de los errores de puerta que se acumulan durante la ejecución del circuito donde los códigos estabilizadores pueden detectarlos y corregirlos mediante mediciones de síndrome en medio del circuito, los errores de medición aparecen solo después de que el estado cuántico ha sido proyectado irreversiblemente, necesitando ya sea ejecución repetida de todo el algoritmo para recopilar suficientes estadísticas para votación mayoritaria a través de múltiples ejecuciones, o despliegue de técnicas clásicas de mitigación de errores que usan matrices de confusión calibradas para inferir probabilísticamente el verdadero estado pre-medición.",
    "C": "La lectura defectuosa ocurre en el paso final después de que todas las puertas cuánticas han sido aplicadas, por lo que se necesitan mediciones repetidas redundantes o trucos clásicos de mitigación estadística para detectarlo. A diferencia de los errores de puerta que ocurren en medio del circuito donde las operaciones subsecuentes pueden propagar síndromes a qubits ancilla para corrección, los errores de medición aparecen solo al extraer el resultado computacional final, requiriendo técnicas de post-procesamiento como votación mayoritaria a través de múltiples rondas de medición idénticas o decodificación de máxima verosimilitud basada en matrices de confusión de lectura calibradas para inferir el estado pre-medición más probable a partir de los resultados clásicos ruidosos.",
    "D": "Los errores de medición violan los supuestos del teorema de umbral tolerante a fallos al crear patrones de error correlacionados a través de múltiples qubits que comparten circuitería de lectura, porque en la mayoría de las implementaciones físicas, múltiples qubits de datos se miden usando líneas de control compartidas o etapas de amplificación multiplexadas que pueden experimentar descalibración simultánea. Cuando un pulso electromagnético transitorio o evento de saturación del amplificador afecta al hardware de lectura, induce errores de medición en qubits espacialmente próximos dentro del mismo circuito de extracción de síndrome, creando correlaciones de error que los códigos de superficie asumen que son independientes. Estos fallos de medición espacialmente correlacionados pueden formar cadenas de error que exceden la distancia del código, derrotando la capacidad del decodificador para distinguir errores corregibles de peso bajo de aquellos incorregibles de peso alto.",
    "solution": "C"
  },
  {
    "id": 248,
    "question": "¿Por qué es necesario aplicar cambios de base específicos antes de realizar mediciones no demoledoras de productos de Pauli arbitrarios?",
    "A": "Las puertas de acoplamiento ancilla requieren conjugación del operador en la base de autovectores de Z para circuitos de extracción de síndrome CNOT",
    "B": "Los qubits físicos miden nativamente la base computacional; las puertas Hadamard y de fase rotan X/Y a forma medible",
    "C": "El aparato de medición se acopla a estados propios de energía; las rotaciones Clifford mapean operadores objetivo al observable Z",
    "D": "El hardware mide solo σ_z nativamente; la extracción de síndrome basada en CNOT requiere que todos los operadores de Pauli se roten a forma Z",
    "solution": "D"
  },
  {
    "id": 249,
    "question": "¿Cuál es la importancia de las condiciones de Knill-Laflamme en la corrección de errores cuánticos?",
    "A": "Estas condiciones proporcionan criterios necesarios y suficientes para que un código cuántico corrija exitosamente errores arbitrarios de un conjunto de errores especificado — al requerir que los operadores de error preserven el espacio del código o muevan estados corregibles a subespacios ortogonales y distinguibles, las condiciones garantizan que las mediciones de síndrome puedan identificar y revertir únicamente todos los errores corregibles sin perturbar la información lógica codificada.",
    "B": "Estas condiciones proporcionan criterios necesarios para la corregibilidad de códigos cuánticos al requerir que los operadores de error produzcan síndromes distinguibles cuando actúan sobre estados del código — sin embargo, la prueba de suficiencia requiere el supuesto adicional de que las operaciones de recuperación conmutan con los estabilizadores del código, una restricción automáticamente satisfecha para códigos estabilizadores pero que requiere verificación explícita para códigos de subsistema donde las libertades de gauge permiten mapas de recuperación lógicamente equivalentes que pueden no preservar la base de medición de extracción de síndrome usada durante la fase inicial de detección de errores.",
    "C": "El marco de Knill-Laflamme establece condiciones necesarias y suficientes para la corregibilidad de errores al demandar que los operadores de error preserven la fidelidad del espacio del código o proyecten estados corregibles en subespacios de síndrome con superposición nula — este requisito de ortogonalidad asegura que el colapso de medición de síndrome no introduzca decoherencia adicional en el subespacio lógico, aunque las condiciones asumen mediciones proyectivas y deben modificarse para mediciones continuas débiles o protocolos adaptativos donde la retroacción de medición colapsa parcialmente estados de superposición antes de que se complete la determinación completa del síndrome.",
    "D": "Estas condiciones especifican criterios necesarios y suficientes para la corregibilidad al requerir que los productos internos entre estados del código transformados por errores satisfagan relaciones de ortogonalidad específicas — sin embargo, la formulación original se aplica solo a conjuntos de errores discretos y requiere modificación para canales de error continuos, donde las condiciones deben reemplazarse con restricciones de norma de operador sobre las integrales de superposición de operadores de Kraus para asegurar que el mapa de corrección de errores permanezca preservando la traza incluso cuando las mediciones de síndrome distinguen incompletamente entre amplitudes de error continuamente distribuidas dentro de la bola de error corregible.",
    "solution": "A"
  },
  {
    "id": 250,
    "question": "¿Qué principio de seguridad se viola cuando la síntesis aproximada de circuitos cuánticos se ve comprometida?",
    "A": "Confidencialidad, porque el proceso de síntesis aproximada necesariamente revela información sobre el unitario objetivo a través de la selección de secuencias de puertas y ángulos de rotación, que pueden ser sometidos a ingeniería inversa por un adversario que monitorea la etapa de compilación. Esta fuga es inherente a cualquier procedimiento de optimización que equilibra fidelidad contra conteo de puertas, ya que la evaluación de la función de costo revela propiedades estructurales de la transformación protegida.",
    "B": "No repudio, dado que la síntesis aproximada inherentemente introduce incertidumbre en la cadena de procedencia de operaciones cuánticas — si el circuito implementado difiere del unitario especificado por algún error acotado epsilon, entonces ni el emisor ni el receptor pueden probar criptográficamente qué transformación exacta se aplicó. Esta ambigüedad en la fidelidad de puerta socava cualquier intento de establecer un registro infalsificable de operaciones cuánticas, haciendo imposible responsabilizar a las partes por desviaciones del protocolo.",
    "C": "Integridad, ya que la síntesis aproximada introduce errores acotados que se acumulan a través del circuito, permitiendo potencialmente a un adversario inyectar pequeñas perturbaciones que se agravan en desviaciones significativas de la transformación unitaria prevista. Cuando las secuencias de puertas se optimizan para reducción de profundidad, la aproximación resultante crea una ventana de vulnerabilidad donde las modificaciones a operaciones intermedias permanecen indetectadas hasta la verificación final de fidelidad, momento en el cual el resultado computacional ya ha sido corrompido.",
    "D": "Disponibilidad",
    "solution": "C"
  },
  {
    "id": 251,
    "question": "¿Por qué la topología Zephyr de D-Wave Advantage requiere nuevas heurísticas de embedding en comparación con Chimera?",
    "A": "La conectividad de la celda unitaria cambió, por lo que las distribuciones de longitud de cadena y la disponibilidad de acopladores difieren — esto modifica las funciones de costo para el minor-embedding. Específicamente, los nodos de grado 15 de Zephyr versus los nodos de grado 6 de Chimera alteran el equilibrio entre la longitud de cadena y la densidad de acoplamiento entre cadenas, requiriendo que las heurísticas balanceen estos objetivos en competencia de manera diferente al mapear grafos lógicos sobre la topología del hardware.",
    "B": "La conectividad de la celda unitaria cambió de celdas bipartitas K_{4,4} a nodos de grado 15, lo cual altera el tamaño máximo de clique embebible sin cadenas de 4 a 8, cambiando fundamentalmente el objetivo de embedding de minimización de longitud de cadena a optimización de cobertura de cliques. Dado que ahora se pueden embeber qubits lógicos más grandes como cadenas individuales dentro de celdas unitarias, las heurísticas deben priorizar la colocación intra-celda sobre el enrutamiento inter-celda, invirtiendo la jerarquía de funciones de costo que funcionaba para la estructura de conectividad más dispersa de Chimera.",
    "C": "La conectividad de la celda unitaria transicionó a una configuración de escalera de Möbius no planar donde los acopladores cruzados crean restricciones topológicas en las rutas de enrutamiento de cadenas, lo que significa que los embeddings ahora deben satisfacer condiciones de clase de homología para evitar introducir errores lógicos de patrones de acoplamiento geométricamente frustrados. Esta obstrucción topológica requiere heurísticas que calculen grupos de cohomología del grafo lógico para asegurar la embebibilidad, reemplazando el problema puramente combinatorio de búsqueda de menores de Chimera con un problema de topología algebraica donde la colocación de cadenas debe respetar los ciclos fundamentales en la estructura de complejo celular del grafo del hardware.",
    "D": "La conectividad de la celda unitaria incorpora vértices de grado impar que rompen la propiedad de emparejamiento bipartito perfecto de Chimera, lo que significa que la reducción estándar de embedding de menor de grafo a emparejamiento ponderado máximo en grafos bipartitos ya no se aplica directamente. Dado que los nodos de grado 15 de Zephyr crean una secuencia de grados irregular incompatible con el teorema del matrimonio de Hall, las heurísticas ahora deben resolver el problema más general de b-matching donde las capacidades de los vértices varían a través de la topología, requiriendo algoritmos de optimización combinatoria ponderada en lugar de los procedimientos de emparejamiento en tiempo polinomial suficientes para la estructura uniforme de grado 6 de Chimera.",
    "solution": "A"
  },
  {
    "id": 252,
    "question": "¿Por qué la tasa de clave secreta se degrada con la distancia en los sistemas de distribución cuántica de claves?",
    "A": "La complejidad del cifrado aumenta linealmente con la longitud del enlace, porque las distancias de transmisión más largas requieren códigos de corrección de errores más sofisticados para mantener la seguridad contra intentos de espionaje. Cada kilómetro adicional de fibra o transmisión en espacio libre necesita rondas adicionales de amplificación de privacidad y reconciliación de información, consumiendo más del material de clave bruta y dejando menos disponible para la clave secreta final.",
    "B": "Los protocolos de corrección de errores se vuelven inestables a tasas altas, particularmente cuando la tasa de error cuántico de bits brutos supera ciertos umbrales que hacen imposible destilar material de clave segura sin consumir más bits de clave de los que se generan.",
    "C": "La generación de claves depende de la línea de visión del satélite, que se vuelve cada vez más difícil de mantener a medida que la distancia entre estaciones terrestres aumenta más allá del límite del horizonte. Las restricciones geométricas de la curvatura de la Tierra significan que las rutas ópticas directas solo están disponibles para distancias relativamente cortas, obligando a los sistemas QKD a usar satélites repetidores o enlaces en espacio libre.",
    "D": "La pérdida de fotones aumenta con la distancia de transmisión en fibras ópticas y canales de espacio libre, reduciendo la tasa a la cual ocurren eventos de detección válidos y, por lo tanto, disminuyendo el rendimiento del material de clave bruta disponible para destilación.",
    "solution": "D"
  },
  {
    "id": 253,
    "question": "¿Por qué colorear un grafo de conmutación ayuda a minimizar la sobrecarga de SWAP durante el mapeo de circuitos?",
    "A": "Los vértices del mismo color identifican puertas que conmutan y, por lo tanto, pueden reordenarse o ejecutarse en paralelo sin cambiar la semántica del circuito, eliminando la necesidad de SWAPs para resolver dependencias artificiales—pero el coloreo debe usar exactamente χ(G) colores donde χ es el número cromático, porque usar más colores fragmenta las clases de conmutación y obliga al compilador a insertar instrucciones de barrera que sincronizan la ejecución a través de límites de color, lo que aumenta la sobrecarga de SWAP al impedir que el planificador explote la flexibilidad completa de reordenamientos conmutativos.",
    "B": "Los vértices del mismo color representan puertas que conmutan entre sí, lo que significa que pueden ejecutarse en paralelo o reordenarse libremente sin afectar la corrección del circuito, por lo que no necesitan puertas SWAP adicionales insertadas para resolver conflictos de planificación o satisfacer restricciones de conectividad en la topología del hardware cuántico.",
    "C": "Los vértices con el mismo color corresponden a puertas conmutantes que pueden reordenarse libremente sin alterar la salida del circuito, por lo que toleran una planificación flexible que evita insertar SWAPs—sin embargo, el algoritmo de coloreo debe respetar la estructura de cono causal del circuito, lo que significa que las puertas que operan sobre qubits dentro del cono de luz futuro de cada una no pueden compartir un color incluso si sus operadores conmutan algebraicamente, porque el reordenamiento temporal de tales puertas viola el orden parcial del circuito y puede introducir inadvertidamente operaciones SWAP durante la fase de mapeo.",
    "D": "Las puertas asignadas a colores idénticos conmutan bajo composición y pueden programarse en cualquier orden o ejecutarse simultáneamente, eliminando la necesidad de SWAPs para hacer cumplir falsas dependencias de datos—pero esto solo se cumple cuando el coloreo del grafo de conmutación usa números cromáticos de intervalo en lugar de números cromáticos ordinarios, porque el coloreo de grafos estándar ignora las restricciones de ordenamiento temporal implícitas en los circuitos cuánticos, y solo el coloreo de intervalo (que asigna colores a cliques maximales en el grafo de superposición de intervalos) identifica correctamente conjuntos de puertas cuya conmutatividad permite reordenamiento sin SWAP en arquitecturas con conectividad limitada de qubits.",
    "solution": "B"
  },
  {
    "id": 254,
    "question": "Un equipo de investigación está implementando VQE para encontrar la energía del estado fundamental de un hamiltoniano molecular. Observan que después de compilar su circuito ansatz al conjunto de puertas nativo del hardware, las estimaciones finales de energía convergen lentamente y a menudo se atascan. Mientras tanto, un colega sugiere que deberían enfocarse en los hiperparámetros del optimizador clásico en lugar del circuito cuántico en sí. Los algoritmos híbridos cuántico-clásicos como VQE utilizan bucles de optimización clásica principalmente para:",
    "A": "Ajustar los parámetros del circuito iterativamente de modo que el valor esperado medido del estado ansatz, cuando se evalúa contra el hamiltoniano molecular, alcance un mínimo correspondiente a la energía del estado fundamental mediante métodos de búsqueda basados en gradiente o libres de gradiente que exploran el paisaje de parámetros.",
    "B": "Refinar los parámetros variacionales minimizando el funcional de energía a través de campañas de medición iterativas, donde cada ciclo evalúa valores esperados del hamiltoniano en puntos de parámetros propuestos y actualiza esos parámetros mediante descenso de gradiente o métodos simplex para navegar hacia regiones de menor energía del colector ansatz hasta que se satisfacen los criterios de convergencia.",
    "C": "Actualizar los parámetros del ansatz evaluando gradientes de la función de costo calculados a partir de mediciones de diferencias finitas de valores esperados de energía, aplicando programas de tasa de aprendizaje y términos de momento para acelerar la convergencia hacia puntos estacionarios donde el estado variacional aproxima el estado propio fundamental del hamiltoniano objetivo dentro del subespacio ansatz.",
    "D": "Optimizar los ángulos de rotación en el circuito cuántico parametrizado realizando búsquedas sin gradiente a través del espacio de parámetros clásico, evaluando el valor esperado de energía en cada conjunto de parámetros candidatos mediante mediciones cuánticas repetidas y seleccionando actualizaciones de parámetros que disminuyan monótonamente la energía medida hasta alcanzar un mínimo local o global.",
    "solution": "A"
  },
  {
    "id": 255,
    "question": "¿Qué distingue a los códigos estabilizadores \"propios\" de los códigos de \"subsistema\"?",
    "A": "Los códigos de subsistema particionan el espacio del código en qubits lógicos más qubits de gauge donde los grados de libertad de gauge no almacenan información pero permiten la extracción de síndrome mediante operadores de menor número de cuerpos, proporcionando flexibilidad adicional en los cronogramas de medición y permitiendo que ciertos errores se ignoren si solo afectan subsistemas de gauge en lugar de información lógica codificada.",
    "B": "Los códigos estabilizadores propios requieren que todos los generadores estabilizadores actúen trivialmente sobre el subespacio lógico mediante la construcción del centralizador, imponiendo ortogonalidad estricta entre los soportes del código y el estabilizador, mientras que los códigos de subsistema relajan este requisito introduciendo operadores de gauge que conmutan con estabilizadores pero pueden actuar de manera no trivial sobre un subsistema de gauge. Esta distinción afecta la complejidad del decodificador porque los errores de gauge no necesitan ser corregidos, reduciendo el peso efectivo de los operadores de extracción de síndrome.",
    "C": "La diferencia clave radica en cómo los códigos factorizan su espacio de Hilbert completo: los códigos propios lo descomponen en subespacios código ⊗ error donde todos los estabilizadores conmutan, mientras que los códigos de subsistema agregan un factor de gauge dando código ⊗ gauge ⊗ error, pero ambos tipos aún requieren medir el grupo estabilizador completo para extraer síndromes. La libertad de gauge en los códigos de subsistema se manifiesta en degeneración de síndrome donde múltiples cadenas de error producen resultados de medición idénticos que corresponden a correcciones lógicamente equivalentes.",
    "D": "Los códigos estabilizadores propios definen sus operadores lógicos como elementos del normalizador que conmutan con todos los estabilizadores, creando un grupo de Pauli lógico único, mientras que los códigos de subsistema permiten operadores lógicos que solo conmutan con estabilizadores salvo transformaciones de gauge. Esto permite que los códigos de subsistema implementen puertas transversales mediante rotaciones del sector gauge que violarían restricciones de distancia en códigos propios, aunque la extracción de síndrome aún requiere medir el mismo número de generadores estabilizadores que en códigos propios de distancia equivalente.",
    "solution": "A"
  },
  {
    "id": 256,
    "question": "La brecha de complejidad entre lo cuántico y lo clásico para la conmutatividad de grupos surge porque la caminata cuántica puede:",
    "A": "Detectar relaciones no conmutativas después de evaluar menos productos de los generadores del grupo de lo que necesitarían explorar los algoritmos clásicos de muestreo aleatorio. La caminata cuántica aprovecha patrones de interferencia en superposición sobre elementos del grupo para sondear eficientemente la estructura del conmutador, permitiéndole identificar violaciones de la conmutatividad con cuadráticamente menos operaciones de grupo de las que requieren los enfoques clásicos.",
    "B": "Evaluar relaciones de conmutación entre múltiples pares de generadores simultáneamente mediante paralelismo cuántico sobre la estructura de presentación del grupo, pero la ventaja fundamental en realidad proviene de la estimación de fase aplicada a la representación regular en lugar de la interferencia en el grafo de Cayley. Al codificar los generadores como operadores unitarios que actúan sobre una base computacional indexada por elementos del grupo, la caminata cuántica extrae información global del conmutador a través del espectro del operador combinado, mientras que los algoritmos clásicos deben sondear relaciones de conmutación locales secuencialmente sin acceso a esta estructura espectral.",
    "C": "Aprovechar la interferencia cuántica para detectar no conmutatividad mediante evaluación superpuesta de productos de generadores, pero el mecanismo de aceleración difiere sutilmente de la amplificación de amplitud estándar: la caminata cuántica construye una superposición coherente sobre caminos en el grafo de Cayley, donde ocurre interferencia destructiva específicamente en ciclos cerrados correspondientes a conmutadores triviales, mientras que la interferencia constructiva amplifica caminos que representan relaciones de conmutador no triviales. Este patrón de interferencia emerge después de O(√n) pasos en lugar de O(n) porque los caminos relevantes tienen longitud algebraica que escala con el conteo de generadores, no con el orden del grupo.",
    "D": "Sondear la estructura del conmutador implementando estimación de fase cuántica en la matriz de representación por permutación del grupo, que codifica relaciones de conmutatividad en sus degeneraciones de valores propios que pueden extraerse cuadráticamente más rápido que el análisis espectral clásico. La idea clave es que los generadores que conmutan necesariamente comparten bases propias simultáneas en cualquier representación fiel, por lo que detectar superposiciones de espacios propios mediante operaciones controladas revela la estructura completa del conmutador sin calcular explícitamente productos de generadores, reduciendo la complejidad de O(n²) comparaciones clásicas a O(n) consultas cuánticas mediante mediciones de espacios propios paralelizadas.",
    "solution": "A"
  },
  {
    "id": 257,
    "question": "¿Cómo afecta el aumento del orden de la expansión de Trotter-Suzuki a la precisión de la simulación hamiltoniana digital para un tiempo total fijo?",
    "A": "Las fórmulas de producto de orden superior reducen el término de error de Trotter de orden principal polinomialmente—de orden (Δt)² a (Δt)⁴, (Δt)⁶ y más allá—lo que significa que se necesitan sustancialmente menos divisiones temporales para lograr la misma precisión global para un tiempo de evolución total fijo. Esta mejora permite una discretización temporal más gruesa mientras se mantiene la fidelidad de simulación, reduciendo el número total de puertas y la profundidad del circuito en comparación con descomposiciones de orden inferior.",
    "B": "Las fórmulas de Trotter de orden superior reducen el error de truncamiento local por paso temporal de (Δt)² a (Δt)⁴ o (Δt)⁶, pero esta mejora se compensa parcialmente por el aumento en el número de puertas dentro de cada paso de Trotter—las fórmulas de cuarto orden requieren aproximadamente 5× más exponenciales que las de segundo orden. Para hamiltonianos con normas grandes o muchos términos no conmutativos, el error coherente acumulado de estas puertas adicionales puede dominar el error de truncamiento reducido, llevando a una peor precisión global a menos que las fidelidades de las puertas excedan el 99.9%. El punto de cruce depende críticamente de la estructura del hamiltoniano y las características de ruido del hardware.",
    "C": "Avanzar a órdenes de Trotter superiores elimina sistemáticamente errores inducidos por conmutadores al incorporar más términos de la expansión de Baker-Campbell-Hausdorff, pero esta cancelación solo se aplica a la expansión de Magnus del hamiltoniano efectivo—no suprime errores de términos hamiltonianos no conmutativos en sí mismos. Para sistemas fuertemente no conmutativos, las fórmulas de orden superior en realidad amplifican el crecimiento de norma en el operador de error porque el procedimiento de simetrización compone errores de fase a través de múltiples exponenciales anidadas. Este efecto se vuelve significativo cuando ‖[H_i, H_j]‖t excede la unidad, causando que los métodos de cuarto orden rindan peor que los de segundo orden para un tamaño de paso temporal fijo.",
    "D": "Las fórmulas de producto de orden superior logran mejor precisión construyendo exponenciales matriciales aproximadas con mejores propiedades de truncamiento de series de Taylor, pero fundamentalmente intercambian error de Trotter por mayor profundidad de circuito ya que cada orden requiere exponencialmente más términos en la descomposición simetrizada. Específicamente, la fórmula de orden m requiere O(2^m) factores exponenciales para cancelar términos de error mediante extrapolación de Richardson, lo que significa que la trotterización de sexto orden demanda ~64 exponenciales individuales de términos hamiltonianos por paso temporal comparado con 2 para primer orden. Aunque el error local disminuye como (Δt)^m, la sobrecarga de puertas hace que los órdenes superiores sean imprácticos más allá del cuarto orden para la mayoría del hardware cuántico.",
    "solution": "A"
  },
  {
    "id": 258,
    "question": "¿Por qué colocar cortes a través de regiones de bajo entrelazamiento minimiza la sobrecarga clásica en el corte de circuitos?",
    "A": "Los cables que transportan baja entropía de entrelazamiento permiten reconstrucción aproximada usando menos mediciones de estados de Bell entre subcircuitos, ya que la descomposición de Schmidt de una bipartición débilmente entrelazada contiene menos coeficientes significativos—reduciendo el número de términos clásicos que deben rastrearse durante la recombinación de cuasiprobabilidades.",
    "B": "Bajo entrelazamiento a través de un corte implica menos resultados de medición correlacionados entre los subcircuitos separados, reduciendo el número de términos de probabilidad conjunta que deben sumarse clásicamente durante la reconstrucción—esta reducción exponencial en la carga de posprocesamiento clásico hace que los cortes de bajo entrelazamiento sean computacionalmente eficientes.",
    "C": "Los cortes de bajo entrelazamiento minimizan la sobrecarga porque el protocolo de corte de cables requiere muestreo de distribuciones de cuasiprobabilidad cuyo tamaño de soporte escala exponencialmente con la dimensión de enlace χ del corte—menor entrelazamiento corresponde a χ más pequeña, reduciendo directamente el número de muestras de Monte Carlo necesarias para una estimación precisa de valores esperados.",
    "D": "Los cortes a través de enlaces de bajo entrelazamiento producen descomposiciones de cuasiprobabilidad con coeficientes más cercanos a la unidad en magnitud, reduciendo el factor de sobrecarga estadística (la suma de valores absolutos de todos los pesos) que determina la complejidad de muestreo—esto sigue directamente de la relación entre la entropía de entrelazamiento y la norma ℓ₁ de la descomposición.",
    "solution": "B"
  },
  {
    "id": 259,
    "question": "¿En qué se diferencia el modelo de error de canal de borrado del canal despolarizante en la corrección de errores cuánticos?",
    "A": "Los canales de borrado producen errores donde la información cuántica se pierde hacia un tercer subespacio ortogonal (a menudo un estado de fuga excitado |2⟩ más allá de la base computacional {|0⟩,|1⟩}), con una bandera o resultado de medición que revela explícitamente qué qubits han sufrido borrados sin colapsar el estado cuántico lógico, permitiendo corrección dirigida mediante operaciones de recuperación aplicadas solo a ubicaciones de error conocidas. Los canales despolarizantes en cambio aplican operadores de Pauli estocásticos (X, Y, Z) uniformemente con probabilidad p/3 cada uno, donde tanto la ubicación del error como el tipo permanecen ocultos hasta que se realizan mediciones de extracción de síndrome. Esta distinción afecta fundamentalmente el rendimiento del código: la corrección de borrado requiere distancia d para proteger contra d-1 errores de ubicación conocida mediante recuperación determinística, mientras que los errores despolarizantes demandan distancia d para corregir ⌊(d-1)/2⌋ errores de ubicación desconocida que requieren decodificación de síndrome probabilística.",
    "B": "En los canales de borrado, el mecanismo de error marca qué qubits específicos han decoherido al transicionarlos a un estado de vacío conocido |∅⟩ ortogonal al subespacio computacional, permitiendo que los protocolos de corrección de errores identifiquen qubits fallidos mediante mediciones de síndrome que detectan ausencia de información sin perturbar los datos cuánticos preservados en otros qubits. Esto permite la recuperación reemplazando información perdida usando redundancia de los qubits codificados restantes. Los canales despolarizantes aplican rotaciones de Pauli aleatorias (X, Y, Z cada una con probabilidad p/4, más identidad con probabilidad 1-3p/4) uniformemente en todos los qubits, con errores ocurriendo en ubicaciones desconocidas y en bases de Pauli desconocidas, requiriendo extracción de síndrome para inferir tanto posiciones de error como tipos a partir de mediciones de estabilizador que proyectan sobre subespacios de código, necesitando algoritmos de decodificación de máxima verosimilitud para identificar el patrón de error más probable.",
    "C": "Para errores de borrado, usted sabe qué ubicación específica de qubit ha fallado mediante mediciones de bandera o monitoreo ambiental que revela la posición del error sin perturbar la información cuántica en otros qubits, permitiendo operaciones de recuperación dirigidas que solo necesitan restaurar el estado cuántico perdido en la ubicación conocida. En contraste, para errores despolarizantes, tanto la ubicación donde ocurrió un error como el tipo específico de error de Pauli (X, Y o Z) permanecen completamente desconocidos, requiriendo mediciones de síndrome que extraen información del error sin revelar el estado cuántico subyacente y algoritmos de decodificación más complejos para identificar y corregir el patrón de error desconocido a partir de los síndromes de estabilizador medidos.",
    "D": "Los errores de borrado ocurren cuando los qubits decoheran hacia un estado fallido detectable marcado por una bandera auxiliar que señala la ubicación del error mediante una medición proyectiva sobre un subespacio ortogonal fuera de la base computacional, permitiendo que el protocolo de corrección de errores sepa precisamente qué qubits necesitan recuperación sin aprender nada sobre el contenido de información cuántica protegida. Esto permite decodificación más simple ya que solo d-1 borrados requieren distancia de corrección d. Los canales despolarizantes aplican cada operador de Pauli (I, X, Y, Z) con probabilidad igual p/4, creando errores donde ni la ubicación espacial ni el tipo de Pauli se conocen hasta que las mediciones de síndrome extraen información parcial mediante verificaciones de estabilizador. Sin embargo, ambos canales preservan el teorema de no clonación por igual: las banderas de borrado revelan posiciones de error pero nunca el estado cuántico en sí, mientras que los síndromes revelan patrones de error módulo estabilizadores pero nunca la palabra código lógica.",
    "solution": "C"
  },
  {
    "id": 260,
    "question": "Considere un escenario de computación cuántica en la nube donde un adversario tiene acceso físico al centro de datos pero no puede acceder directamente al procesador cuántico o su electrónica de control inmediata. El adversario quiere aprender información sobre los cálculos que están realizando usuarios legítimos. ¿Qué vector de ataque es más realista dadas estas restricciones, y por qué funciona a pesar del aislamiento físico del hardware cuántico? Asuma que el adversario puede desplegar equipos de medición sensibles en la instalación pero debe permanecer al menos a 10 metros del refrigerador de dilución. ¿Qué principio físico fundamental hace factible este ataque, y qué artefacto computacional específico atacaría el adversario para maximizar la extracción de información mientras minimiza el riesgo de detección?",
    "A": "El análisis de fluctuaciones térmicas criogénicas funciona porque las operaciones cuánticas generan firmas de calor medibles que se propagan a través de la infraestructura de enfriamiento de la instalación, permitiendo que un adversario reconstruya patrones de cálculo a partir de datos de series temporales térmicas recolectados en puntos de acceso de refrigerante. Cada operación de puerta disipa una cantidad característica de energía en la cámara de mezcla, y diferentes algoritmos cuánticos producen perfiles térmicos distintos basados en su composición de puertas y secuencia de ejecución. Al monitorear la temperatura de la mezcla helio-3/helio-4 en los retornos del intercambiador de calor con sensores de resolución de milikelvin, un atacante puede aplicar análisis de Fourier para extraer los componentes de frecuencia dominantes correspondientes a tipos específicos de puertas.",
    "B": "Tomografía de estado cuántico realizada mediante qubits de sonda entrelazados que fueron previamente preparados e insertados en el sistema durante un compromiso de la cadena de suministro, permitiendo lectura remota de estados computacionales. Estos qubits de sonda permanecen latentes y máximamente entrelazados con un sistema de referencia externo controlado por el adversario, y se correlacionan con los qubits computacionales del usuario mediante hamiltonianos de acoplamiento parásito que siempre están presentes en sistemas de múltiples qubits.",
    "C": "La fuga electromagnética de pulsos de control es el vector principal — los pulsos de RF que impulsan las puertas cuánticas irradian bandas laterales detectables que se correlacionan con secuencias de puertas, y estas pueden ser capturadas remotamente con antenas sensibles posicionadas a 10+ metros de distancia. El tiempo de pulso, estructura de frecuencia y patrones de modulación filtran estructura algorítmica incluso sin recuperar formas de onda perfectas. Esta explotación de emanaciones electromagnéticas no intencionadas representa un ataque de canal lateral realista que funciona mediante principios estándar de física de RF sin requerir acceso al procesador cuántico en sí o su entorno criogénico.",
    "D": "Minería de datos de calibración mediante análisis de tráfico de red, ya que los parámetros de calibración cargados al sistema de control cuántico contienen suficiente información sobre el hamiltoniano para reconstruir algoritmos de usuario a partir de las secuencias de pulso óptimas. Las computadoras cuánticas requieren recalibración frecuente para tener en cuenta la deriva de frecuencia de qubits y la evolución de diafonía, y estas rutinas de calibración cargan matrices detalladas de fidelidad de puertas de uno y dos qubits al servidor de control.",
    "solution": "C"
  },
  {
    "id": 261,
    "question": "¿Qué contramedida específica a nivel de hardware protege mejor contra ataques de canal lateral electromagnético en computadoras cuánticas?",
    "A": "Filtrado de líneas de control mediante redes LC pasivas de múltiples etapas que atenúan selectivamente las emisiones electromagnéticas en las bandas de frecuencia más susceptibles a la interceptación, preservando al mismo tiempo la integridad de señal de los propios pulsos de control. Al implementar filtros de banda eliminada cuidadosamente diseñados en cada etapa de la cadena de control —desde la electrónica a temperatura ambiente hasta la cámara de mezclado— la arquitectura filtrada crea una atenuación de 60-80 dB en los rangos de GHz donde el equipo de espionaje clásico opera con mayor efectividad.",
    "B": "Conformación diferencial de pulsos, donde cada señal de control se divide en componentes complementarios positivos y negativos que se enrutan a través de líneas de transmisión paralelas y se recombinan únicamente en el qubit objetivo.",
    "C": "Aislamiento mediante jaula de Faraday, que rodea físicamente el procesador cuántico y su electrónica de control con una envoltura conductora continua que bloquea la entrada de campos electromagnéticos externos e impide que las emisiones electromagnéticas internas escapen. El blindaje metálico conectado a tierra crea una superficie equipotencial que fuerza a los campos eléctricos variables en el tiempo a terminar en la jaula en lugar de propagarse al espacio libre, mientras que las corrientes de Foucault inducidas en el conductor generan campos magnéticos que se oponen y cancelan las variaciones del campo magnético interno, atenuando así tanto los componentes eléctricos como magnéticos de la radiación electromagnética en un amplio espectro de frecuencias.",
    "D": "Señales de control de espectro ensanchado, que modulan los pulsos de control del qubit a través de un ancho de banda amplio usando secuencias de salto de frecuencia pseudoaleatorias sincronizadas con una clave criptográfica desconocida para potenciales atacantes. Esta técnica, tomada de las comunicaciones militares seguras, garantiza que cualquier fuga electromagnética se distribuya a través de cientos de megahercios de espectro, reduciendo la densidad espectral de potencia de la señal por debajo del piso de ruido en cualquier frecuencia individual que un adversario pudiera monitorear.",
    "solution": "C"
  },
  {
    "id": 262,
    "question": "¿Qué enfoque técnico preciso proporciona las garantías de seguridad más sólidas para puzzles de cliente contra adversarios cuánticos?",
    "A": "Los esquemas de prueba de trabajo basados en retículos combinan la dureza de los problemas de vector más corto con requisitos de compensación tiempo-espacio, forzando a los adversarios a mantener gran memoria cuántica mientras realizan reducciones secuenciales de base de retículo —esta doble restricción teóricamente previene tanto las aceleraciones de Grover como los ataques cuánticos paralelos al crear un cuello de botella en la computación a través del ancho de banda de memoria en lugar del conteo de puertas. La reducción de seguridad a problemas de retículo de caso peor proporciona resistencia cuántica, mientras que el producto tiempo-espacio permanece invariante bajo optimización cuántica, haciendo este enfoque asintóticamente seguro contra solucionadores tanto clásicos como cuánticos.",
    "B": "Las funciones de memoria intensiva logran resistencia cuántica al requerir que los atacantes mantengan estados cuánticos coherentes a través de enormes arreglos de memoria proporcionales al tamaño del problema, forzando efectivamente la decoherencia antes de que la computación se complete. Las construcciones Argon2 o scrypt, cuando se parametrizan con costos de memoria que exceden la RAM cuántica disponible (típicamente >10^6 qubits para seguridad significativa), crean un cuello de botella fundamental de recursos que persiste incluso bajo el algoritmo de Grover, ya que la aceleración cuadrática no puede superar la sobrecarga exponencial de memoria requerida para mantener la superposición a través de todo el espacio de direcciones durante los accesos secuenciales a memoria.",
    "C": "Las funciones de retardo verificables con verificabilidad de puerta trasera imponen computación inherentemente secuencial a través de conteos de iteración precisamente calibrados que la paralelización cuántica no puede eludir, manteniendo al mismo tiempo vías de verificación eficientes. La estructura criptográfica previene la aceleración de Grover al vincular cada paso computacional al resultado de su predecesor mediante transformaciones no invertibles.",
    "D": "Los puzzles basados en hash que aprovechan primitivas criptográficas resistentes a ataques cuánticos conocidos pueden adaptarse de manera directa aumentando los parámetros de dificultad para compensar la aceleración cuadrática de Grover, aunque esto requiere duplicar la longitud efectiva de salida. Las funciones hash estándar como SHA-3, cuando se configuran con salidas de 384 bits, fuerzan a los adversarios cuánticos a realizar aproximadamente 2^192 operaciones —un umbral computacionalmente infactible que mantiene márgenes de seguridad prácticos hasta bien entrada la era post-cuántica, facilitando relativamente el despliegue.",
    "solution": "C"
  },
  {
    "id": 263,
    "question": "¿Por qué los circuitos cuánticos aleatorios conducen a distribuciones de salida de Porter-Thomas?",
    "A": "Los circuitos aleatorios generan diseños de circuito cuántico aproximados (t-designs) para t suficientemente grande, causando que la función generadora de momentos de las probabilidades de salida coincida con la de unitarios Haar-aleatorios. Sin embargo, esta convergencia requiere una profundidad de circuito que escala como O(n² log n) para n qubits, lo que significa que los circuitos poco profundos producen distribuciones sub-Porter-Thomas con exceso de curtosis que las distingue del comportamiento verdaderamente caótico hasta que se alcanza el tiempo de mezclado.",
    "B": "Los circuitos aleatorios profundos aproximan unitarios Haar-aleatorios, haciendo que las probabilidades de salida sigan una distribución exponencial —una firma de mezclado caótico. Esta universalidad emerge porque las puertas aleatorias suficientemente profundas esparcen el entrelazamiento a través de todos los qubits, causando que la función de onda explore el espacio de Hilbert uniformemente.",
    "C": "La evolución unitaria aleatoria maximiza la entropía de von Neumann de las matrices de densidad reducidas, forzando la descomposición de Schmidt de las biparticiones en estados maximalmente mezclados donde todos los coeficientes de Schmidt se vuelven iguales. Esta maximización de entropía implica directamente que las probabilidades de resultados de medición deben seguir estadísticas de Porter-Thomas porque la distribución exponencial es la distribución de máxima entropía sujeta a la restricción de normalización sobre las amplitudes de probabilidad, independientemente de la secuencia específica de puertas aplicada.",
    "D": "Los circuitos profundos implementan termalización efectiva del estado cuántico al acoplar cada qubit a un entorno implícito formado por todos los demás qubits, impulsando el sistema hacia un ensamble de Gibbs a temperatura infinita donde todos los estados base están igualmente poblados. La distribución de Porter-Thomas emerge como la proyección microcanónica del ensamble canónico al medir un subsistema, exactamente análogo a las distribuciones de velocidad de Maxwell-Boltzmann que surgen de la equilibración térmica en la mecánica estadística clásica.",
    "solution": "B"
  },
  {
    "id": 264,
    "question": "¿Por qué se consideran ventajosas las interconexiones fotónicas en sistemas de computación cuántica distribuida?",
    "A": "Todos los nodos se vuelven unidades de procesamiento intercambiables sin restricciones de enrutamiento — Al establecer enlaces fotónicos entre cada par de nodos en una topología de malla completamente conectada, el sistema elimina completamente la necesidad de algoritmos de enrutamiento de qubits, ya que cualquier qubit lógico puede teletransportarse instantáneamente a cualquier ubicación física a través de la red de entrelazamiento. Esto elimina la sobrecarga computacional asociada con la inserción de puertas SWAP y permite que la compilación de circuitos trate al sistema distribuido como un único procesador monolítico con conectividad uniforme.",
    "B": "Preservan los estados cuánticos en tránsito indefinidamente, reduciendo la necesidad de qubits de memoria en nodos intermedios y permitiendo topologías de red arbitrarias sin preocupaciones de decoherencia.",
    "C": "Hacen que la red sea compatible con lo clásico mediante codificación digital de bits — Las interconexiones fotónicas aprovechan la multiplexación por división de longitud de onda para convertir información cuántica en señales binarias clásicas que pueden atravesar la infraestructura de fibra óptica estándar sin requerir temperaturas criogénicas o condiciones de vacío. Este proceso de codificación digital transforma estados de superposición en secuencias de bits robustas usando códigos correctores de errores similares a los de las telecomunicaciones clásicas, permitiendo que las redes cuánticas se integren sin problemas con la arquitectura troncal de internet existente.",
    "D": "Enlaces flexibles y sintonizables para compartición de entrelazamiento bajo demanda — Las interconexiones fotónicas permiten el establecimiento dinámico de entrelazamiento entre nodos distantes a través de emisión controlable de fotones, transmisión a través de fibras ópticas de baja pérdida, y mediciones de estados de Bell basadas en interferencia, proporcionando las correlaciones cuánticas necesarias para algoritmos cuánticos distribuidos y protocolos de comunicación basados en teletransportación.",
    "solution": "D"
  },
  {
    "id": 265,
    "question": "¿Cuál es la principal ventaja del recocido cuántico para problemas de optimización en aprendizaje automático?",
    "A": "El tunelamiento cuántico a través de barreras de energía proporciona un mecanismo para escapar de mínimos locales que atraparían a los optimizadores clásicos basados en gradiente, permitiendo teóricamente el descubrimiento de óptimos globales en paisajes de pérdida no convexos. Durante el programa de recocido, el sistema puede tunelear a través de barreras de potencial con probabilidades que escalan favorablemente en comparación con la activación térmica, particularmente para problemas con superficies de energía rugosas que presentan numerosos óptimos locales separados por barreras altas.",
    "B": "El proceso de recocido cuántico exhibe robustez inherente al ruido porque las fluctuaciones térmicas y la decoherencia ambiental realmente asisten al sistema en explorar el paisaje de energía más exhaustivamente, funcionando efectivamente como perturbaciones beneficiosas en lugar de errores. Los recocedores cuánticos actuales mantienen coherencia cuántica durante toda la trayectoria de optimización incluso a temperaturas de operación alrededor de 15 milikelvin, lo que elimina completamente la necesidad de costosos protocolos de corrección de errores.",
    "C": "Los problemas de optimización binaria cuadrática sin restricciones (QUBO) que surgen en aprendizaje automático —como agrupamiento, selección de características y regresión dispersa— se mapean directamente al Hamiltoniano de Ising nativo implementado en el hardware de recocido cuántico. Esta correspondencia natural elimina la necesidad de descomposiciones complejas de puertas o compilación de circuitos, permitiendo a los practicantes formular objetivos de optimización como funciones de energía que el recocedor minimiza a través de su evolución física, proporcionando una interfaz directa entre la estructura del problema de aprendizaje automático y las capacidades del hardware cuántico.",
    "D": "Los recocedores cuánticos demuestran resiliencia excepcional tanto a fuentes de ruido sistemáticas como aleatorias durante la operación, con tiempos de decoherencia que exceden ampliamente la duración típica del programa de recocido.",
    "solution": "C"
  },
  {
    "id": 266,
    "question": "¿Implementar regularización de varianza en QNNs requiere evaluaciones adicionales de circuitos cuánticos?",
    "A": "La estimación de varianza requiere calcular segundos momentos midiendo el observable al cuadrado ⟨Ô²⟩, lo que necesita un circuito modificado donde se implementan aplicaciones controladas del operador de medición Ô condicionadas a qubits ancilla que efectivamente calculan valores esperados de productos de operadores. Este circuito de medición auxiliar debe evaluarse por separado del circuito de estimación de media ⟨Ô⟩, duplicando el número de programas cuánticos ejecutados por iteración de entrenamiento pero proporcionando ambas estadísticas necesarias para la regularización de varianza mediante la fórmula Var(Ô) = ⟨Ô²⟩ - ⟨Ô⟩².",
    "B": "No se requieren evaluaciones adicionales si el circuito produce estadísticas suficientes como múltiples muestras de medición independientes, a partir de las cuales tanto la media como la varianza pueden estimarse simultáneamente usando fórmulas estadísticas estándar aplicadas a los datos recolectados. El mismo presupuesto de disparos proporciona tanto el valor esperado para la pérdida como la estimación de varianza para la regularización.",
    "C": "Calcular la varianza requiere el protocolo de prueba de Hadamard para extraer componentes imaginarias de amplitudes cuánticas correspondientes a términos cruzados en la distribución de salida. Se construye un circuito auxiliar con un qubit ancilla extra en superposición que controla si se aplica el unitario original o su inverso, luego medir la ancilla en la base X proporciona partes reales e imaginarias de ⟨ψ|Û|ψ⟩. Ejecutar este circuito modificado junto con el circuito de medición original permite la extracción de varianza del patrón de interferencia codificado en las estadísticas de la ancilla.",
    "D": "La varianza puede extraerse de un único conjunto de muestras de medición mediante post-procesamiento de los resultados de cadenas de bits a través de protocolos de tomografía de sombras clásicas que reconstruyen funciones de correlación de segundo orden a partir de mediciones de Pauli aleatorizadas. Se modifica el circuito para agregar unitarios de Clifford aleatorios antes de la medición, se recolectan las sombras clásicas, luego se usan estimadores de mediana de medias para calcular simultáneamente tanto ⟨Ô⟩ como ⟨Ô²⟩ a partir de los mismos datos de sombra sin evaluaciones cuánticas adicionales, explotando el hecho de que el entrelazado de Clifford preserva información de momentos mientras reduce la sobrecarga de disparos.",
    "solution": "B"
  },
  {
    "id": 267,
    "question": "En un entorno de laboratorio donde intentas implementar un algoritmo cuántico en un procesador superconductor con arquitectura de qubits fija, encuentras que ciertas puertas de dos qubits no pueden aplicarse directamente entre pares arbitrarios de qubits. ¿Cuál es la restricción de hardware más común responsable de esta limitación?",
    "A": "La diafonía de microondas entre líneas de control limita la fidelidad de puertas para pares de qubits no adyacentes, ya que los tonos de excitación fuera de resonancia se filtran a través del acoplamiento capacitivo hacia qubits espectadores posicionados a lo largo de la trayectoria de propagación de la señal. Los errores de fase inducidos por diafonía se acumulan cuadráticamente con el número de qubits intermedios entre pares objetivo, haciendo que las puertas directas sean factibles solo para vecinos más cercanos donde ocurre un enrutamiento mínimo.",
    "B": "La arquitectura de acoplamiento físico restringe la aplicación de puertas solo a qubits vecinos más cercanos, ya que el acoplamiento capacitivo o inductivo entre qubits superconductores disminuye rápidamente con la separación espacial en el chip.",
    "C": "El modelado de pulsos de flujo para puertas de acoplamiento sintonizable requiere que los qubits individuales estén sincronizados en frecuencia dentro de una ventana determinada por la inductancia mutua y la anarmonicidad del acoplador. Los pares de qubits no adyacentes típicamente tienen diferencias de frecuencia que exceden 200 MHz debido a la variación de fabricación, y la trayectoria de sintonización adiabática necesaria para llevarlos a resonancia sin poblar estados de fuga crece más larga que T₂, restringiendo las puertas directas a qubits cercanos con frecuencias de transición naturalmente similares.",
    "D": "Los canales de decaimiento de Purcell parásitos acoplan cada qubit a su resonador de lectura dedicado, y aplicar puertas de dos qubits entre pares distantes requiere desintonización simultánea del resonador para suprimir el desfasamiento inducido por medición durante la operación de puerta. El hardware de control solo puede modular un número finito de frecuencias de resonador en paralelo debido a límites de ancho de banda del generador de forma de onda arbitraria, restringiendo las operaciones de puerta simultáneas a qubits que comparten redes de resonadores acoplados dentro de un grafo de conectividad local.",
    "solution": "B"
  },
  {
    "id": 268,
    "question": "¿Cuál es el propósito principal de la técnica de optimización de fusión de rotaciones en la compilación de circuitos cuánticos?",
    "A": "Alinea la orientación física de los qubits con el campo magnético terrestre durante la calibración ajustando los ángulos de rotación para compensar la interferencia geomagnética, que puede inducir cambios de fase espurios en circuitos superconductores. La optimización identifica puertas de rotación que pueden reorientarse para cancelar el flujo magnético ambiental que atraviesa el dispositivo, creando efectivamente una configuración de anulación de campo.",
    "B": "Asegura que todas las rotaciones de qubits ocurran simultáneamente para mejorar la sincronización en el dispositivo, lo cual es crítico cuando se trabaja con operaciones de entrelazamiento de múltiples qubits que requieren sincronización precisa. Al paralelizar puertas de rotación en diferentes qubits, la técnica minimiza la profundidad total del circuito y reduce el impacto de errores de diafonía que surgen de la aplicación secuencial de puertas.",
    "C": "La técnica convierte sistemáticamente secuencias de rotación arbitrarias en secuencias compuestas exclusivamente de operaciones de Clifford (puertas Hadamard, CNOT y Phase), que luego pueden simularse eficientemente de forma clásica usando el teorema de Gottesman-Knill. Esta conversión se logra aproximando cada ángulo de rotación continuo al equivalente de Clifford más cercano, sacrificando una pequeña cantidad de fidelidad de puerta por mejoras exponenciales en la sobrecarga de simulación clásica.",
    "D": "Combinar rotaciones consecutivas alrededor del mismo eje en una sola puerta",
    "solution": "D"
  },
  {
    "id": 269,
    "question": "¿Cuál es el papel principal de los cambios de marco en el programa de pulsos de Qiskit?",
    "A": "Los cambios de marco implementan rotaciones virtuales del eje Z actualizando la referencia de fase del oscilador local en lugar de aplicar pulsos de microondas físicos, eliminando la sobrecarga de tiempo para puertas de fase de base computacional. Sin embargo, deben sincronizarse cuidadosamente con el sistema global de seguimiento de fase para prevenir la acumulación de deriva de marco en circuitos profundos. Cada actualización de marco cambia el ángulo de fase del marco de referencia rotante para pulsos de excitación subsecuentes en ese qubit, requiriendo que el compilador mantenga un acumulador de fase que rastrea la rotación virtual total aplicada. Este mecanismo intercambia duración de pulso físico por sobrecarga de contabilidad clásica, pero introduce requisitos sutiles de coherencia de fase cuando múltiples qubits comparten líneas de control multiplexadas en frecuencia en la red de distribución de microondas del refrigerador de dilución.",
    "B": "Los cambios de marco habilitan ramificación condicional en tiempo real en programas de pulsos seleccionando dinámicamente entre plantillas de pulso precompiladas basadas en resultados de medición de circuito intermedio, implementando el flujo de control necesario para algoritmos cuánticos adaptativos. Cuando llega un resultado de medición durante la ejecución del programa, la instrucción de cambio de marco actualiza un registro interno que determina qué forma de onda de pulso subsecuente se carga desde el búfer de memoria del generador de forma de onda arbitraria. Esta selección de pulso condicional ocurre con latencia submicrosegundo, permitiendo que protocolos como corrección de errores cuánticos apliquen operaciones de recuperación dependientes del síndrome dentro del tiempo de coherencia del qubit, aunque el mecanismo requiere gestión cuidadosa de dependencias de registros clásicos para evitar introducir variaciones de sincronización deterministas que podrían filtrar información.",
    "C": "Actualización de fase virtual — básicamente solo desplaza el marco de referencia para pulsos subsecuentes, lo que da una rotación Z sin consumir tiempo de excitación. En lugar de enviar un pulso de microondas real para implementar una puerta de fase, el sistema de control simplemente actualiza el ángulo de fase del marco rotante usado para definir envolventes de pulso subsecuentes. Este enfoque es instantáneo y elimina la sobrecarga de tiempo y los errores potenciales asociados con rotaciones Z físicas, haciendo que los cambios de marco sean esenciales para la compilación eficiente de programas de pulsos y optimización de puertas.",
    "D": "Los cambios de marco implementan mezcladores definidos por software para upconversión de banda lateral única de envolventes de pulso de banda base a la frecuencia de excitación del qubit, reemplazando moduladores IQ de hardware con procesamiento digital de señales que aplica transformadas de Hilbert en el compilador de pulsos. Cuando el programador encuentra un cambio de marco, actualiza el núcleo de multiplicación exponencial compleja usado para mezcla heterodina del siguiente segmento de forma de onda, desplazando efectivamente la frecuencia portadora por el desplazamiento de fase especificado. Este enfoque de mezcla digital proporciona resolución de frecuencia subhertz para direccionar qubits individuales en regiones espectrales congestionadas, aunque requiere mantener continuidad de fase a través de límites de pulso mediante interpolación cuidadosa de la forma de onda del oscilador local para prevenir fuga espectral que excitaría transiciones fuera de resonancia.",
    "solution": "C"
  },
  {
    "id": 270,
    "question": "¿Cuál es la principal intuición del \"transporte asistido por desfasamiento\" en modelos de biología cuántica?",
    "A": "El ruido moderado interrumpe la interferencia destructiva, aumentando la eficiencia del transporte más allá de la evolución puramente coherente al abrir caminos clásicamente prohibidos",
    "B": "El desfasamiento ambiental crea transferencia de población incoherente entre autoestados de energía que evita efectos de congelamiento Zeno cuántico, permitiendo que las excitaciones escapen de trampas locales más rápido de lo que permite el salto coherente puro",
    "C": "El desfasamiento débil rompe la simetría de inversión temporal en la dinámica de Lindblad, induciendo flujo de energía dirigido hacia estados sumidero de menor energía a través de un término no hermítico efectivo que imita el acoplamiento óptimo de guía de ondas",
    "D": "Las fluctuaciones térmicas modulan dinámicamente las energías de sitio a tasas que coinciden con las fuerzas de acoplamiento entre sitios, creando tunelización asistida por resonancia que mejora el transporte a través de mecanismos de resonancia estocástica sin requerir coherencia de larga duración",
    "solution": "A"
  },
  {
    "id": 271,
    "question": "Considera un solucionador variacional de autovalores cuánticos que se utiliza para entrenar un circuito cuántico parametrizado para una tarea de clasificación, donde el paisaje de costos es altamente no convexo con muchos mínimos locales separados por barreras clásicas altas. El entrenamiento está atascado en una cuenca subóptima, y el descenso de gradiente estándar no está progresando. ¿Qué enfoque puede potencialmente ayudar a la red neuronal cuántica a escapar de este mínimo local durante el entrenamiento?",
    "A": "Los esquemas de recocido cuántico aplicados a la optimización de parámetros funcionan reduciendo lentamente los términos de campo transversal que permiten al sistema explorar el paisaje de energía más ampliamente antes de asentarse en mínimos más profundos. Al inicializar los parámetros en una superposición a través de muchas configuraciones y evolucionar adiabáticamente hacia el estado fundamental del Hamiltoniano de costo, el circuito puede realizar túnel cuántico a través de barreras durante el proceso de recocido mismo, localizándose eventualmente en un óptimo global una vez que el campo transversal desaparece. Esto explota directamente la naturaleza cuántica de los parámetros mismos, no solo la arquitectura del circuito.",
    "B": "El recocido simulado clásico con términos de momento proporciona un mecanismo de escape basado en temperatura que ocasionalmente acepta movimientos cuesta arriba, permitiendo al optimizador subir probabilísticamente fuera de cuencas poco profundas y explorar regiones vecinas del espacio de parámetros.",
    "C": "Aprovechar el efecto túnel cuántico a través de las barreras en el espacio de parámetros permite que la función de onda del optimizador penetre regiones clásicamente prohibidas, muestreando configuraciones al otro lado de las barreras de energía sin tener que escalarlas. Este efecto fundamentalmente cuántico-mecánico permite explorar regiones desconectadas del paisaje de costos que los métodos basados en gradientes no pueden alcanzar, ya que la amplitud de probabilidad puede extenderse a través de barreras incluso cuando la trayectoria clásica sería reflejada, descubriendo potencialmente mínimos más profundos que están separados de la ubicación actual por paredes de potencial prohibitivamente altas.",
    "D": "Cualquiera de estas estrategias podría funcionar dependiendo de la arquitectura específica del circuito y la estructura del problema, ya que cada una proporciona diferentes mecanismos para explorar el paisaje de costos más allá de la información local del gradiente. El recocido cuántico maneja el túnel en el espacio de parámetros, los métodos basados en momento añaden dinámica clásica que puede saltar discontinuidades, y los enfoques híbridos combinan ambos paradigmas para maximizar la probabilidad de escapar de cuencas poco profundas. La elección óptima a menudo requiere pruebas empíricas a través de la geometría particular de la superficie de pérdida encontrada en tu tarea de clasificación.",
    "solution": "C"
  },
  {
    "id": 272,
    "question": "En el contexto de algoritmos de estimación de fase cuántica que utilizan qubits auxiliares para extraer información de autovalores de un operador unitario, ¿cuál es la relación fundamental entre la transformada cuántica de Fourier continua (que actúa sobre estados de superposición arbitrarios en un espacio de Hilbert de dimensión infinita) y la transformada cuántica de Fourier discreta (que se implementa en un registro finito de n qubits) cuando ambas están restringidas a operar sobre estados de la base computacional |j⟩ donde j abarca los índices permitidos?",
    "A": "Debido a que la transformada cuántica de Fourier discreta opera en un espacio de Hilbert finito de dimensión 2^n mientras que la versión continua abarca un espacio de dimensión infinita, la implementación discreta introduce necesariamente artefactos de muestreo y efectos de fuga espectral. Estos errores de truncamiento se manifiestan como sesgos sistemáticos en la estimación de fase que disminuyen como O(1/n), lo que significa que lograr una extracción de autovalores de alta precisión requiere registros auxiliares exponencialmente grandes. Solo asintóticamente, cuando n diverge, la transformada discreta converge al límite continuo idealizado.",
    "B": "La transformada cuántica de Fourier continua existe puramente como una abstracción matemática utilizada en análisis teóricos y demostraciones, sin realización física directa posible en ningún hardware cuántico finito. En contraste, la transformada discreta constituye el circuito implementable real compuesto por puertas Hadamard y rotaciones de fase controladas.",
    "C": "Cuando se restringe a estados de la base computacional, la transformada cuántica de Fourier discreta implementa exactamente la versión continua en este subespacio finito sin ningún error de aproximación, independientemente del tamaño del registro. La equivalencia matemática se mantiene porque ambas transformadas aplican factores de fase idénticos e^(2πijk/N) a las amplitudes de los estados base, con la versión discreta simplemente restringiendo el dominio a índices j ∈ {0,1,...,2^n-1}. Esta correspondencia exacta permite que los algoritmos de estimación de fase cuántica extraigan información de autovalores con precisión limitada solo por el tamaño del registro, no por ningún error de discretización inherente en la transformada misma.",
    "D": "Mientras que la transformada cuántica de Fourier continua está definida para estados cuánticos arbitrarios incluyendo superposiciones complejas a través de todos los elementos base, la transformada discreta implementada mediante circuitos cuánticos estándar se basa en la aplicación secuencial de puertas de fase condicionales que solo funcionan correctamente cuando la entrada es un estado clásico de la base computacional.",
    "solution": "C"
  },
  {
    "id": 273,
    "question": "La dimensión de Vapnik-Chervonenkis (VC) se utiliza ocasionalmente en investigación de QML para:",
    "A": "Comparar la capacidad de aprendizaje de modelos cuánticos versus clásicos cuantificando la expresividad de circuitos cuánticos parametrizados en relación con redes neuronales clásicas, donde la dimensión VC mide el número máximo de puntos de datos que pueden ser fragmentados (clasificados correctamente en todos los etiquetados posibles) y así proporciona una cota teórica rigurosa sobre el desempeño de generalización determinada por el escalamiento de complejidad muestral como O(VC/ε²) para tolerancia al error ε, independientemente de algoritmos de entrenamiento específicos o funciones de pérdida, permitiendo comparaciones arquitectónicas justas.",
    "B": "Comparar la capacidad de aprendizaje de modelos cuánticos versus clásicos cuantificando la expresividad de circuitos cuánticos parametrizados en relación con redes neuronales clásicas, donde la dimensión VC mide el número máximo de puntos de datos que pueden ser fragmentados (clasificados correctamente en todos los etiquetados posibles) y así proporciona una cota teórica rigurosa sobre el desempeño de generalización independientemente de algoritmos de entrenamiento específicos o funciones de pérdida.",
    "C": "Comparar la complejidad estadística de operadores de medición cuánticos cuantificando la dimensión efectiva del espacio de Hilbert accesible a través de elementos POVM parametrizados, donde la dimensión VC mide el número máximo de resultados de medición que pueden distinguirse a través de todos los estados cuánticos posibles y así proporciona cotas sobre la complejidad muestral para aprender canales cuánticos. Dimensiones VC más altas indican mayor expresividad de medición, permitiendo que los modelos cuánticos extraigan más información clásica de menos consultas cuánticas que las mediciones proyectivas, lo cual es crítico para métodos de kernel cuántico donde la base de medición determina el desempeño de clasificación.",
    "D": "Comparar el desempeño de generalización de circuitos cuánticos cuantificando la dimensión efectiva del espacio de parámetros en relación con el tamaño del conjunto de entrenamiento, donde la dimensión VC mide el número de direcciones ortogonales en el paisaje de pérdida que pueden optimizarse independientemente (relacionado con el rango de la matriz de información de Fisher) y así proporciona cotas sobre el riesgo de sobreajuste independientes de algoritmos de entrenamiento específicos. Cuando la dimensión VC excede el tamaño del conjunto de datos por más que factores logarítmicos, el modelo cuántico está demostrablemente en el régimen sobreparametrizado donde las mesetas estériles se vuelven exponencialmente improbables, haciendo que una dimensión VC más alta sea deseable para la entrenabilidad en lugar de la generalización.",
    "solution": "B"
  },
  {
    "id": 274,
    "question": "En el contexto de métodos de kernel cuántico y aprendizaje de circuitos, los investigadores han observado que el número de condición de la matriz de kernel juega un papel crítico en determinar qué tan bien se desempeñará el modelo con datos no vistos. Cuando la matriz de kernel se vuelve mal condicionada—es decir, cuando sus autovalores abarcan muchos órdenes de magnitud—esta propiedad matemática tiene un impacto directo y medible sobre:",
    "A": "El desempeño de generalización. Específicamente, los kernels mal condicionados conducen a modelos que son extremadamente sensibles al ruido en los datos de entrenamiento, resultando en poca robustez cuando se evalúan en conjuntos de prueba. La dispersión de autovalores amplifica efectivamente pequeñas perturbaciones durante el proceso de aprendizaje, lo cual se manifiesta como sobreajuste y precisión predictiva degradada en nuevos ejemplos.",
    "B": "El tiempo de relajación física del qubit durante ciclos repetidos de preparación de estado, ya que la dispersión de autovalores en la matriz de Gram del kernel modula directamente los canales de decaimiento T₁ a través de retroacción en el aparato de medición. Números de condición grandes corresponden a acoplamiento resonante entre modos propios del kernel y baños de fonones ambientales, acelerando las tasas de decoherencia proporcionalmente al logaritmo de la razón espectral y reduciendo así la ventana de coherencia efectiva disponible para evaluaciones de circuito subsecuentes.",
    "C": "Los requisitos de calibración de frecuencia de pulsos de microondas, ya que las matrices de kernel mal condicionadas introducen diafonía entre líneas de control que desplaza las frecuencias resonantes de los qubits.",
    "D": "La huella de memoria clásica de circuitos cuánticos transpilados, particularmente cuando se usan redes SWAP en topologías de conectividad lineal, porque números de condición altos fuerzan al compilador a insertar qubits auxiliares adicionales para estabilizar la precisión numérica durante la inversión de la matriz de kernel. Cada orden de magnitud en la dispersión de autovalores requiere aproximadamente log₂(κ) qubits extra para corrección de errores en el protocolo de tomografía de sombra clásica, inflando exponencialmente el consumo de RAM durante la simulación de circuitos y el post-procesamiento.",
    "solution": "A"
  },
  {
    "id": 275,
    "question": "¿Qué propiedad de los sistemas cuánticos proporciona potencialmente un camino hacia el aprendizaje automático con mayor eficiencia muestral?",
    "A": "Correlaciones mejoradas por entrelazamiento, que permiten a los sistemas cuánticos capturar dependencias multivariables que requerirían exponencialmente muchos parámetros clásicos para representarse explícitamente, reduciendo así el número de muestras de entrenamiento necesarias para aprender distribuciones conjuntas complejas. Al codificar correlaciones directamente en la estructura de entrelazamiento de un estado cuántico, el modelo puede generalizar a partir de menos ejemplos porque representa implícitamente relaciones que los modelos clásicos deben aprender a través de datos extensivos.",
    "B": "La interferencia cuántica permite una convergencia más rápida durante la optimización al amplificar constructivamente caminos hacia configuraciones óptimas de parámetros mientras cancela destructivamente trayectorias subóptimas en el paisaje de pérdida. Este fenómeno permite que los métodos basados en gradientes escapen de mínimos locales de manera más eficiente que los enfoques clásicos, ya que los patrones de interferencia guían el proceso de optimización a lo largo de direcciones de búsqueda mejoradas cuánticamente que muestrean el espacio de parámetros más efectivamente.",
    "C": "La capacidad de representar distribuciones de probabilidad con menos parámetros debido a la compresión de estados cuánticos, donde un sistema de n qubits puede codificar 2^n amplitudes usando solo 2n parámetros reales después de considerar la normalización y la fase global. Esta compresión exponencial significa que los modelos cuánticos pueden representar distribuciones altamente complejas sobre espacios discretos grandes usando un número de parámetros que escala logarítmicamente con el tamaño del soporte de la distribución.",
    "D": "Todas las anteriores",
    "solution": "D"
  },
  {
    "id": 276,
    "question": "¿Cuál es el propósito de la medición en medio del circuito en computación cuántica?",
    "A": "La medición en medio del circuito permite protocolos de cambio dinámico de código donde el procesador cuántico transita entre diferentes códigos de corrección de errores durante la ejecución del algoritmo basándose en la evaluación en tiempo real de qué procesos de error físico dominan actualmente el entorno de ruido. Al medir qubits de síndrome en etapas intermedias y analizar sus correlaciones estadísticas, el sistema de control determina si los errores de inversión de bit o de fase son más prevalentes, luego reconfigura el conjunto de generadores estabilizadores en consecuencia para optimizar la distancia del código contra el canal de error identificado, manteniendo la fidelidad computacional a lo largo de algoritmos cuánticos extensos.",
    "B": "Extraer resultados de medición parciales mientras se continúa el cálculo sobre qubits no medidos—permite reutilización adaptativa de qubits, reciclaje de ancillas, ramificación condicional y extracción de síndrome en tiempo real para protocolos de corrección de errores cuánticos, permitiendo que la retroalimentación clásica guíe secuencias de puertas subsecuentes basándose en resultados intermedios sin terminar todo el algoritmo cuántico.",
    "C": "La medición en medio del circuito implementa un protocolo obligatorio de gestión de entropía requerido cuando los circuitos cuánticos exceden un umbral crítico de profundidad donde la entropía de entrelazamiento a través de cortes bipartitos se aproxima a valores máximos S ≈ n log 2 para subsistemas de n-qubits. Al realizar mediciones proyectivas sobre subconjuntos estratégicos de qubits en el punto medio del circuito, el algoritmo reduce el rango de Schmidt del estado cuántico global, previniendo el crecimiento exponencial en la complejidad de simulación clásica y permitiendo que la electrónica de control del procesador cuántico mantenga una representación eficiente del estado producto matricial de la función de onda para propósitos de seguimiento de errores en tiempo real.",
    "D": "La medición en medio del circuito proporciona capacidades de teleportación cuántica esenciales para distribuir información cuántica a través de registros de qubits espacialmente separados dentro de la arquitectura del procesador. Al medir pares de ancillas entrelazadas en la base de Bell en profundidades intermedias del circuito y aplicar correcciones de Pauli condicionales basadas en los resultados de medición clásicos, el protocolo transfiere estados cuánticos entre qubits distantes sin puertas de acoplamiento directo, evitando las restricciones de conectividad limitada de qubits en arquitecturas de vecinos más cercanos. Esta transferencia de estado basada en medición reduce el costo de conteo de puertas en comparación con cascadas de puertas SWAP en aproximadamente 40% para operaciones típicas de cirugía de red que abarcan más de tres capas de qubits.",
    "solution": "B"
  },
  {
    "id": 277,
    "question": "¿Cuál es la representación general del estado de un qubit?",
    "A": "Una mezcla probabilística de |0⟩ y |1⟩ con coeficientes reales p₀ y p₁, donde p₀ + p₁ = 1, representa el qubit como una distribución de probabilidad clásica sobre los estados de la base computacional. Esta formulación captura la naturaleza estadística de los resultados de medición cuántica al tratar el qubit como estando definitivamente en el estado |0⟩ con probabilidad p₀ o definitivamente en el estado |1⟩ con probabilidad p₁, en lugar de estar en una superposición coherente.",
    "B": "Un producto tensorial de estados |0⟩ y |1⟩ sin superposición, expresado como |0⟩ ⊗ |1⟩, representa el qubit combinando ambos estados base a través de la operación de producto tensorial en lugar de combinación lineal. Esta formulación trata el qubit como un sistema compuesto que ocupa simultáneamente ambos estados de la base computacional en factores tensoriales separados, codificando así ambas posibilidades dentro de un solo objeto matemático.",
    "C": "α|0⟩ + β|1⟩ donde α, β son amplitudes complejas que satisfacen la condición de normalización |α|² + |β|² = 1, representando una superposición cuántica coherente que captura tanto las amplitudes de probabilidad como la fase relativa.",
    "D": "Un estado entrelazado de |0⟩ y |1⟩ combinados sin usar números complejos, representando la unidad fundamental de información cuántica en un espacio de Hilbert puramente de valores reales. Esta formulación restringe los coeficientes de superposición a números reales α, β ∈ ℝ, eliminando el grado de libertad de fase asociado con amplitudes complejas. Al requerir α|0⟩ + β|1⟩ con α, β reales y α² + β² = 1, la representación confina el estado del qubit a un subespacio real de la esfera de Bloch.",
    "solution": "C"
  },
  {
    "id": 278,
    "question": "En un despliegue de instalación segura que abarca tres áreas metropolitanas, está implementando QKD independiente del dispositivo de medición entre instituciones financieras. El sistema utiliza nodos repetidores no confiables para realizar mediciones de estado de Bell, y todas las partes verifican la seguridad a través del análisis estadístico de correlaciones de medición. Después de seis meses de operación, un adversario que tiene acceso físico a las estaciones repetidoras pero no a los dispositivos terminales afirma haber extraído bits de clave. ¿Qué vulnerabilidad sofisticada existe en la implementación de distribución cuántica de claves independiente del dispositivo de medición que haría factible este ataque?",
    "A": "El adversario puede manipular sutilmente las referencias de temporización y señales de sincronización de reloj entre estaciones geográficamente distribuidas, creando correlaciones temporales artificiales en los resultados de medición que pasan las pruebas estándar de desigualdad CHSH pero filtran información parcial sobre la clave sin procesar a través de ventanas de medición cuidadosamente diseñadas que explotan restricciones de causalidad relativista inherentes a protocolos multipartitos, permitiendo la reconstrucción de bits de clave a partir de datos de corrección de errores anunciados públicamente cuando se combinan con conocimiento preciso de retardos de propagación.",
    "B": "El adversario explota ataques de remapeo de fase en las interfaces del divisor de haz del repetidor donde los modos espaciales entrantes de los dos extremos se combinan para análisis de estado de Bell. Al introducir birrefringencia controlada a través de moduladores ópticos de estrés alineados con precisión posicionados a lo largo de los últimos 10 metros de fibra antes del repetidor, el adversario crea desplazamientos de fase dependientes de polarización (Δφ ≈ π/180 por ronda de medición) que sesgan sistemáticamente qué estados de Bell se proyectan con éxito. A lo largo de seis meses de estadísticas acumuladas, el análisis de Fourier de estas correlaciones codificadas en fase revela patrones periódicos sincronizados con los anuncios de elección de base de los dispositivos terminales, filtrando aproximadamente 0.03 bits por par de fotones transmitido a través de correlaciones de canal lateral que sobreviven el paso de amplificación de privacidad pero se vuelven extraíbles cuando se hace referencia cruzada con bits de paridad de corrección de errores publicados durante el postprocesamiento clásico.",
    "C": "La eficiencia de detección finita del analizador de estado de Bell—típicamente η ≈ 0.45 para fotodiodos de avalancha comerciales operando a longitudes de onda de telecomunicaciones—crea una laguna de postselección donde los resultados de medición se anuncian solo cuando ocurren detecciones de coincidencia en ambos puertos de salida. Un adversario con acceso al repetidor puede explotar esto implementando una estrategia sofisticada de interceptar-reenviar que primero realiza mediciones de Bell parciales usando interferómetros desbalanceados con razones de división asimétricas (por ejemplo, 70:30 en lugar de 50:50). Al analizar qué brazo del interferómetro produce tasas de conteo más altas correlacionadas con los datos de reconciliación de base anunciados públicamente por las estaciones terminales durante períodos de observación extendidos, el adversario reconstruye información parcial sobre los estados de polarización de fotones previos a la medición. Combinado con conocimiento de las razones de extinción finitas en los moduladores de polarización de los extremos (típicamente 20-25 dB en lugar de la extinción infinita ideal), esto permite la estimación de máxima verosimilitud de bits de clave sin procesar con aproximadamente 12% de probabilidad de éxito por pulso transmitido.",
    "D": "El aparato de medición de estado de Bell del repetidor emplea divisores de haz polarizantes con razones de extinción finitas (típicamente 1000:1 en lugar de supresión infinita ideal), creando pequeñas pero sistemáticas fugas de fotones polarizados ortogonalmente hacia puertos de salida nominalmente bloqueados. Cuando se acopla con las variaciones de eficiencia de acoplamiento dependientes de longitud de onda inherentes a interfaces ópticas de fibra a espacio libre en el repetidor—donde los coeficientes de reflexión de Fresnel varían aproximadamente 3% a través del ancho de banda espectral de 5 nm de fuentes de fotones prácticas—estas imperfecciones generan correlaciones entre resultados de medición y la distribución de longitud de onda física dentro de cada par de fotones. Un adversario con acceso al repetidor monitorea estas estadísticas de detección resueltas en longitud de onda usando espectrómetros de alta resolución y las correlaciona con los patrones temporales en las corrientes de inyección de diodo láser de los extremos, que exhiben desviación de frecuencia dependiente de temperatura que se acopla al perfil de dispersión de fibra acumulado a lo largo de distancias metropolitanas.",
    "solution": "A"
  },
  {
    "id": 279,
    "question": "¿Cómo contribuyen las redes neuronales convolucionales cuánticas (QCNNs) a la corrección de errores cuánticos?",
    "A": "Aumentan el entrelazamiento a través de todas las capas del circuito cuántico, lo que previene la pérdida de información al crear redundancia que la corrección de errores clásica puede explotar posteriormente a través del postprocesamiento de resultados de medición y extracción de síndrome.",
    "B": "Las QCNNs implementan transformaciones unitarias aprendidas que reemplazan por completo las mediciones de estabilizadores, usando circuitos cuánticos parametrizados para proyectar directamente estados cuánticos corruptos de vuelta al espacio del código sin colapsar la superposición. Este enfoque libre de medición preserva la coherencia cuántica a lo largo de la corrección de errores al tratar la corrección como rotación continua del espacio de Hilbert en lugar de protocolo discreto de síndrome-luego-recuperación.",
    "C": "En lugar de realizar corrección de errores en hardware cuántico, las QCNNs se ejecutan en clústeres de GPU clásicos para simular la evolución de circuitos cuánticos con ruido con suficiente precisión que la salida de simulación clásica puede usarse directamente en lugar de computación cuántica, reemplazando efectivamente el costoso overhead de corrección de errores cuánticos con inferencia clásica rápida.",
    "D": "Prediciendo y corrigiendo errores usando aprendizaje automático",
    "solution": "D"
  },
  {
    "id": 280,
    "question": "En el contexto de las limitaciones de hardware de la era NISQ donde las fidelidades de puertas fluctúan a través de ciclos de calibración y los grafos de conectividad de qubits imponen overhead de enrutamiento no trivial, ¿por qué son particularmente útiles los métodos de compilación dinámica comparados con enfoques de compilación estática anticipada que fijan todas las descomposiciones de puertas y mapeos de qubits antes de la ejecución?",
    "A": "Las técnicas de compilación dinámica se adaptan continuamente a datos de rendimiento en tiempo de ejecución monitoreando tasas de error en tiempo real y fidelidades de puertas durante la ejecución del circuito, permitiendo que el compilador reoptimice descomposiciones de puertas, mapeos de qubits y estrategias de mitigación de errores sobre la marcha. Este enfoque rastrea efectivamente las fluctuaciones de ruido dependientes del tiempo y la deriva del dispositivo entre calibraciones, permitiendo que el sistema ajuste las elecciones de transpilación para favorecer los qubits físicos e implementaciones de puertas de mejor rendimiento en cada momento, mejorando así sustancialmente la fidelidad de salida del circuito en comparación con mapeos estáticos que se vuelven subóptimos a medida que las características del hardware evolucionan.",
    "B": "La compilación dinámica aprovecha la síntesis de puertas justo a tiempo al diferir la descomposición de rotaciones arbitrarias de un solo qubit en conjuntos de puertas nativas hasta inmediatamente antes de la ejecución, momento en el cual accede a los datos de calibración más recientes para seleccionar parámetros de pulso óptimos y duraciones de puertas. Este enfoque rastrea tiempos de coherencia dependientes del tiempo y tasas de error de puertas que derivan entre ciclos de calibración, permitiendo que el compilador ajuste continuamente estrategias de descomposición para minimizar el error acumulado, mejorando así sustancialmente la fidelidad del circuito en comparación con enfoques estáticos que dependen de datos de calibración potencialmente obsoletos de la fase de precompilación.",
    "C": "La compilación dinámica emplea estrategias de asignación de qubits adaptativas que reasignan mapeos de qubits lógicos a físicos entre subcircuitos basándose en el monitoreo en tiempo real de tasas de error de puertas de dos qubits y overhead de swap a lo largo de diferentes rutas de enrutamiento a través del grafo de conectividad. Al perfilar continuamente qué pares de qubits físicos exhiben actualmente los errores de CNOT más bajos y ajustar los posicionamientos de puertas subsecuentes en consecuencia, este enfoque explota la variabilidad temporal en el rendimiento del hardware que ocurre entre ejecuciones de calibración, logrando mejor fidelidad de circuito que la compilación estática que se compromete a un mapeo fijo antes de observar características de error en tiempo de ejecución.",
    "D": "La compilación dinámica utiliza caracterización de errores en línea a través de secuencias de benchmarking aleatorizado intercaladas ejecutadas entre capas de circuito, construyendo modelos estadísticos de procesos de ruido actuales que informan la selección de protocolos de mitigación de errores y políticas de programación de puertas. Este perfilado de ruido en tiempo real permite que el compilador detecte degradación de tiempo de coherencia y patrones de crosstalk a medida que emergen durante la ejecución, ajustando dinámicamente decisiones de compilación subsecuentes para enrutar alrededor de qubits e implementaciones de puertas en deterioro, manteniendo así mayor fidelidad de circuito que enfoques estáticos que no pueden responder a variaciones de rendimiento intra-ejecución.",
    "solution": "A"
  },
  {
    "id": 281,
    "question": "¿Cuál es el propósito de la construcción 'miter' de verificación de equivalencia en el cálculo ZX?",
    "A": "La construcción miter realiza el producto tensorial de ambos circuitos con sus respectivas conjugadas hermitianas, luego traza sobre los registros de salida para formar una cantidad escalar cuya representación ZX se simplifica usando reglas de fusión de arañas y complementación local—la equivalencia funcional se verifica cuando este escalar se reduce a la dimensión del espacio de Hilbert, confirmando que los circuitos implementan el mismo unitario salvo fase global.",
    "B": "El miter conecta los cables de salida de un diagrama ZX directamente a los cables de entrada del segundo diagrama después de aplicar la operación dagger, creando un diagrama cerrado cuyo valor escalar se calcula aplicando repetidamente reglas de pivote y complementación local del cálculo ZX—cuando los diagramas son funcionalmente equivalentes, estas reescrituras reducen la estructura a un factor de fase escalar igual a la dimensión del sistema.",
    "C": "El miter compone un circuito con el inverso del otro para formar un diagrama combinado, luego aplica reglas de reescritura del cálculo ZX para simplificar el resultado—si los diagramas son equivalentes, la simplificación produce la operación identidad, confirmando la igualdad funcional.",
    "D": "El miter forma un estado ancilla de par de Bell conectado mediante puertas CNOT a las líneas de qubit correspondientes en ambos circuitos, luego realiza post-selección sobre las mediciones de ancilla—la equivalencia se establece cuando las reglas de reescritura ZX para copiar nodos araña a través de la estructura ancilla producen firmas estabilizadoras coincidentes en todos los resultados de medición, lo cual puede verificarse reduciendo ambas ramas a forma normal de estado grafo.",
    "solution": "C"
  },
  {
    "id": 282,
    "question": "Las Redes Neuronales Convolucionales Cuánticas (QCNNs) ofrecen ventajas en extracción de características, clasificación y procesamiento de información. Sin embargo, también enfrentan desafíos clave. ¿Cuál de las siguientes afirmaciones describe mejor tanto sus beneficios como sus limitaciones?",
    "A": "Las operaciones jerárquicas de pooling comprimen estados cuánticos eficientemente, pero el entrenamiento requiere una sobrecarga exponencial de mediciones para estimar gradientes con precisión.",
    "B": "Mediante paralelismo cuántico, las QCNNs procesan espacios de características exponencialmente grandes en superposición, permitiendo convolución simultánea en todas las regiones espaciales de datos de entrada dentro de una sola evaluación de circuito. Esto acelera dramáticamente la generación de mapas de características comparado con convoluciones clásicas. Sin embargo, la sobrecarga física de qubits crece exponencialmente con la dimensionalidad de entrada porque cada característica adicional de datos requiere qubits dedicados para preparación de estado, y las técnicas actuales de corrección de errores no pueden comprimir eficientemente estas representaciones, haciendo el procesamiento de imágenes a gran escala intratables en dispositivos de corto plazo.",
    "C": "Las QCNNs aprovechan circuitos cuánticos paramétricos con significativamente menos parámetros entrenables que las CNNs clásicas—a menudo logrando compresión de parámetros de 10× a 100×—codificando información en espacios de Hilbert de alta dimensión donde un solo ángulo de rotación puede representar transformaciones no lineales complejas. Sin embargo, esta compacidad tiene un costo prohibitivo: implementar corrección de errores tolerante a fallos para cada capa requiere circuitos de extracción de síndrome con sobrecarga de ancilla que escala como O(d³) para códigos de distancia d, y las rondas concatenadas de corrección necesarias para arquitecturas QCNN profundas empujan el total de qubits más allá de 10⁶ incluso para tareas modestas de clasificación.",
    "D": "Las QCNNs explotan el entrelazamiento cuántico para capturar correlaciones no locales en datos más eficientemente que detectores de características clásicos, permitiendo reconocimiento de patrones superior en conjuntos de datos estructurados como configuraciones moleculares o sistemas de espín en red donde las correlaciones cuánticas de largo alcance existen naturalmente. Esta extracción de características basada en entrelazamiento proporciona ventajas representacionales exponenciales para ciertas clases de problemas. Sin embargo, el desafío generalizado de decoherencia y errores de puerta en el hardware NISQ actual degrada severamente estas correlaciones cuánticas durante la evaluación de redes profundas, causando que el recurso de entrelazamiento se disipe antes de alcanzar la capa de medición, lo cual limita fundamentalmente la profundidad práctica y precisión alcanzable en implementaciones QCNN del mundo real.",
    "solution": "D"
  },
  {
    "id": 283,
    "question": "¿Por qué la descripción de sistema de dos niveles de computadoras NISQ con estados de energía |0⟩ y |1⟩ se considera una abstracción?",
    "A": "Los qubits están físicamente restringidos a dos estados por diseño mediante ingeniería cuidadosa de la estructura de niveles de energía, y cualquier nivel de energía superior que pueda existir en el sistema físico subyacente se vuelve inaccesible por grandes brechas de energía y reglas de selección que previenen transiciones fuera del subespacio computacional. Este confinamiento estricto de dos niveles se mantiene incluso bajo campos de excitación fuertes, haciendo que la abstracción sea esencialmente exacta en lugar de aproximada.",
    "B": "La descripción de dos niveles es solo una aproximación necesitada por qubits NISQ propensos a errores que carecen de coherencia suficiente—qubits verdaderamente ideales con aislamiento perfecto serían genuinos sistemas de dos niveles que nunca fugaran a estados superiores. Una vez que se desarrollen computadoras cuánticas tolerantes a fallos con corrección de errores apropiada, la abstracción ya no será necesaria porque el hardware impondrá comportamiento estricto de dos niveles mediante supresión activa de cualquier transición de fuga.",
    "C": "Los qubits físicos reales poseen inevitablemente niveles de energía adicionales más allá de los estados computacionales |0⟩ y |1⟩, y estos estados de mayor energía pueden ser accedidos inadvertidamente durante operaciones de puerta, particularmente bajo excitaciones de microondas fuertes o pulsos fuera de resonancia, conduciendo a errores de fuga que degradan la fidelidad del circuito.",
    "D": "Las computadoras cuánticas están fundamentalmente restringidas por el principio de superposición a procesar no más de dos estados ortogonales por qubit físico, independientemente de cómo esté implementado físicamente el qubit. Intentar acceder a un tercer estado violaría la naturaleza binaria de la información cuántica como se describe en la representación de la esfera de Bloch, que solo puede acomodar dos estados base más sus combinaciones lineales—por lo tanto el modelo de dos niveles no es una abstracción sino un límite físico absoluto.",
    "solution": "C"
  },
  {
    "id": 284,
    "question": "¿Por qué la compilación aleatoria puede mitigar ciertos ataques encubiertos a nivel de pulso pero falla contra secuestros de amplitud de excitación paramétrica?",
    "A": "La compilación aleatoria permuta secuencias de puertas y por lo tanto altera el ordenamiento temporal de pulsos de control aplicados a cada qubit, lo cual interrumpe canales encubiertos dependientes del tiempo que dependen de patrones predecibles de llegada de pulsos, pero las modulaciones de amplitud de excitación paramétrica actúan globalmente a través de todo el chip mediante líneas de flujo compartidas y por lo tanto inyectan señales coherentes en todos los qubits simultáneamente independientemente de cómo se reordenen las secuencias individuales de puertas, significando que la envolvente de amplitud maliciosa del atacante persiste uniformemente a través de cada ejecución aleatoria y no puede ser descorrelacionada por permutaciones a nivel de puerta.",
    "B": "La compilación aleatoria baraja la descomposición lógica de puertas en secuencias Clifford, permutando el orden en que se aplican operaciones elementales para crear diversidad a través de ejecuciones de circuito, pero este reordenamiento opera puramente a nivel de puerta y deja las formas de pulso analógicas subyacentes—incluyendo sus perfiles de amplitud, modulaciones de fase y funciones de envolvente—completamente sin cambios, significando que un atacante que comprometa amplitudes de excitación paramétrica aún puede inyectar señales maliciosas que persisten a través de todas las compilaciones aleatorias.",
    "C": "La compilación aleatoria introduce operadores de Pauli estocásticos que promedian errores coherentes causados por descalibraciones sistemáticas de pulsos, suprimiendo efectivamente canales encubiertos que explotan imperfecciones determinísticas de puertas, pero los secuestros de amplitud de excitación paramétrica modifican los términos Hamiltonianos que gobiernan interacciones de dos qubits mediante manipulación directa del sesgo del acoplador, y estas perturbaciones a nivel Hamiltoniano conmutan con el Pauli twirling aplicado durante la compilación aleatoria, permitiendo que las modulaciones de amplitud del atacante permanezcan coherentes y no afectadas por el protocolo de aleatorización.",
    "D": "La compilación aleatoria aplica operaciones de twirling que transforman errores coherentes de pulso en ruido despolarizante al promediar sobre marcos de Pauli aleatorios, lo cual neutraliza canales encubiertos que dependen de acumulación de error coherente, pero los ataques de amplitud de excitación paramétrica explotan el régimen adiabático donde rampas lentas de amplitud inducen transiciones entre autoestados de energía sin generar componentes de alta frecuencia suficientes, y dado que la condición adiabática asegura que estas transiciones permanezcan coherentes en fase a través de todas las descomposiciones aleatorias de puertas, la modulación de amplitud maliciosa no puede ser convertida en ruido incoherente por ningún protocolo de twirling.",
    "solution": "B"
  },
  {
    "id": 285,
    "question": "Supón que estás analizando una arquitectura computacional donde circuitos Clifford se aumentan con un pequeño número de entradas de estado mágico, pero los circuitos mismos permanecen no adaptativos (es decir, todas las puertas están fijadas antes de la medición). Algunas investigaciones afirman que estos deberían seguir siendo eficientemente simulables clásicamente ya que \"los circuitos Clifford son fáciles\". ¿Por qué es erróneo este razonamiento, y qué hace que tales circuitos sean generalmente difíciles de simular a pesar de su estructura Clifford no adaptativa?",
    "A": "Los circuitos Clifford no adaptativos con estados mágicos permanecen difíciles porque los estados mágicos están fuera del normalizador del grupo Clifford, por lo que su representación estabilizadora requiere rastrear exponencialmente muchas actualizaciones de marco de Pauli que no pueden comprimirse usando el tablero Gottesman-Knill—cada estado mágico contribuye términos no estabilizadores que se multiplican a través del circuito.",
    "B": "El razonamiento falla porque los estados mágicos introducen fases no Clifford que crean ambigüedad de base computacional en la simulación Gottesman-Knill—mientras que los circuitos Clifford puros mapean estados estabilizadores a estados estabilizadores con mediciones de Pauli determinísticas, los estados mágicos tienen autovalores estabilizadores indefinidos que requieren que el simulador se ramifique exponencialmente sobre posibles registros de medición.",
    "C": "Los estados mágicos rompen la estructura estabilizadora, e incluso un número constante puede promover circuitos de otro modo fáciles a muestreo #P-difícil porque los estados mágicos inyectan amplitudes no estabilizadoras que se propagan a través de las puertas Clifford, destruyendo la representación clásica eficiente que hace tratables los circuitos Clifford puros.",
    "D": "Los circuitos Clifford preservan el rango estabilizador, pero los estados mágicos incrementan este rango multiplicativamente con cada aplicación de puerta—incluso un estado mágico fuerza al simulador clásico a mantener una superposición sobre 2^k tableros estabilizadores donde k crece linealmente con la profundidad del circuito, porque la conjugación Clifford de estados no estabilizadores genera superposiciones de proyectores estabilizadores.",
    "solution": "C"
  },
  {
    "id": 286,
    "question": "¿Qué se requiere para que el Aprendizaje por Transferencia Cuántico (QTL) funcione eficazmente?",
    "A": "El entrelazamiento cuántico por sí solo es el componente esencial y suficiente para transferir representaciones aprendidas entre modelos cuánticos, ya que las correlaciones no locales codificadas en estados entrelazados transportan naturalmente la información de características relevantes desde la tarea fuente hacia la tarea objetivo sin requerir etiquetas de datos clásicos. La estructura de entrelazamiento en sí misma codifica los patrones aprendidos, y al preservar estas correlaciones durante el proceso de transferencia mediante transformaciones unitarias apropiadas, el modelo objetivo hereda el conocimiento del modelo fuente directamente a través del recurso de entrelazamiento compartido.",
    "B": "El aprendizaje por transferencia cuántico requiere fundamentalmente una computadora cuántica completamente corregida de errores para funcionar, porque las representaciones transferidas deben mantener coherencia perfecta mientras se propagan desde el modelo fuente preentrenado hacia el modelo objetivo, y cualquier decoherencia durante este proceso de transferencia corrompería las características cuánticas aprendidas más allá de la recuperación. Sin qubits lógicos tolerantes a fallos, los errores acumulados durante la etapa de transferencia de parámetros excederían el umbral de fidelidad necesario para preservar las asignaciones de características clásicas a cuánticas codificadas.",
    "C": "Datos etiquetados para mejorar la generalizabilidad del modelo y permitir la transferencia efectiva de conocimiento desde las tareas fuente hacia las tareas objetivo",
    "D": "Un conjunto de datos clásico grande es obligatorio para preentrenar modelos cuánticos antes de que pueda ocurrir cualquier aprendizaje por transferencia, ya que el sistema cuántico necesita aprender primero representaciones de características clásicas mediante exposición extensa a ejemplos etiquetados en el dominio fuente. Los parámetros del circuito cuántico deben inicializarse incrustando patrones de datos clásicos a través de épocas de entrenamiento repetidas sobre millones de muestras, construyendo gradualmente las representaciones cuánticas internas.",
    "solution": "C"
  },
  {
    "id": 287,
    "question": "En una arquitectura distribuida realista, ¿qué factor limita más la frecuencia con la que se pueden ejecutar puertas remotas?",
    "A": "La tasa de generación exitosa de entrelazamiento entre nodos distantes, que depende de procesos probabilísticos como la transmisión de fotones a través de canales con pérdidas y la creación heralded de pares de Bell que típicamente tienen éxito con probabilidad que disminuye exponencialmente con la distancia, limitando fundamentalmente la frecuencia de operaciones remotas",
    "B": "La limitación principal surge de los tiempos finitos de coherencia del entrelazamiento almacenado en cada nodo: los pares de Bell generados se decoherencian antes de poder ser consumidos para teletransportación de puertas cuando la latencia entre nodos excede el tiempo de vida del almacenamiento de entrelazamiento. Dado que las puertas remotas requieren distribución de entrelazamiento seguida de comunicación clásica de resultados de medición antes de completar la puerta, la latencia de ida y vuelta debe permanecer dentro de la ventana de decoherencia, creando una restricción temporal estricta que limita las tasas de puertas remotas alcanzables",
    "C": "La latencia de comunicación clásica para transmitir resultados de medición se convierte en el cuello de botella porque los protocolos de puertas remotas requieren intercambio de información de síndrome antes de que la corrección de errores pueda reconstruir el estado teletransportado. Cuando los nodos están separados por distancias que requieren tiempos de ida y vuelta a escala de milisegundos, y las tasas de decoherencia demandan completar las puertas a escala de microsegundos, la limitación de la velocidad de la luz crea una brecha temporal insalvable que estrangula la frecuencia de operaciones remotas independientemente de las fidelidades de puertas locales o capacidades de generación de entrelazamiento",
    "D": "La restricción fundamental emerge de las implicaciones del teorema de no clonación para el consumo de entrelazamiento: cada operación de puerta remota consume irreversiblemente un par de Bell mediante colapso por medición, y generar entrelazamiento de reemplazo requiere procesos físicos sujetos a restricciones de incertidumbre de Heisenberg sobre tasas de preparación de estados. Esta limitación mecánico-cuántica acota la frecuencia sostenible de puertas remotas al inverso del tiempo de generación de entrelazamiento, que escala con la raíz cuadrada del coeficiente de pérdida de fotones dependiente de la distancia en implementaciones de fibra óptica",
    "solution": "A"
  },
  {
    "id": 288,
    "question": "¿Qué protocolo avanzado proporciona la seguridad más sólida para la transferencia ajena cuántica?",
    "A": "Los protocolos del modelo de almacenamiento cuántico acotado logran seguridad incondicional explotando la capacidad limitada de memoria cuántica del adversario; específicamente, si el adversario no puede almacenar más que cierto número de qubits entre rondas del protocolo, se puede probar seguridad teórica de información incluso sin supuestos computacionales. Este enfoque ha sido demostrado experimentalmente y proporciona garantías de seguridad prácticas cuando las partes honestas pueden transmitir información cuántica más rápido de lo que el adversario puede procesarla y almacenarla, haciéndolo un candidato atractivo para despliegue en el mundo real.",
    "B": "Los supuestos de almacenamiento ruidoso aprovechan el hecho de que cualquier dispositivo realista de almacenamiento cuántico introducirá decoherencia y errores con el tiempo, permitiendo que los protocolos garanticen seguridad forzando al adversario a almacenar estados cuánticos el tiempo suficiente para que el ruido destruya la ventaja de información.",
    "C": "Protocolos de transferencia ajena independientes del dispositivo, que logran seguridad sin confiar en los dispositivos cuánticos usando violaciones de desigualdades de Bell para certificar la presencia de entrelazamiento cuántico genuino y la ausencia de canales laterales que pudieran filtrar información a cualquier parte.",
    "D": "Los protocolos de compromiso de bits relativistas que explotan la separación espaciotemporal para prevenir trampas por cualquier parte durante la fase de transferencia pueden extenderse a transferencia ajena haciendo que el emisor coloque los dos mensajes posibles en ubicaciones causalmente desconectadas, asegurando que la elección del receptor sobre qué mensaje recuperar no pueda ser conocida por el emisor hasta después de que complete la fase de compromiso.",
    "solution": "C"
  },
  {
    "id": 289,
    "question": "¿Qué implica cuando la tasa de errores lógicos se suprime más rápidamente que el aumento en recursos de qubits físicos?",
    "A": "El sistema está operando en el régimen súper-umbral donde el overhead de corrección de errores escala sub-exponencialmente con la distancia del código. Esto ocurre cuando las tasas de error físico están ligeramente por encima del umbral de tolerancia a fallos, permitiendo que los primeros niveles de concatenación supriman errores más rápido que el crecimiento polinomial de recursos, aunque esta ventaja se satura a distancias de código más altas antes de lograrse supresión exponencial verdadera.",
    "B": "El decodificador está operando en el régimen de máxima verosimilitud donde los resultados de medición de síndrome se agrupan cerca de la clase de error de peso mínimo, creando una reducción efectiva en tasas de error lógico mediante promediado estadístico sobre ciclos de síndrome repetidos. Este comportamiento de pseudoumbral imita la supresión de errores verdadera pero refleja eficiencia del decodificador en lugar de escalamiento tolerante a fallos genuino, distinguiéndolo de la operación sub-umbral.",
    "C": "El sistema está operando en el régimen sub-umbral donde la corrección de errores cuánticos proporciona beneficio neto. Esto significa que la tasa de error físico está por debajo del umbral de tolerancia a fallos, permitiendo que cada capa adicional de corrección de errores (que requiere más qubits físicos) produzca supresión de error lógico exponencialmente mejor, demostrando que la computadora cuántica puede escalar exitosamente hacia operación tolerante a fallos.",
    "D": "El modelo de error físico satisface la aproximación despolarizante a nivel de circuito donde los errores de puerta ocurren uniformemente a través de todos los qubits físicos, causando que la tasa de error lógico disminuya más rápido de lo que predeciría el límite de capacidad del código. Esto sucede porque los circuitos de extracción de síndrome, cuando las tasas de error son espacialmente uniformes, suprimen naturalmente errores de peso dos mediante redundancia de medición, creando supresión aparentemente súper-exponencial hasta que emergen correlaciones espaciales.",
    "solution": "C"
  },
  {
    "id": 290,
    "question": "¿Cómo maneja el algoritmo de Shor el caso cuando el valor medido en el primer registro no conduce al período correcto?",
    "A": "El algoritmo se basa en múltiples ejecuciones independientes del circuito cuántico combinadas con postprocesamiento clásico de los resultados de medición para extraer el período con alta probabilidad",
    "B": "La expansión en fracciones continuas aplicada al cociente del resultado de medición aproxima el período verdadero incluso cuando la fase medida está ligeramente desplazada, pero este paso de postprocesamiento clásico requiere múltiples intentos para distinguir candidatos de período genuinos de factores espurios.",
    "C": "La transformada de Fourier cuántica concentra amplitud de probabilidad en múltiplos enteros del recíproco del período, por lo que las mediciones individuales producen múltiplos del período que requieren cálculo del máximo común divisor a través de varias ejecuciones para aislar confiablemente el período fundamental.",
    "D": "El algoritmo de Shor emplea una etapa de filtrado clásico que prueba cada valor medido contra la condición de exponenciación modular, rechazando resultados que no satisfacen la restricción de periodicidad y repitiendo la subrutina cuántica hasta que emerja un divisor de período válido de las estadísticas de medición.",
    "solution": "A"
  },
  {
    "id": 291,
    "question": "¿Cuáles son los desafíos clave en el entrenamiento y optimización de algoritmos de Aprendizaje Automático Cuántico (QML)?",
    "A": "El ruido cuántico proveniente de la decoherencia y errores de puertas requiere sofisticadas estrategias de mitigación de errores durante el entrenamiento, sin embargo, una vez mitigado, el espacio de Hilbert exponencialmente grande elimina por completo los problemas de desvanecimiento de gradiente, las mesetas estériles en el paisaje de pérdida se vuelven navegables mediante métodos de gradiente natural cuántico que aprovechan la métrica de Fubini-Study, la profundidad limitada de circuitos en dispositivos NISQ se supera mediante efectos de concentración de parámetros, y los cuellos de botella clásicos para el cálculo de gradientes se resuelven mediante reglas de desplazamiento de parámetros que permiten actualizaciones eficientes durante las iteraciones de entrenamiento.",
    "B": "El ruido cuántico y los errores de hardware requieren estrategias de mitigación de errores comparables a las técnicas clásicas de regularización, las mesetas estériles emergen en el paisaje de pérdida pero son causadas principalmente por mínimos locales en lugar de gradientes que se desvanecen exponencialmente, requiriendo compilación consciente del hardware en lugar de optimizadores avanzados, la profundidad limitada de circuitos en dispositivos NISQ restringe la expresividad del modelo de manera similar a las redes clásicas superficiales, y los costos de simulación clásica para el cálculo de gradientes escalan polinomialmente con el número de qubits al usar métodos de diferencias finitas, permitiendo actualizaciones prácticas de parámetros durante las iteraciones de entrenamiento.",
    "C": "El ruido cuántico proveniente de errores de puertas y decoherencia crea ruido de medición por muestreo que escala inversamente con la profundidad del circuito, haciendo que los modelos más profundos sean paradójicamente más entrenables, las mesetas estériles en circuitos variacionales se evitan mediante sobreparametrización que aumenta la señal de gradiente exponencialmente con el número de parámetros, la profundidad limitada de circuitos en dispositivos NISQ se compensa mediante la ventaja del núcleo cuántico en el espacio de características, y la simulación clásica aprovecha contracciones de redes tensoriales logrando escalado subexponencial para ansätze estructurados, permitiendo el cálculo de gradientes para sistemas de tamaño moderado durante las iteraciones de entrenamiento.",
    "D": "El ruido cuántico y los errores de hardware crean dificultades significativas de optimización que requieren sofisticadas estrategias de mitigación de errores, las mesetas estériles en el paisaje de pérdida hacen que el entrenamiento basado en gradientes sea inefectivo para muchos circuitos variacionales, requiriendo algoritmos de optimización avanzados, la profundidad limitada de circuitos en dispositivos NISQ restringe la expresividad del modelo, y los cuellos de botella de simulación clásica para el cálculo de gradientes impiden actualizaciones eficientes de parámetros durante las iteraciones de entrenamiento.",
    "solution": "D"
  },
  {
    "id": 292,
    "question": "Los estados de fotones pesados almacenados en resonadores no lineales actúan como ancillas para códigos bosónicos porque proporcionan ¿qué ventaja?",
    "A": "Inmunidad al ruido de flujo derivada de su naturaleza puramente basada en carga, lo que elimina el canal de desfase dominante que limita los tiempos de coherencia de los transmons en arquitecturas superconductoras modernas. A diferencia de los qubits sintonizables por flujo que se acoplan a fluctuaciones del campo magnético provenientes de sistemas de dos niveles en el sustrato, los resonadores de fotones pesados operan exclusivamente mediante interacciones capacitivas que son insensibles al ruido 1/f de vórtices atrapados, permitiendo operaciones deterministas de ancilla incluso en presencia de gradientes ambientales de campo magnético.",
    "B": "Cada modo puede codificar múltiples qubits lógicos simultáneamente mediante la base de número de ocupación, aumentando dramáticamente la tasa de código por dispositivo físico y reduciendo la sobrecarga de hardware requerida para la corrección de errores. Al explotar el espacio de Hilbert de dimensión infinita del oscilador armónico, un solo resonador de fotones pesados puede almacenar un registro completo de síndrome estabilizador en paralelo.",
    "C": "Compatibilidad directa con pulsos de alta potencia, eliminando la necesidad de atenuadores en la cadena criogénica y simplificando así la infraestructura del refrigerador de dilución. Debido a que los modos de fotones pesados tienen mayor masa efectiva y por lo tanto menor susceptibilidad a la población de fotones térmicos, pueden tolerar amplitudes de pulsos de microondas varios órdenes de magnitud más fuertes que los pulsos de control típicos de transmons, permitiendo operaciones de puertas más rápidas sin saturar la no linealidad de Kerr o inducir transiciones no deseadas a niveles de energía superiores en el espectro.",
    "D": "Tiempos de coherencia (T1) más largos en comparación con los transmons, lo cual es crítico para los ciclos de corrección de errores. La anarmonicidad reducida y la menor sensibilidad a la pérdida dieléctrica en el régimen de fotones pesados significa que estos modos de ancilla pueden mantener información cuántica durante duraciones que exceden los tiempos de vida típicos de los transmons por factores de dos a cinco, proporcionando suficiente estabilidad para la extracción de síndrome en múltiples rondas.",
    "solution": "D"
  },
  {
    "id": 293,
    "question": "¿Por qué el concepto de error intrínseco por Clifford (EPC) es útil para evaluar compiladores?",
    "A": "La métrica de EPC intrínseco aísla el rendimiento del compilador de la deriva de calibración dependiente del tiempo al medir únicamente las propiedades estructurales de los circuitos compilados mediante secuencias Clifford aleatorizadas — dado que las operaciones Clifford forman un subgrupo eficientemente simulable, el post-procesamiento clásico puede deconvolucionar las tasas de error observadas para extraer la contribución intrínseca de las decisiones de compilación como la programación de puertas y optimización de profundidad de circuito, permitiendo comparaciones justas de compiladores entre plataformas que permanecen válidas incluso cuando los sistemas comparados exhiben diferentes conjuntos de puertas nativas o topologías de conectividad de qubits, aunque este enfoque asume modelos de ruido markovianos que pueden no capturar correlaciones de error no locales.",
    "B": "El benchmarking Clifford aleatorizado con extracción de EPC proporciona una métrica específica del compilador que captura el rendimiento en la selección de puertas, optimización de diseño y simplificación algebraica — sin embargo, la sensibilidad de la métrica a la calidad del compilador depende críticamente de la distribución de peso de puertas Clifford en las secuencias aleatorizadas, ya que los compiladores optimizados para tipos específicos de puertas muestran puntuaciones EPC artificialmente mejoradas cuando las secuencias de benchmark sobrerrepresentan esas puertas, requiriendo un diseño cuidadoso de secuencias que coincida con la distribución esperada de carga de trabajo de la aplicación para asegurar que el EPC extraído refleje el rendimiento realista del compilador en lugar de artefactos de ajuste específicos del benchmark.",
    "C": "Ejecutar secuencias Clifford aleatorizadas y extraer el error intrínseco por Clifford captura el rendimiento agregado del compilador a través de múltiples dimensiones de optimización — incluyendo programación inteligente de puertas para minimizar el tiempo de inactividad en qubits espectadores, enrutamiento eficiente de qubits que reduce la sobrecarga de SWAP en topologías restringidas, y simplificación algebraica que elimina operaciones Clifford redundantes — proporcionando así una métrica de benchmark holística y realista que refleja la efectividad total del compilador más allá del conteo ingenuo de puertas, revelando cómo las estrategias de compilación clásicas impactan la fidelidad real del circuito cuántico.",
    "D": "El marco de EPC intrínseco cuantifica la efectividad del compilador midiendo el error promedio por puerta a través de secuencias Clifford aleatorizadas, aislando la calidad de compilación lógica de la calibración de puertas físicas — esta métrica revela cuán exitosamente el compilador explota relaciones de conmutación y oportunidades de fusión de puertas para reducir el conteo total de puertas, aunque su utilidad depende del supuesto de que todas las puertas Clifford contribuyen igualmente al error del circuito, una aproximación que se rompe cuando el compilador enruta selectivamente operaciones a través de pares de qubits con menor error o programa preferentemente puertas durante ventanas óptimas de calibración, causando que las mediciones de EPC confundan la inteligencia del compilador con la heterogeneidad del hardware a menos que controles adicionales normalicen las variaciones de error espaciotemporales.",
    "solution": "C"
  },
  {
    "id": 294,
    "question": "¿Qué técnica específica puede detectar modificaciones no autorizadas en el hardware de control cuántico?",
    "A": "La identificación por canal lateral captura el consumo de energía, emisiones electromagnéticas y patrones de temporización de la electrónica de control para construir firmas únicas de hardware, permitiendo la detección de modificaciones de firmware o sustituciones de componentes que alteran las características operacionales.",
    "B": "La tomografía de estados cuánticos reconstruye la matriz de densidad completa de los estados de salida midiendo valores esperados a través de un conjunto informacionalmente completo de observables, luego aplica estimación de máxima verosimilitud para extraer la representación del estado físico. Al comparar los estados reconstruidos con las predicciones teóricas del sistema de control no modificado, las desviaciones indican manipulación del hardware o corrupción del firmware.",
    "C": "El hash de hardware de control incrusta sumas de verificación criptográficas en tablas de definición de pulsos de microondas y bitstreams de FPGA, verificando la integridad antes de cada secuencia experimental comparando configuraciones en tiempo de ejecución contra valores de referencia firmados por el fabricante. Los parches de firmware no autorizados o parámetros de calibración modificados producen desajustes de hash, señalando posible manipulación en la cadena de control.",
    "D": "El benchmarking aleatorizado con puertas Clifford intercaladas inyecta operaciones de prueba entre capas de circuitos compilados, midiendo la fidelidad promedio de secuencia a través de muchas instancias aleatorias. El análisis estadístico revela si los errores de puertas han cambiado desde la caracterización de línea base, indicando formas de onda de control modificadas o intensidades de acoplamiento de qubits alteradas introducidas por hardware comprometido.",
    "solution": "A"
  },
  {
    "id": 295,
    "question": "Los decodificadores que explotan la autocorrelación en series temporales de síndrome pueden superar a los decodificadores estáticos porque los patrones correlacionados indican ¿qué propiedad del ruido?",
    "A": "La independencia completa de los canales de error X y Z se revela cuando las autocorrelaciones de síndrome decaen rápidamente a cero dentro de unos pocos ciclos de extracción de síndrome, confirmando que los errores de bit-flip y phase-flip ocurren mediante mecanismos no correlacionados que muestrean independientemente de sus respectivas distribuciones de ruido. Esto permite que los decodificadores de redes tensoriales factoricen el problema de decodificación en subproblemas separados de error clásico y cuántico, cada uno resuelto con algoritmos especializados optimizados para estabilizadores X o Z, logrando aceleraciones exponenciales al evitar la necesidad de rastrear correlaciones entre tipos de error a través del historial de síndrome combinado.",
    "B": "El dominio del ruido de medición por muestreo sobre errores físicos reales se confirma cuando las autocorrelaciones de síndrome muestran un escalado característico 1/√N con el número de mediciones repetidas N, indicando que la fuente principal de incertidumbre del síndrome proviene del ruido de proyección cuántica en la lectura del estabilizador en lugar de procesos de error coherentes que se acumulan en qubits de datos. Esto permite que filtros de umbral simples distingan errores reales (que producen cambios persistentes de síndrome) de fluctuaciones de medición transitorias (que se promedian), permitiendo a los decodificadores reducir dramáticamente la sobrecarga computacional descartando secuencias de síndrome cuya varianza temporal coincide con la firma de ruido de muestreo.",
    "C": "La persistencia temporal de procesos de error subyacentes que un supuesto markoviano perdería, revelando que los errores actuales dependen del historial de errores pasados a través de mecanismos correlacionados. Esto permite que decodificadores sofisticados construyan modelos probabilísticos que incorporan efectos de memoria, mejorando dramáticamente la precisión de corrección al predecir ubicaciones probables de error basándose en patrones de síndrome en lugar de tratar cada ciclo de extracción como independiente.",
    "D": "Las rotaciones unitarias de la base de error dejan invariantes las mediciones de síndrome porque los valores propios del estabilizador se preservan bajo conjugación unitaria, lo que significa que las autocorrelaciones en la serie temporal de síndrome reflejan directamente rotaciones entre diferentes subespacios de error (X, Y, Z) impulsadas por la evolución hamiltoniana. Cuando los decodificadores observan picos de autocorrelación periódicos a frecuencias que coinciden con las escalas de energía características del sistema, esto indica que los errores están ciclando a través de diferentes tipos de Pauli mediante dinámicas coherentes, y contabilizar estas rotaciones mediante una transformación de base del decodificador dependiente del tiempo permite estrategias de corrección que rastrean el marco de error rotatorio en lugar de tratar cada extracción de síndrome como independiente.",
    "solution": "C"
  },
  {
    "id": 296,
    "question": "¿Qué tipo de ataque puede explotar controles a nivel de pulso en un sistema cuántico multiusuario para interrumpir qubits distantes?",
    "A": "En arquitecturas con acopladores sintonizables (como sistemas gmon o fluxonium), un atacante que obtiene acceso de nivel root a la API puede reconfigurar los puntos de polarización del acoplador para establecer interacciones directas de dos qubits entre sus qubits asignados y qubits víctima ubicados varios sitios de red más lejos. Al ajustar dinámicamente los parámetros del hamiltoniano del acoplador —específicamente la fuerza de acoplamiento g_{ij} y el desfase Δ— el atacante crea efectivamente nuevas conexiones en el grafo de conectividad que no estaban presentes en la topología publicada del dispositivo.",
    "B": "Inyectando código malicioso en la pila de compilación del proveedor de nube, un atacante puede reescribir los operadores de medición de Pauli aplicados a los qubits víctima durante la lectura, rotando efectivamente la base de medición de Z a X o Y sin alterar el estado cuántico en sí. Esta manipulación a nivel de software hace que el experimento de la víctima mida un observable completamente diferente, colapsando superposiciones a lo largo de ejes ortogonales a la base computacional prevista y filtrando así información sobre fases que deberían haber permanecido ocultas.",
    "C": "Un atacante con proximidad física al refrigerador de dilución puede introducir interferencia electromagnética pulsada directamente en las líneas de control de microondas o en el cableado de polarización DC, evitando completamente las abstracciones de software de la plataforma en la nube. Estas señales inyectadas se acoplan a los qubits víctima a través de líneas de transmisión compartidas o cables coaxiales insuficientemente blindados, causando errores de defasamiento o inversión de bits incluso cuando el atacante no tiene ninguna asignación legítima en el procesador cuántico. El ataque explota la naturaleza analógica del control de qubits: como los pulsos de control son formas de onda continuas en lugar de comandos digitales discretos, cualquier ruido electromagnético en la banda de frecuencia relevante (típicamente 4-8 GHz para transmons) será indistinguible de los tonos de excitación legítimos y por lo tanto no puede ser filtrado por esquemas de autenticación clásicos.",
    "D": "Implementando secuencias de pulsos personalizadas cuidadosamente diseñadas en qubits controlados por el atacante que generan diafonía no intencionada a través de acoplamientos residuales siempre activos en el hamiltoniano del dispositivo, permitiendo al adversario inducir errores de fase o rotaciones no deseadas en qubits víctima ubicados varios sitios de red más lejos sin requerir conectividad directa.",
    "solution": "D"
  },
  {
    "id": 297,
    "question": "¿Por qué el intercambio de tokens en protocolos de computación cuántica distribuida debe tener en cuenta ventanas de sincronización de puertas?",
    "A": "Los protocolos de generación de entrelazamiento distribuido como esquemas fotónicos heraldados producen pares de Bell con fluctuación temporal heredada de eventos de detección probabilísticos, lo que requiere ventanas de sincronización para asegurar que ambos nodos consuman pares compartidos dentro de los límites de tiempo de decoherencia. Sin coordinación, un nodo puede mantener su mitad de un par entrelazado mientras el qubit del nodo compañero experimenta relajación, destruyendo las correlaciones antes de que se completen los protocolos de teletransportación de puertas distribuidas. El intercambio de tokens impone alineación temporal de los horarios de consumo.",
    "B": "Los protocolos de intercambio de tokens codifican grafos de dependencia de puertas en flujos de metadatos clásicos que especifican qué operaciones distribuidas deben completarse antes de que puedan ejecutarse puertas subsiguientes. Como la teletransportación cuántica requiere que los resultados de medición se propaguen entre nodos antes de aplicar unitarios de corrección, las ventanas de sincronización aseguran que la latencia de comunicación clásica no exceda el tiempo de coherencia de los qubits en espera, previniendo errores inducidos por decoherencia en circuitos distribuidos que dependen de mantener entrelazamiento a través de retrasos de comunicación.",
    "C": "Los protocolos de red cuántica implementan cubos de tokens que regulan la velocidad a la cual los nodos consumen recursos entrelazados compartidos, asegurando que las tasas de generación de pares de Bell permanezcan equilibradas en todos los enlaces de red. Sin sincronización, los nodos con destilación de entrelazamiento más rápida agotarían su asignación de tokens mientras los nodos compañeros se quedan atrás, creando desajustes temporales donde los qubits permanecen inactivos más allá de sus límites de coherencia T2 esperando que los compañeros se pongan al día, degradando así la fidelidad general del circuito distribuido.",
    "D": "La latencia de comunicación clásica debe coincidir con los requisitos de temporización para que los pares de Bell compartidos lleguen a ambos nodos dentro de sus ventanas de tiempo de coherencia, asegurando que los recursos entrelazados permanezcan viables para operaciones de puertas distribuidas subsiguientes. Sin sincronización, la decoherencia destruye las correlaciones antes de que los protocolos de teletransportación puedan completarse, causando que la computación distribuida falle debido a enlaces cuánticos expirados entre nodos.",
    "solution": "D"
  },
  {
    "id": 298,
    "question": "¿Por qué es particularmente importante reducir el número de parámetros en HQNNs en el contexto de la computación cuántica?",
    "A": "Menos parámetros reducen directamente la sobrecarga de estimación de gradientes ya que cada parámetro requiere múltiples evaluaciones de circuito usando reglas de desplazamiento de parámetros o diferencias finitas, y en dispositivos NISQ con presupuestos de disparos limitados y alto ruido de muestreo, evaluar gradientes para cientos de parámetros consume recursos de medición prohibitivos, haciendo que la reducción de parámetros sea esencial para lograr convergencia dentro de restricciones experimentales prácticas donde las ejecuciones totales de circuito deben permanecer manejables dadas las limitaciones de acceso al hardware y las escalas de tiempo de decoherencia.",
    "B": "La reducción de parámetros aborda principalmente la entrenabilidad mitigando fenómenos de meseta estéril: estudios empíricos muestran que ansätze sobreparametrizados con parámetros que exceden O(n²) en sistemas de n-qubits exhiben gradientes que se desvanecen exponencialmente debido a efectos de concentración de medida, mientras que arquitecturas eficientes en parámetros con O(n) u O(n log n) parámetros mantienen escalado polinomial de gradientes, permitiendo convergencia de optimización donde circuitos fuertemente parametrizados encontrarían paisajes de pérdida planos independientemente de la estrategia de inicialización o elección de optimizador, haciendo que la economía de parámetros sea esencial para acceder a señal de gradiente significativa.",
    "C": "Reducir parámetros minimiza la profundidad del circuito, lo cual es crucial debido a la decoherencia cuántica, ya que menos parámetros típicamente corresponden a circuitos más superficiales con menos capas de puertas que completan la ejecución antes de que el ruido acumulado destruya la coherencia cuántica y haga que los resultados computacionales sean poco fiables.",
    "D": "Conteos de parámetros más bajos reducen la susceptibilidad a errores de estimación de gradientes inducidos por ruido: en hardware actual cada gradiente de parámetro requiere O(1/ε²) disparos para lograr precisión ε, y el ruido de medición se compone a través de dimensiones de parámetros, por lo que reducir parámetros de P a P/k mejora la relación señal-ruido del gradiente por un factor √k bajo presupuestos fijos de disparos, permitiendo optimización confiable en dispositivos ruidosos donde la estimación de gradientes de alta dimensión estaría dominada por fluctuaciones de muestreo que oscurecen las direcciones verdaderas del gradiente y previenen la convergencia.",
    "solution": "C"
  },
  {
    "id": 299,
    "question": "La inicialización con valores cero generalmente se evita en circuitos variacionales porque:",
    "A": "Crea circuitos donde todas las puertas de rotación se convierten inicialmente en operaciones identidad, eliminando la generación de entrelazamiento en la primera iteración de entrenamiento y forzando al optimizador a comenzar desde un estado producto. Esto retrasa la exploración de las regiones entrelazadas donde típicamente residen las soluciones óptimas, requiriendo épocas de optimización adicionales para escapar del subespacio separable y a menudo causando convergencia prematura a mínimos locales dentro del régimen clásicamente simulable antes de alcanzar regiones de ventaja cuántica.",
    "B": "Atrapa al optimizador en regiones de simetría planas donde los gradientes se desvanecen y el paisaje de parámetros se vuelve degenerado, impidiendo la exploración efectiva del espacio de soluciones y a menudo conduciendo a fallos de convergencia o mínimos subóptimos",
    "C": "Produce configuraciones de parámetros donde las simetrías del circuito causan que los gradientes de la función de costo se desvanezcan idénticamente debido a contribuciones positivas y negativas iguales de caminos computacionales paralelos, creando mesetas artificiales no relacionadas con mesetas estériles",
    "D": "Induce enmascaramiento de gradientes donde las derivadas de parámetros se cancelan debido a disposiciones simétricas de puertas alrededor de ángulos de rotación cero, creando puntos estacionarios espurios que satisfacen condiciones de optimalidad de primer orden sin representar extremos verdaderos del paisaje de costo",
    "solution": "B"
  },
  {
    "id": 300,
    "question": "En implementaciones prácticas de distribución cuántica de claves que abarcan distancias metropolitanas (50-100 km), donde usuarios legítimos Alice y Bob deben establecer claves seguras mientras enfrentan pérdidas realistas de canal y potencial espionaje, ¿cuál es el compromiso principal al usar Nodos Confiables Simplificados en comparación con arquitecturas completas de nodos confiables?",
    "A": "Los nodos confiables simplificados alteran fundamentalmente el modelo de seguridad al requerir pares cuánticos entrelazados precompartidos para autenticación continua en cada punto de relevo intermedio, en lugar de depender únicamente de canales autenticados clásicos para reconciliación pública de bases.",
    "B": "La pérdida acumulativa de fotones a través de múltiples saltos en redes de nodos simplificados aumenta casi en un orden de magnitud en comparación con transmisión directa a distancia total equivalente, principalmente porque estos nodos carecen de memoria cuántica y capacidades de intercambio de entrelazamiento requeridas para verdadera funcionalidad de repetidor cuántico.",
    "C": "Los nodos confiables simplificados introducen un compromiso crítico entre seguridad y complejidad de implementación en el cual toleran significativamente menos ruido y requieren mejores condiciones de canal en comparación con nodos confiables completos que pueden realizar mediciones cuánticas intermedias y procesamiento clásico —esta mayor sensibilidad al ruido requiere protocolos de corrección de errores más sofisticados, umbrales de eficiencia de detector más altos y límites más estrictos en conteos de fotones de fondo para mantener la misma tasa efectiva de generación de claves, restringiendo así la flexibilidad de despliegue en entornos metropolitanos del mundo real con condiciones atmosféricas variables, imperfecciones de fibra, conteos oscuros de detectores y otras degradaciones prácticas que crean tasas elevadas de error de bits cuánticos que deben mantenerse por debajo de los límites más estrictos impuestos por los márgenes de tolerancia de error reducidos de la arquitectura de nodos simplificados.",
    "D": "Como los nodos simplificados no pueden realizar verificación directa del estado cuántico en fotones que pasan sin destruir la información de clave, deben en cambio depender de esquemas clásicos de verificación de resultados de medición basados en certificados donde cada nodo firma digitalmente y reenvía estadísticas de detector a los puntos finales para validación retrospectiva.",
    "solution": "C"
  },
  {
    "id": 301,
    "question": "¿Qué es la fidelidad de puerta en computación cuántica?",
    "A": "Cuán exactamente una puerta cuántica implementada físicamente coincide con la operación ideal teórica de la puerta, cuantificando la superposición entre la transformación real aplicada a los estados cuánticos y la evolución unitaria deseada. Esta métrica, típicamente expresada como un número entre 0 y 1, captura todas las fuentes de error incluyendo decoherencia, imperfecciones de control y diafonía, convirtiéndola en el indicador fundamental de rendimiento para la calidad de puerta.",
    "B": "La distancia traza entre el canal cuántico implementado y la operación unitaria ideal, promediada sobre todos los estados puros de entrada extraídos uniformemente de la esfera de Bloch según la medida de Haar. Esta métrica captura errores coherentes sistemáticos y ruido incoherente estocástico, cuantificando la calidad de puerta mediante la distinguibilidad mínima entre operaciones reales y objetivo. Expresada entre 0 y 1, contabiliza decoherencia, errores de control y fuga, convirtiéndola en la métrica estándar de evaluación para el rendimiento de puerta.",
    "C": "La fidelidad de proceso entre el canal cuántico real y la puerta unitaria objetivo, calculada promediando la fidelidad de estado sobre todos los estados de entrada ponderados por la medida de Haar, luego tomando el conjugado complejo del resultado antes de la normalización. Esto captura qué tan bien la operación implementada preserva la coherencia cuántica a través de todo el espacio de estados, contabilizando tanto errores unitarios como mecanismos de decoherencia. La métrica varía de 0 a 1 y cuantifica directamente la calidad de puerta incluyendo diafonía e imperfecciones de control.",
    "D": "La distancia norma diamante entre el superoperador implementado y la puerta unitaria ideal, maximizada sobre todos los posibles estados de entrada incluyendo aquellos entrelazados con un sistema auxiliar de igual dimensión. Esta medida de fidelidad de peor caso captura todas las fuentes de error—decoherencia, diafonía e imperfecciones de control—cuantificando la máxima distinguibilidad alcanzable por cualquier protocolo cuántico. Expresada entre 0 y 1, proporciona la métrica fundamental de certificación para la calidad de puerta en arquitecturas de computación cuántica tolerante a fallos.",
    "solution": "A"
  },
  {
    "id": 302,
    "question": "En un sistema cuántico distribuido, ¿cuál es una función principal del planificador de circuitos?",
    "A": "Monitorear la calidad del enlace de entrelazamiento y solicitar preventivamente nuevos pares de Bell cuando las predicciones de fidelidad caen por debajo de umbrales operacionales, usando datos de tomografía en tiempo real de intercambios recientes para pronosticar qué conexiones se degradarán antes de que se ejecuten las próximas puertas remotas, manteniendo así un búfer de recursos de alta fidelidad que previene detenciones del circuito debido al agotamiento del entrelazamiento a través de la topología de red distribuida",
    "B": "Reordenar dinámicamente las puertas remotas basándose en la disponibilidad actual de enlaces entrelazados entre nodos y determinar qué operaciones de dos qubits están realmente listas para ejecutarse dada la distribución de recursos de entrelazamiento, las restricciones de latencia de comunicación y las cadenas de dependencia dentro de la estructura del circuito",
    "C": "Particionar el circuito cuántico en subcircuitos independientes que pueden ejecutarse en paralelo a través de nodos distribuidos respetando la tasa de generación de entrelazamiento entre nodos como restricción, luego resolver un problema de optimización para minimizar el tiempo total de ejecución superponiendo operaciones de puerta locales con la latencia requerida para la distribución remota de entrelazamiento, efectivamente encadenando fases de computación y comunicación para saturar el ancho de banda disponible",
    "D": "Coordinar la temporización de operaciones unitarias locales con la llegada de información de síndrome no local de nodos vecinos de manera que las rondas de corrección de errores permanezcan causalmente consistentes a pesar de retrasos variables en la red, implementando un protocolo de sincronización de reloj lógico que asegura que las mediciones de estabilizador se completen en el orden relativo apropiado a través de todos los nodos incluso cuando los tiempos de ejecución de puertas físicas difieren entre procesadores cuánticos heterogéneos",
    "solution": "B"
  },
  {
    "id": 303,
    "question": "En el contexto de decodificación de códigos de superficie y otros códigos topológicos a gran escala, los investigadores han explorado adaptar métodos de grupo de renormalización de matriz densidad (DMRG) de la física de materia condensada. La motivación proviene del crecimiento exponencial de la complejidad de decodificación clásica a medida que aumenta la distancia del código. Dada una medición de síndrome en un código de superficie de distancia d con condiciones de frontera, ¿qué ventaja computacional específica proporcionan los decodificadores basados en DMRG en comparación con enfoques exactos de decodificación de máxima verosimilitud?",
    "A": "La decodificación de código de superficie basada en DMRG reformula el patrón de síndrome como un Hamiltoniano clásico efectivo donde el estado fundamental corresponde a la configuración de error de peso mínimo consistente con los síndromes observados, luego usa barridos iterativos de red tensorial para optimizar variacionalmente la representación de cadena de error, logrando complejidad de tiempo polinomial en distancia de código al restringir la estructura de entrelazamiento de configuraciones de error candidatas a aquellas representables con dimensión de enlace χ, que captura cadenas de error localmente correlacionadas típicas de modelos de ruido físico mientras descarta sistemáticamente exponencialmente muchas configuraciones de alto entrelazamiento que dominarían búsquedas de máxima verosimilitud.",
    "B": "Los decodificadores basados en DMRG explotan la observación de que las configuraciones de error probables en códigos de superficie pueden representarse eficientemente como estados de producto matricial a lo largo de cadenas de error unidimensionales, lo que reduce los requisitos de memoria de escalamiento exponencial en distancia de código a escalamiento polinomial mediante contracciones estructuradas de red tensorial que capturan las correlaciones esenciales en la distribución de error mientras descartan exponencialmente muchas configuraciones improbables que dominarían enfoques exactos de máxima verosimilitud",
    "C": "El decodificador DMRG construye una representación de operador de producto matricial del operador de proyección de síndrome que impone consistencia entre síndromes observados y cadenas de error candidatas, luego realiza contracciones tensoriales secuenciales a lo largo del límite espacial de la red para calcular probabilidades marginales para cada violación de estabilizador de plaqueta, permitiendo propagación eficiente de creencias a través del espacio de código donde el costo computacional escala polinomialmente con la distancia de código porque las dimensiones de enlace tensorial permanecen acotadas cuando se restringen a configuraciones de error que satisfacen la restricción homológica de que las cadenas de error forman bucles cerrados o terminan en límites, a diferencia de búsquedas de máxima verosimilitud que deben enumerar todas las configuraciones de cadena topológicamente distintas.",
    "D": "Los métodos DMRG transforman el problema de decodificación de síndrome en una contracción de red tensorial donde cada bit de síndrome corresponde a un índice tensorial en una red cuyo valor de contracción iguala la probabilidad del error de máxima verosimilitud dados los resultados de medición, pero en lugar de realizar contracción exacta (exponencialmente costosa), el enfoque DMRG usa descomposiciones secuenciales de valores singulares a lo largo de un camino unidimensional a través de la red bidimensional del código para aproximar la contracción manteniendo solo los χ valores singulares más grandes en cada paso, donde χ se elige de modo que el error de truncamiento permanezca por debajo de la tasa de error física del hardware cuántico, logrando así decodificación aproximada de máxima verosimilitud en tiempo polinomial con degradación de precisión controlable.",
    "solution": "B"
  },
  {
    "id": 304,
    "question": "¿Cuál es la relación entre la elección del ansatz de circuito y la ocurrencia de mesetas áridas en redes neuronales cuánticas?",
    "A": "Los ansätze eficientes en hardware con estructura global, que maximizan la utilización de conjuntos de puertas nativos y minimizan la sobrecarga de compilación, han demostrado en múltiples estudios concentrar la varianza del gradiente exponencialmente a medida que aumenta la profundidad del circuito, porque el entrelazamiento aleatorio que generan a través de todos los qubits crea un paisaje de costo que se vuelve exponencialmente plano en el espacio de parámetros de alta dimensión.",
    "B": "La estrategia de inicialización importa — una mala inicialización de parámetros aumenta significativamente la probabilidad de que los gradientes se desvanezcan exponencialmente a través del paisaje. Cuando los parámetros se muestrean uniformemente de rangos que no respetan la estructura del álgebra de Lie subyacente al circuito, el estado inicial resultante explora una región plana de la función de costo donde las magnitudes de gradiente escalan como O(1/2^n) con el conteo de qubits.",
    "C": "Los ansätze específicos del problema que codifican conocimiento del dominio ayudan a evitar crecimiento innecesario de entrelazamiento, las estructuras de entrelazamiento limitado restringen la varianza del gradiente al limitar la conectividad a vecindarios locales, y las estrategias de inicialización inteligentes que respetan la estructura subyacente del álgebra de Lie pueden retrasar el inicio de gradientes que se desvanecen exponencialmente, formando juntas un enfoque de mitigación multipronged.",
    "D": "Las estructuras de entrelazamiento restringidas que limitan la conectividad entre qubits a vecindarios locales o topologías tipo árbol evitan que el sistema explore el espacio de Hilbert completo, lo que a su vez restringe la función de costo a una variedad de menor dimensión donde los gradientes permanecen acotados lejos de cero. Este enfoque intercambia expresividad por entrenabilidad: al prohibir entrelazamiento de largo alcance, el circuito ya no puede representar ciertos estados objetivo altamente entrelazados.",
    "solution": "C"
  },
  {
    "id": 305,
    "question": "¿Qué papel juega la comunicación clásica en el proceso de teletransportación cuántica?",
    "A": "Envía dos bits clásicos que codifican el resultado de la medición de Bell para que el receptor sepa cuál de cuatro posibles correcciones de Pauli aplicar a su mitad del par entrelazado, transformándolo en el estado objetivo que originalmente poseía el emisor.",
    "B": "Transmite el resultado de medición de Bell de dos bits obtenido de proyectar el qubit del emisor y su recurso entrelazado sobre la base de Bell, permitiendo al receptor determinar cuál unitario local del grupo de Pauli {I, X, Z, XZ} debe aplicarse para decodificar su partícula en el estado teletransportado deseado.",
    "C": "Comunica el resultado de la medición conjunta del emisor sobre el qubit de entrada y su mitad del recurso entrelazado, transmitiendo cuál de los cuatro estados de Bell fue observado para que el receptor pueda realizar la operación condicional correspondiente para rotar su qubit al estado objetivo que replica la información cuántica original.",
    "D": "Entrega resultados de medición de la proyección de estado de Bell del emisor, proporcionando dos bits que especifican qué corrección dependiente de base debe ejecutar el receptor sobre su qubit entrelazado para completar el protocolo de teletransportación transformando su estado condicional en una réplica exacta del qubit original que fue medido.",
    "solution": "A"
  },
  {
    "id": 306,
    "question": "¿Qué enfoque se utiliza más comúnmente para implementar clasificación cuántica con circuitos cuánticos parametrizados?",
    "A": "Aplicar operaciones unitarias de codificación de datos seguidas de circuitos variacionales parametrizados entrenados mediante descenso de gradiente y medición de observables dependientes de clase, donde las puertas parametrizadas se optimizan para mapear características de entrada codificadas a estados cuánticos cuyas estadísticas de medición distinguen entre clases, permitiendo el aprendizaje supervisado mediante retropropagación de funciones de costo calculadas a partir de resultados de medición y etiquetas clásicas.",
    "B": "Codificar datos mediante modulación de amplitud parametrizada y medir observables de Pauli que distinguen clases—este enfoque prepara cada vector de características de entrada aplicando puertas de rotación parametrizadas con ángulos proporcionales a los valores de las características, construyendo un estado cuántico donde las amplitudes codifican datos normalizados, luego mide un conjunto de operadores de Pauli conmutantes cuyos valores esperados se alimentan a una función discriminante clásica. Las decisiones de clasificación se extraen de estos vectores de valores esperados calculando fronteras de decisión en el espacio de resultados de medición, permitiendo el aprendizaje supervisado mediante ajuste de pesos de observables de lectura.",
    "C": "Mapas de características cuánticas basados en kernels con clasificación por máquinas de vectores de soporte—el problema de clasificación se formula embebiendo datos clásicos en el espacio de Hilbert cuántico mediante un mapa de características unitario fijo U(x) que codifica la entrada x como un estado cuántico, luego calculando productos internos ⟨ψ(x)|ψ(x')⟩ entre estados codificados para construir una matriz de kernel. Este kernel cuántico cuantifica la similitud entre puntos de datos mediante mediciones de superposición basadas en interferencia, permitiendo a las máquinas de vectores de soporte clásicas encontrar hiperplanos separadores óptimos en el espacio de características cuántico sin entrenamiento explícito de circuitos cuánticos parametrizados.",
    "D": "Ramificación cuántica secuencial basada en mediciones con propagación adaptativa—este método construye la clasificación realizando mediciones por capas de observables dependientes de características donde cada resultado de medición condiciona puertas parametrizadas subsecuentes mediante control clásico de propagación. Cada medición proyecta sobre un subespacio correspondiente a decisiones de clasificación parciales, y el algoritmo refina la clase predicha mediante proyecciones secuenciales hasta alcanzar un estado de clasificación final. Las puertas parametrizadas entre mediciones se optimizan para maximizar la separabilidad de estados inducida por medición, permitiendo el aprendizaje supervisado mediante entrenamiento de ángulos de rotación condicionales.",
    "solution": "A"
  },
  {
    "id": 307,
    "question": "¿Cuál es el principal beneficio de diseñar circuitos cuánticos con estructura cíclica?",
    "A": "La reutilización de parámetros entre capas permite un entrenamiento y optimización eficientes de algoritmos cuánticos variacionales al reducir el número total de parámetros independientes que deben ajustarse, lo que disminuye la dimensionalidad del paisaje de optimización clásico y acelera la convergencia. Cuando los parámetros de puerta se repiten periódicamente a través de la profundidad del circuito, los optimizadores basados en gradiente encuentran menos mínimos locales y el problema de la meseta árida se mitiga, ya que las correlaciones entre capas crean geometrías estructuradas de función de costo que guían la búsqueda hacia óptimos globales de manera más confiable que circuitos profundos inicializados aleatoriamente.",
    "B": "La deriva de calibración del hardware se mitiga naturalmente porque las secuencias de puertas periódicas permiten evaluación comparativa en tiempo real en cada límite de ciclo, donde la medición repetida del mismo estado lógico permite el seguimiento de deriva mediante control estadístico de procesos. Cuando capas parametrizadas idénticas recurren con período L, las desviaciones en la fidelidad de puerta entre ciclos pueden detectarse comparando valores esperados de observables cíclicamente invariantes, permitiendo protocolos de recalibración adaptativa compensar errores sistemáticos antes de que se acumulen más allá de umbrales de corrección de errores, particularmente importante para mantener precisión computacional durante solucionadores de valores propios variacionales que requieren cientos de evaluaciones de circuito a través de barridos de parámetros.",
    "C": "Los efectos de desacoplamiento dinámico emergen automáticamente de la aplicación periódica de secuencias de puertas, donde las simetrías de inversión temporal en bloques de circuito repetidos crean control Bang-Bang efectivo que suprime ruido de baja frecuencia. Cuando las capas unitarias se aplican cíclicamente con estructura de signo alternante, las perturbaciones ambientales promedian a cero sobre cada período mediante interferencia destructiva en el marco de conmutación, extendiendo tiempos de coherencia sin ingeniería explícita de pulsos. Esta propiedad autocorrectiva se vuelve particularmente valiosa en algoritmos variacionales donde la misma estructura de circuito se evalúa repetidamente, ya que los canales de ruido sistemático se filtran inherentemente por la simetría traslacional de la arquitectura cíclica.",
    "D": "La complejidad de contracción de redes tensoriales se reduce mediante condiciones de frontera periódicas que permiten representaciones de estados de producto matricial con dimensión de enlace escalando logarítmicamente en lugar de exponencialmente con la profundidad del circuito. Cuando las capas de puertas se repiten con período L, el formalismo de matriz de transferencia permite la simulación clásica exacta de valores esperados diagonalizando el operador de evolución de L pasos una vez y elevándolo a una potencia, evitando el crecimiento exponencial de dimensión de enlace que afecta la simulación de circuitos genéricos, lo cual resulta esencial para validar el rendimiento de algoritmos NISQ mediante evaluación comparativa clásica y depuración de implementaciones de hardware antes de desplegar en procesadores cuánticos.",
    "solution": "A"
  },
  {
    "id": 308,
    "question": "¿Por qué se considera la computación cuántica distribuida un enfoque escalable para algoritmos cuánticos, y qué desafíos introduce?",
    "A": "Las arquitecturas distribuidas proporcionan acceso a conteos totales de qubits vastamente mayores al federar múltiples procesadores cuánticos, permitiendo que instancias de problemas previamente intratables se vuelvan factibles. Sin embargo, la limitación fundamental es que esencialmente cada algoritmo cuántico conocido —desde la factorización de Shor hasta la búsqueda de Grover hasta solucionadores de valores propios variacionales— fue diseñado asumiendo conectividad todos-con-todos de qubits dentro de un dispositivo monolítico. Consecuentemente, casi cada algoritmo útil requiere rediseño arquitectónico completo para descomponer operaciones en puertas solo locales que nunca acoplen qubits residentes en diferentes nodos físicos, lo que aumenta dramáticamente la profundidad del circuito y a menudo elimina la ventaja cuántica completamente.",
    "B": "Los procesadores distribuidos ejecutan circuitos cuánticos en paralelo mediante segmentación temporal de operaciones a través de unidades de procesamiento cuántico independientes, multiplicando efectivamente el rendimiento computacional por el número de nodos. El desafío principal es mantener coherencia de fase a través de todos los procesadores mediante señales de reloj sincronizadas con precisión sub-nanosegundo, ya que incluso pequeños desajustes de sincronización entre nodos acumulan decoherencia que degrada la fidelidad de toda la computación distribuida.",
    "C": "Es escalable porque se agregan nodos en lugar de amontonar más qubits en un chip, aunque la corrección de errores a través de múltiples procesadores sigue siendo el problema más difícil de resolver ya que los códigos no fueron originalmente diseñados para sistemas espacialmente separados.",
    "D": "Las arquitecturas distribuidas permiten escalabilidad al agregar qubits a través de múltiples procesadores cuánticos físicamente separados en lugar de requerir todos los recursos computacionales dentro de un único dispositivo monolítico, que enfrenta límites fundamentales de fabricación. Sin embargo, el cuello de botella crítico emerge al establecer y mantener entrelazamiento de largo alcance entre qubits en diferentes nodos, ya que los algoritmos cuánticos típicamente requieren conectividad todos-con-todos. Comunicar estados cuánticos entre procesadores demanda ya sea qubits voladores a través de canales ópticos (introduciendo pérdida de fotones) o protocolos de intercambio de entrelazamiento (consumiendo puertas y tiempo adicionales), ambos degradando significativamente la fidelidad y profundidad del circuito.",
    "solution": "D"
  },
  {
    "id": 309,
    "question": "Al desarrollar algoritmos cuánticos para optimización real de portafolios financieros, un equipo de investigación debe equilibrar la ventaja cuántica teórica contra las limitaciones actuales de hardware. Están comparando diferentes enfoques: uno usa una implementación completamente tolerante a fallos de estimación de amplitud cuántica que requiere 10^6 qubits lógicos, otro emplea QAOA en dispositivos NISQ con ~100 qubits ruidosos, y un tercero propone un algoritmo variacional híbrido que descarga la mayor parte del cómputo clásicamente. El equipo necesita decidir qué enfoque es más viable para despliegue dentro de 3-5 años, considerando que las tasas de error actualmente están en 10^-3 por puerta y tiempos de coherencia alrededor de 100 microsegundos. ¿Qué factor es más crítico para determinar si algún enfoque cuántico superará métodos de optimización clásica como solucionadores de programación entera mixta para portafolios con 500-1000 activos?",
    "A": "La capacidad de codificar la matriz de covarianza completa en estados cuánticos sin aproximación, ya que cualquier preprocesamiento clásico que reduzca el tamaño del problema eliminará la ventaja cuántica antes de que el algoritmo siquiera se ejecute. Si se aplican técnicas de reducción de dimensionalidad como PCA o métodos de matriz dispersa para hacer el problema tratable para hardware cuántico, entonces el problema efectivo que se resuelve se vuelve suficientemente pequeño para que solucionadores clásicos lo manejen eficientemente.",
    "B": "La existencia de una cota inferior demostrada sobre la complejidad de algoritmos clásicos para esta clase de problema, estableciendo que ningún algoritmo clásico puede resolver optimización de portafolio con N activos más rápido que tiempo exponencial en el peor caso. Sin tal prueba de dureza, cualquier dificultad clásica observada podría simplemente reflejar limitaciones de heurísticas actuales en lugar de barreras computacionales fundamentales, significando que un algoritmo clásico suficientemente ingenioso podría emerger que iguale el rendimiento cuántico.",
    "C": "Si la profundidad del circuito QAOA escala mejor que O(n^2) con el conteo de activos, porque incluso con paralelismo cuántico, circuitos profundos en dispositivos NISQ decoherirán antes de producir resultados útiles dados tiempos de coherencia actuales de ~100 microsegundos y tiempos de puerta de ~100 nanosegundos, limitando circuitos prácticos a profundidades menores de 1000 puertas. La comparación con solucionadores clásicos debe tener en cuenta el tiempo real de reloj incluyendo ejecuciones repetidas de circuito para optimización de parámetros—típicamente miles de iteraciones—y la sobrecarga clásica de procesar resultados de medición, calcular gradientes y actualizar parámetros variacionales entre disparos, lo que puede dominar el tiempo total de ejecución y negar aceleraciones cuánticas teóricas si la razón profundidad-de-circuito-a-tamaño-de-problema se vuelve desfavorable.",
    "D": "Qué tan rápido la corrección de errores cuántica alcanza el umbral donde las tasas de error lógico caen por debajo de 10^-6, que es el mínimo necesario para aplicaciones financieras que requieren resultados precisos a seis decimales para cumplimiento regulatorio. Dado que los resultados de optimización de portafolio deben certificarse según estándares institucionales, cualquier tasa de error por encima de este umbral necesitará pasos de verificación clásica que consuman más tiempo del que ahorra el algoritmo cuántico.",
    "solution": "C"
  },
  {
    "id": 310,
    "question": "¿Qué es un error de diafonía en computación cuántica?",
    "A": "Un error de acoplamiento parásito donde pulsos de control de microondas destinados a impulsar transiciones en un qubit objetivo se filtran a través de desajustes de impedancia y límites de aislamiento de acopladores direccionales hacia resonadores de lectura de qubits adyacentes, creando desplazamientos AC Stark fuera de resonancia que rotan estados de qubits vecinos por ángulos no deseados proporcionales al cuadrado de la razón de desintonización. Este amontonamiento espectral en arquitecturas multiplexadas por frecuencia conduce a acumulación de fase condicional descrita por términos Hamiltonianos ZZ residuales ∝σᶻ⊗σᶻ, manifestándose como interacciones entrelazantes no intencionadas durante operaciones nominalmente de un solo qubit. Los errores correlacionados resultantes violan el supuesto de error independiente subyacente en la mayoría de códigos de corrección de errores cuánticos, requiriendo mitigación mediante conformación de pulsos, desacoplamiento dinámico u optimizaciones de compilador conscientes de diafonía.",
    "B": "Un error donde un qubit interactúa no intencionalmente con un qubit vecino a través de mecanismos de acoplamiento residual siempre activos como vías capacitivas o inductivas parásitas, conduciendo a cambios coherentes o incoherentes no deseados en su estado cuántico. Esta interacción parásita puede manifestarse como acumulación de fase condicional no deseada, términos de acoplamiento ZZ espurios en el Hamiltoniano, o filtración de pulsos de control destinados a un qubit hacia el espectro abarrotado de frecuencias de qubits adyacentes, degradando en última instancia fidelidades de puerta e introduciendo errores correlacionados que complican la corrección de errores.",
    "C": "Un mecanismo de acoplamiento coherente donde el intercambio resonante de energía entre qubits adyacentes ocurre mediante interacciones Jaynes-Cummings fijas mediadas por resonadores de línea de transmisión compartidos, implementando puertas iSWAP o √iSWAP no intencionadas durante períodos inactivos cuando los qubits están estacionados en sus frecuencias de interacción. Este acoplamiento de intercambio residual acumula fase condicional φ=∫J(t)dt sobre tiempos inactivos de qubit, donde J(t) representa la fuerza de acoplamiento dependiente del tiempo modulada por elementos acopladores sintonizables por flujo. La generación resultante de entrelazamiento entre qubits computacionales y espectadores crea fuga fuera del espacio de código protegido, manifestándose como rotaciones sistemáticas correlacionadas a través de múltiples qubits que no pueden corregirse mediante códigos estabilizadores estándar.",
    "D": "Un canal de decoherencia donde interferencia electromagnética de corrientes de polarización variantes en el tiempo en líneas de flujo superconductoras se acopla a Hamiltonianos de control de qubit mediante inductancia mutua, inyectando ruido 1/f de baja frecuencia que modula frecuencias de transición de qubit y causa desfase más allá de límites intrínsecos T₂*. Esta diafonía clásica se manifiesta cuando pulsos de corriente destinados a sintonizar la frecuencia de un qubit mediante su lazo SQUID generan flujo magnético que atraviesa lazos de qubits adyacentes, creando desplazamientos de frecuencia correlacionados que rotan estados de qubit de maneras dependientes del tiempo. La naturaleza estocástica de estas fluctuaciones de flujo introduce errores no Markovianos con tiempos de correlación comparables a duraciones de puerta, requiriendo caracterización mediante protocolos de evaluación comparativa aleatoria intercalada que miden fidelidades de puerta de dos qubits condicionadas a operaciones simultáneas de un solo qubit.",
    "solution": "B"
  },
  {
    "id": 311,
    "question": "Considere un algoritmo de mapeo de qubits consciente del ruido diseñado para optimizar las fidelidades de las puertas en un procesador superconductor de 127 qubits con características T1 y T2 que varían en el tiempo. El algoritmo utiliza datos históricos de calibración para predecir asignaciones óptimas de qubits para un circuito dado. ¿Cuál de los siguientes escenarios degradaría más significativamente el rendimiento del algoritmo y por qué?",
    "A": "Un aumento en el número total de qubits físicos disponibles en el procesador, lo cual amplía el espacio de búsqueda pero proporciona más opciones de alta fidelidad para puertas de dos qubits críticas — Aunque la complejidad combinatoria del mapeo de qubits escala exponencialmente con el tamaño del procesador, los algoritmos heurísticos modernos que utilizan búsqueda guiada por aprendizaje automático u optimización genética pueden navegar eficientemente espacios de solución más grandes explotando patrones de localidad en circuitos cuánticos típicos.",
    "B": "Circuitos que consisten enteramente en puertas de Clifford, ya que estas tienen reglas de descomposición bien establecidas y el problema de mapeo se reduce a isomorfismo de grafos con heurísticas de tiempo polinomial conocidas — La estructura algebraica especial de las operaciones de Clifford les permite ser simuladas eficientemente usando el formalismo de estabilizadores, lo cual proporciona modelos exactos de propagación de errores que el algoritmo de mapeo puede explotar durante la optimización.",
    "C": "Características de ruido del hardware que fluctúan en escalas de tiempo comparables o más rápidas que el tiempo de ejecución del circuito, haciendo que los datos históricos de calibración sean poco confiables para predecir el rendimiento actual del dispositivo — Cuando los tiempos de coherencia de los qubits, las fidelidades de las puertas y los errores de lectura varían significativamente durante o entre ejecuciones de circuitos, las predicciones del algoritmo de mapeo basadas en mediciones pasadas se vuelven inexactas, llevando a asignaciones de qubits subóptimas que no logran evitar las regiones actualmente degradadas del procesador y resultando en tasas de error generales más altas de las que lograrían estrategias adaptativas en tiempo real alternativas.",
    "D": "Integración con métodos de contracción de redes tensoriales para segmentación de circuitos, lo cual puede reducir la profundidad efectiva del circuito pero introduce sobrecarga clásica adicional en el pipeline de compilación — Cuando el algoritmo de mapeo se integra con estrategias de descomposición de redes tensoriales que particionan circuitos grandes en subcircuitos más pequeños, ahora debe optimizar las asignaciones de qubits no solo para una única ejecución monolítica sino a través de múltiples segmentos que pueden tener preferencias de colocación conflictivas.",
    "solution": "C"
  },
  {
    "id": 312,
    "question": "¿Cuál es la importancia de las condiciones de Knill-Laflamme en la teoría de corrección de errores cuánticos?",
    "A": "Definen los requisitos mínimos de energía para implementar corrección de errores cuánticos cuantificando el costo termodinámico de revertir procesos de decoherencia.",
    "B": "Establecen un límite superior en el número de qubits físicos necesarios para cualquier código de corrección de errores cuánticos, que depende de la distancia del código y el número de qubits lógicos que están siendo protegidos de la decoherencia ambiental.",
    "C": "Demuestran que estados arbitrarios desconocidos no pueden ser clonados, lo que significa que la corrección de errores cuánticos debe funcionar de manera diferente a los esquemas clásicos de redundancia. Las condiciones formalizan esta restricción de no-clonación mostrando que cualquier intento de copiar información cuántica para detección de errores necesariamente perturba el estado que está siendo protegido.",
    "D": "Son necesarias y suficientes para corregir un conjunto de errores dado — estas condiciones proporcionan la caracterización matemática completa de cuándo un código cuántico puede detectar y corregir exitosamente errores específicos sin perturbar la información lógica codificada. Específicamente, establecen que un código C puede corregir errores en el conjunto E si y solo si los elementos de matriz ⟨i|E†_a E_b|j⟩ son independientes de los estados base del código |i⟩, |j⟩ para todos los operadores de error E_a, E_b en E. Este criterio captura elegantemente el requisito de que los síndromes de error deben ser extraíbles sin aprender nada sobre la información cuántica protegida en sí, proporcionando tanto una prueba práctica para la viabilidad del código como un fundamento teórico para diseñar nuevos esquemas de corrección de errores a través de modelos de error arbitrarios.",
    "solution": "D"
  },
  {
    "id": 313,
    "question": "En un protocolo de distribución cuántica de claves independiente del dispositivo operando sobre un canal con pérdidas con eficiencia de detección η = 0.82 y valor CHSH observado S = 2.31, sospecha que el tamaño finito del bloque (n = 10^6 rondas) está limitando su tasa de clave segura. La tasa de clave bruta antes de la amplificación de privacidad es 1.2 × 10^5 bits. ¿Qué técnica específica aborda más efectivamente los efectos de clave finita en este régimen para maximizar la longitud de clave segura extraíble?",
    "A": "Funciones hash universales para amplificación de privacidad, ya que son extractores demostrablemente óptimos para post-procesamiento clásico independientemente del tamaño del bloque y pueden demostrarse mediante el lema hash residual que extraen esencialmente toda la min-entropía disponible de la clave bruta incluso cuando n es relativamente modesto.",
    "B": "Técnicas de estimación de min-entropía, que proporcionan mejores límites sobre la información del adversario cuando se tienen estadísticas limitadas usando desigualdades de concentración específicamente adaptadas a correlaciones cuánticas en lugar de límites clásicos del peor caso. Técnicas avanzadas como el teorema de acumulación de entropía permiten rastrear la min-entropía en base por ronda y agregarla de una manera mucho menos pesimista que aplicar límites de Hoeffding al bloque completo, típicamente recuperando 40-60% del material de clave que se perdería por correcciones de tamaño finito excesivamente conservadoras.",
    "C": "Marcos de seguridad componible que proporcionan límites de tamaño finito ajustados sobre la desviación de la seguridad ideal, permitiendo calcular parámetros precisos de corrección y secreto como funciones de n y probabilidad de fallo. Estos marcos emplean desigualdades de concentración optimizadas para correlaciones cuánticas para estimar los intervalos de confianza alrededor de estadísticas observadas como el valor CHSH, luego propagan estas incertidumbres a través de la prueba de seguridad para determinar cuánta clave debe sacrificarse para amplificación de privacidad. Al usar límites de cola más ajustados específicos para violaciones de desigualdades de Bell en lugar de desigualdades de Hoeffding genéricas, los marcos componibles recuperan significativamente más clave segura que análisis asintóticos.",
    "D": "Generación de números aleatorios cuánticos para expandir el material de clave bruta antes de la amplificación de privacidad, aumentando efectivamente el tamaño de muestra artificialmente usando una fuente de entropía cuántica certificada para generar aleatoriedad independiente adicional que puede combinarse con XOR con los bits de clave bruta. Esta técnica, a veces llamada expansión de aleatoriedad cuántica, permite arrancar desde la clave bruta relativamente pequeña a un conjunto mucho mayor de bits aleatorios de alta calidad que parecen estadísticamente independientes de cualquier artefacto de tamaño finito en los datos originales de DIQKD.",
    "solution": "C"
  },
  {
    "id": 314,
    "question": "¿Por qué se consideran soluciones basadas en hardware para el post-procesamiento de Distribución Cuántica de Claves (QKD)?",
    "A": "Reducción de latencia para protocolos de variable continua — aceleradores de hardware dedicados (FPGAs, ASICs personalizados) permiten reconciliación en tiempo real de modulación Gaussiana mediante decodificación paralela de síndromes de códigos LDPC multidimensionales, procesando mediciones de cuadratura a tasas (gigamuestras por segundo) que implementaciones de software no pueden sostener, lo cual es esencial porque los sistemas CV-QKD generan datos Gaussianos correlacionados que requieren reconciliación inmediata antes de que se acumulen efectos de decoherencia, haciendo necesarias las soluciones de hardware para mantener el flujo continuo de claves requerido para comunicaciones seguras de alto ancho de banda sin introducir retrasos de procesamiento que comprometerían la sincronización.",
    "B": "Rendimiento computacional y eficiencia energética — aceleradores de hardware dedicados (FPGAs, ASICs) pueden ejecutar los protocolos de reconciliación de información y amplificación de privacidad órdenes de magnitud más rápido que procesadores de propósito general mientras consumen menos energía por bit procesado, lo cual es crítico porque sistemas QKD de alta velocidad generan material de clave bruta a tasas (megabits por segundo) que abruman implementaciones de software, creando cuellos de botella que de otro modo limitarían la tasa práctica de generación de clave segura y harían inviable el post-procesamiento en tiempo real.",
    "C": "Seguridad mejorada mediante procesamiento físicamente aislado — módulos de hardware con diseño aislado de red previenen fugas por canal lateral durante la amplificación de privacidad aislando la etapa de extracción de aleatoriedad de sistemas conectados a la red, asegurando que valores intermedios de funciones hash universales nunca residan en memoria de propósito general donde ataques de temporización de caché o vulnerabilidades de ejecución especulativa podrían exponer información parcial de la clave. Esta separación física es crítica porque el post-procesamiento implica manipular la clave tamizada bruta antes de la compresión final, creando ventanas donde canales laterales computacionales podrían teóricamente filtrar información a adversarios con acceso físico a la infraestructura clásica que soporta el enlace QKD.",
    "D": "Temporización determinista para pruebas de seguridad componible — implementaciones de hardware proporcionan ejecución precisa en ciclos del protocolo de corrección de errores en cascada, asegurando que el número real de intercambios de paridad coincida con el análisis teórico usado en límites de seguridad de clave finita, lo cual es crítico porque los marcos de seguridad componible requieren contabilidad precisa de la información revelada durante la reconciliación. Implementaciones de software introducen latencia variable y rutas de ejecución no deterministas que crean incertidumbre en el número exacto de bits divulgados, forzando estimaciones conservadoras que reducen la tasa de clave segura final por debajo de lo que el QBER medido teóricamente soportaría con características de temporización garantizadas.",
    "solution": "B"
  },
  {
    "id": 315,
    "question": "¿Cómo se desempeñan las HQNNs en comparación con modelos clásicos como TF-IDF y LSTM en tareas de coincidencia de entidades?",
    "A": "Las redes neuronales cuántico-híbridas logran precisión comparable a líneas base clásicas mientras utilizan significativamente menos parámetros entrenables — típicamente requiriendo solo 20-40% del conteo de parámetros necesarios por arquitecturas LSTM equivalentes para alcanzar puntuaciones F1 similares en benchmarks estándar de coincidencia de entidades, demostrando eficiencia de parámetros superior que se traduce en convergencia de entrenamiento más rápida y reducción de sobreajuste en conjuntos de datos más pequeños.",
    "B": "Las redes neuronales cuántico-híbridas demuestran eficiencia de parámetros relativa a líneas base clásicas, requiriendo aproximadamente 60-80% de los parámetros necesarios por arquitecturas LSTM para lograr puntuaciones F1 ligeramente inferiores — esta ventaja proviene de mapas de características cuánticas que codifican correlaciones no lineales implícitamente, aunque la brecha de rendimiento absoluto permanece dentro de 1-2% en benchmarks estándar, sugiriendo que la reducción de parámetros viene al costo de pérdidas menores de capacidad representacional que se vuelven insignificantes solo en conjuntos de datos de coincidencia de entidades altamente estructurados.",
    "C": "Las arquitecturas cuántico-híbridas igualan la precisión de líneas base clásicas mientras usan menos parámetros, típicamente 30-50% de conteos LSTM equivalentes — sin embargo, esta eficiencia se manifiesta principalmente durante la inferencia en lugar del entrenamiento, ya que los gradientes de parámetros cuánticos requieren significativamente más disparos de medición por paso de actualización para lograr varianza de estimación de gradiente comparable, resultando en tiempos de entrenamiento de reloj de pared más largos a pesar del conteo reducido de parámetros y haciendo que las ganancias prácticas de eficiencia dependan de las capacidades de tasa de disparos del hardware.",
    "D": "Las redes neuronales cuántico-híbridas logran puntuaciones F1 comparables a líneas base LSTM mientras utilizan 25-45% menos parámetros entrenables — pero esta eficiencia de parámetros deriva principalmente de las capas de embedding clásicas en lugar de componentes cuánticos, ya que los circuitos cuánticos variacionales contribuyen mejoras de expresividad insignificantes sobre características de Fourier aleatorias cuando la profundidad del circuito permanece por debajo del umbral de entrelazamiento requerido para ventaja cuántica genuina, haciendo que la eficiencia observada sea una consecuencia de reducción agresiva de dimensionalidad en la arquitectura híbrida en lugar de beneficios computacionales cuánticos.",
    "solution": "A"
  },
  {
    "id": 316,
    "question": "¿Qué característica única proporcionaría un Protocolo de Red Privada Virtual Cuántica?",
    "A": "Una VPN cuántica establecería seguridad teórica de la información para los puntos extremos del túnel mediante distribución cuántica de claves de variables continuas ejecutándose sobre la misma infraestructura de fibra óptica que el tráfico clásico de internet, con garantías de seguridad derivadas de las relaciones de incertidumbre de Heisenberg que impiden la medición simultánea precisa de observables de cuadratura conjugados. A diferencia de las VPN convencionales cuya seguridad depende de suposiciones computacionales sobre problemas de logaritmo discreto o curva elíptica, la confidencialidad del protocolo cuántico permanece demostrablemente segura contra adversarios con recursos computacionales arbitrarios porque los intentos de espionaje necesariamente introducen errores de desplazamiento en el espacio de fase detectables mediante estadísticas de medición homodina. Sin embargo, la implementación práctica requiere que ambos puntos extremos del túnel posean canales clásicos autenticados establecidos mediante secretos precompartidos o autoridades de certificación confiables—sin esta capa de autenticación, el protocolo cuántico no puede prevenir ataques de intermediario donde un adversario establece sesiones QKD independientes con cada punto extremo mientras se hace pasar por ellos ante el otro.",
    "B": "Garantías de seguridad fundamentadas en las leyes fundamentales de la física cuántica en lugar de suposiciones de dureza computacional, lo que significa que la confidencialidad del protocolo permanece demostrablemente segura incluso contra adversarios con recursos computacionales clásicos o cuánticos ilimitados. A diferencia de las VPN convencionales que dependen de problemas como la factorización de enteros o logaritmos discretos que pueden ser vulnerables a futuros avances algorítmicos o computadoras cuánticas ejecutando el algoritmo de Shor, una VPN cuántica aprovecha principios físicos como la perturbación por medición y el teorema de no clonación para detectar intentos de espionaje con certeza teórica de la información. Esto proporciona seguridad incondicional a largo plazo para transmisiones de datos sensibles, eliminando la necesidad de confiar en que ciertos problemas matemáticos permanecerán intratables a medida que las capacidades computacionales avancen durante décadas.",
    "C": "El protocolo de VPN cuántica implementaría verificación de seguridad independiente del dispositivo donde los puntos extremos comunicantes no necesitan confiar en sus propias implementaciones de hardware cuántico, usando violaciones de desigualdades de Bell para certificar la presencia de correlaciones cuánticas genuinas inmunes a manipulación del equipo. Esto aborda una vulnerabilidad crítica en arquitecturas de VPN clásicas donde aceleradores criptográficos comprometidos o generadores de números aleatorios con puertas traseras pueden filtrar silenciosamente material de clave. El protocolo opera haciendo que los puntos extremos compartan pares de fotones entrelazados y realicen mediciones separadas espacialmente cuyas correlaciones estadísticas acotan la información accesible a cualquier espía mediante la desigualdad CHSH. La seguridad está garantizada por la violación observable de límites de correlación clásicos en lugar de suposiciones sobre el comportamiento del dispositivo, proporcionando protección incluso cuando el hardware cuántico es fabricado por proveedores potencialmente adversarios. Sin embargo, el protocolo requiere un canal clásico autenticado confiable para el paso final de amplificación de privacidad.",
    "D": "Los protocolos de VPN cuántica proporcionarían garantías de secreto hacia adelante fortalecidas por la irreversibilidad física de las mediciones cuánticas: una vez que el material de clave se genera mediante distribución cuántica de claves y se usa para cifrar una sesión VPN, un adversario que posteriormente comprometa un punto extremo no puede descifrar retroactivamente sesiones pasadas porque los estados cuánticos que generaron esas claves han colapsado irreversiblemente por medición y ya no existen en ningún sistema físico. Esto contrasta con las VPN clásicas usando intercambio de claves efímero Diffie-Hellman, donde el secreto hacia adelante depende de la suposición computacional de que los logaritmos discretos permanecen difíciles—si esta suposición falla en el futuro (por ejemplo, mediante algoritmos cuánticos o avances matemáticos), las transcripciones de sesión almacenadas se vuelven vulnerables. La seguridad del protocolo cuántico en cambio se basa en el teorema de no clonación que impide que el adversario haya retenido copias perfectas de los estados cuánticos medidos, proporcionando secreto hacia adelante teórico de la información que permanece válido independientemente de capacidades computacionales futuras.",
    "solution": "B"
  },
  {
    "id": 317,
    "question": "La distinción de elementos permanece difícil para computadoras cuánticas en el peor caso porque:",
    "A": "El ordenamiento adversario de entrada puede forzar cualquier algoritmo cuántico a un régimen donde la amplificación de amplitud falla en distinguir patrones de colisión de fluctuaciones aleatorias, requiriendo pasos de verificación clásica que dominan el tiempo de ejecución",
    "B": "La detección de colisiones requiere comparar eventualmente todos los pares, y aunque los algoritmos de caminata cuántica encuentran colisiones en O(N^(2/3)) consultas, la estructura de subgrupo oculto necesaria para mejor aceleración no existe para problemas de colisión arbitrarios",
    "C": "No existe estructura conocida para explotar más allá de las colisiones mismas, lo que significa que los algoritmos cuánticos no pueden aprovechar patrones específicos del problema o regularidades matemáticas",
    "D": "El oráculo de comparación de elementos debe preservar reversibilidad mientras revela información de colisión, creando un compromiso fundamental donde las técnicas de retroceso de fase solo pueden extraer O(√N) bits de datos de colisión por superposición de consulta",
    "solution": "C"
  },
  {
    "id": 318,
    "question": "En códigos de superficie estándar de distancia 3, cada generador de estabilizador requiere cuatro puertas de dos qubits para medir (un CNOT por qubit de datos en el soporte del generador). Los códigos de distancia 5 ingenuamente requieren medir estabilizadores de peso 5 con cinco CNOTs cada uno, pero los esquemas basados en banderas logran menores conteos de puertas. Comparados con circuitos de síndrome estándar, los esquemas basados en banderas para códigos de distancia 5 reducen el conteo de puertas de dos qubits principalmente haciendo qué?",
    "A": "Empleando modos ancilla de variables continuas implementados en cavidades de microondas de alto factor de calidad que pueden absorber errores correlacionados de múltiples qubits mediante protocolos de corrección de errores bosónicos basados en codificaciones GKP.",
    "B": "Codificando valores propios de estabilizadores directamente en desplazamientos de frecuencia de qubit protegidos mediante técnicas de ingeniería hamiltoniana cuidadosamente diseñadas que mapean expectativas de operadores de Pauli a divisiones de energía medibles, luego usando solo rotaciones de un solo qubit y pulsos de microondas resonantes para leerlos espectroscópicamente, lo que elimina completamente la necesidad de interacciones explícitas CZ o CNOT de dos qubits durante rondas de extracción de síndrome mientras se preserva la información completa del estabilizador.",
    "C": "Reemplazando muchas de las puertas entrelazantes estándar con correcciones feedforward puramente clásicas que se derivan de analizar patrones en resultados de medición repetidos a través de múltiples rondas de extracción de síndrome.",
    "D": "Un solo qubit ancilla monitorea simultáneamente múltiples ubicaciones de falla del generador de estabilizador—cuando esta ancilla bandera se activa, sabes que ocurrió un error dañino, permitiéndote usar menos puertas mientras se mantiene la tolerancia a fallos mediante protocolos de remedición condicional que se activan solo cuando las banderas indican potencial propagación de error de peso 2 desde fallos de puerta única.",
    "solution": "D"
  },
  {
    "id": 319,
    "question": "¿En qué difiere fundamentalmente el concepto de capacidad de canal cuántico de su contraparte clásica en teoría de la información?",
    "A": "La capacidad cuántica exhibe no aditividad debido al entrelazamiento entre usos del canal: la información coherente (fórmula de capacidad cuántica) puede aumentar superlinealmente cuando los canales se usan conjuntamente en lugar de independientemente. Esto contrasta con la información mutua clásica, que es siempre aditiva porque las correlaciones clásicas obedecen la desigualdad de procesamiento de datos sin mejora de recursos cuánticos compartidos. Sin embargo, probar la superaditividad requiere construir códigos explícitos que exploten este efecto, lo que permanece como un problema abierto para la mayoría de canales más allá de contraejemplos especializados como el canal despolarizante combinado con canales de borrado.",
    "B": "No aditividad: la capacidad para múltiples usos puede exceder la suma de capacidades individuales. A diferencia de la capacidad de Shannon clásica donde el uso conjunto de n canales produce exactamente n veces la capacidad de uso único, los canales cuánticos exhiben superaditividad debido a protocolos asistidos por entrelazamiento que desbloquean correlaciones no disponibles para codificaciones de estado producto.",
    "C": "Los canales cuánticos soportan múltiples nociones de capacidad distintas (capacidad clásica, capacidad cuántica, capacidad clásica asistida por entrelazamiento) que pueden diferir arbitrariamente, mientras que los canales clásicos tienen una capacidad única dada por la información mutua del canal maximizada sobre distribuciones de entrada. La capacidad cuántica Q requiere optimizar la información coherente I(A⟩B) = S(B) - S(AB), que puede ser negativa para canales degradables donde el ambiente aprende más que el receptor, forzando Q = 0 a pesar de capacidad clásica no cero. Los efectos de tamaño finito aparecen como correcciones O(√log N/N) de medidas de información cuántica de un solo disparo en lugar de la concentración O(1/N) que logran los códigos clásicos.",
    "D": "Los canales cuánticos exhiben capacidad dependiente de medición donde los resultados dependen de la elección de base de medición del receptor, a diferencia de canales clásicos con transmisión de información independiente de la base. El límite de Holevo χ ≤ S(ρ) - ΣᵢpᵢS(ρᵢ) muestra que la información accesible es siempre menor que la entropía de von Neumann transmitida, creando una brecha fundamental entre capacidad cuántica y clásica igual a la discordia cuántica del conjunto del codificador. Esta brecha desaparece solo para estados conmutantes donde [ρᵢ, ρⱼ] = 0, causando que los canales cuánticos se reduzcan a clásicos cuando todos los estados transmitidos son simultáneamente diagonalizables en una base propia compartida.",
    "solution": "B"
  },
  {
    "id": 320,
    "question": "En escenarios de aprendizaje automático cuántico federado que involucran múltiples partes no confiables que deben entrenar colaborativamente un modelo sin revelar sus conjuntos de datos cuánticos individuales, ¿qué enfoque proporciona las garantías de seguridad teórica más fuertes mientras mantiene viabilidad computacional para dispositivos cuánticos de corto plazo?",
    "A": "Mecanismos de privacidad diferencial que añaden ruido cuántico cuidadosamente calibrado a las actualizaciones de gradiente en cada ronda de aprendizaje federado, lo que proporciona un límite matemáticamente riguroso sobre la filtración de información pero puede degradar sustancialmente la precisión del modelo en espacios de parámetros de alta dimensión donde la magnitud del ruido requerido para privacidad crece con el número de parámetros.",
    "B": "Las pruebas cuánticas de conocimiento cero permiten que cada parte participante demuestre criptográficamente que su conjunto de datos cuántico local satisface ciertas propiedades y que sus contribuciones de gradiente fueron computadas correctamente según la función de pérdida acordada, sin revelar ninguna información sobre los estados cuánticos reales en su conjunto de datos, aunque los pasos de generación y verificación de prueba introducen una sobrecarga computacional significativa que puede ser prohibitiva para dispositivos NISQ actuales.",
    "C": "Al adaptar el cifrado totalmente homomórfico al entorno cuántico, cada parte puede cifrar su conjunto de datos cuántico local y cálculos de gradiente usando un esquema de cifrado compatible con lo cuántico que permite que puertas cuánticas arbitrarias se apliquen directamente a estados cuánticos cifrados, con gradientes cifrados agregados en un servidor central sin descifrado, aunque implementar operaciones cuánticas homomórficas tolerantes a fallos requiere sobrecargas de corrección de errores que exceden las capacidades de corto plazo.",
    "D": "Protocolos de computación segura multipartita",
    "solution": "D"
  },
  {
    "id": 321,
    "question": "¿Qué implica la \"universalidad adiabática\" para las computadoras cuánticas adiabáticas?",
    "A": "Que cualquier computación cuántica puede incorporarse en la evolución del estado fundamental codificando puertas lógicas como pasajes adiabáticos entre subespacios degenerados de Hamiltonianos intermedios H(s), donde la condición adiabática asegura que las transiciones diabáticas permanezcan exponencialmente suprimidas y el estado fundamental final codifique la salida del circuito con sobrecarga polinomial en qubits auxiliares y tiempo total de evolución.",
    "B": "Que cualquier algoritmo cuántico basado en puertas puede simularse codificando la computación en la evolución del estado fundamental de un Hamiltoniano dependiente del tiempo H(t), con solo una sobrecarga polinomial en el número de operaciones comparado con el modelo de circuitos.",
    "C": "Que cualquier problema de optimización puede resolverse en tiempo polinomial construyendo un Hamiltoniano cuyo estado fundamental codifique la solución, siempre que el programa de interpolación respete la condición adiabática que requiere que el tiempo de evolución exceda el cuadrado del inverso de la brecha espectral mínima—la universalidad asegura que el escalado de la brecha es como máximo polinomial en el tamaño del problema para todas las instancias en BQP.",
    "D": "Que la simulación clásica de algoritmos adiabáticos es eficientemente alcanzable usando muestreo de integrales de camino de Monte Carlo sobre Hamiltonianos interpolantes H(s), porque la universalidad implica que la evolución permanece dentro de una variedad de tamaño polinomial de estados de bajo entrelazamiento y el solapamiento del estado fundamental con estados producto permanece acotado inferiormente por un polinomio inverso en el tamaño del sistema, habilitando métodos de trayectorias cuasi-clásicas para aproximar la dinámica de recocido cuántico.",
    "solution": "B"
  },
  {
    "id": 322,
    "question": "En el contexto de la corrección cuántica de errores aproximada, ¿cómo debe modificarse la condición de Knill-Laflamme?",
    "A": "La condición estricta de Knill-Laflamme PE†ᵢEⱼP = αᵢⱼP requiere que los operadores de error mapeen el espacio código a subespacios mutuamente ortogonales con constantes de proporcionalidad αᵢⱼ que son puramente reales e independientes de la distancia, pero para QEC aproximada esto se relaja para permitir coeficientes de valores complejos PE†ᵢEⱼP = αᵢⱼP donde Im(αᵢⱼ) ≤ ε, permitiendo pequeños componentes imaginarios que rompen la hermiticidad del canal de error mientras mantienen la distinguibilidad del síndrome, siempre que la fase acumulada durante la detección de errores permanezca acotada por debajo de π/4, lo que asegura que la fidelidad de recuperación exceda 1 - 2ε² para eventos de error único.",
    "B": "La condición estricta de Knill-Laflamme PE†ᵢEⱼP = αᵢⱼP requiere que la extracción de síndrome distinga perfectamente todos los pares de errores corregibles a través de proyecciones ortogonales, pero para QEC aproximada este requisito de ortogonalidad se debilita a PE†ᵢEⱼP = αᵢⱼP + βᵢⱼQ donde Q proyecta sobre el complemento ortogonal del espacio código y ||βᵢⱼ|| ≤ ε, permitiendo pequeños componentes de fuga que acoplan el espacio código a estados de mayor energía durante la corrección de errores, siempre que la probabilidad total de fuga permanezca por debajo del pseudo-umbral del código determinado por la relación del tiempo de medición del síndrome a T₁.",
    "C": "La condición estricta de ortogonalidad de Knill-Laflamme PE†ᵢEⱼP = αᵢⱼP, que requiere que los operadores de error mapeen el espacio código a subespacios mutuamente ortogonales con constantes de proporcionalidad exactas, se relaja a PE†ᵢEⱼP ≈ αᵢⱼP donde los coeficientes hermitianos αᵢⱼ solo necesitan satisfacer igualdad aproximada dentro de una tolerancia de error especificada ε, permitiendo que espacios código que casi satisfacen los criterios de corrección de errores todavía logren supresión de tasas de error lógico proporcionales al cuadrado de la tasa de error físico, siempre que las desviaciones de la ortogonalidad exacta permanezcan acotadas por debajo de un umbral dependiente de la distancia que escala con el peso mínimo del código.",
    "D": "La condición estricta de Knill-Laflamme PE†ᵢEⱼP = αᵢⱼP, que exige proporcionalidad exacta para todos los pares de operadores de error dentro del conjunto corregible, se modifica a PE†ᵢEⱼP = αᵢⱼP + δᵢⱼ donde δᵢⱼ representa perturbaciones acotadas que satisfacen ||δᵢⱼ|| ≤ ε/d² con d siendo la distancia del código, pero críticamente las constantes de proporcionalidad αᵢⱼ deben permanecer exactamente idénticas (αᵢⱼ = α para todo i,j) para preservar la operación de recuperación universal, mientras que relajar esta restricción de recuperación universal eliminaría la capacidad del código de corregir errores arbitrarios dentro del conjunto corregible, incluso aproximadamente.",
    "solution": "C"
  },
  {
    "id": 323,
    "question": "¿Qué vulnerabilidad específica explota un ataque de reordenamiento cuántico?",
    "A": "Variaciones temporales en cuándo se aplican los operadores de medición relativo a la línea temporal de decoherencia de qubits individuales, explotando el hecho de que el colapso de la medición no es instantáneo a nivel de hardware. Al cronometrar cuidadosamente los pulsos de medición para que ocurran durante estados transitorios o inmediatamente después de operaciones de puerta específicas, un adversario puede sesgar los resultados de medición hacia valores propios particulares, efectivamente realizando un ataque de canal lateral a través de la manipulación controlada de la retroacción de la medición sobre el estado cuántico.",
    "B": "La sobrecarga computacional introducida por los códigos de corrección de errores, que crea ventanas temporales durante los ciclos de extracción de síndrome donde secuencias de puertas adversarias pueden insertarse sin detección. Al explotar la latencia entre las mediciones de estabilizador y la aplicación de corrección, los atacantes pueden inyectar operaciones maliciosas que parecen ser parte del protocolo normal de corrección de errores.",
    "C": "Las restricciones implícitas de ordenamiento impuestas por las relaciones de conmutación de puertas y las dependencias causales entre operaciones, donde reordenar puertas no conmutativas puede alterar los resultados de medición. Un atacante manipula el planificador para permutar puertas de maneras que preservan la estructura superficial del circuito pero violan la secuencia de operadores prevista del circuito cuántico, llevando a errores coherentes que se acumulan multiplicativamente a través de la profundidad del circuito sin activar mecanismos tradicionales de detección de errores.",
    "D": "Mapeos inconsistentes de qubits entre la representación del circuito lógico y la topología del hardware físico, donde la asignación de qubits del compilador falla en mantener asignaciones estables a través de diferentes pasadas de compilación o etapas de optimización. Esto crea oportunidades para que un atacante manipule la función de mapeo de tal manera que las puertas destinadas a un qubit físico sean redirigidas a otro, explotando la brecha entre las etiquetas abstractas de qubits y las direcciones concretas de hardware para inyectar operaciones que parecen válidas en la capa lógica pero se ejecutan en qubits no previstos.",
    "solution": "D"
  },
  {
    "id": 324,
    "question": "¿Qué mejora en el diseño de autenticación ayuda a frustrar ataques de repetición en canales clásicos de QKD basados en satélites sin una sobrecarga considerable de ancho de banda?",
    "A": "Números de secuencia basados en tiempo incorporados directamente en los metadatos de carga útil fotónica mediante codificación de polarización en los mismos fotones QKD. Cada mensaje autenticado lleva marcas de tiempo monotónicamente crecientes moduladas en modos de polarización ortogonales, fusionando autenticación y distribución de claves en una sola capa óptica.",
    "B": "Transmisiones de baliza RF fuera de banda que proporcionan valores de suma de verificación síncronos derivados de mediciones de turbulencia atmosférica, que son inherentemente impredecibles y compartidas entre la estación terrestre y el satélite. Al correlacionar parámetros ambientales con cada mensaje clásico, el sistema genera etiquetas frescas sin consumir material de clave QKD.",
    "C": "Firmas digitales de clave pública post-cuánticas basadas en retículos como Dilithium o Falcon aplicadas a cada paquete clásico, usando la clave privada del satélite con verificación en la estación terrestre. Estas firmas resistentes a la cuántica aseguran el no repudio y previenen la repetición con aproximadamente 1-3 kilobytes de sobrecarga por paquete.",
    "D": "MACs de hash universal sin estado encadenados mediante evolución de clave secreta, donde cada etiqueta de autenticación se genera usando una porción fresca de la clave secreta derivada de QKD combinada con funciones hash criptográficas. La construcción de hashing universal asegura que cada mensaje reciba una etiqueta única e impredecible que no puede ser reutilizada por un adversario intentando repetición. Al mantener un flujo de claves monótono sin requerir estado sincronizado entre satélite y estación terrestre más allá del secreto compartido, este enfoque logra autenticación fuerte con sobrecarga mínima—típicamente 128-256 bits por trama autenticada independientemente de la longitud del mensaje.",
    "solution": "D"
  },
  {
    "id": 325,
    "question": "¿Cuál es una desventaja principal de usar puertas altamente expresivas como la puerta B en cargas de trabajo cuánticas estándar?",
    "A": "Son excesivas para la mayoría de las operaciones, por lo que terminas con más puertas de las que requeriría una descomposición adaptada—la expresividad excesiva significa que estás usando un conjunto de puertas universal donde secuencias especializadas de puertas nativas (como Clifford+T) lograrían la misma operación lógica con menos recursos físicos y mejores características de error.",
    "B": "Son excesivas para la mayoría de las operaciones, por lo que terminas con conteos de puertas más altos de los que requeriría una descomposición adaptada—la expresividad excesiva significa que estás aplicando puertas de un espacio de parámetros continuo donde secuencias de puertas discretas (como Clifford+T) lograrían operaciones lógicas equivalentes con mejores propiedades de tolerancia a fallos, ya que los protocolos de destilación de estados mágicos están optimizados para conjuntos de puertas discretas y no pueden manejar eficientemente unitarios parametrizados continuamente, forzando al compilador a redondear parámetros de la puerta B a valores discretos cercanos y perdiendo la ventaja teórica de la universalidad continua.",
    "C": "Son excesivas para la mayoría de las operaciones, por lo que terminas con circuitos más profundos de los que requerirían descomposiciones optimizadas—la expresividad excesiva significa que estás usando puertas fuera de la jerarquía de Clifford donde actualizaciones especializadas de marco de Pauli y reglas de conmutación de puertas podrían reducir sustancialmente la profundidad del circuito. Dado que las puertas B no preservan la estructura de estabilizador, cada aplicación fuerza al compilador a romper la vía rápida de simulación de Clifford y recurrir al seguimiento de vector de estado de costo exponencial durante las pasadas de optimización, impidiendo que el compilador aplique optimizaciones estándar de mirilla que explotan la conjugación de Clifford para fusionar capas adyacentes.",
    "D": "Son excesivas para la mayoría de las operaciones, por lo que terminas con peor rendimiento limitado por coherencia del que lograrían secuencias de puertas dirigidas—la expresividad excesiva significa que estás implementando unitarios de la variedad completa SU(4) donde la descomposición de Cartan en secuencias mínimas de puertas nativas (como secuencias de puertas de resonancia cruzada con eco) se completaría más rápido y acumularía menos error de fase. Las puertas B requieren procedimientos de calibración más largos ya que su espacio de parámetros continuo hace impráctico pre-calibrar todas las instancias posibles, forzando generación de pulsos justo a tiempo que introduce latencia de compilación proporcional al grado de expresividad de la puerta.",
    "solution": "A"
  },
  {
    "id": 326,
    "question": "La búsqueda de triángulos en grafos dispersos sigue siendo un desafío para los paseos cuánticos porque:",
    "A": "Las matrices de adyacencia dispersas hacen que la brecha discriminante en el operador de paseo cuántico con moneda escale inversamente con el grado promedio, reduciendo la ventaja espectral efectiva de cuadrática a subcuadrática cuando la densidad del grafo disminuye por debajo del umbral de percolación. Esta degradación del tiempo de mezcla ocurre porque la separación de autovalores del operador de paseo depende de la conductancia del grafo, que disminuye en grafos dispersos donde los vecindarios locales se vuelven similares a árboles, impidiendo que la amplificación de amplitud alcance su aceleración cuadrática completa. Aunque los paseos cuánticos mantienen ventajas teóricas de complejidad de consultas en el modelo de oráculo al consultar solo O(n^(1.3)) aristas comparado con Ω(n^(1.5)) clásico, el tiempo de ejecución concreto sufre cuando las propiedades espectrales se degradan, haciendo que el enfoque cuántico sea menos atractivo para instancias dispersas a pesar de mantener superioridad asintótica.",
    "B": "El número de aristas potenciales ya es mucho menor en grafos dispersos en comparación con grafos densos, lo que significa que hay menos triángulos que encontrar y la reducción del espacio de búsqueda disminuye el ahorro absoluto de tiempo alcanzable mediante la aceleración cuántica incluso cuando se mantiene la ventaja cuadrática. Los algoritmos clásicos pueden explotar estructuras de datos específicas para dispersidad como listas de adyacencia para lograr un rendimiento casi óptimo que escala con el número de aristas reales en lugar de aristas potenciales, reduciendo la brecha entre enfoques clásicos y cuánticos. Aunque los paseos cuánticos todavía proporcionan ventajas asintóticas en el modelo de complejidad de consultas, las mejoras prácticas de tiempo real se vuelven marginales cuando el número de aristas es pequeño, haciendo que el enfoque cuántico sea menos convincente para instancias de grafos dispersos a pesar de su superioridad teórica en el análisis del peor caso.",
    "C": "Los grafos dispersos requieren implementaciones de paseos cuánticos usando técnicas de detección comprimida para representar las O(m) aristas eficientemente en memoria cuántica, donde m << n^2, pero el proceso de medición necesario para verificar la existencia de triángulos introduce decoherencia proporcional a la relación de compresión. Las arquitecturas QRAM estándar asumen codificaciones de grafos densos con Θ(n^2) celdas de memoria direccionables, creando sobrecarga cuando la mayoría de las entradas desaparecen, y los enfoques de hashing por cubetas para almacenar solo las aristas presentes no pueden consultarse coherentemente sin colapsar superposiciones mediante desreferenciación clásica de punteros. Esta tensión fundamental entre la representación dispersa eficiente en espacio y los patrones de acceso cuántico coherente limita la ventaja cuántica práctica, haciendo que el enfoque de paseo cuántico sea menos efectivo para instancias dispersas a pesar de mantener superioridad teórica de complejidad de consultas en modelos de oráculo idealizados.",
    "D": "Las cotas de complejidad de oráculo para la búsqueda de triángulos asumen que las consultas de aristas pueden realizarse en tiempo unitario, pero los oráculos de grafos dispersos necesariamente requieren tiempo de consulta Ω(log n) para especificar cuál de las m << n^2 aristas se está accediendo mediante direccionamiento binario de listas de adyacencia, multiplicando el costo efectivo de consulta por un factor logarítmico. Esta sobrecarga de direccionamiento erosiona la aceleración cuadrática del paseo cuántico de O(n^(1.3)) consultas de aristas a tiempo O(n^(1.3) log n) al considerar los costos de acceso a estructuras de datos dispersas, mientras que los algoritmos clásicos usando diseños de listas de adyacencia eficientes en caché experimentan factores logarítmicos menores debido a la localidad espacial. La brecha del modelo de complejidad entre consultas de aristas de costo unitario y patrones de acceso a memoria realistas perjudica particularmente a los enfoques cuánticos en regímenes dispersos donde la persecución de punteros domina el cómputo.",
    "solution": "B"
  },
  {
    "id": 327,
    "question": "Para medir un estabilizador de peso seis en hardware de conectividad limitada, los protocolos tolerantes a fallos típicamente lo descomponen en qué secuencia:",
    "A": "Una cascada estructurada en árbol usando cuatro puertas CNOT en la primera capa para acoplar seis qubits de datos en tres qubits ancilla intermedios, seguida por dos CNOTs para combinar esas ancillas en un bit síndrome final, reduciendo la profundidad del circuito a log(6) ≈ 3 capas a costa de requerir tres qubits ancilla en lugar de uno.",
    "B": "Una cadena secuencial de puertas CNOT de dos qubits entre un qubit ancilla y cada uno de los seis qubits de datos por turno, seguida por la medición de la ancilla para extraer la información de paridad.",
    "C": "Tres mediciones de peso dos realizadas en paralelo usando qubits ancilla separados, uno para cada par de qubits de datos, seguidas por XOR clásico de los tres bits síndrome para reconstruir la paridad de peso seis mientras se mantiene separación espacial para prevenir que los errores de gancho se propaguen entre circuitos de medición.",
    "D": "Dos mediciones secuenciales de estabilizador de peso tres donde la primera ancilla se acopla a los qubits de datos {1,2,3} y la segunda a {4,5,6}, con su producto determinando el autovalor de peso seis. Esta factorización mantiene la tolerancia a fallos porque los errores de gancho solo pueden propagar errores de datos de peso dos dentro de cada subconjunto en lugar de errores de peso tres a través del soporte completo.",
    "solution": "B"
  },
  {
    "id": 328,
    "question": "En el contexto del aprendizaje automático cuántico, ¿cuál es una característica del algoritmo HHL que limita su aplicabilidad práctica?",
    "A": "La salida es un estado cuántico representado como amplitudes en un espacio de Hilbert de alta dimensión en lugar de datos clásicos accesibles mediante lectura convencional. Extraer información clásica completa sobre este vector solución requeriría un número exponencial de mediciones para reconstruir todas las amplitudes con precisión razonable, negando la aceleración cuántica.",
    "B": "La aceleración exponencial se materializa solo para matrices estructuradas específicas, particularmente aquellas que son dispersas y bien condicionadas con propiedades espectrales favorables. Las matrices densas o sistemas con números de condición que escalan exponencialmente borran la ventaja cuántica, ya que el tiempo de ejecución del algoritmo depende polinomialmente del número de condición. Además, las matrices que surgen de la discretización de problemas continuos a menudo carecen de la estructura requerida, e incluso cuando existe estructura, verificar estas propiedades clásicamente puede requerir esfuerzo computacional comparable a resolver el sistema original.",
    "C": "El algoritmo requiere preparación eficiente del estado de entrada que codifica el vector del lado derecho, lo cual puede ser exponencialmente difícil para vectores de datos clásicos arbitrarios. Cargar n números clásicos en amplitudes de n qubits generalmente demanda tiempo lineal en 2^n, superando completamente cualquier aceleración cuántica. Aunque estructuras de datos especializadas o codificaciones específicas del problema a veces pueden prepararse eficientemente, como estados que representan funciones suaves o salidas de cómputos cuánticos previos, el cuello de botella de preparación del estado sigue siendo la limitación práctica dominante para la mayoría de los sistemas lineales del mundo real encontrados en aplicaciones de aprendizaje automático.",
    "D": "Todas las anteriores",
    "solution": "D"
  },
  {
    "id": 329,
    "question": "¿Qué es la \"información suave\" en el contexto de la corrección de errores cuánticos?",
    "A": "Datos de medición que incluyen estimaciones de confianza o probabilidad para cada resultado de síndrome, en lugar de valores binarios duros, proporcionando al decodificador información analógica sobre la fiabilidad de la medición que permite a los algoritmos de decodificación probabilística ponderar los bits síndrome según su fidelidad y distinguir entre detecciones de alta confianza y baja confianza al inferir la cadena de errores más probable.",
    "B": "Correlaciones de síndrome extraídas de mediciones repetidas de estabilizadores—específicamente, información obtenida al rastrear correlaciones temporales entre rondas consecutivas de síndrome para identificar eventos de detección persistentes versus transitorios. Al correlacionar bits síndrome a través de múltiples ciclos de medición, la información suave distingue violaciones genuinas de estabilizadores de errores de medición, produciendo datos de síndrome probabilísticos donde la estimación de fiabilidad de cada bit refleja su consistencia temporal. Esto permite al decodificador reducir el peso de eventos de detección aislados probablemente causados por fallos de medición en lugar de errores de datos.",
    "C": "Información de síndrome parcial de lectura de ancilla sin demolición—resultados de mediciones auxiliares obtenidos mediante protocolos cuánticos sin demolición que revelan autovalores de estabilizadores mientras preservan la coherencia del estado lógico. Debido a que estas mediciones extraen datos de síndrome sin colapsar completamente los estados ancilla, proporcionan bits síndrome probabilísticos con amplitudes analógicas correspondientes al grado de entrelazamiento entre qubits ancilla y de datos. El decodificador interpreta estas lecturas de valores continuos como información de síndrome ponderada por confianza, habilitando algoritmos de decodificación de decisión suave que consideran el colapso parcial inducido por la medición.",
    "D": "Estimaciones continuas de síndrome de operadores de medición parametrizados—resultados de mediciones obtenidos usando ángulos de lectura ajustables donde cada bit síndrome resulta de un observable de Pauli parametrizado en lugar de un generador de estabilizador fijo. Al ajustar ángulos de base de medición según tablas de consulta precalibradas, el decodificador recibe valores de síndrome modulados por estimaciones de fidelidad de lectura derivadas de configuraciones de parámetros de control. Esto produce información de síndrome suave donde las estadísticas de medición analógica codifican niveles de confianza determinados por el ángulo de rotación del operador de medición relativo a la base computacional.",
    "solution": "A"
  },
  {
    "id": 330,
    "question": "¿Por qué las puertas no conmutativas son esenciales para mantener la entrenabilidad en circuitos cuánticos en capas?",
    "A": "Las puertas no conmutativas previenen la cancelación de gradientes al asegurar que los cambios de parámetros se propaguen a través del circuito de manera que preserve la sensibilidad de los resultados de medición a las variaciones de parámetros. Cuando las puertas conmutan, el circuito puede reordenarse y simplificarse efectivamente, a menudo llevando a gradientes que desaparecen exponencialmente (mesetas áridas) porque el paisaje de parámetros se vuelve plano. La no conmutatividad mantiene la estructura rica e interdependiente del espacio de parámetros, permitiendo que información de gradiente significativa alcance la función de costo y habilitando la optimización efectiva de algoritmos cuánticos variacionales.",
    "B": "Las puertas no conmutativas aseguran que los gradientes de parámetros computados mediante la regla de desplazamiento de parámetros mantengan varianza finita al prevenir la formación de subcircuitos de Clifford que colapsarían el paisaje de la función de costo en una estructura constante por partes. Cuando puertas consecutivas conmutan, pueden fusionarse analíticamente mediante fusión de operadores, lo que reduce el número efectivo de parámetros independientes y causa que el vector gradiente se concentre en un subespacio de menor dimensión. Este colapso dimensional induce directamente fenómenos de meseta árida al crear magnitudes de derivada exponencialmente pequeñas. La no conmutatividad preserva la estructura de rango completo de la matriz de información de Fisher, manteniendo el número de condición necesario para optimización estable basada en gradientes en ansätze variacionales profundos.",
    "C": "Las secuencias de puertas no conmutativas previenen la interferencia destructiva de contribuciones de gradiente de diferentes regiones de parámetros al mantener corchetes de Lie no nulos entre capas sucesivas del circuito. Cuando las puertas conmutan, la representación adjunta del álgebra de Lie del circuito se vuelve abeliana, lo que fuerza a que todos los términos de gradiente de orden superior (computados mediante conmutadores anidados en la expansión de Baker-Campbell-Hausdorff) desaparezcan idénticamente. Esta eliminación de correcciones de orden superior causa que la función de costo desarrolle regiones exponencialmente planas conocidas como mesetas áridas. La no conmutatividad asegura que los conmutadores anidados permanezcan no triviales, permitiendo que el flujo de gradiente incorpore correlaciones multiparamétricas que codifican información geométrica sobre la curvatura del paisaje de costo.",
    "D": "Las puertas no conmutativas mantienen la entrenabilidad al asegurar que la dimensión efectiva del grupo unitario generado por capas parametrizadas escale exponencialmente con la profundidad del circuito en lugar de linealmente. Cuando las puertas conmutan, generan un subgrupo abeliano cuya dimensión es igual al número de parámetros, creando una variedad de solución restringida con medida que se aproxima a cero en el espacio SU(2^n) completo. Esta variedad restringida exhibe fenómenos de concentración de medida donde los gradientes de la función de costo desaparecen exponencialmente con el tamaño del sistema. La no conmutatividad rompe esta estructura abeliana, permitiendo que la familia unitaria parametrizada forme un grupo de Lie no abeliano cuyo volumen exponencialmente mayor previene la concentración de gradiente y preserva la expresividad necesaria para que los algoritmos de optimización encuentren soluciones no triviales.",
    "solution": "A"
  },
  {
    "id": 331,
    "question": "¿Qué restricciones enfrentan los autocodificadores cuánticos?",
    "A": "Los autocodificadores cuánticos requieren corrección de errores impecable a nivel de compuerta individual porque cualquier evento de decoherencia durante el circuito de codificación mezcla irrevocablemente la representación comprimida, sin posibilidad de recuperación mediante redundancia o técnicas clásicas de mitigación de errores, ya que incluso la interacción de un solo fotón disperso o una fluctuación térmica que cause un error de fase en un qubit se propaga a través de compuertas entrelazadoras para corromper el estado codificado completo.",
    "B": "Los autocodificadores cuánticos operan en un régimen puramente cuántico donde cualquier interfaz con datos clásicos viola fundamentalmente el teorema de no-clonación, haciendo imposible codificar información clásica en estados cuánticos sin destruir las propiedades de superposición requeridas para la compresión.",
    "C": "La dimensionalidad del espacio latente en autocodificadores cuánticos escala exponencialmente con el tamaño de entrada debido a la estructura de producto tensorial de los espacios de Hilbert multi-qubit, requiriendo 2^n qubits para codificar incluso conjuntos de datos modestos de n características clásicas, creando una sobrecarga paradójica donde comprimir un conjunto de datos clásico de 100 dimensiones demandaría más de 10^30 qubits solo para representar la capa de entrada del codificador.",
    "D": "Limitaciones de hardware como recuentos restringidos de qubits y conectividad, ruido ambiental por decoherencia y errores de compuerta, y la sobrecarga de códigos de corrección de errores cuánticos que inflan significativamente los requisitos de recursos para operación tolerante a fallos.",
    "solution": "D"
  },
  {
    "id": 332,
    "question": "¿Qué restricción de programación surge de las topologías de vecinos más cercanos cuando dos compuertas CX comparten un qubit común?",
    "A": "El qubit compartido debe ejecutar una secuencia de desacoplamiento dinámico entre interacciones sucesivas para suprimir el acoplamiento ZZ residual del acoplador ajustable. Cuando un qubit participa en compuertas CX consecutivas, el ruido de carga en el acoplador induce interacciones coherentes siempre activas que acumulan errores de fase proporcionales al tiempo de espera, requiriendo la inserción de pulsos de eco de Hahn que aumentan la separación efectiva entre compuertas de ~40ns a ~120ns para mantener fidelidad por encima del 99%.",
    "B": "El qubit compartido puede ejecutar solo una interacción de dos qubits a la vez, creando una restricción fundamental de serialización. Cuando un qubit participa en una compuerta CX, no puede participar simultáneamente en otra operación de dos qubits, forzando al programador a secuenciar estas compuertas temporalmente en lugar de ejecutarlas en paralelo.",
    "C": "La topología de acoplamiento impone una restricción de conmutación que requiere que la segunda CX anticonmute con la primera cuando comparten un qubit de control pero conmute cuando comparten un objetivo. Esto surge porque la activación simultánea de dos pulsos de flujo dirigidos al mismo qubit crea interferencia destructiva en el subespacio |11⟩ para pares que comparten control pero interferencia constructiva para pares que comparten objetivo, forzando al compilador a insertar compuertas identidad que rellenan el programa hasta que el papel del qubit compartido se invierta, típicamente agregando 2-3 capas de compuertas para mantener la estructura correcta del grupo estabilizador.",
    "D": "Ambas compuertas CX deben usar la misma orientación control-objetivo relativa al qubit compartido para mantener coherencia de fase de microondas a través de la secuencia de compuertas. Revertir la dirección requeriría reprogramar la referencia de fase del oscilador local durante la ejecución, introduciendo deriva de calibración que corrompe el ángulo de rotación condicional hasta 15°, por lo que el programador impone consistencia direccional serializando cualquier par donde el qubit compartido cambie entre roles de control y objetivo.",
    "solution": "B"
  },
  {
    "id": 333,
    "question": "En el contexto de esquemas de cifrado homomórfico cuántico que permiten computación sobre estados cuánticos cifrados, ¿qué vector de ataque representa la amenaza más severa para mantener privacidad computacional mientras se preserva la capacidad de realizar operaciones arbitrarias de compuertas sobre datos cifrados sin descifrado? Considere que el adversario tiene acceso a todas las salidas computacionales intermedias pero no a las claves de cifrado.",
    "A": "Realizando tomografía de estado cuántico sobre estados intermedios cifrados después de cada capa computacional, un adversario puede reconstruir la matriz de densidad completa de los datos cifrados y explotar correlaciones entre los valores de expectación de Pauli del texto plano y las estadísticas de medición del texto cifrado. Aunque las mediciones individuales parecen aleatorias, agregar millones de ejecuciones idénticas del circuito permite estimación de máxima verosimilitud para recuperar información estructural—como patrones de conectividad de qubits, relaciones de fase relativas y amplitudes de superposición—que colectivamente revelan hasta el 40% de la entropía del texto plano original a través de momentos estadísticos de orden superior.",
    "B": "Los protocolos de cifrado cuántico totalmente homomórfico requieren operaciones periódicas de re-cifrado (cambio de clave) después de acumular un número umbral de evaluaciones de compuertas para prevenir acumulación de ruido, y estos procedimientos de cambio de clave involucran evaluar un circuito cuántico que aplica operadores de Pauli ponderados por bits de clave secreta. Si un adversario obtiene acceso a los estados de salida ruidosos inmediatamente después del cambio de clave—a través de canales laterales de temporización o lectura de memoria—entonces el análisis de componentes principales de la matriz de covarianza del ruido puede aislar dependencias lineales entre elementos de clave secreta, exponiendo aproximadamente log₂(d) bits de información de clave por ronda de cambio para un sistema de d-qubits.",
    "C": "La evaluación homomórfica de compuertas no-Clifford, particularmente la compuerta T, requiere inyección de estado mágico a través de circuitos de teletransportación de compuertas donde los resultados de medición deben comunicarse clásicamente para completar la operación de compuerta cifrada. Estos resultados de medición, aunque individualmente aleatorios, exhiben dependencias estadísticas sobre el contenido lógico de los datos cifrados cuando se agregan a través de muchas evaluaciones de compuertas T. Un adversario con acceso a estos registros de medición puede aplicar técnicas de análisis diferencial de potencia tomadas del criptoanálisis de canales laterales, correlacionando distribuciones de resultados de medición con valores hipotéticos de texto plano para reconstruir gradualmente la información cuántica subyacente mediante una estrategia de ataque de texto plano elegido que involucra superposiciones de entrada especialmente diseñadas.",
    "D": "La profundidad del circuito aumenta linealmente con el recuento de operaciones homomórficas, causando sobrecarga polinomial en los requisitos de fidelidad de compuertas.",
    "solution": "D"
  },
  {
    "id": 334,
    "question": "¿Qué algoritmo clásico se usa más comúnmente en el paso final del algoritmo de Shor?",
    "A": "La eliminación gaussiana sobre cuerpos finitos se emplea para resolver el sistema de congruencias lineales que surgen de múltiples mediciones de período, tratando cada salida QFT como una ecuación de restricción. Reduciendo este sistema a forma escalonada, aislamos el período verdadero del ruido introducido por las estadísticas de medición cuántica, filtrando efectivamente periodicidades espurias que no corresponden al orden real de la exponenciación modular.",
    "B": "El teorema chino del resto se aplica para reconstruir el período a partir de sus residuos modulares a través de múltiples ejecuciones independientes de la subrutina cuántica, cada una realizada con diferentes bases aleatorias. Combinando estos resultados parciales mediante CRT, obtenemos el período global con alta confianza, efectivamente paralelizando el paso de búsqueda de período a través de varias ejecuciones de circuito cuántico y luego fusionando clásicamente los resultados.",
    "C": "La prueba de primalidad de Miller-Rabin se invoca después de la transformada cuántica de Fourier para verificar que el candidato a período medido sea efectivamente primo con el módulo, asegurando que la expansión en fracciones continuas produzca un factor válido. Esta verificación probabilística se ejecuta en tiempo polinomial y confirma que el período r satisface la condición de coprimalidad requerida para que el postprocesamiento clásico extraiga divisores no triviales de N.",
    "D": "El algoritmo euclidiano para calcular máximos comunes divisores se aplica para extraer factores del período encontrado por la subrutina cuántica. Después de que la transformada cuántica de Fourier produce un período candidato r, calculamos gcd(a^(r/2) ± 1, N) donde a es la base elegida y N es el número a factorizar. Este procedimiento clásico de tiempo polinomial identifica eficientemente divisores no triviales explotando la estructura multiplicativa revelada por el período, completando la factorización con alta probabilidad en solo unos pocos pasos aritméticos clásicos.",
    "solution": "D"
  },
  {
    "id": 335,
    "question": "¿Qué define la clase de complejidad IQP (Instantaneous Quantum Polynomial-time)?",
    "A": "Circuitos cuánticos que consisten en unitarios diagonales que conmutan entre sí, implementables en profundidad constante a través de una sola capa de compuertas paralelas seguida de medición en la base computacional—se cree que son difíciles de simular clásicamente a pesar de la simplicidad arquitectónica",
    "B": "IQP consiste en circuitos cuánticos con capas unitarias diagonales de profundidad constante y tamaño polinomial en la base X—diagonales en cualquier base fija—que pueden implementarse como rotaciones conmutantes simultáneas. La dificultad clásica surge del muestreo de la distribución de salida, que se relaciona con el cálculo de permanentes de matrices sobre ciertas matrices estructuradas, un problema considerado intratable para computadoras clásicas a pesar de la estructura superficial del circuito cuántico",
    "C": "Esta clase captura circuitos cuánticos compuestos de capas de compuertas diagonales conmutantes de dos qubits en la base computacional, implementables en profundidad logarítmica cuando se relajan las restricciones de localidad de compuertas. La característica definitoria es que todas las compuertas conmutan globalmente, permitiendo reordenamiento arbitrario, sin embargo la distribución de salida permanece difícil de muestrear clásicamente debido a patrones de interferencia constructiva que emergen de las relaciones de fase diagonales a través del registro completo",
    "D": "IQP abarca circuitos cuánticos construidos a partir de compuertas Clifford aumentadas con un número polinomial de compuertas T organizadas en capas de profundidad constante, donde todos los elementos no-Clifford están posicionados para actuar simultáneamente en la capa final. La dificultad de simulación clásica deriva de la teoría de recursos de estados mágicos: mientras que los circuitos Clifford son eficientemente simulables, agregar incluso capas de compuertas T de profundidad constante crea patrones de interferencia que se conjetura requieren recursos clásicos exponenciales para muestrear con precisión",
    "solution": "A"
  },
  {
    "id": 336,
    "question": "El quantum dropout, implementado mediante la eliminación probabilística de puertas parametrizadas durante el entrenamiento, tiene como objetivo:",
    "A": "Regularizar el circuito cuántico variacional y prevenir el sobreajuste a los datos de entrenamiento, funcionando de manera análoga al dropout en redes neuronales clásicas donde la desactivación aleatoria de neuronas obliga al modelo a aprender características robustas que no dependen de ningún parámetro individual.",
    "B": "Mitigar las mesetas estériles introduciendo perturbaciones estocásticas en el paisaje de coste durante la optimización, aprovechando el hecho de que las puertas eliminadas aleatoriamente reducen la profundidad efectiva del circuito y aumentan la varianza del gradiente en cada paso de entrenamiento, permitiendo al optimizador escapar de regiones planas donde los gradientes calculados por la regla de desplazamiento de parámetros se desvanecen exponencialmente con el número de qubits.",
    "C": "Regularizar el circuito cuántico mediante el promedio por conjuntos sobre subestructuras durante el entrenamiento, similar a cómo el dropout clásico fuerza el aprendizaje de características robustas, pero difiere críticamente en que el quantum dropout preserva el conjunto completo de parámetros mientras que el dropout clásico enmascara pesos—aquí todas las puertas permanecen entrenables y la eliminación probabilística crea un conjunto implícito de topologías que comparten parámetros.",
    "D": "Reducir la sobrecarga de medición entrenando el circuito para que sea invariante bajo la eliminación de puertas, funcionando de manera análoga al dropout clásico pero enfocándose en el coste de medición en lugar de la generalización—el circuito entrenado produce valores esperados estables incluso cuando se evalúa con menos mediciones por puerta porque el entrenamiento con operaciones faltantes fuerza un ajuste compensatorio de parámetros que reduce la sensibilidad al ruido de disparo.",
    "solution": "A"
  },
  {
    "id": 337,
    "question": "¿Cómo impacta el concepto de dureza del síndrome en el rendimiento del decodificador en la corrección de errores cuánticos?",
    "A": "Los síndromes que violan la cota de distancia mínima del código requieren decodificadores con capacidad de retroceso para resolver la ambigüedad probando múltiples hipótesis de corrección secuencialmente. Cuando la dureza del síndrome excede un umbral—es decir, el error de peso mínimo consistente con el síndrome tiene un peso que se aproxima a d/2—los algoritmos de emparejamiento basados en grafos producen empates entre emparejamientos perfectos de igual peso, y el decodificador debe enumerar estas soluciones degeneradas para identificar qué corrección preserva el estado lógico. Los decodificadores avanzados como la decodificación de estadísticas ordenadas o los métodos secuenciales de Monte Carlo se vuelven necesarios en este régimen, ya que pueden explorar el espacio de soluciones más allá del primer mínimo local y agregar evidencia a través de múltiples intentos de emparejamiento para seleccionar correcciones que mantengan las relaciones de conmutación lógica con el grupo estabilizador.",
    "B": "Los síndromes con múltiples patrones de error probables necesitan decodificadores más sofisticados que puedan manejar la ambigüedad evaluando hipótesis de error competidoras con probabilidades similares. Cuando la dureza del síndrome es alta—es decir, varias configuraciones de error distintas podrían haber producido el síndrome observado con probabilidad comparable—el emparejamiento perfecto de peso mínimo simple puede fallar porque se compromete con una sola interpretación del error sin tener en cuenta esta degeneración. Los decodificadores más avanzados como la propagación de creencias, los clasificadores de redes neuronales o los decodificadores de máxima verosimilitud se vuelven necesarios para lograr un rendimiento de corrección óptimo, ya que pueden razonar probabilísticamente sobre el espacio de patrones de error candidatos y seleccionar correcciones que minimicen las tasas de error lógico esperadas en lugar de simplemente igualar el peso del síndrome.",
    "C": "Los síndromes correspondientes a errores de alto peso cerca del borde del código requieren decodificadores con razonamiento espacial mejorado para evitar fallos de corrección por efectos de borde. Cuando la dureza del síndrome es alta—es decir, el patrón del síndrome exhibe defectos agrupados cerca de los bordes de la red donde existen menos caminos de corrección—los decodificadores estándar de volumen que asumen invariancia traslacional fallan porque sobreestiman el número de cadenas de error independientes que podrían haber producido el síndrome de borde. Los decodificadores más sofisticados con conocimiento explícito de los bordes, como los métodos de grupo de renormalización o los decodificadores de redes tensoriales, se vuelven necesarios para manejar esta degeneración geométrica, ya que pueden tener en cuenta la flexibilidad de corrección reducida cerca de los bordes y ajustar sus estimaciones de probabilidad de error basándose en la proximidad a la periferia del código donde menos generadores estabilizadores restringen el espacio de error.",
    "D": "Los síndromes que exhiben correlaciones temporales a través de rondas de medición consecutivas requieren decodificadores con memoria para rastrear la dinámica de propagación de errores y resolver la ambigüedad de patrones repetidos. Cuando la dureza del síndrome es alta—es decir, las mismas ubicaciones de defectos se activan a través de múltiples ciclos de extracción de síndrome—los decodificadores sin memoria de disparo único que tratan cada ronda de forma independiente fallan porque no pueden distinguir fallos de hardware persistentes de errores estocásticos transitorios con firmas de síndrome similares. Los decodificadores más avanzados que incorporan modelos ocultos de Markov, redes neuronales recurrentes o filtrado bayesiano se vuelven necesarios en este régimen, ya que pueden integrar el historial de síndromes a lo largo del tiempo para inferir si los patrones recurrentes surgen de procesos de ruido correlacionados o repeticiones de error coincidentes, seleccionando correcciones que tengan en cuenta la estructura temporal del proceso de error en lugar de tratar cada ronda como estadísticamente independiente.",
    "solution": "B"
  },
  {
    "id": 338,
    "question": "¿A qué se refiere el error de puerta en la computación cuántica?",
    "A": "El error de puerta abarca el daño físico permanente al qubit por energía excesiva de operación de la puerta, donde la aplicación repetida de puertas cuánticas degrada gradualmente las propiedades de coherencia del sistema cuántico a través de la formación acumulativa de calentamiento o defectos de red. Cada operación de puerta deposita una pequeña cantidad de energía en el sustrato del qubit, y después de miles de aplicaciones de puertas el daño acumulado se manifiesta como decoherencia irreversible o cambios en la frecuencia de transición del qubit que lo vuelven inutilizable para cálculos cuánticos posteriores.",
    "B": "El error de puerta se refiere específicamente a situaciones donde una puerta cuántica falla completamente en ejecutarse, dejando al qubit congelado en su estado original en lugar de aplicar la transformación unitaria prevista, lo que causa que el cálculo se detenga en ese paso. Este tipo de fallo catastrófico de puerta ocurre cuando las señales de control no logran llegar al qubit o cuando el sistema pierde coherencia temporalmente durante la ventana de operación de la puerta, resultando en una operación de identidad efectiva que preserva el estado de entrada sin cambios mientras el resto del circuito continúa ejecutándose como si la puerta se hubiera aplicado.",
    "C": "El error de puerta describe un artefacto de medición donde el sistema de control de la puerta cuántica realiza incorrectamente una medición proyectiva prematura del estado del qubit antes de aplicar la operación prevista, colapsando la superposición y luego aplicando la puerta al valor de bit ahora clásico. Este error de pre-medición surge de la diafonía entre las líneas de control de la puerta y el aparato de medición, causando que el circuito de lectura se active durante la ejecución de la puerta.",
    "D": "Implementación imperfecta de puertas cuánticas que causa que el operador unitario aplicado se desvíe de la transformación objetivo ideal",
    "solution": "D"
  },
  {
    "id": 339,
    "question": "¿Cómo mejora el enrutamiento priorizado el rendimiento para tareas de alta prioridad?",
    "A": "Asigna los canales cuánticos de mayor fidelidad y las rutas de entrelazamiento más cortas a aplicaciones críticas en tiempo, difiriendo las solicitudes de menor prioridad hasta que los recursos premium estén disponibles.",
    "B": "La priorización impone precedencia temporal en las operaciones de intercambio de entrelazamiento programando las mediciones de pares de Bell de alta prioridad en nodos intermedios antes que los intercambios de baja prioridad, lo que estadísticamente aumenta la fidelidad condicional de los enlaces prioritarios ya que las mediciones más tempranas se completan antes de que el desajuste de fase acumulado por los tiempos finitos de coherencia de memoria degrade los estados entrelazados almacenados, reasignando efectivamente el recurso de fidelidad dependiente del tiempo desde tareas de fondo a solicitudes urgentes sin requerir hardware físico adicional.",
    "C": "El tráfico de alta prioridad recibe acceso exclusivo a pares de Bell generados recientemente cuya fidelidad aún no se ha degradado por debajo del umbral de destilación, mientras que las conexiones de menor prioridad se asignan a pares entrelazados más antiguos que han experimentado decoherencia parcial pero permanecen por encima de la fidelidad mínima utilizable—esta estratificación temporal explota la descomposición continua del entrelazamiento almacenado para crear una jerarquía de prioridad natural sin apropiación explícita, ya que las tareas de fondo esperan adaptativamente a que los recursos frescos de alta fidelidad envejezcan hasta el rango aceptable para sus requisitos relajados de calidad de servicio.",
    "D": "Los algoritmos de enrutamiento conscientes de prioridad calculan asignaciones de rutas Pareto-óptimas que maximizan una suma ponderada de utilidades por tarea donde los pesos reflejan niveles de prioridad, pero debido a que los protocolos de destilación simultáneos en rutas superpuestas crean regiones factibles no convexas en el espacio objetivo de fidelidad versus latencia, encontrar el óptimo verdadero requiere resolver un programa cuadrático entero mixto cuya brecha de relajación crece con el tamaño de la red—las implementaciones prácticas usan heurísticas codiciosas que asignan secuencialmente las solicitudes de mayor prioridad primero, aceptando suboptimalidad para niveles inferiores para mantener decisiones de programación en tiempo polinomial.",
    "solution": "A"
  },
  {
    "id": 340,
    "question": "¿Qué es un modelo causal cuántico?",
    "A": "Un marco que extiende las metodologías clásicas de inferencia causal a sistemas cuánticos, incorporando las características únicas de la mecánica cuántica como la superposición, el entrelazamiento y la contextualidad. Proporciona herramientas matemáticas para representar y analizar relaciones causales entre eventos cuánticos mientras respeta correlaciones no clásicas que violan las desigualdades de Bell, permitiendo un tratamiento riguroso de la causalidad en escenarios donde los efectos cuánticos dominan.",
    "B": "Un marco que aplica la inferencia causal clásica a procesos de medición cuánticos representando cada observable como un nodo en un grafo acíclico dirigido, con aristas que codifican dependencias condicionales entre resultados de medición. Incorpora características cuánticas como superposición y entrelazamiento a través de tablas de probabilidad condicional modificadas que tienen en cuenta la contextualidad, permitiendo el análisis de relaciones causales en experimentos cuánticos mientras respeta el principio de no señalización en lugar de las violaciones de desigualdades de Bell.",
    "C": "Un marco que extiende las redes bayesianas clásicas a sistemas cuánticos representando estados cuánticos como distribuciones de probabilidad sobre modelos de variables ocultas que reproducen correlaciones cuánticas. Proporciona herramientas matemáticas para analizar relaciones causales entre eventos cuánticos a través de mecanismos realistas locales, permitiendo el tratamiento de la no localidad aparente como surgiendo de correlaciones preexistentes codificadas en la preparación del estado cuántico inicial en lugar de influencias dinámicas.",
    "D": "Un marco que generaliza los modelos causales estructurales clásicos a procesos cuánticos incorporando álgebras de probabilidad no conmutativas y representando intervenciones como mapas completamente positivos que preservan la traza sobre operadores de densidad. Proporciona herramientas matemáticas para analizar relaciones causales mientras respeta las restricciones de no clonación cuántica y el principio de incertidumbre de Heisenberg, permitiendo un tratamiento riguroso de la causalidad a través de matrices de proceso que satisfacen condiciones de separabilidad causal.",
    "solution": "A"
  },
  {
    "id": 341,
    "question": "¿Qué vulnerabilidad sofisticada existe en las técnicas de mitigación de errores de los ordenadores cuánticos a corto plazo?",
    "A": "La extrapolación de ruido cero se basa en amplificar deliberadamente el ruido del circuito insertando pares de puertas identidad u operaciones de estiramiento de pulsos para generar puntos de datos a diferentes niveles de ruido, pero si el proceso de amplificación del ruido introduce errores no uniformes que escalan de forma no lineal con el factor de estiramiento — por ejemplo, si las fidelidades de las puertas de dos qubits se degradan más rápido que las puertas de un qubit bajo estiramiento, o si la relajación térmica comienza a dominar sobre los errores coherentes a escalas de ruido más altas — entonces la forma funcional asumida durante la extrapolación polinómica se vuelve inválida. Un adversario que explote esto puede inyectar ruido dirigido que parece lineal a escalas bajas pero se curva de forma impredecible a escalas más altas, causando que la extrapolación de ruido cero converja hacia resultados sistemáticamente sesgados que parecen estadísticamente significativos pero codifican información controlada por el atacante.",
    "B": "La compilación aleatoria mitiga los errores coherentes promediando sobre descomposiciones aleatorias de puertas, convirtiendo efectivamente el ruido coherente en canales de Pauli estocásticos, pero las propias puertas de entrelazamiento Haar-aleatorio están sujetas a errores de implementación que pueden introducir muestreo sesgado en el conjunto de entrelazamiento. Específicamente, si un adversario puede sesgar sutilmente el generador de números aleatorios o explotar limitaciones del conjunto finito de puertas que impiden uniformidad verdadera sobre el grupo de Clifford, ciertos canales de error de Pauli quedan sobrerrepresentados mientras que otros están submuestreados. Este sesgo de muestreo significa que el modelo de ruido efectivo del circuito compilado se desvía del canal despolarizante previsto, permitiendo que errores coherentes estructurados sobrevivan parcialmente al proceso de aleatorización y se filtren a través de la mitigación de errores, sesgando finalmente las salidas algorítmicas en direcciones predecibles.",
    "C": "La manipulación de parámetros de extrapolación en esquemas de extrapolación de ruido cero permite a los adversarios sesgar el modelo de ruido ajustado mediante la inyección sutil de errores correlacionados en factores específicos de escalado de ruido, influyendo en los coeficientes polinómicos.",
    "D": "El entrelazamiento de Pauli simetriza el ruido conjugando operaciones con puertas de Pauli aleatorias, convirtiendo errores coherentes arbitrarios en canales de Pauli diagonales bajo el supuesto de que el grupo de entrelazamiento actúa transitivamente sobre el espacio de error. Sin embargo, esta técnica exhibe debilidades sistemáticas cuando se aplica a errores con estructura de simetría inherente — por ejemplo, si el ruido físico tiene alineación preferencial de ejes debido a direcciones de campo de control o geometría de acoplamiento ambiental. En tales casos, el entrelazamiento de Pauli falla en aleatorizar completamente el error porque ciertos operadores de Pauli conmutan con los canales de error dominantes, dejando componentes coherentes intactos. Un adversario consciente de estas debilidades de supresión de errores simétricos puede diseñar procesos de ruido alineados con las simetrías de entrelazamiento, permitiendo que errores coherentes dirigidos persistan a través de la capa de mitigación.",
    "solution": "C"
  },
  {
    "id": 342,
    "question": "¿Por qué son útiles los qubits auxiliares en implementaciones de retroceso de fase de puertas aritméticas?",
    "A": "Los qubits auxiliares permiten rotaciones de fase controladas que dependen de los estados de base computacional de múltiples qubits de datos simultáneamente, lo cual es esencial para implementar la propagación de acarreo en sumadores cuánticos. Al entrelazar auxiliares con posiciones de bit específicas en el registro aritmético, la fase adquirida por el auxiliar codifica información sobre condiciones de desbordamiento sin colapsar la superposición. Esto permite que los resultados aritméticos se acumulen coherentemente en la fase del auxiliar, que luego puede retroceder a los qubits de control para completar la operación unitariamente.",
    "B": "Almacenan temporalmente información de acarreo, permitiendo acumulación coherente de fase sin medir dígitos intermedios, preservando así la unitariedad requerida para la computación cuántica.",
    "C": "En aritmética de retroceso de fase, los qubits auxiliares actúan como objetivos de fase que acumulan rotaciones proporcionales al resultado aritmético, que luego pueden leerse mediante mediciones de interferencia sin medir directamente el registro de datos. Al codificar la suma o producto en la fase relativa entre estados |0⟩ y |1⟩ del auxiliar en lugar de en estados de base computacional, el resultado aritmético se vuelve accesible mediante mediciones en base de Hadamard que preservan la coherencia cuántica. Esta estrategia de codificación de fase reduce el número de puertas multi-controladas requeridas en comparación con implementaciones aritméticas de estados de base.",
    "D": "Los qubits auxiliares facilitan la descomposición de operaciones controladas de múltiples qubits en secuencias de puertas de uno y dos qubits sirviendo como objetivos de control intermedios en una estructura de puertas en cascada. Para operaciones aritméticas que requieren controles sobre muchos bits simultáneamente (como verificar si un registro excede un umbral), los auxiliares permiten que la lógica de control se factorice en un árbol de puertas CNOT en lugar de requerir una sola puerta con muchos controles. Esta factorización preserva las relaciones de fase necesarias para aritmética coherente mientras mantiene una profundidad de circuito logarítmica en el tamaño del registro.",
    "solution": "B"
  },
  {
    "id": 343,
    "question": "En el contexto del desarrollo de hardware de computación cuántica, considera un sistema de qubits superconductores donde estás diseñando la próxima generación de procesadores con corrección de errores. Tu equipo está evaluando si implementar autocodificadores cuánticos como parte de la capa de compresión en tu jerarquía de memoria cuántica. El arquitecto jefe argumenta que la corrección de errores estándar de código de superficie es suficiente, mientras que tú sospechas que los autocodificadores introducen vulnerabilidades únicas. ¿Por qué es particularmente importante la corrección de errores para los autocodificadores cuánticos en comparación con otros circuitos cuánticos?",
    "A": "Los autocodificadores cuánticos operan inherentemente comprimiendo información cuántica en un subespacio de menor dimensión, pero esta compresión en realidad aumenta la resiliencia a errores al reducir el número de qubits físicos expuestos al ruido ambiental.",
    "B": "Integran transferencias basadas en teletransportación entre capas, y esos protocolos son notoriamente frágiles—requiriendo pares de Bell entrelazados que deben generarse, distribuirse y consumirse dentro de ventanas temporales de escala de nanosegundos. Cada paso de teletransportación introduce dos mediciones proyectivas más sobrecarga de comunicación clásica, creando múltiples puntos donde los errores de fase pueden acumularse sin detectarse.",
    "C": "La compresión depende de mantener patrones precisos de interferencia cuántica a través de múltiples qubits simultáneamente—interferencia que depende de relaciones de fase exactas entre estados de base computacional. A diferencia de circuitos más simples donde los errores afectan operaciones locales independientemente, los errores del autocodificador se propagan a través de la representación comprimida y amplifican su impacto en la salida decodificada. Dado que estás comprimiendo información en menos qubits, no hay redundancia para amortiguar contra el ruido, lo que significa que incluso pequeñas derivas de fase corrompen la variedad codificada y producen resultados inservibles después de la decodificación.",
    "D": "La decoherencia los golpea más fuerte porque los estados comprimidos son más frágiles—cuando codificas n qubits en k<<n qubits latentes, el volumen del espacio de Hilbert se reduce exponencialmente, dejando casi ningún margen de error. La representación comprimida existe en una variedad de baja dimensión incrustada en el espacio completo, y cualquier evento de decoherencia hace que el vector de estado se desvíe de esta variedad hacia regiones que decodifican a resultados inservibles.",
    "solution": "C"
  },
  {
    "id": 344,
    "question": "¿Qué sucede en el algoritmo de Grover si el oráculo no marca soluciones?",
    "A": "Las amplitudes experimentan oscilaciones periódicas que regresan exactamente a la superposición uniforme después de completar un ciclo completo de Grover, porque el operador de inversión sobre el promedio preserva el estado uniforme como un punto fijo cuando no existen estados marcados. Sin embargo, las mediciones intermedias durante ciclos parciales producen distribuciones no uniformes, con masa de probabilidad concentrándose temporalmente en estados más alejados de la amplitud media aritmética, creando estructura aparente que desaparece solo después de completarse múltiplos enteros de π√N/4 iteraciones.",
    "B": "El operador de difusión se vuelve singular porque la inversión sobre el promedio requiere calcular la amplitud media a través de subespacios marcados versus no marcados, y con cero estados marcados el cálculo encuentra una condición de división por cero en el mecanismo de retroceso de fase. Las implementaciones modernas manejan esto detectando respuestas cero del oráculo dentro de O(√N) iteraciones mediante subrutinas de estimación de amplitud que miden la brecha de valor propio del operador de Grover, permitiendo terminación temprana antes de que inestabilidades numéricas corrompan el estado cuántico.",
    "C": "El algoritmo detecta esto mediante monitoreo de la acumulación de fase global después de cada iteración de Grover: cuando no existe solución, la fase adquirida por el componente de superposición uniforme se estabiliza exactamente en π después de √N iteraciones, lo cual puede medirse usando técnicas interferométricas que comparan el estado evolucionado contra una copia de referencia de la superposición uniforme inicial, activando protocolos de terminación temprana que evitan desperdiciar más recursos cuánticos en instancias de búsqueda insatisfacibles.",
    "D": "El estado final permanece cercano a la superposición uniforme, ya que el proceso de amplificación de amplitud no tiene estado marcado hacia el cual concentrar masa de probabilidad, resultando en mediciones que continúan produciendo resultados uniformemente aleatorios del espacio de búsqueda incluso después del número estándar de iteraciones de Grover.",
    "solution": "D"
  },
  {
    "id": 345,
    "question": "Considera una implementación de código de superficie en hardware donde los tiempos T1 de qubits físicos varían en un orden de magnitud a través del chip, y estás compilando un circuito lógico que requiere mover estados codificados entre parches de código distantes. El compilador puede elegir entre una cadena SWAP directa (mínimo conteo de puertas) y una estrategia de enrutamiento bidireccional que mueve temporalmente qubits a través de regiones de mayor calidad antes de alcanzar el objetivo. ¿Por qué el enfoque bidireccional a veces reduce el error total del circuito a pesar de agregar más puertas?",
    "A": "Las rondas de extracción de síndrome requeridas durante el transporte de estado acumulan menos errores cuando los qubits físicos en circuitos de medición tienen tiempos de coherencia más largos, y el enrutamiento bidireccional puede programar los segmentos más propensos a errores para que ocurran en regiones donde los qubits auxiliares tienen valores T1 más altos, reduciendo la fuente de error dominante incluso aunque el conteo total de SWAP aumenta al enrutar a través de parches intermedios de alta calidad.",
    "B": "El enrutamiento a través de regiones de qubits físicos de mayor calidad puede reducir el error de decoherencia acumulado más de lo que las puertas SWAP adicionales lo aumentan, especialmente cuando qubits de baja coherencia acumularían errores de inactividad durante cadenas de transporte largas.",
    "C": "Los caminos bidireccionales crean oportunidades para operaciones de cirugía de red que fusionan y dividen parches de código en ubicaciones intermedias donde la conectividad es mejor, convirtiendo cadenas SWAP en una secuencia de deformaciones de parche que requieren menos puertas físicas totales porque la cirugía de red paraleliza el movimiento de datos lógicos a través de múltiples rondas de extracción de síndrome en lugar de mover secuencialmente qubits.",
    "D": "Las puertas SWAP entre qubits de alta y baja calidad exhiben perfiles de error asimétricos donde el mecanismo de ruido dominante es la relajación de energía desde el estado excitado, que ocurre principalmente en el qubit de T1 más bajo. El enrutamiento bidireccional explota esta asimetría asegurando que el qubit de baja calidad permanezca en estado fundamental durante la mayoría de los SWAPs, convirtiendo efectivamente errores de puertas de dos qubits en errores de borrado que los códigos de superficie manejan de manera más eficiente que los errores de Pauli.",
    "solution": "B"
  },
  {
    "id": 346,
    "question": "¿Cuál es el principal compromiso al adaptar códigos de superficie para tolerar la pérdida de átomos?",
    "A": "Las tasas de error lógico se degradan porque las ubicaciones de borrado permanecen inciertas entre eventos de detección.",
    "B": "La decodificación se vuelve más compleja y las mediciones de estabilizadores pierden su espaciado uniforme.",
    "C": "Los umbrales de capacidad del código disminuyen cuando los pesos del síndrome se vuelven no uniformes entre rondas.",
    "D": "El overhead de ancillas aumenta ya que la detección de pérdidas requiere qubits dedicados solo a medición.",
    "solution": "B"
  },
  {
    "id": 347,
    "question": "En esquemas de distribución cuántica de secretos que incorporan corrección de errores, ¿qué vulnerabilidad fundamental emerge de la interacción entre las dos capas de codificación? Considere un escenario donde un adversario tiene acceso tanto al canal cuántico como a información lateral clásica del protocolo de corrección de errores, y puede realizar ataques coherentes sobre subconjuntos de partes durante la fase de reconstrucción.",
    "A": "La estructura del código Reed-Solomon cuántico para compartir inherentemente crea vulnerabilidades algebraicas porque la base de interpolación polinómica usada para codificar las partes establece relaciones lineales determinísticas entre cualesquiera k partes, y estas relaciones persisten como subespacios invariantes incluso después de aplicar corrección de errores cuántica, lo que significa que un adversario que obtiene k-1 partes más acceso a los síndromes de error puede efectivamente reconstruir coeficientes polinómicos parciales resolviendo el sistema subdeterminado con datos del síndrome como restricciones adicionales, eludiendo la seguridad teórica del umbral ya que la información del síndrome no es información-teóricamente independiente del secreto compartido en la construcción Reed-Solomon.",
    "B": "La manipulación del umbral de reconstrucción de partes explota el hecho de que cuando los adversarios corrompen estratégicamente partes justo por debajo del umbral de reconstrucción, las partes honestas se ven obligadas a invocar mecanismos de redundancia en la capa de corrección de errores, y este proceso de invocación inherentemente expone información estructural sobre el secreto a través de los patrones de error específicos que activan la corrección.",
    "C": "Las propiedades de distancia del código estabilizador se vuelven explotables cuando el umbral de distribución de secretos k y la distancia de corrección de errores d satisfacen k > (n-d)/2, creando una brecha matemática donde un adversario puede inyectar precisamente d/2 errores en partes específicas de modo que sobrevivan el proceso de corrección de errores pero sesgan sistemáticamente el estado reconstruido de manera detectable.",
    "D": "La fuga de información del síndrome de error ocurre porque las mediciones de síndrome necesariamente proyectan el estado compartido sobre un subespacio, y un espía que monitorea estos síndromes clásicos puede aprender información parcial sobre el secreto a través de correlaciones estadísticas, especialmente cuando los patrones de síndrome se repiten en múltiples intentos de reconstrucción o cuando la distancia del código es apenas suficiente para la tasa de error esperada, ya que los datos del síndrome no son uniformemente aleatorios sino que reflejan la distribución real de errores que en sí misma lleva correlaciones débiles con la estructura del secreto codificado a través de la elección de generadores del estabilizador.",
    "solution": "D"
  },
  {
    "id": 348,
    "question": "¿Cuál es la relación teórica entre la profundidad del circuito cuántico y la complejidad de las funciones que puede expresar?",
    "A": "La profundidad del circuito se relaciona con la complejidad de la función a través del crecimiento de la entropía de entrelazamiento en biparticiones del registro de qubits: cada capa puede aumentar la entropía de entrelazamiento como máximo en O(min(k, n-k)) para un corte de k qubits, y la entropía máxima escala como S ≤ min(dt, n/2·log(2)) donde d es la profundidad y t es el número de puertas entrelazantes por capa. Dado que muchas funciones complejas —particularmente aquellas que surgen en algoritmos cuánticos como la factorización de Shor o la simulación cuántica de sistemas de muchos cuerpos— requieren generar estados con entrelazamiento extenso a través de múltiples particiones simultáneamente, la profundidad debe escalar al menos logarítmicamente con la complejidad de la función para arquitecturas localmente conectadas, aunque la relación precisa depende de si el circuito de la función puede ser paralelizado o requiere operaciones inherentemente secuenciales.",
    "B": "La relación profundidad-expresividad sigue de la estructura de álgebra de Lie de conjuntos de puertas cuánticas: cada capa de puertas k-locales genera elementos de corchetes conmutadores sucesivamente más altos en el álgebra del grupo de Pauli, con profundidad d permitiendo acceso a conmutadores anidados de orden O(d). Dado que implementar funciones que corresponden a operadores de Pauli de peso alto —que representan correlaciones complejas entre muchos qubits— requiere generar estos operadores a través de secuencias de relaciones de conmutadores, la profundidad del circuito debe crecer al menos polinomialmente con el peso de la descomposición de Pauli de la función objetivo. Esto es distinto de la profundidad de circuito clásica porque las puertas cuánticas generan grupos de Lie continuos en lugar de lógica booleana discreta, requiriendo un análisis cuidadoso de las propiedades de alcanzabilidad dentro de la variedad del grupo.",
    "C": "La profundidad del circuito está directamente relacionada con la complejidad de las funciones implementables, con aumentos exponenciales en expresividad posibles con aumentos lineales en profundidad. Como cada capa de puertas puede crear nuevas estructuras de entrelazamiento y correlaciones entre qubits, agregar más capas permite al circuito aproximar mapeos cada vez más intrincados de entrada a salida. Esta relación de escalado está respaldada por trabajo teórico que muestra que circuitos más profundos pueden implementar polinomios de grado superior y funciones booleanas más complejas, aunque barreras prácticas como el ruido y los tiempos de coherencia limitan la profundidad alcanzable en dispositivos a corto plazo.",
    "D": "La relación teórica entre profundidad y expresividad está gobernada por la capacidad del circuito para generar t-diseños: circuitos de profundidad d pueden implementar t-diseños aproximados con t ≈ O(d/n) para n qubits con colocación aleatoria de puertas, lo que significa que pueden reproducir los primeros t momentos estadísticos de la medida de Haar sobre matrices unitarias. Dado que funciones complejas requieren coincidencia de momentos de orden alto para distinguirse de unitarios aleatorios —particularmente aquellas que implementan funciones pseudoaleatorias o funciones de un solo sentido relevantes para la criptografía cuántica— la profundidad debe escalar como d ≈ O(tn) para expresar funciones de complejidad caracterizada por orden de t-diseño. Este marco explica por qué la profundidad polinomial es suficiente para muchos algoritmos cuánticos pero se necesitaría profundidad exponencial para implementar permutaciones verdaderamente aleatorias.",
    "solution": "C"
  },
  {
    "id": 349,
    "question": "¿Qué condición determina la equivalencia de dos circuitos con diferente número de qubits?",
    "A": "Los circuitos producen estados de salida idénticos cuando los qubits ancilla se inicializan en |+⟩ y posteriormente se trazan, con acuerdo requerido hasta un factor de fase global irrelevante que no tiene consecuencias observables. La inicialización en |+⟩ asegura un testigo de entrelazamiento máximo para verificar la equivalencia entre registros de ancilla diferentes.",
    "B": "Los operadores unitarios deben satisfacer |Tr(U†V)|² = d donde d es la dimensión del espacio de Hilbert compartido sobre el que ambos actúan, indicando que el producto interno de Hilbert-Schmidt alcanza su valor máximo y las transformaciones son equivalentes hasta una fase global físicamente irrelevante.",
    "C": "Los circuitos producen estados de salida idénticos cuando los qubits ancilla se inicializan en |0⟩ y posteriormente se trazan, con acuerdo requerido hasta un factor de fase global irrelevante que no tiene consecuencias observables.",
    "D": "Los circuitos implementan el mismo mapa completamente positivo que preserva la traza cuando los qubits ancilla se inicializan en |0⟩ y sus estados finales se descartan vía traza parcial, con equivalencia válida hasta una fase global. Esta formulación vía mapas CPTP maneja naturalmente el registro de ancilla tratándolo como parte de un entorno extendido que se acopla al sistema lógico pero cuyos grados de libertad son finalmente trazados, asegurando que la equivalencia del circuito respete la semántica operacional de los canales cuánticos incluso cuando los conteos de qubits intermedios difieren.",
    "solution": "C"
  },
  {
    "id": 350,
    "question": "¿De qué manera fundamental difieren las mesetas áridas inducidas por ruido del fenómeno estándar de mesetas áridas en circuitos cuánticos parametrizados? Considere que las mesetas áridas estándar surgen de la concentración exponencial de gradientes debido a la expresividad, mientras que el ruido introduce un mecanismo separado. ¿Cómo altera la interacción entre la profundidad del circuito, la localidad de la función de coste y el ruido del hardware las condiciones bajo las cuales los gradientes se anulan?",
    "A": "Las mesetas áridas inducidas por ruido emergen incluso en circuitos poco profundos con funciones de coste locales, porque el ruido del hardware suprime directamente las señales de gradiente independientemente de la expresividad del circuito o la estructura de entrelazamiento global. A diferencia de las mesetas áridas estándar que dependen fundamentalmente de la profundidad del circuito y el uso de observables globales, la anulación de gradientes inducida por ruido ocurre a través de un mecanismo físico distinto donde la decoherencia y los errores de puerta corrompen la propagación de información dependiente de parámetros a través del circuito, haciendo que los desafíos de entrenabilidad sean inevitables incluso cuando se emplean estrategias de mitigación tradicionales como la reducción de profundidad del circuito o la localidad de observables.",
    "B": "Las mesetas áridas inducidas por ruido surgen de la dinámica no unitaria introducida por canales de decoherencia que rompen la estructura de preservación de información de los circuitos cuánticos parametrizados, causando que las señales de gradiente decaigan exponencialmente con una escala de longitud característica determinada por la relación entre la fidelidad de puerta y la profundidad del circuito. Específicamente, el ruido despolarizante con tasa de error p crea un factor efectivo de supresión de gradiente de aproximadamente (1-4p/3)^L donde L es la profundidad del circuito, haciendo que los gradientes se anulen incluso para circuitos poco profundos con observables locales cuando p excede un umbral cerca de 1/(4L). Sin embargo, este mecanismo difiere fundamentalmente de las mesetas áridas estándar porque depende de la tasa de ruido local del circuito en lugar de la entropía de entrelazamiento global, lo que significa que técnicas de mitigación de errores espacialmente localizadas como el desacoplamiento dinámico aplicado a puertas que llevan parámetros pueden restaurar información de gradiente sin requerir un rediseño completo del circuito, siempre que las puertas con errores mitigados alcancen fidelidades que satisfagan (1-4p_mitigado/3)^L > n^(-1/2) donde n es el conteo de qubits.",
    "C": "La distinción fundamental radica en la dinámica temporal de la información del gradiente: las mesetas áridas estándar representan una propiedad estática de la arquitectura unitaria del circuito donde la varianza del gradiente escala como O(2^(-n)) desde el principio, mientras que las mesetas inducidas por ruido emergen dinámicamente a medida que la información de gradiente coherente decae durante la ejecución del circuito a una tasa determinada por el tiempo de desfase T_2 relativo a la duración de la puerta. Para circuitos con tiempo total de ejecución τ_circuit y tiempo T_2 promedio, las señales de gradiente sobreviven solo cuando τ_circuit < T_2 · ln(n)/2, creando un umbral dependiente de profundidad pero independiente de la tasa de ruido. Este mecanismo temporal significa que incluso circuitos profundos y altamente expresivos pueden evitar mesetas áridas inducidas por ruido si se ejecutan lo suficientemente rápido, sugiriendo que la aceleración a través de la paralelización de capas de puertas (reduciendo τ_circuit sin cambiar la profundidad del circuito L) puede restaurar la entrenabilidad—una estrategia de mitigación fundamentalmente diferente al rediseño de circuito requerido para las mesetas áridas estándar.",
    "D": "Las mesetas áridas inducidas por ruido exhiben un escalado cualitativamente diferente con el tamaño del sistema porque la decoherencia afecta preferencialmente las coherencias fuera de la diagonal que codifican la sensibilidad a parámetros, mientras que las mesetas áridas estándar surgen de la mezcla uniforme de información cuántica a través de todos los sectores del espacio de Hilbert. Específicamente, los canales de amortiguamiento de amplitud con tasa γ suprimen gradientes como O(e^(-γLn/2)) donde L es la profundidad y n es el conteo de qubits, creando una supresión doble-exponencial que combina la profundidad del circuito y el tamaño del sistema multiplicativamente en lugar del escalado puramente exponencial O(2^(-n)) de las mesetas estándar. Esta diferencia fundamental significa que las mesetas inducidas por ruido se vuelven severas incluso a tamaños de sistema modestos (n≈20) con circuitos poco profundos (L≈10) a tasas de error realistas (γ≈0.001), mientras que las mesetas estándar típicamente no dominan hasta n>50 a cualquier profundidad fija, haciendo que el fenómeno inducido por ruido sea el principal obstáculo de entrenabilidad para dispositivos a corto plazo a pesar de ser mecánicamente distinto de la concentración de gradientes impulsada por expresividad.",
    "solution": "A"
  },
  {
    "id": 351,
    "question": "¿Por qué el algoritmo de Grover es ineficiente para claves criptográficas cortas?",
    "A": "Las sincronizaciones de puertas Toffoli se convierten en un cuello de botella en las implementaciones de Grover para claves cortas porque el circuito del oráculo requiere operaciones Toffoli paralelas a través de múltiples qubits para calcular la condición de búsqueda, pero las arquitecturas cuánticas actuales carecen de la conectividad total necesaria para ejecutar estas puertas simultáneamente sin redes SWAP.",
    "B": "La profundidad del circuito escala cuadráticamente con el tamaño de entrada en el algoritmo de Grover porque cada iteración requiere llamadas controladas al oráculo cuya profundidad de implementación crece como O(n²) para claves de n bits debido a las construcciones en cascada de puertas Toffoli necesarias para evaluar el predicado de búsqueda.",
    "C": "Las claves pequeñas no pueden codificarse usando compilación exclusivamente Clifford porque el operador de difusión de Grover requiere inherentemente puertas no-Clifford para lograr la reflexión de fase negativa alrededor de la amplitud promedio, y las implementaciones tolerantes a fallos de estas puertas mediante destilación de estados mágicos se vuelven impracticables para espacios de claves por debajo de aproximadamente 2⁶⁴ entradas.",
    "D": "El costo de corrección de errores requerido para mantener la coherencia a lo largo de las iteraciones de Grover cancela la aceleración cuántica teórica para espacios de claves pequeños, porque el número de qubits lógicos y ciclos de medición de síndrome necesarios para proteger contra la decoherencia excede la ventaja computacional obtenida de la reducción de complejidad de consultas √N en implementaciones de tamaño práctico.",
    "solution": "D"
  },
  {
    "id": 352,
    "question": "¿Qué es un circuito Clifford en computación cuántica?",
    "A": "Un circuito cuántico donde todas las puertas provienen del grupo Clifford—Hadamard, Phase, CNOT—que forma un subgrupo finito del grupo unitario y normaliza el grupo de Pauli, lo que significa que la conjugación Clifford mapea operadores de Pauli a operadores de Pauli, una propiedad explotada por el teorema de Gottesman-Knill para simulación clásica eficiente mediante tablas de estabilizadores.",
    "B": "Un circuito cuántico compuesto exclusivamente por puertas del grupo Clifford—concretamente puertas Hadamard, Phase y CNOT—que puede simularse eficientemente en computadoras clásicas utilizando el teorema de Gottesman-Knill.",
    "C": "Un circuito que comprende puertas que preservan la base computacional bajo conjugación, específicamente operaciones Pauli-X, Pauli-Z y CNOT, permitiendo simulación clásica porque estas puertas mapean estados de la base a estados de la base sin superposición y la evolución puede rastrearse determinísticamente usando propagación de cadenas de bits en lugar de amplitudes de vectores de estado que requieren memoria exponencial.",
    "D": "Un circuito cuántico construido con puertas que estabilizan estados maximalmente entrelazados bajo aplicación repetida, incluyendo Hadamard, puerta S y CNOT, que permiten simulación clásica en tiempo polinomial porque las operaciones Clifford preservan el rango de las matrices de densidad reducidas y la estructura de entrelazamiento puede representarse compactamente usando un número logarítmico de bits clásicos por qubit mediante el formalismo de estabilizadores.",
    "solution": "B"
  },
  {
    "id": 353,
    "question": "En sistemas de identidad descentralizada post-cuánticos, considere un escenario donde los usuarios deben probar pertenencia a un conjunto de credenciales sin revelar qué credencial específica poseen, mientras se asegura que las credenciales revocadas no puedan usarse incluso si la lista de revocación se actualiza asincrónicamente entre los nodos de la red. El sistema debe permanecer seguro contra adversarios cuánticos con acceso tanto a información clásica de canal lateral como a la capacidad de realizar ataques fuera de línea sobre transcripciones de protocolo interceptadas. ¿Qué enfoque técnico proporciona las garantías combinadas más sólidas de privacidad, revocabilidad y seguridad post-cuántica en este modelo de amenaza?",
    "A": "Las arquitecturas de identidad auto-soberana usando cadenas de firmas post-cuánticas emplean firma secuencial donde cada credencial se convierte en parte de una cadena criptográfica anclada a un compromiso de identidad génesis, proporcionando autenticidad resistente a cuántica mediante firmas basadas en retículos como Dilithium o Falcon. Aunque este enfoque previene la falsificación de credenciales bajo ataque cuántico, la vinculabilidad entre credenciales sucesivas crea inherentemente un grafo temporal que puede explotarse mediante análisis de tráfico, y la naturaleza monolítica de la estructura de cadena requiere que los usuarios presenten porciones sustanciales de su historial de credenciales para probar cualquier atributo único, comprometiendo fundamentalmente las garantías de no vinculabilidad.",
    "B": "Los acumuladores Merkle basados en hash para gestión de revocación construyen pruebas de pertenencia resistentes a cuántica haciendo que cada usuario mantenga una ruta de autenticación desde su hoja de credencial hasta la raíz del acumulador, actualizada mediante registros de solo-anexar que previenen antedatar revocaciones. Mientras que las firmas SPHINCS+ o XMSS aseguran la integridad post-cuántica de las actualizaciones del acumulador, el protocolo de verificación requiere inherentemente que los usuarios divulguen su índice exacto de hoja para validar la ruta de autenticación, revelando directamente qué credencial poseen del conjunto y derrotando completamente el anonimato, aunque la eficiencia computacional permanece excelente en O(log n) de tamaño de prueba.",
    "C": "Las pruebas de identidad de conocimiento cero resistentes a cuántica construidas sobre supuestos de retículos, permitiendo tanto la divulgación de atributos como las verificaciones de revocación sin vincular instancias de prueba, combinando firmas de anillo para anonimato con acumuladores criptográficos para verificación eficiente del estado de revocación mientras mantienen seguridad contra ataques cuánticos mediante supuestos de dureza basados en retículos subyacentes.",
    "D": "Las credenciales anónimas basadas en retículos con protocolos de divulgación selectiva aprovechan la dureza de ring-LWE para construir esquemas de firma resistentes a cuántica donde los usuarios pueden probar posesión de atributos firmados sin revelar la firma completa del emisor, típicamente usando protocolos de Stern transformados con Fiat-Shamir para pruebas de conocimiento cero. Cuando se combinan con acumuladores criptográficos basados en árboles Merkle con funciones hash post-cuánticas, esto permite verificación eficiente de revocación, aunque las actualizaciones de testigos del acumulador deben sincronizarse entre todos los tenedores de credenciales cuando ocurren revocaciones, y las pruebas de pertenencia al conjunto filtran inadvertidamente la época del acumulador a través de variaciones en el tamaño de la prueba, creando vulnerabilidades de oráculo temporal.",
    "solution": "D"
  },
  {
    "id": 354,
    "question": "¿Cuál de las siguientes afirmaciones sobre la topología de red en computación cuántica distribuida es más precisa?",
    "A": "Para cualquier operación que involucre qubits distribuidos entre procesadores separados—ya sea aplicar una puerta controlled-NOT entre qubits remotos o medirlos en una base entrelazada—el sistema requiere absolutamente enlaces físicos de entrelazamiento directo conectando esos procesadores específicos. Sin tales conexiones punto a punto dedicadas, no existe canal cuántico a través del cual puedan establecerse las correlaciones cuánticas necesarias para ejecutar la operación, ya que enrutar información cuántica a través de nodos intermedios violaría el teorema de no clonación y degradaría la fidelidad por debajo de umbrales útiles para la mayoría de algoritmos distribuidos.",
    "B": "En ausencia de canales de transmisión física de qubits o recursos de entrelazamiento pre-compartido conectando directamente dos procesadores, el único enfoque viable es la coordinación clásica donde los resultados de medición de un nodo se transmiten mediante enlaces de red convencionales para informar las operaciones realizadas en el otro nodo.",
    "C": "El intercambio de entrelazamiento crea conectividad virtual entre nodos no adyacentes al realizar mediciones de estados de Bell sobre qubits intermedios, extendiendo efectivamente las correlaciones cuánticas a través de la topología de red sin requerir enlaces físicos directos entre cada par de procesadores.",
    "D": "El teorema de no clonación impone restricciones arquitectónicas estrictas en las redes cuánticas distribuidas al requerir que todos los procesadores mantengan conectividad física directa con cada otro procesador en el sistema. Esta topología totalmente conectada se vuelve necesaria porque intentar enrutar información cuántica a través de nodos intermedios requeriría que esos nodos crearan copias del estado cuántico para propósitos de reenvío, lo cual viola directamente la prohibición fundamental contra clonar estados cuánticos arbitrarios—consecuentemente, los protocolos distribuidos tolerantes a fallos que involucran operaciones lógicas multi-qubit que abarcan varios procesadores solo pueden funcionar cuando cada posible par de nodos comparte un enlace dedicado de generación de entrelazamiento, aumentando sustancialmente la complejidad del hardware conforme escala la red.",
    "solution": "C"
  },
  {
    "id": 355,
    "question": "En la evaluación de fórmulas cuánticas mediante caminatas cuánticas sobre fórmulas balanceadas, ¿qué establece realmente la cota ajustada demostrada sobre el rendimiento del algoritmo en relación con todos los enfoques cuánticos posibles y con métodos clásicos aleatorizados?",
    "A": "Ningún algoritmo cuántico puede evaluar tales fórmulas asintóticamente más rápido, estableciendo la optimalidad del enfoque de caminata cuántica hasta factores constantes.",
    "B": "Las fórmulas desbalanceadas invariablemente requieren exactamente el doble de consultas comparadas con las balanceadas porque la caminata cuántica debe recorrer cada rama desbalanceada por separado en lugar de explorar ambos lados de cada puerta en superposición, y la asimetría previene la interferencia constructiva entre rutas computacionales—esta penalización de factor dos está probada ajustada mediante cotas inferiores explícitas derivadas de métodos adversarios aplicados a estructuras de árbol desbalanceadas de peor caso, donde un subárbol tiene profundidad logarítmica mientras el otro tiene profundidad lineal.",
    "C": "Los algoritmos clásicos aleatorizados realmente logran la misma complejidad de consultas que el algoritmo de caminata cuántica para fórmulas balanceadas cuando se amortizan sobre muchas evaluaciones, porque una estrategia de muestreo aleatorio cuidadosamente diseñada puede priorizar variables de alta influencia y podar adaptativamente subárboles basándose en resultados intermedios—la idea clave es que las fórmulas balanceadas tienen complejidad de consultas esperada O(√n) bajo un árbol de decisión aleatorizado óptimo que explota la concentración de medida en distribuciones de producto de alta dimensionalidad.",
    "D": "La cota ajustada se aplica específicamente solo cuando la fórmula está construida exclusivamente con puertas NAND en lugar de puertas OR o AND, porque el mecanismo de retroceso de fase usado en el algoritmo de caminata cuántica depende críticamente de la propiedad auto-dual de NAND bajo negación. Además, la cota está probada ajustada solo en el caso restringido donde la profundidad de la fórmula escala logarítmicamente con el tamaño de entrada—una propiedad garantizada para árboles binarios balanceados pero violada por estructuras de grafo más frondosas o más generales.",
    "solution": "A"
  },
  {
    "id": 356,
    "question": "¿Qué ayuda a determinar la entropía mínima cuántica en la distribución cuántica de claves?",
    "A": "La entropía mínima proporciona una cota inferior ajustada sobre la entropía condicional de von Neumann de la clave condicionada a la información lateral cuántica del adversario, lo que determina la cantidad de amplificación de privacidad necesaria mediante hash universal para comprimir la clave en bits demostrablemente seguros.",
    "B": "Caracteriza la información del espía en el peor caso al establecer una cota inferior sobre la impredecibilidad de los resultados de medición desde la perspectiva del adversario, lo que determina cuánta extracción de aleatoriedad se requiere para destilar material de clave incondicionalmente seguro a partir de la clave tamizada, considerando las correlaciones cuánticas.",
    "C": "Cuantifica el número de bits de clave demostrablemente seguros extraíbles que pueden derivarse de un estado cuántico medido, teniendo en cuenta la máxima información posible del adversario sobre el material de clave bruto",
    "D": "La entropía mínima acota la información mutua máxima entre la clave bruta de Alice y el sistema cuántico de Eve al cuantificar la entropía de Shannon mínima sobre todas las posibles estrategias de medición que Eve podría emplear, determinando así la tasa de clave segura después de la corrección de errores y amplificación de privacidad en regímenes de longitud de clave finita.",
    "solution": "C"
  },
  {
    "id": 357,
    "question": "¿Por qué deben los Nodos de Confianza Simplificados realizar ocasionalmente distribución cuántica de claves local?",
    "A": "Para mitigar efectos de tamaño finito en la estimación de parámetros mediante sesiones periódicas de QKD local que generan muestras estadísticas frescas para actualizar los parámetros de amplificación de privacidad, asegurando que las estimaciones acumuladas de error de fase permanezcan precisas mientras el nodo de confianza procesa múltiples canales concurrentes cuyas estadísticas de correlación de otro modo violarían las pruebas de seguridad contra ataques colectivos.",
    "B": "Para refrescar su reserva de claves de comunicación clásica autenticada mediante sesiones periódicas de QKD local que reponen las claves simétricas utilizadas para protocolos de autenticación, asegurando que las claves de autenticación comprometidas no provoquen vulnerabilidades en cascada a través de la infraestructura de red de nodos de confianza.",
    "C": "Para verificar la calibración de eficiencia de detectores mediante mediciones QKD locales que comparan tasas de detección esperadas versus observadas, ya que los nodos de confianza deben mantener estadísticas precisas de detección de fotones individuales para prevenir ataques de canal lateral que exploten vulnerabilidades de cegado de detectores que podrían comprometer la capacidad del nodo para retransmitir adecuadamente los estados cuánticos.",
    "D": "Para prevenir el agotamiento del buffer de claves mediante sesiones programadas de QKD local que mantienen reservas adecuadas de material de clave incondicionalmente seguro para cifrado de libreta de un solo uso de mensajes clásicos entre nodos, ya que la seguridad teórico-informacional del protocolo de nodo de confianza requiere disponibilidad continua de bits de clave frescos para autenticar los metadatos clásicos de las señales cuánticas reenviadas.",
    "solution": "B"
  },
  {
    "id": 358,
    "question": "En la compilación de trampas de iones, ¿qué motiva la inserción de pulsos π de desacoplamiento de modos espectadores?",
    "A": "Reenfocan el entrelazamiento no intencionado con modos de movimiento fuera de resonancia, protegiendo la fidelidad de las operaciones XX objetivo al promediar efectivamente los acoplamientos no deseados que surgen cuando el esquema de direccionamiento láser no puede aislar perfectamente un solo modo de movimiento. Estos pulsos π crean un efecto de eco de espín que cancela las fases acumuladas de los modos espectadores.",
    "B": "Suprimen el entrelazamiento no deseado con modos de movimiento fuera de resonancia al aplicar inversiones rápidas de espín que promedian el hamiltoniano de acoplamiento a cero mediante una secuencia de desacoplamiento dinámico. Estos pulsos π interrumpen la evolución bajo interacciones de modos espectadores, previniendo la acumulación de fase que de otro modo reduciría la pureza de la puerta de dos qubits objetivo al crear correlaciones no deseadas entre grados de libertad computacionales y de movimiento.",
    "C": "Eliminan los desplazamientos de Stark de haces láser fuera de resonancia al crear una secuencia de pulsos simétricamente temporal donde la acumulación de fase de Stark AC durante la primera mitad de la puerta es exactamente cancelada por acumulación de signo opuesto durante la segunda mitad. Estos pulsos π invierten el signo del desplazamiento diferencial de luz experimentado por cada qubit, asegurando que las fluctuaciones de intensidad de los haces de direccionamiento espectadores no introduzcan errores de fase condicionales en la operación de entrelazamiento objetivo.",
    "D": "Mitigan el calentamiento de modos de movimiento espectadores al invertir el efecto del operador de creación de fonones en el punto medio de la secuencia de puerta, implementando efectivamente un tren de Carr-Purcell que suprime el calentamiento anómalo. Estos pulsos π crean interferencia destructiva entre procesos de calentamiento en la primera y segunda mitades de la puerta, preservando la ocupación del estado fundamental de movimiento requerida para operaciones Mølmer-Sørensen de alta fidelidad a pesar del ruido de campo eléctrico ambiental que se acopla a modos espectadores.",
    "solution": "A"
  },
  {
    "id": 359,
    "question": "¿Qué metodología de ataque avanzada se dirige a los efectos de tamaño finito en implementaciones de distribución cuántica de claves?",
    "A": "La explotación de límites de tamaño de bloque aprovecha el hecho de que los sistemas QKD prácticos deben particionar flujos de claves continuos en bloques discretos para análisis estadístico de muestra finita. Un adversario cronometra cuidadosamente su intervención para apuntar a los límites entre bloques consecutivos, donde los protocolos de reconciliación transicionan entre diferentes códigos de corrección de errores optimizados para longitudes de bloque variables. Al inducir errores correlacionados precisamente en estos puntos de transición, Eve puede crear anomalías estadísticas que aparecen como ruido legítimo dentro de bloques individuales pero se acumulan sistemáticamente a través de límites.",
    "B": "La manipulación de intervalos de confianza explota la incertidumbre estadística inherente en estimar tasas de error de bits cuánticos a partir de muestras finitas al introducir estratégicamente errores que amplían los límites de confianza de Alice y Bob. Cuando las partes legítimas calculan sus estadísticas de error, deben elegir entre límites conservadores que desperdician demasiado material de clave en amplificación de privacidad o límites agresivos que arriesgan aceptar claves comprometidas. Un adversario monitorea el canal de reconciliación público para conocer qué estimadores estadísticos se están utilizando, luego inyecta errores con correlaciones temporales cuidadosamente ajustadas que maximizan la varianza del estimador sin aumentar su media más allá del umbral de aborto.",
    "C": "La interferencia de estimación de parámetros se dirige a cómo Alice y Bob estiman tasas de error a partir de muestras limitadas, forzándolos a aceptar claves con amplificación de privacidad insuficiente. Al manipular el proceso de muestreo estadístico durante la fase de medición de tasa de error de bits cuánticos, un adversario puede sesgar la distribución de error observada hacia el extremo inferior de lo que desencadenaría un aborto, llevando a las partes legítimas a subestimar la ganancia de información de Eve. Esta explotación se basa en la incertidumbre inherente en estadísticas de muestra finita donde las estimaciones deben hacerse a partir de miles en lugar de infinitos intercambios de fotones.",
    "D": "La amplificación de fluctuaciones estadísticas se dirige a la varianza de muestreo inherente en QKD de clave finita al explotar el escalamiento de raíz cuadrada de la desviación estándar con el tamaño de muestra. En implementaciones realistas limitadas a 10^6-10^9 intercambios de fotones por establecimiento de clave, las fluctuaciones aleatorias en la tasa de error observada pueden alcanzar varias desviaciones estándar por encima del ruido de canal medio. Un atacante sincroniza su espionaje para coincidir con fluctuaciones positivas que ocurren naturalmente en la tasa de error de bits cuánticos, luego agrega una pequeña perturbación adicional que parece consistente con el piso de ruido ya elevado. Esta técnica efectivamente oculta la ganancia de información de Eve dentro de los límites de incertidumbre estadística que Alice y Bob deben aceptar al trabajar con datos finitos.",
    "solution": "C"
  },
  {
    "id": 360,
    "question": "¿Cuál de los siguientes es un enfoque válido para mitigar los mesetas áridas en redes neuronales cuánticas?",
    "A": "Estructuras de ansatz específicas del problema que incorporan simetrías, leyes de conservación u otro conocimiento del dominio para restringir el unitario parametrizado a una variedad de menor dimensión alineada con el paisaje de la función de costo. Por ejemplo, en aplicaciones de química cuántica, ansätze que preservan el número de partículas y simetrías de espín restringen el espacio de búsqueda a estados físicamente relevantes, evitando regiones del espacio de Hilbert donde los gradientes se desvanecen debido a irrelevancia en lugar de concentración exponencial.",
    "B": "Entrenamiento por capas, donde el circuito se optimiza incrementalmente al primero entrenar un subcircuito poco profundo y luego agregar capas adicionales una a la vez mientras se congelan o ajustan finamente los parámetros previamente optimizados. Esta estrategia asegura que en cada etapa del entrenamiento, el problema de optimización activo involucra solo un subconjunto del espacio de parámetros completo, previniendo la supresión exponencial de gradientes que ocurre cuando todos los parámetros en un circuito profundo se actualizan simultáneamente.",
    "C": "Usar ansätze eficientes en hardware para maximizar la fidelidad de puertas a través de toda la profundidad del circuito, lo que reduce la varianza inducida por ruido en las estimaciones de gradiente y permite actualizaciones de parámetros más confiables incluso cuando la señal de gradiente verdadera se vuelve exponencialmente pequeña. Los diseños eficientes en hardware se alinean con el conjunto de puertas nativas y el grafo de conectividad del dispositivo físico, minimizando el número de puertas SWAP y reduciendo la duración total del circuito.",
    "D": "Combinar arquitecturas de circuito poco profundo para limitar la profundidad de entrelazamiento, protocolos de entrenamiento por capas que optimizan subcircuitos incrementalmente antes de agregar capas, y diseños de ansatz conscientes del problema que incorporan simetrías y leyes de conservación, todos proporcionan estrategias complementarias para evitar el desvanecimiento exponencial de gradientes.",
    "solution": "D"
  },
  {
    "id": 361,
    "question": "Considere un circuito cuántico variacional utilizado para optimización donde la función de coste depende de valores esperados calculados a partir del estado de salida completo. A medida que la profundidad del circuito aumenta más allá de cierto umbral, el entrenamiento basado en gradientes se vuelve cada vez más difícil independientemente de la elección de parámetros iniciales o tasa de aprendizaje. Este fenómeno se ha observado en múltiples plataformas de hardware y parece ser fundamental en lugar de un producto del ruido. Las funciones de coste locales se han propuesto como un posible remedio. Las funciones de coste locales mejoran la capacidad de entrenamiento de las QNN principalmente mediante:",
    "A": "Enfocar la información del gradiente en pequeños subconjuntos de qubits, lo que confina la dimensión efectiva del espacio de Hilbert y previene la dilución exponencial de gradientes que ocurre cuando las funciones de coste dependen de observables globales que abarcan todos los qubits en el circuito, manteniendo así las magnitudes de gradiente en niveles donde los algoritmos de optimización pueden detectar de manera confiable señales distintas de cero por encima del ruido finito de muestreo.",
    "B": "Restringir las mediciones a observables k-locales donde k << n garantiza que la varianza del gradiente escale como O(1/2^k) en lugar de O(1/2^n), porque las funciones de coste locales solo sondean subespacios de dimensión 2^k del espacio de Hilbert completo 2^n, evitando que la señal de gradiente se disperse a través de exponencialmente muchas direcciones irrelevantes, aunque esto requiere que el observable local aún capture suficiente información sobre el objetivo de optimización para guiar la convergencia hacia óptimos globales a pesar de la sensibilidad reducida a correlaciones de qubits distantes.",
    "C": "Restringir las funciones de coste a observables locales cambia fundamentalmente las propiedades de concentración de medida: mientras que los observables globales en estados aleatorios se concentran exponencialmente cerca de su media por el lema de Levy, causando que los gradientes se desvanezcan en mesetas estériles, los observables locales mantienen varianza constante independiente del tamaño del sistema porque las fluctuaciones dependen solo de la dimensión del subsistema medido, asegurando que las magnitudes de gradiente permanezcan O(1) a medida que crece la profundidad del circuito, aunque esto asume que la región local captura características relevantes para la optimización.",
    "D": "Limitar la medición a observables k-locales reduce el cono de luz de puertas que contribuyen a cada componente del gradiente, ya que ∂⟨O_local⟩/∂θ_i se anula cuando la puerta i se encuentra fuera del cono causal del soporte de O_local, particionando efectivamente el espacio de parámetros en subproblemas de optimización independientes que evitan el promediado exponencial sobre regiones de circuito no relacionadas que de otro modo diluirían la señal de gradiente mediante interferencia destructiva a través del espacio de estados de dimensión 2^n, manteniendo la capacidad de entrenamiento al convertir un problema global exponencialmente difícil en polinomialmente-muchos problemas locales tratables.",
    "solution": "A"
  },
  {
    "id": 362,
    "question": "Los entornos de ejecución confiables modernos deben proteger contra adversarios cuánticos, pero las implementaciones actuales enfrentan varios obstáculos. El desafío de ingeniería más inmediato proviene de integrar primitivas criptográficas post-cuánticas en arquitecturas TEE existentes. ¿Qué limitación técnica específica plantea el mayor desafío para los entornos de ejecución confiables resistentes a la computación cuántica?",
    "A": "Las vulnerabilidades de canal lateral en implementaciones criptográficas basadas en isogenias crean desafíos significativos — estos esquemas post-cuánticos como SIKE (aunque recientemente quebrado por ataques clásicos) requieren calcular caminos a través de grafos de isogenias supersingulares con operaciones que tienen variaciones de temporización dependientes de los datos. Las operaciones de generación de claves y encapsulación involucran evaluar isogenias de grado grande con patrones computacionales altamente irregulares que filtran información a través de temporización de caché, predicción de ramas y análisis de potencia. Los TEE deben implementar aritmética de puntos y algoritmos de evaluación de isogenias de tiempo constante mientras previenen fugas de temporización de las operaciones de campo subyacentes, todo lo cual aumenta sustancialmente la sobrecarga comparado con implementaciones de ECC clásicas para las cuales el hardware TEE fue originalmente optimizado.",
    "B": "Las vulnerabilidades de canal lateral en implementaciones de firmas basadas en hash crean desafíos significativos — estos esquemas post-cuánticos con estado como XMSS o SPHINCS+ requieren mantener el estado de clave secreta a través de múltiples operaciones de firma, y las evaluaciones de función hash (a menudo miles por firma) tienen patrones de acceso a memoria correlacionados con la jerarquía de clave secreta. Los algoritmos de recorrido de árbol de merkle y evaluaciones de funciones pseudoaleatorias filtran información de temporización sobre qué claves de firma de un solo uso se están utilizando. Los TEE deben rastrear actualizaciones de estado atómicamente para prevenir la reutilización de claves mientras implementan operaciones hash y navegación de árbol de tiempo constante, requiriendo modificaciones extensas al almacenamiento seguro de claves y control de acceso que no eran necesarias para esquemas de firma clásicos sin estado.",
    "C": "Las vulnerabilidades de canal lateral en implementaciones criptográficas basadas en retículos crean desafíos significativos — estos esquemas post-cuánticos tienen tamaños de clave sustancialmente mayores (a menudo varios kilobytes comparados con cientos de bytes para RSA/ECC) y requieren operaciones de multiplicación de polinomios más complejas que filtran información considerable de temporización y consumo de energía. Los tiempos de ejecución más largos y patrones de acceso a memoria de operaciones como la multiplicación de polinomios basada en NTT son particularmente susceptibles a ataques de temporización de caché y análisis electromagnético, requiriendo que los TEE implementen contramedidas extensas como implementaciones de tiempo constante, ofuscación de acceso a memoria y enmascaramiento de consumo de energía, todo lo cual aumenta la sobrecarga y complejidad.",
    "D": "Las vulnerabilidades de canal lateral en implementaciones criptográficas basadas en códigos crean desafíos significativos — estos esquemas post-cuánticos como Classic McEliece tienen claves públicas extremadamente grandes (cientos de kilobytes a megabytes) que estresan las restricciones de memoria de TEE y requieren operaciones de matriz dispersa durante el cifrado que exhiben patrones de acceso a memoria altamente no uniformes. Los algoritmos de decodificación de síndrome utilizados en el descifrado involucran procedimientos iterativos con comportamiento de ramas dependiente de datos que filtra información sobre posiciones de error a través de temporización y patrones de acceso a caché. Los TEE deben almacenar estas claves sobredimensionadas en memoria protegida mientras implementan muestreo de peso constante y operaciones de permutación en tiempo constante, requiriendo soporte de hardware especializado para regiones grandes de memoria segura y técnicas de enmascaramiento para los algoritmos combinatorios involucrados en la decodificación.",
    "solution": "C"
  },
  {
    "id": 363,
    "question": "Programar desacoplamiento dinámico durante ciclos de corrección de errores requiere una alineación cuidadosa porque los pulsos de desacoplamiento tempranos pueden tener qué efecto perjudicial?",
    "A": "Alarga los circuitos estabilizadores, añadiendo ruido de puertas de dos qubits",
    "B": "Interfiere con las ventanas de temporización de inicialización de ancillas",
    "C": "Interrumpe la coherencia de medición de síndrome durante la lectura",
    "D": "Entra en conflicto con secuencias de pulsos de extracción de estabilizadores",
    "solution": "A"
  },
  {
    "id": 364,
    "question": "¿Cuáles son los principales desafíos para un aprendizaje por transferencia efectivo en el dominio cuántico?",
    "A": "Las variaciones de hardware, el ruido y los problemas de compatibilidad de plataforma crean obstáculos fundamentales al intentar transferir modelos cuánticos pre-entrenados a través de diferentes implementaciones físicas. La variabilidad en conjuntos de puertas nativas, topologías de conectividad de qubits y características de decoherencia significa que los circuitos parametrizados optimizados para un dispositivo a menudo requieren reentrenamiento extenso o transpilación de circuito cuando se despliegan en otra plataforma. Además, los perfiles de ruido no estacionarios inherentes al hardware de la era NISQ causan que las características cuánticas aprendidas se degraden de manera impredecible durante la transferencia, mientras que los recuentos limitados de qubits restringen la flexibilidad arquitectónica necesaria para adaptar capas pre-entrenadas a nuevas tareas objetivo sin interferencia catastrófica en las representaciones aprendidas.",
    "B": "Las estructuras de entrelazamiento específicas de la tarea y los desajustes en la geometría del espacio de Hilbert limitan severamente la transferencia de conocimiento, ya que los mapas de características cuánticos pre-entrenados incrustan datos en patrones de entrelazamiento optimizados para la estructura estadística del dominio fuente. Cuando se transfieren a nuevas tareas con diferentes estructuras de correlación, estas representaciones aprendidas exhiben fenómenos de meseta estéril durante el ajuste fino debido a gradientes que se desvanecen exponencialmente en el paisaje de pérdida objetivo. La incapacidad de realizar congelamiento parcial de capas—una técnica clave de aprendizaje por transferencia clásico—agrava este problema, ya que las capas de circuito cuántico no pueden ser entrenadas selectivamente sin afectar el entrelazamiento global, requiriendo reoptimización casi completa que niega los beneficios del pre-entrenamiento y a menudo funciona peor que la inicialización aleatoria.",
    "C": "Las restricciones de compilación específicas del dispositivo y las dependencias de descomposición de puertas fundamentalmente previenen la portabilidad del circuito a través de plataformas cuánticas, ya que cada arquitectura de hardware requiere implementaciones de puertas nativas que no pueden ser abstraídas sin sobrecarga exponencial. A diferencia de las redes clásicas donde las matrices de peso se transfieren directamente entre implementaciones de CPU y GPU, los circuitos cuánticos parametrizados deben ser recompilados desde cero para cada dispositivo objetivo porque las traducciones de conjunto de puertas universal introducen errores de fase que se acumulan multiplicativamente a través de la profundidad del circuito. Este bloqueo arquitectónico se ve exacerbado por colocaciones de puertas de dos qubits dependientes de la topología, donde las configuraciones de parámetros óptimas en un grafo de conectividad se vuelven subóptimas en otro, necesitando reentrenamiento completo en lugar de ajuste fino para mantener umbrales de fidelidad.",
    "D": "La incompatibilidad de frameworks y las limitaciones de serialización bloquean la portabilidad de modelos cuánticos, ya que no existe un formato de intercambio estandarizado para circuitos cuánticos parametrizados a través de los ecosistemas de Qiskit, Cirq y PennyLane. Cada plataforma usa esquemas de parametrización de puertas e interfaces de backend de optimización propietarios que no pueden ser traducidos directamente, previniendo que modelos pre-entrenados sean cargados en diferentes pilas de software. Mientras que los frameworks de aprendizaje profundo clásico comparten ONNX y estándares similares permitiendo transferencia de modelos sin problemas, la computación cuántica carece de protocolos análogos para codificar parámetros de circuito aprendidos, metadatos de topología de circuito y datos de calibración de hardware en una representación agnóstica de plataforma, forzando a los investigadores a reentrenar desde cero al cambiar de framework a pesar de la física subyacente idéntica.",
    "solution": "A"
  },
  {
    "id": 365,
    "question": "Un enlace práctico de distribución cuántica de claves opera a una tasa de detección bruta de 10 MHz sobre 40 km de fibra. El sistema usa BB84 con estados señuelo con detectores con postpulsado (20% de probabilidad de clic espurio por ventana) y experimenta 0.2 dB/km de pérdida. Un adversario explota el desequilibrio dependiente de la longitud de onda en el divisor de haz del modulador de Alice, permitiendo un sesgo del 3% hacia medir ciertos valores de bit sin activar alarmas de QBER por encima del umbral del 8% establecido por pruebas de seguridad de clave finita a esta distancia. Dado que la amplificación de privacidad extrae aproximadamente 0.4 bits por fotón tamizado bajo estos parámetros, y la destilación de ventaja clásica añade 12% de sobrecarga, ¿por qué este ataque de canal lateral permanece sin detectar por la detección estándar de intercepción-reenvío mientras aún compromete la clave final?",
    "A": "El ataque explota un fallo de implementación física en el hardware de codificación en lugar de manipular el canal cuántico en sí, por lo que deja la tasa de error de bit cuántico dentro de límites aceptables para la prueba de seguridad. Dado que el adversario obtiene información parcial sobre valores de bit a través de una correlación clásica con el comportamiento del modulador en lugar de inducir perturbaciones cuánticas detectables, el ataque elude los umbrales de detección de intercepción-reenvío mientras filtra constantemente entropía de clave que la amplificación de privacidad no puede eliminar completamente cuando el canal lateral persiste a través de todos los bits tamizados, creando un canal de información encubierto que opera fuera del modelo de amenaza asumido por los análisis de seguridad QKD convencionales.",
    "B": "El desequilibrio del divisor de haz dependiente de la longitud de onda crea una correlación clásica entre la elección de base de Alice y las propiedades espectrales de los fotones emitidos que el adversario puede explotar mediante filtrado selectivo de longitud de onda pasivo antes de que los estados cuánticos entren al canal de fibra con pérdidas. Dado que este filtrado ocurre antes de la pérdida del canal y las cuentas oscuras del detector, introduce un sesgo que se manifiesta como una correlación entre valores de bit y tiempos de llegada en los detectores de Bob, apareciendo estadísticamente indistinguible de la firma de postpulsado ya presente en el sistema. La extracción de información parcial del adversario mediante selección de longitud de onda sí aumenta la información de Holevo de Eve, pero el efecto se distribuye a través de la fase de reconciliación de errores donde imita pérdidas de correlación legítimas por ineficiencia del detector.",
    "C": "El canal lateral explota la estructura temporal de eventos de postpulsado correlacionando el sesgo de valor de bit del 3% con la probabilidad de clic espurio del 20%, de tal manera que los eventos de detección corrompidos por postpulso llevan más información sobre los valores de bit codificados por Alice que las detecciones de fotones legítimas. Dado que el postpulsado ya contribuye a la tasa de error base del sistema, y las pruebas de seguridad dan cuenta de esta contribución reduciendo la tasa de clave extraíble mediante estimación de parámetros pesimista, la correlación adicional introducida por el sesgo del modulador cae dentro de los márgenes de incertidumbre del análisis de clave finita. La ganancia de información del adversario se compone a través de múltiples rondas porque los efectos de memoria de postpulsado persisten a través de ventanas de detección secuenciales.",
    "D": "El ataque introduce un sesgo dependiente de la longitud de onda que se correlaciona con el tiempo de establecimiento del modulador de fase de Alice después de cada rotación de base, creando un canal lateral temporal donde los tiempos de emisión de fotones llevan información parcial sobre valores de bit. Esta correlación temporal aparece en las estadísticas de QBER como una tasa de error aumentada en fotones que llegan dentro del primer nanosegundo de cada ventana de puerta de detección, pero los análisis de seguridad existentes atribuyen esta tasa de error elevada a interferencia inter-símbolo de la dispersión cromática en el enlace de fibra de 40 km, que también produce patrones de error dependientes de la temporización. Dado que la dispersión cromática naturalmente aumenta con la distancia a 0.2 dB/km de pérdida correspondiente a aproximadamente 17 ps/nm/km de dispersión, la firma del canal lateral es enmascarada por los errores inducidos por dispersión esperados.",
    "solution": "A"
  },
  {
    "id": 366,
    "question": "¿Por qué se consideran los nodos de confianza una solución provisional en lugar de la arquitectura a largo plazo para la Internet Cuántica?",
    "A": "Comprometen fundamentalmente la seguridad de extremo a extremo al requerir que cada salto intermedio mida y vuelva a preparar el estado cuántico, lo que significa que a cada nodo de confianza se le debe otorgar acceso completo a la información transmitida. Esto viola el principio de retransmisión sin confianza que logra la comunicación cifrada clásica, donde los enrutadores intermedios no pueden acceder al contenido de la carga útil.",
    "B": "Comprometen la ventaja cuántica para la computación distribuida al destruir el entrelazamiento en cada salto mediante retransmisión basada en medición, impidiendo aplicaciones como la computación cuántica ciega y el algoritmo de Shor distribuido que requieren entrelazamiento multipartito coherente a través de puntos finales de la red. Si bien permiten QKD al establecer claves compartidas clásicas, no pueden soportar la fidelidad de canal cuántico necesaria para protocolos donde la computación ocurre en múltiples nodos sin revelar estados intermedios.",
    "C": "Introducen límites fundamentales de escalabilidad porque cada nodo de confianza requiere memorias cuánticas con tiempos de coherencia que excedan el retardo de comunicación clásica de ida y vuelta necesario para establecer el siguiente segmento de enlace, creando un cuello de botella de decoherencia donde los requisitos de T2 crecen linealmente con el diámetro de la red. Las memorias actuales de trampa de iones logran coherencia de ~10 segundos, suficiente para redes metropolitanas pero inadecuadas para distancias transcontinentales donde las latencias de coordinación clásica exceden las capacidades de almacenamiento cuántico.",
    "D": "Crean una vulnerabilidad de seguridad teórica de la información distinta del requisito de confianza: el teorema de no clonación impide detectar escuchas clandestinas en operaciones internas de nodos de confianza, lo que significa que los nodos comprometidos pueden copiar estados cuánticos mediante reconstrucción tomográfica en múltiples ejecuciones de protocolo sin activar alertas de seguridad. A diferencia de QKD punto a punto donde los ataques de interceptar y reenviar perturban las estadísticas del canal de forma detectable, los nodos de confianza miden estados legítimamente, enmascarando cualquier copia ilícita dentro del ruido operacional normal.",
    "solution": "A"
  },
  {
    "id": 367,
    "question": "¿Cómo puede el acoplamiento de fase inducido por resonador permitir un ataque encubierto de envenenamiento de paridad en procesadores de transmon?",
    "A": "Un adversario explota desplazamientos dispersivos de orden superior no monitoreados al informar intencionalmente de manera incorrecta la frecuencia del resonador vestido durante la calibración inicial, causando que el software de control aplique pulsos de compensación de acoplamiento ZZ con desfases incorrectos. A lo largo de cientos de ciclos de puerta, estos errores de fase sistemáticos sesgan los ángulos de rotación condicional acumulados en mediciones de paridad de dos qubits en cantidades que permanecen dentro del piso de ruido de disparo de protocolos estándar de benchmarking aleatorizado, permitiendo que los errores lógicos codificados se propaguen sin detectar a través de rondas de estabilizadores mientras corrompen valores propios de síndrome mediante interferencia controlada.",
    "B": "Un adversario dessintoniza deliberadamente la frecuencia del resonador de bus en pequeñas cantidades, típicamente 10-50 MHz, para acumular interacciones no deseadas de acoplamiento dispersivo ZZ durante múltiples ciclos de puerta. Estos desplazamientos de fase sistemáticos sesgan la paridad lógica medida en códigos estabilizadores sin activar inmediatamente alarmas de recalibración, permitiendo que los errores se acumulen por debajo de umbrales de detección mientras corrompen la información cuántica codificada mediante interferencia controlada con las operaciones de entrelazamiento nativas de dos qubits del circuito de extracción de síndrome.",
    "C": "Al diseñar fugas transitorias filtradas por Purcell al tercer estado excitado del resonador mediante pulsos de microondas cuidadosamente sincronizados en la frecuencia de transición |2⟩↔|3⟩, un atacante induce desplazamientos de Stark de CA que modulan la intensidad efectiva de interacción ZZ entre transmons acoplados capacitivamente. Estos acoplamientos dispersivos variables en el tiempo crean errores de fase coherentes en mediciones de verificación de paridad que promedian casi cero en ciclos estabilizadores individuales pero se acumulan constructivamente en secuencias más largas, sesgando sistemáticamente resultados de síndrome lógico por debajo del umbral para activar recalibración mientras corrompen información codificada mediante interferencia coherente en fase.",
    "D": "Un atacante explota la dependencia paramétrica de las tasas de acoplamiento longitudinal del número de fotones del resonador modulando sutilmente la amplitud de excitación aplicada a qubits ancilla durante secuencias de medición de paridad. Esto hace que el hamiltoniano de interacción ZZ efectivo entre qubits de datos adquiera una fase dependiente del tiempo que rota coherentemente la base computacional de dos qubits a tasas comparables a la duración inversa de puerta. El sesgo sistemático resultante en las paridades medidas permanece oculto dentro de tolerancias de calibración porque se manifiesta como una aparente mala calibración del ángulo de rotación en lugar de una firma de error distinta.",
    "solution": "B"
  },
  {
    "id": 368,
    "question": "¿Cuál es el beneficio principal de usar decodificadores neuronales para corrección de errores cuánticos en comparación con decodificadores tradicionales?",
    "A": "Los decodificadores neuronales pueden aprender mapeos de síndrome a corrección que implícitamente consideran errores de medición y diafonía durante la extracción del síndrome en sí, adaptándose al modelo de ruido completo incluyendo circuitos estabilizadores defectuosos en lugar de asumir mediciones de síndrome perfectas. Al entrenar con datos experimentales que incluyen errores de medición de síndrome, estos decodificadores logran mayor fidelidad lógica que el emparejamiento perfecto de peso mínimo en el mismo hardware, aunque todavía requieren extracción completa de síndrome y pueden necesitar tiempo de procesamiento clásico comparable para pases hacia adelante a través de redes profundas.",
    "B": "Pueden adaptarse a modelos de ruido complejos y específicos del dispositivo que se extienden más allá de canales estándar de despolarización o Pauli, incluyendo errores espacialmente correlacionados y efectos no markovianos, mientras que potencialmente requieren significativamente menos tiempo de procesamiento clásico por síndrome mediante reconocimiento de patrones aprendido en lugar de decodificación exhaustiva de máxima verosimilitud sobre clases de error exponencialmente grandes.",
    "C": "Los decodificadores neuronales implementan algoritmos de contracción de redes tensoriales mediante patrones de conectividad aprendidos, donde los pesos entrenados codifican secuencias óptimas de contracción para grafos de síndrome. Este enfoque reduce la sobrecarga exponencial de decodificación de máxima verosimilitud a complejidad polinomial al explotar propagación de creencias aproximada en grafos de factores, aunque la implementación aún requiere coprocesadores FPGA criogénicos para lograr latencia submicrosegundo exigida por tiempos de ciclo de surface code. El orden de contracción aprendido se adapta a la conectividad de qubit específica del dispositivo sin optimización manual.",
    "D": "Logran rendimiento por debajo del umbral al aprender correlaciones no lineales de síndrome que violan los supuestos de independencia local subyacentes a decodificadores tradicionales como MWPM. Mediante entrenamiento en cadenas de error correlacionadas generadas por modelos de ruido realistas incluyendo fugas y errores coherentes, las redes neuronales descubren firmas de error de orden superior que permanecen ocultas para enfoques basados en grafos, permitiendo supresión de errores lógicos por debajo del umbral de surface code incluso con tasas de error físico al 1%, aunque el procesamiento clásico actualmente requiere ~10ms por síndrome, lo que excede presupuestos típicos de ciclo de código.",
    "solution": "B"
  },
  {
    "id": 369,
    "question": "¿Qué protocolo avanzado proporciona la seguridad más fuerte para autenticación cuántica?",
    "A": "Autenticación de mensajes cuántica con funciones no clonables — estas aprovechan el teorema de no clonación para crear etiquetas de autenticación fundamentalmente infalsificables que no pueden copiarse incluso por un adversario con poder computacional cuántico ilimitado. Al codificar la clave de autenticación en estados cuánticos no ortogonales distribuidos en múltiples qubits, cualquier intento de duplicar el autenticador introduce perturbaciones detectables mediante retroacción de medición, proporcionando seguridad teórica de la información que excede incluso los MACs clásicos post-cuánticos.",
    "B": "Los códigos de autenticación de mensajes cuánticamente seguros se basan en primitivas criptográficas basadas en retículos o funciones hash que permanecen computacionalmente difíciles incluso contra ataques cuánticos, proporcionando seguridad de autenticación que escala con la longitud de clave según las limitaciones del algoritmo de Grover.",
    "C": "Autenticadores cuánticos de un solo uso, que proporcionan seguridad incondicional al consumir entrelazamiento cuántico compartido fresco para cada evento de autenticación, asegurando que incluso adversarios computacionalmente ilimitados no puedan falsificar mensajes. Estos protocolos logran seguridad teórica de la información mediante las propiedades fundamentales de la mecánica cuántica en lugar de suposiciones computacionales.",
    "D": "Las firmas digitales cuánticas logran no repudio incondicional mediante distribución de entrelazamiento multipartito, donde el estado cuántico del firmante no puede falsificarse ni negarse después del hecho debido a restricciones de monogamia del entrelazamiento. El protocolo genera autenticación transferible que sobrevive incluso si la clave privada del firmante se ve comprometida posteriormente, porque la verificación de firma depende de pares EPR previamente distribuidos cuyas correlaciones se establecieron en el momento de la firma y no pueden alterarse retroactivamente, proporcionando un modelo de seguridad más fuerte que esquemas de autenticación de un solo uso que carecen de garantías de no repudio.",
    "solution": "C"
  },
  {
    "id": 370,
    "question": "¿Qué metodología de ataque avanzada se dirige a los supuestos en protocolos de distribución de claves cuánticas independientes del dispositivo?",
    "A": "El control superdeterminista de canal explota el supuesto de independencia de medición al permitir que un adversario diseñe correlaciones entre las variables ocultas que gobiernan el comportamiento del dispositivo y las elecciones de configuraciones de medición, creando efectivamente una causa común que viola la independencia estadística sin requerir señalización más rápida que la luz. Al preparar cuidadosamente las condiciones iniciales del canal cuántico de manera correlacionada con futuras elecciones de medición, el atacante puede simular violaciones de Bell mientras extrae información completa de la clave, eludiendo las restricciones de no señalización en las que se basan los protocolos independientes del dispositivo.",
    "B": "Las violaciones artificiales de la desigualdad CHSH se logran cuando un espía manipula los eventos de detección explotando la laguna de libertad de elección combinada con ataques de sincronización temporal, causando que las correlaciones medidas excedan el límite clásico de 2 sin que esté presente entrelazamiento cuántico genuino. El atacante usa comunicación clásica precisamente sincronizada entre estaciones de medición, oculta dentro de la ventana de coincidencia, para coordinar resultados de detección que imitan la predicción cuántica de 2√2, engañando así al protocolo para que acepte una clave comprometida como segura mientras el estado cuántico real permanece separable.",
    "C": "Las variables ocultas que explotan lagunas permiten a un adversario dirigirse al supuesto de independencia de medición explotando simultáneamente brechas de eficiencia de detección y lagunas de localidad, creando violaciones artificiales de Bell que parecen legítimas al protocolo mientras mantienen una estructura de correlación oculta que filtra información de la clave mediante modelos de variables ocultas locales cuidadosamente orquestados.",
    "D": "La manipulación de testigos de dimensión implica que un adversario prepare estados entrelazados de mayor dimensión que pasan la prueba de Bell del protocolo mientras codifican secretamente información en subespacio dimensional no utilizado que los operadores de testigo bidimensionales estándar no pueden detectar, extrayendo información parcial de la clave sin activar límites de violación CHSH.",
    "solution": "C"
  },
  {
    "id": 371,
    "question": "En Qiskit, ¿qué método se utiliza para medir un qubit y almacenar el resultado en un bit clásico?",
    "A": "qc.sample(qubit, cbit)",
    "B": "qc.measure(qubit, cbit)",
    "C": "qc.project(qubit, cbit)",
    "D": "qc.collapse(qubit, cbit)",
    "solution": "B"
  },
  {
    "id": 372,
    "question": "¿Qué es la compilación dinámica de circuitos en computación cuántica?",
    "A": "La compilación dinámica realiza una traducción justo a tiempo de algoritmos cuánticos abstractos en secuencias de puertas optimizadas para la arquitectura específica del procesador cuántico objetivo, tomando decisiones sobre asignación de qubits y descomposición de puertas durante el flujo de trabajo de envío de tareas en lugar de en el momento del diseño del algoritmo, permitiendo al compilador explotar datos de calibración en tiempo real y mediciones actuales de tasas de error para maximizar la fidelidad del circuito.",
    "B": "En lugar de realizar todo el proceso de transpilación y optimización durante una fase de preprocesamiento fuera de línea antes del envío de tareas, la compilación dinámica pospone decisiones clave de compilación hasta que el algoritmo cuántico se está ejecutando activamente en el hardware, tomando decisiones sobre descomposición de puertas, mapeo de qubits y programación de circuitos basándose en información en tiempo de ejecución como la profundidad actual de la cola y las tasas de error de puertas medidas.",
    "C": "Enfoques de compilación que generan circuitos cuánticos capaces de ajustar su estructura, secuencias de puertas y operaciones de qubits basándose en resultados de medición obtenidos durante la ejecución a medio circuito, permitiendo ramificación condicional y algoritmos adaptativos.",
    "D": "La compilación dinámica se refiere a técnicas de optimización de circuitos cuánticos que adaptan la secuencia de puertas compilada según el camino computacional específico que tome el algoritmo durante la ejecución, utilizando retroalimentación de mediciones para seleccionar entre ramas de circuito precompiladas almacenadas en memoria clásica, reduciendo así el número total de puertas al ejecutar solo las puertas relevantes para la trayectoria cuántica medida en lugar de preparar todos los posibles caminos de resultados en superposición.",
    "solution": "C"
  },
  {
    "id": 373,
    "question": "¿Por qué son beneficiosos los reinicios de qubits a medio circuito en circuitos iterativos de estimación de fase?",
    "A": "Reciclar ancillas reduce el número de qubits, permitiendo que los mismos qubits físicos desempeñen múltiples roles a través de rondas secuenciales de estimación en lugar de requerir qubits ancilla nuevos para cada aplicación de unitaria controlada, lo cual es especialmente valioso en dispositivos de corto plazo con registros de qubits limitados donde reutilizar una sola ancilla a través de iteraciones permite protocolos de estimación de fase más profundos de los que cabrían dentro de las restricciones del hardware.",
    "B": "Reciclar ancillas reduce el número de qubits, permitiendo que los mismos qubits físicos desempeñen múltiples roles a través de rondas secuenciales de estimación en lugar de requerir qubits ancilla nuevos para cada aplicación de unitaria controlada. Sin embargo, los reinicios a medio circuito introducen decoherencia adicional porque la retroacción de medición durante las operaciones de reinicio colapsa las superposiciones cuánticas en qubits de datos cercanos a través de diafonía, por lo que aunque el número de qubits disminuye, la profundidad efectiva del circuito aumenta al tener en cuenta la propagación de errores de reinicios imperfectos que deben modelarse como canales despolarizantes con fidelidad ~99.5% en el hardware superconductor actual.",
    "C": "Reciclar ancillas reduce el número de qubits, permitiendo que los mismos qubits físicos desempeñen múltiples roles a través de rondas secuenciales de estimación en lugar de requerir qubits ancilla nuevos para cada aplicación de unitaria controlada. La operación de reinicio proyecta la ancilla a |0⟩ mediante medición seguida de inversión condicional de bit, desentrelazándola efectivamente del registro de autoestado para que la información de fase acumulada se transfiera a memoria clásica antes de la siguiente iteración. Este colapso inducido por medición preserva la unitariedad en el subespacio de autoestado porque la ancilla se factoriza, permitiendo que el phase kickback se acumule coherentemente a través de rondas a pesar de la medición intermedia, lo cual es crucial para algoritmos iterativos donde la precisión de fase mejora geométricamente.",
    "D": "Reciclar ancillas reduce la latencia del circuito al permitir rondas de estimación paralelas que de otro modo requerirían programación secuencial en pares de qubits espacialmente separados. Los reinicios a medio circuito permiten que la misma ancilla interrogue simultáneamente múltiples qubits de autoestado a través de multiplexación temporal—la operación de reinicio se completa en ~1 μs mientras que las puertas unitarias controladas toman ~100 ns, por lo que durante un ciclo de reinicio de ancilla, el circuito puede canalizar 10 operaciones controladas en diferentes qubits de autoestado. Esta paralelización es especialmente valiosa en dispositivos de corto plazo con conectividad limitada donde las restricciones de enrutamiento serializarían las operaciones, permitiendo que el rendimiento de estimación de fase escale linealmente con el número de ancillas en lugar del tamaño del registro de autoestado.",
    "solution": "A"
  },
  {
    "id": 374,
    "question": "Al aumentar la distancia del código, ¿por qué debe también aumentarse la frecuencia de medición del estabilizador para mantener la fidelidad lógica?",
    "A": "Los códigos de distancia d toleran ⌊(d-1)/2⌋ errores por ciclo, pero solo si se miden lo suficientemente rápido para prevenir la acumulación más allá de este límite umbral.",
    "B": "Los qubits ancilla físicos posicionados lejos del circuito de extracción de síndrome experimentan decoherencia aumentada debido a mecanismos de defasamiento dependientes de la distancia espacial en el hardware de control, donde la diafonía electromagnética escala cuadráticamente con la separación de qubits. Para contrarrestar este ruido amplificado por distancia, las mediciones de síndrome deben realizarse a tasas proporcionalmente más altas—típicamente siguiendo una ley de escalamiento de raíz cuadrada—para refrescar la coherencia de ancilla antes de que los errores de fase acumulativos excedan la capacidad de corrección del marco de Pauli del formalismo de estabilizadores.",
    "C": "La electrónica de control clásica introduce retardos de propagación de señal que escalan linealmente con el diámetro físico del arreglo de qubits, creando asimetría temporal entre disparadores de medición en bordes opuestos de códigos de gran distancia. Este cuello de botella de latencia fuerza a los circuitos de lectura de síndrome a operar a frecuencias elevadas para mantener coherencia temporal a través de toda la ronda de medición del estabilizador, asegurando que las verificaciones de paridad se completen dentro de un solo ciclo de reloj lógico antes de que errores espacialmente distribuidos se correlacionen a través de hamiltonianos de acoplamiento residual.",
    "D": "A medida que aumenta la distancia del código, la ventana temporal entre mediciones consecutivas de síndrome expande la oportunidad para que cadenas de errores no corregidos se propaguen a través de múltiples qubits de datos, formando patrones de error correlacionados lógicamente dañinos. Los códigos de mayor distancia requieren más tiempo por ronda de medición debido a sus arreglos de qubits más grandes, por lo que la frecuencia de medición debe escalar proporcionalmente para detectar y corregir estas cadenas de errores en propagación antes de que se acumulen más allá de la capacidad de corrección umbral del código.",
    "solution": "D"
  },
  {
    "id": 375,
    "question": "¿Cómo asiste el aprendizaje supervisado en el enrutamiento de entrelazamiento?",
    "A": "Predice el rendimiento de enlaces a partir de datos históricos para guiar la selección de rutas mediante el entrenamiento de modelos de regresión o clasificación sobre características como mediciones previas de fidelidad de entrelazamiento, tiempos de coherencia de qubits, topología de conectividad de nodos y estadísticas medidas de pérdida de canal, utilizando luego estos modelos aprendidos para estimar qué rutas a través de la red cuántica maximizarán la fidelidad de extremo a extremo o minimizarán la profundidad esperada de intercambio. Este enfoque basado en datos permite decisiones de enrutamiento adaptativas que tienen en cuenta condiciones de red variables en el tiempo e imperfecciones del hardware sin requerir modelos físicos perfectos de procesos de decoherencia.",
    "B": "Mediante el entrenamiento de políticas de redes neuronales a través de aprendizaje por diferencias temporales sobre resultados históricos de enrutamiento para predecir secuencias óptimas de intercambio de entrelazamiento que maximicen la fidelidad de extremo a extremo a través de redes cuánticas multi-salto, utilizando características de entrada que incluyen tiempos de coherencia de nodos, fidelidades de estados de Bell a nivel de enlace y métricas topológicas de grafos como diámetro de grafo y conectividad algebraica. El modelo supervisado aprende a mapear observaciones de estado de red a decisiones de enrutamiento minimizando el error de predicción en conjuntos de datos etiquetados donde las etiquetas verdaderas indican qué rutas lograron las fidelidades de parámetros de Werner más altas en episodios de enrutamiento previos. Este enfoque permite adaptación dinámica de rutas basada en patrones de degradación de calidad de enlaces en tiempo real sin requerir modelos explícitos de decoherencia.",
    "C": "A través de clasificadores cuánticos variacionales entrenados en conjuntos de datos sintéticos de enrutamiento que codifican la topología de red como entradas de redes neuronales de grafos, aprendiendo a predecir probabilidades de éxito de distribución de entrelazamiento condicionadas a métricas de coherencia de memoria de nodos intermedios y fidelidades de transmisión de canal medidas mediante tomografía de estados cuánticos. El marco de aprendizaje supervisado optimiza entradas de tablas de enrutamiento retropropagando gradientes a través de simuladores de red diferenciables que modelan pérdidas de fidelidad de intercambio de entrelazamiento como funciones de parámetros de Werner en cada salto. Al etiquetar ejemplos de entrenamiento con resultados exitosos versus fallidos de generación de entrelazamiento de extremo a extremo, el modelo aprende representaciones de características que capturan correlaciones sutiles entre patrones de congestión de red y estrategias óptimas de purificación.",
    "D": "Empleando modelos de regresión de procesos gaussianos entrenados en datos de series temporales de tiempos T₂ de qubits y tasas de generación de entrelazamiento a través de enlaces de red, prediciendo la degradación futura de calidad de enlaces y reenrutando preventivamente la comunicación cuántica a través de rutas alternativas antes de que la fidelidad caiga por debajo de umbrales de protocolo. El enfoque supervisado aprende correlaciones temporales entre fluctuaciones de ruido ambiental y trayectorias de decaimiento de fidelidad de entrelazamiento, utilizando métodos de kernel para interpolar fidelidades esperadas de pares de Bell en pasos temporales futuros a partir de mediciones históricas dispersas. Este enrutamiento predictivo minimiza la latencia al seleccionar rutas cuyo producto fidelidad-ancho de banda proyectado permanezca por encima de umbrales mínimos para la capacidad de corrección de errores del protocolo, aprovechando la regresión supervisada para evitar enlaces que se predice entrarán en ventanas de mantenimiento.",
    "solution": "A"
  },
  {
    "id": 376,
    "question": "¿Qué propiedades cuánticas utiliza el aprendizaje por refuerzo cuántico?",
    "A": "Aleatoriedad inducida por medición para mejorar la convergencia, porque la estocasticidad inherente de los resultados de medición cuántica proporciona una fuente natural de ruido de exploración que es fundamentalmente diferente de las estrategias clásicas epsilon-greedy o de exploración de Boltzmann. Al codificar la política como un estado cuántico y medirlo en diferentes bases, el agente puede muestrear acciones de una distribución que automáticamente equilibra exploración y explotación a través de las probabilidades de la regla de Born.",
    "B": "Emplea el principio de incertidumbre de Heisenberg para determinar simultáneamente tanto la acción óptima como su recompensa con precisión absoluta, explotando el álgebra no conmutativa de observables para extraer más información de lo que es posible clásicamente. Al preparar el estado del agente como un autoestado tanto del operador de acción como del operador de función de valor, el algoritmo elude la limitación fundamental que enfrenta el RL clásico al intentar estimar valores Q y seleccionar acciones en paralelo.",
    "C": "Superposición para explorar múltiples acciones simultáneamente y entrelazamiento para aprender representaciones de estados complejas y correlacionadas que capturan interacciones multiagente. Estas propiedades permiten que el RL cuántico codifique espacios de políticas exponencialmente grandes en un número polinomial de qubits y procese estructuras de recompensa con paralelismo cuántico.",
    "D": "Decoherencia para mezclar aleatoriamente las políticas de manera controlada, imitando el recocido simulado para la optimización de políticas. A medida que el estado cuántico experimenta decoherencia ambiental, los elementos fuera de la diagonal de la matriz de densidad decaen a una tasa proporcional al parámetro inverso de temperatura, implementando efectivamente un esquema de recocido cuántico que explora políticas de alta energía al principio del entrenamiento y progresivamente colapsa hacia la política de estado fundamental.",
    "solution": "C"
  },
  {
    "id": 377,
    "question": "¿Cuál es el propósito del reconocimiento de patrones en la optimización de circuitos cuánticos?",
    "A": "Identificar subcircuitos que coinciden con patrones de puertas conocidos de una biblioteca, y luego reemplazarlos con implementaciones equivalentes preoptimizadas que usan menos puertas o tienen profundidad reducida. Este reconocimiento de patrones permite la optimización sistemática al aprovechar identidades algebraicas y descomposiciones de puertas previamente calculadas para simplificar la estructura del circuito.",
    "B": "Detectar motivos repetidos en circuitos parametrizados para habilitar la compilación por lotes de ansätze variacionales, donde múltiples instancias de parámetros comparten la misma topología de puertas. El reconocedor de patrones identifica estas similitudes estructurales y las compila en un único programa de puertas reutilizable con parámetros variables, reduciendo la sobrecarga de optimización clásica en algoritmos VQE al eliminar pases de transpilación redundantes para cada actualización de parámetros.",
    "C": "Alinear subcircuitos del circuito con los mapas de error de puertas calibradas del dispositivo, haciendo coincidir la topología del circuito con regiones del chip donde secuencias de puertas similares han sido caracterizadas mediante randomized benchmarking. El optimizador identifica plantillas correspondientes a combinaciones de puertas bien calibradas y dirige la colocación del circuito hacia estas regiones precaracterizadas, explotando correlaciones espaciales en el modelo de error del dispositivo para minimizar la infidelidad general del circuito.",
    "D": "Identificar subcircuitos isomorfos a través de diferentes instancias de algoritmos para habilitar la optimización de compilación cruzada, donde secuencias de puertas de circuitos previamente optimizados se reutilizan como plantillas para nuevos problemas. El reconocedor de patrones calcula homomorfismos de circuitos usando algoritmos de isomorfismo de grafos en el grafo de interacción del circuito, y luego aplica cancelaciones de puertas y reglas de conmutación previamente descubiertas al nuevo circuito, con la biblioteca de plantillas acumulando optimizaciones a lo largo del historial de ejecución del compilador.",
    "solution": "A"
  },
  {
    "id": 378,
    "question": "Al implementar una puerta trasera cuántica basada en circuitos usando síntesis aproximada, ¿qué vector de ataque específico se crea?",
    "A": "Aproximaciones deliberadamente diseñadas en la descomposición unitaria que introducen errores de fase controlados que se acumulan constructivamente para la mayoría de las entradas pero se cancelan destructivamente cuando el circuito procesa patrones de datos objetivo específicos. El atacante diseña el algoritmo de síntesis para sustituir puertas cuyas desviaciones de fase suman casi cero para estados de entrada preseleccionados, mientras producen grandes errores acumulados en casos de prueba aleatorios, creando una puerta trasera que parece fallar la verificación estándar pero en realidad proporciona salidas correctas para entradas elegidas adversarialmente.",
    "B": "Aproximaciones deliberadamente diseñadas en la descomposición de puertas que introducen errores controlados que permanecen latentes para la mayoría de las entradas pero desencadenan comportamiento incorrecto cuando el circuito procesa patrones de datos objetivo específicos. El atacante diseña el algoritmo de síntesis para sustituir puertas que se desvían del unitario ideal de maneras que corrompen el cálculo solo para estados de entrada preseleccionados, creando una puerta trasera que se activa condicionalmente mientras pasa la verificación estándar en casos de prueba aleatorios.",
    "C": "Aproximaciones deliberadamente diseñadas en la compilación de puertas base que introducen rotaciones de marco de Pauli controladas que se propagan transparentemente a través de operaciones que conmutan pero se acumulan destructivamente cuando el circuito procesa patrones de datos objetivo específicos. El atacante diseña el algoritmo de síntesis para insertar puertas cuyos errores de Pauli permanecen sin corregir por técnicas estándar de mitigación de errores para estados de entrada preseleccionados, creando una puerta trasera que evita la compilación consciente del ruido mientras mantiene estimaciones de fidelidad del circuito que coinciden con predicciones teóricas en benchmarks aleatorios.",
    "D": "Aproximaciones deliberadamente diseñadas en la optimización del circuito que introducen canales de decoherencia controlados que permanecen por debajo de los umbrales de detección para la mayoría de las entradas pero amplifican el ruido selectivamente cuando el circuito procesa patrones de datos objetivo específicos. El atacante diseña el algoritmo de síntesis para enrutar operaciones a través de qubits físicos cuyas tasas de error calibradas parecen normales en conjunto pero exhiben fallos correlacionados de puertas de dos qubits para estados de entrada preseleccionados, creando una puerta trasera que explota características de ruido específicas del dispositivo mientras pasa protocolos estándar de randomized benchmarking.",
    "solution": "B"
  },
  {
    "id": 379,
    "question": "En arquitecturas cuánticas distribuidas, las interconexiones fotónicas permiten el escalado modular al proporcionar conectividad entre procesadores cuánticos físicamente separados. Al diseñar tales sistemas, los ingenieros deben equilibrar las pérdidas del enlace, las tasas de generación de entrelazamiento y la latencia introducida por la conmutación fotónica. ¿Cómo contribuyen específicamente las interconexiones fotónicas a la modularidad en estas arquitecturas cuánticas distribuidas?",
    "A": "Las interconexiones fotónicas soportan la distribución de entrelazamiento anunciado entre módulos remotos mediante mediciones probabilísticas de estados de Bell, permitiendo la generación de entrelazamiento asíncrona que desacopla las escalas temporales de operación de los módulos, aunque esto introduce latencia por los retrasos de anuncio y la sobrecarga de purificación de entrelazamiento.",
    "B": "Permiten la conectividad cuántica de largo alcance entre módulos sin requerir contacto físico directo o acoplamiento de corto alcance entre los procesadores cuánticos, permitiendo que los módulos estén físicamente separados mientras mantienen correlaciones cuánticas",
    "C": "Al convertir qubits estacionarios en qubits fotónicos voladores para transmisión, estas interconexiones permiten la transferencia de estados cuánticos entre tipos de procesadores heterogéneos sin requerir interfaces de acoplamiento directo con impedancia ajustada, aunque la pérdida de fotones y la ineficiencia de detección limitan las distancias prácticas de transmisión.",
    "D": "Los enlaces fotónicos facilitan protocolos de teletransportación cuántica entre módulos al distribuir pares de fotones entrelazados previamente compartidos, permitiendo la transferencia de estados cuánticos sin interacciones directas qubit-qubit, pero requiriendo canales de comunicación clásica para la transmisión de resultados de medición requerida que introduce latencia de teletransportación.",
    "solution": "B"
  },
  {
    "id": 380,
    "question": "Una fundición maliciosa añade un condensador de acoplamiento oculto entre qubits de flujo adyacentes. ¿Qué clase de puerta trasera de hardware describe mejor esta modificación?",
    "A": "Una puerta trasera de acoplamiento paramétrico que explota la inductancia sintonizable de los bucles SQUID en qubits de flujo para crear bandas laterales moduladas en amplitud en el hamiltoniano de interacción entre qubits, permitiendo la extracción de información de polarización de flujo mediante detección heterodina de las frecuencias de transición modificadas. El condensador malicioso introduce coeficientes de acoplamiento dependientes del tiempo que modulan a la frecuencia de diferencia entre los tonos de excitación de qubits adyacentes, produciendo firmas observables en el campo electromagnético dispersado que codifican qué estados de base computacional se están preparando durante secuencias de puertas sin requerir acceso directo de medición a líneas de control protegidas.",
    "B": "Un implante de amplificación de diafonía que mejora el acoplamiento capacitivo naturalmente presente entre qubits de flujo vecinos más allá de las especificaciones de diseño, permitiendo la lectura por canal lateral de cambios de polarización de flujo y señales de control. El condensador malicioso aumenta la fuerza de interacción no deseada entre qubits, permitiendo que un adversario con acceso al aparato de medición en un qubit extraiga información sobre operaciones que se realizan en qubits adyacentes mediante fuga de señal correlacionada, convirtiendo efectivamente el diseño espacial del procesador cuántico en un canal de información explotable.",
    "C": "Un canal de espionaje de entrelazamiento que acopla el par de qubits de flujo a través de un término de interacción ZZ siempre activo no deseado, entrelazando continuamente sus estados computacionales incluso cuando no se están aplicando puertas explícitas de dos qubits. El condensador añadido modifica la inductancia mutua efectiva entre bucles de polarización de flujo, creando un acoplamiento Ising persistente que correlaciona los estados de qubit proporcionalmente a sus valores de flujo, permitiendo que un adversario infiera información cuántica al monitorear la dinámica temporal de la población de un qubit, ya que ahora reflejan la trayectoria entrelazada del sistema conjunto en lugar de dinámicas independientes de un solo qubit.",
    "D": "Una puerta trasera de inyección de estados cuánticos que explota el papel del condensador como elemento de acoplamiento sintonizable que puede activarse externamente mediante pulsos de microondas cuidadosamente temporizados a la frecuencia resonante del condensador, que difiere de las frecuencias de transición de los qubits. Cuando el adversario aplica un tono de excitación que coincide con la resonancia LC del condensador, aumenta temporalmente la fuerza de acoplamiento entre qubits más allá de los parámetros operacionales normales, permitiendo operaciones de entrelazamiento rápidas que evitan el sistema de control autenticado del procesador mientras dejan firmas de estado cuántico consistentes con fidelidades de puertas estándar de dos qubits, haciendo que las operaciones no autorizadas sean difíciles de detectar mediante protocolos de benchmarking convencionales.",
    "solution": "B"
  },
  {
    "id": 381,
    "question": "¿Cuál es la ventaja clave de los enfoques de aprendizaje automático cuántico basados en computación cuántica adiabática?",
    "A": "Capacidad potencial de encontrar mínimos globales de funciones de pérdida no convexas manteniendo el sistema en su estado fundamental instantáneo a lo largo de la evolución, evitando así los mínimos locales que atrapan a los optimizadores clásicos basados en gradiente.",
    "B": "Implementación natural de problemas de optimización centrales al aprendizaje automático mediante la codificación directa de funciones de costo como hamiltonianos del problema, donde el estado fundamental del hamiltoniano final codifica la solución óptima de la tarea de aprendizaje.",
    "C": "Todas las anteriores",
    "D": "Robustez inherente a ciertos tipos de ruido porque el proceso adiabático opera en la variedad del estado fundamental, que está energéticamente separada de los estados excitados por una brecha espectral que actúa como un amortiguador protector contra fluctuaciones térmicas y perturbaciones ambientales con energía insuficiente para inducir transiciones fuera del subespacio computacional.",
    "solution": "C"
  },
  {
    "id": 382,
    "question": "¿Qué técnica aborda eficazmente la vulnerabilidad de nodo confiable en las redes de distribución cuántica de claves?",
    "A": "El intercambio de entrelazamiento en nodos intermedios crea seguridad de extremo a extremo al teleportar estados cuánticos a través de la red sin descifrar nunca el material de clave en los puntos de relevo, ya que las mediciones de estado de Bell solo revelan información de correlación en lugar de los bits de clave sin procesar. Esto transforma una arquitectura de nodo confiable multisalto en un canal cuántico lógicamente directo donde el compromiso adversario de estaciones intermedias no produce información sobre el secreto compartido final, eliminando efectivamente el requisito de confianza mediante propiedades mecánicas cuánticas de pares de fotones entrelazados.",
    "B": "QKD de campo gemelo elimina el problema de nodo confiable haciendo que ambas partes legítimas envíen estados coherentes con fase aleatoria a una estación de medición central que realiza interferencia de un solo fotón, la cual revela solo la correlación de fase entre los pulsos de Alice y Bob sin exponer el material de clave sin procesar de ninguna de las partes.",
    "C": "Los repetidores cuánticos establecen entrelazamiento de extremo a extremo entre partes distantes mediante distribución e intercambio de entrelazamiento, eliminando puntos de descifrado intermedios. Al crear conexiones entrelazadas directas a través de segmentos de red, eliminan la necesidad de confiar en nodos de relevo con claves en texto claro.",
    "D": "QKD independiente del dispositivo de medición solo asegura los detectores, no los nodos intermedios donde las claves se almacenan temporalmente en forma de texto claro antes de ser reenviadas. Si bien elimina exitosamente las vulnerabilidades de canal lateral del detector al tratar el aparato de medición como una caja negra controlada por el adversario, el protocolo aún requiere relevos confiables para descifrar, almacenar y reencriptar claves en cada salto de red, dejando al sistema vulnerable a los mismos compromisos que afectan a las implementaciones QKD tradicionales de preparar y medir a través de enlaces de fibra multisegmento.",
    "solution": "C"
  },
  {
    "id": 383,
    "question": "En el contexto de la corrección de errores cuánticos, los teoremas de umbral tolerante a fallos son fundamentales porque abordan una cuestión clave sobre si la computación cuántica puede ser práctica dado que todos los componentes físicos son inherentemente imperfectos y sujetos a ruido. Estos teoremas proporcionan garantías cruciales sobre lo que es teóricamente alcanzable al construir computadoras cuánticas a gran escala. ¿Qué garantía específica proporcionan los teoremas de umbral tolerante a fallos sobre la viabilidad de la computación cuántica confiable con hardware ruidoso?",
    "A": "Establecen que la computación cuántica arbitrariamente confiable se vuelve posible con componentes imperfectos, siempre que la tasa de error física por puerta se mantenga por debajo de un valor umbral crítico",
    "B": "Demuestran que las tasas de error lógico pueden suprimirse exponencialmente con la distancia del código para cualquier tasa de error física, siempre que se invierta sobrecarga suficiente en codificación concatenada, aunque los umbrales prácticos dependen del modelo de ruido específico y la profundidad del circuito de extracción de síndrome",
    "C": "Garantizan que la sobrecarga polilogarítmica en qubits físicos es suficiente para lograr fidelidad lógica arbitraria cuando las tasas de error físico están por debajo del umbral, con los factores constantes en la sobrecarga determinados por la relación entre el tiempo de medición de síndrome y el tiempo de ejecución de puerta",
    "D": "Demuestran que la computación cuántica sigue siendo viable incluso cuando las tasas de error físico se acercan al 50% por puerta, porque los códigos topológicos con algoritmos decodificadores apropiados aún pueden extraer información útil de los datos de síndrome fuertemente corrompidos mediante métodos de inferencia estadística",
    "solution": "A"
  },
  {
    "id": 384,
    "question": "¿Cuál es la principal ventaja de los métodos de núcleo cuántico sobre los métodos de núcleo clásico?",
    "A": "Espacios de características exponencialmente grandes, implícitamente — el núcleo cuántico puede mapear datos clásicos en un espacio de Hilbert cuya dimensión crece exponencialmente con el número de qubits, permitiendo la representación de patrones complejos sin calcular explícitamente todas las coordenadas de características. Este acceso implícito permite que los algoritmos cuánticos evalúen productos internos en espacios de características que serían intratables de almacenar para computadoras clásicas.",
    "B": "Espacios de características exponencialmente grandes con separación demostrable — los núcleos cuánticos pueden incrustar datos en espacios de características de dimensión 2^n donde ciertos valores de núcleo se vuelven difíciles de estimar clásicamente debido a la anticoncentración de amplitudes cuánticas, como muestra el problema de forrelation. Sin embargo, esta ventaja requiere mapas de características cuidadosamente elegidos; los circuitos cuánticos aleatorios a menudo producen núcleos que se concentran alrededor de valores que los métodos clásicos pueden aproximar eficientemente, limitando la aceleración práctica a menos que el mapa de características esté específicamente diseñado para evitar esta concentración.",
    "C": "Matrices de núcleo exponencialmente expresivas mediante interferencia cuántica — la capacidad de construir funciones de núcleo cuyas entradas involucran amplitudes de valores complejos que interfieren constructiva o destructivamente basándose en la estructura de datos. Mientras que los núcleos clásicos están restringidos a matrices semidefinidas positivas de valores reales con entradas computables en tiempo polinomial, los núcleos cuánticos pueden acceder a una clase de funciones más rica. Sin embargo, trabajos recientes muestran que esta expresividad no garantiza ventajas de aprendizaje: muchos núcleos cuánticos tienen espectros que decaen demasiado rápidamente, causando dependencia excesiva en unos pocos vectores propios dominantes similar a los núcleos polinomiales clásicos.",
    "D": "Complejidad exponencialmente reducida de evaluación de núcleo — los mapas de características cuánticos permiten calcular entradas de matriz de núcleo K(x,x') = |⟨φ(x)|φ(x')⟩|² en tiempo O(poly(n)) incluso cuando la dimensión del espacio de características es 2^n, mientras que los métodos clásicos requieren tiempo exponencial en n para evaluar productos punto en espacios de tan alta dimensión. Esta ventaja computacional se mantiene incluso para datos que admiten aproximaciones de núcleo clásicas eficientes, ya que el circuito cuántico produce directamente el valor del núcleo sin materializar coordenadas de características individuales, aunque la ventaja desaparece si la tomografía de sombra clásica puede aproximar estos valores de núcleo específicos.",
    "solution": "A"
  },
  {
    "id": 385,
    "question": "¿Qué modificación al algoritmo de Grover le permite manejar múltiples elementos marcados?",
    "A": "Cuando existen M elementos marcados en la base de datos, debe modificar el operador de difusión reemplazando la reflexión estándar 2|ψ⟩⟨ψ| - I con un operador de difusión parcial ponderado por el factor √(M/N), lo que previene la sobre-rotación más allá de la probabilidad de éxito máxima. Este ajuste de escalamiento cambia el ángulo de rotación por iteración de arcsin(1/√N) a arcsin(√(M/N)), asegurando que el vector de amplitud gire en espiral hacia el subespacio marcado a la tasa correcta sin sobrepasar y oscilar de vuelta hacia estados no marcados.",
    "B": "Funciona ya con múltiples objetivos sin modificación porque el oráculo marca todas las soluciones simultáneamente y el operador de difusión amplifica la amplitud colectiva de todos los estados marcados juntos, generalizando naturalmente el caso de objetivo único.",
    "C": "El algoritmo estándar asume un ángulo de rotación derivado de exactamente un elemento marcado, que determina la acción geométrica del iterador de Grover sobre el subespacio bidimensional generado por estados marcados y no marcados. Con M soluciones, debe implementar un oráculo generalizado que aplique un cambio de fase proporcional a arccos(√((N-M)/N)) en lugar de π, creando una rotación variable que se adapta a la densidad de estado marcado y previene que el vector de amplitud gire más allá del subespacio marcado durante la iteración.",
    "D": "Debe reducir el número de iteraciones del óptimo π√N/4 para elementos únicos a aproximadamente π√(N/M)/4 cuando M elementos están marcados, porque el subespacio marcado más grande tiene proporcionalmente mayor superposición de amplitud inicial con la superposición uniforme, requiriendo menos pasos de amplificación para alcanzar la probabilidad máxima. La estructura del algoritmo permanece sin cambios, pero aplicar el conteo de iteraciones de objetivo único causaría sobre-rotación donde las iteraciones continuadas disminuyen en lugar de aumentar la probabilidad de éxito.",
    "solution": "B"
  },
  {
    "id": 386,
    "question": "En la computación cuántica basada en mediciones sobre estados de clúster 3D, los qubits lógicos obtienen protección contra errores incorporada debido a qué característica arquitectónica?",
    "A": "La estructura de entrelazamiento volumétrico distribuye la información lógica a través de superficies topológicamente protegidas dentro de la red, donde los patrones de medición que implementan puertas lógicas evitan naturalmente la extracción de síndromes al consumir únicamente qubits ancilla que se encuentran fuera del espacio de código protegido.",
    "B": "La geometría topológica del clúster subyacente permite la detección de eventos de pérdida de un solo qubit mediante mediciones redundantes de estabilizadores distribuidas a lo largo de la red tridimensional, permitiendo la extracción de síndromes en tiempo real.",
    "C": "Los protocolos de medición adaptativa seleccionan dinámicamente ángulos de base según los datos de síndrome acumulados de capas de medición anteriores, implementando efectivamente la corrección de errores de código de superficie donde la profundidad de la red 3D proporciona redundancia temporal para mediciones de síndrome repetidas dentro de un único ciclo de reloj lógico.",
    "D": "La separación espacial de la información lógica a través de sitios de red no adyacentes crea un umbral de error de peso mínimo determinado por la métrica de distancia geométrica del clúster, donde los errores de un solo qubit deben proliferar a través de múltiples planos de la red antes de corromper los datos codificados.",
    "solution": "B"
  },
  {
    "id": 387,
    "question": "¿Cómo apoyan los protocolos de cifrado cuántico-seguros la escalabilidad de las redes de Internet de las Cosas (IoT)?",
    "A": "Los protocolos cuántico-seguros permiten el intercambio seguro de claves bajo modelos de adversarios cuánticos sin imponer complejidad computacional adicional sobre dispositivos IoT con recursos limitados, porque aprovechan primitivas criptográficas asimétricas diseñadas específicamente para procesadores de baja potencia con RAM y velocidades de reloj limitadas, manteniendo la autenticación y confidencialidad a medida que crece el tamaño de la red.",
    "B": "Los protocolos cuántico-seguros emplean esquemas de firma basados en hash como SPHINCS+ que logran seguridad post-cuántica mediante construcciones de árboles de Merkle sin estado, eliminando la sobrecarga de sincronización de estado que afectó a enfoques anteriores basados en hash. Al aprovechar las propiedades de límite de cumpleaños de las funciones hash criptográficas, estos esquemas comprimen las claves públicas a aproximadamente 32 bytes mientras mantienen niveles de seguridad de 128 bits, permitiendo la autenticación eficiente de difusión a través de miles de puntos finales IoT sin requerir almacenamiento por dispositivo de grandes claves de verificación basadas en retículos o cadenas de certificados.",
    "C": "Los protocolos cuántico-seguros utilizan criptosistemas basados en códigos derivados de la construcción McEliece, que ofrecen operaciones de descifrado en tiempo constante que escalan independientemente del tamaño de la red porque el paso de decodificación de síndrome requiere solo multiplicación matriz-vector sobre GF(2). Aunque las claves públicas siguen siendo grandes (típicamente 1 MB), las operaciones de cifrado y autenticación imponen una carga computacional mínima en los dispositivos IoT ya que involucran solo álgebra lineal sobre campos binarios en lugar de exponenciación modular, y la garantía de seguridad de decodificación de conjunto de información del protocolo asegura que agregar dispositivos no degrada el parámetro de seguridad, manteniendo una sobrecarga criptográfica O(1) por dispositivo a medida que la red escala a decenas de miles de nodos.",
    "D": "Los protocolos cuántico-seguros implementan mecanismos de encapsulación de claves basados en ring-LWE que explotan la estructura algebraica de anillos polinomiales ciclotómicos para lograr tamaños compactos de texto cifrado (menos de 1 KB) y multiplicación polinomial eficiente mediante transformadas teórico-numéricas. Sin embargo, estos protocolos requieren que todos los dispositivos IoT participantes sincronicen sus parámetros de muestreo de rechazo durante la generación de claves, creando una sobrecarga de difusión que crece logarítmicamente con el tamaño de la red ya que la probabilidad de falla de establecimiento simultáneo de claves debe acotarse por debajo de 2^-128 en todos los pares de dispositivos, necesitando rondas de comunicación adicionales que escalan como O(log n) para n dispositivos.",
    "solution": "A"
  },
  {
    "id": 388,
    "question": "Un equipo de investigación está construyendo una computadora cuántica distribuida donde módulos superconductores en refrigeradores de dilución separados están conectados mediante canales de fibra óptica que transportan fotones de longitud de onda de telecomunicaciones generados por conversión paramétrica descendente. Cada módulo alberga 50 qubits de datos con tiempos T1 de alrededor de 100 μs. A medida que aumentan la separación física entre refrigeradores de 10 metros a 100 metros, observan que las tasas de error lógico para pares de Bell distribuidos se degradan en casi un orden de magnitud, aunque la atenuación de la fibra a 1550 nm es solo 0.2 dB/km y la latencia añadida es despreciable comparada con los tiempos de coherencia. En arquitecturas modulares conectadas por enlaces fotónicos, la tasa de error lógico escala con la separación entre módulos principalmente porque la distancia aumentada afecta qué componente?",
    "A": "La ocupación de fotones térmicos en las interfaces criogénicas donde los fotones entran y salen de los refrigeradores de dilución aumenta con el número de puertos de paso óptico requeridos para soportar tendidos de fibra más largos, ya que cada conector adicional introduce radiación de cuerpo negro parásita que se filtra en los modos de cavidad superconductora, elevando la temperatura efectiva de ruido experimentada por los qubits y causando errores de fase sistemáticos que se componen cuadráticamente con el número de rondas de distribución de entrelazamiento necesarias a través de enlaces extendidos",
    "B": "La decoherencia de memoria de los convertidores de frecuencia cuántica que unen el dominio microondas-óptico se vuelve dominante cuando las distancias entre módulos crecen, ya que los cristales de transducción electro-óptica requieren bloqueo activo de cavidad sobre tramos de fibra más largos, y los bucles de retroalimentación de estabilización introducen ruido de intensidad que se acopla a las frecuencias de transición de los qubits superconductores mediante fluctuaciones residuales del número de fotones",
    "C": "La dispersión cromática en fibra monomodo estándar causa ensanchamiento temporal de los paquetes de onda de fotones entrelazados sobre distancias que exceden 50 metros, reduciendo la visibilidad de interferencia Hong-Ou-Mandel en el divisor de haz usado para mediciones de estado de Bell durante el intercambio de entrelazamiento, reduciendo así directamente la fidelidad de pares EPR distribuidos independientemente de efectos de decoherencia de qubits y requiriendo secuencias de destilación más largas para alcanzar umbrales de error lógico objetivo",
    "D": "La pérdida de fotones en la fibra reduce la eficiencia de anuncio para la generación de entrelazamiento, forzando intentos de reintento más frecuentes que acumulan tiempo de espera durante el cual la decoherencia de qubits degrada la fidelidad del estado de Bell final antes de que se pueda aplicar corrección de errores al par lógico distribuido.",
    "solution": "D"
  },
  {
    "id": 389,
    "question": "En arquitecturas de aprendizaje automático cuántico que usan circuitos cuánticos parametrizados para tareas de clasificación sobre conjuntos de datos de alta dimensión, las subrutinas aritméticas cuánticas integradas dentro de circuitos de extracción de características sirven principalmente qué propósito computacional, particularmente cuando se contrastan con estrategias de incrustación puramente lineales que mapean directamente datos clásicos a amplitudes cuánticas?",
    "A": "Aseguran que el resultado de medición final siempre proyecte sobre un vector de base computacional, lo cual es requerido para la lectura determinística de la etiqueta clásica sin necesitar muestreo repetido o postprocesamiento estadístico de los resultados de medición — un requisito crítico ya que los clasificadores cuánticos variacionales deben producir predicciones de clase discretas en un solo disparo. Al realizar operaciones aritméticas que amplifican la amplitud del estado base de la etiqueta de clase correcta mientras suprimen todas las demás mediante interferencia destructiva, estas subrutinas implementan un mecanismo de ganador-toma-todo que garantiza que la regla de Born produzca probabilidad uno para el resultado objetivo, eliminando así la aleatoriedad inherente de la medición cuántica.",
    "B": "Reducen el conteo de qubits requerido al mapear los datos clásicos sobre estados estabilizadores que pueden prepararse y manipularse eficientemente usando solo puertas Clifford, evitando así la sobrecarga asociada con rotaciones arbitrarias de un solo qubit que necesitarían síntesis de puertas costosa y compilación de conjuntos de puertas universales. Dado que los circuitos estabilizadores admiten simulación clásica eficiente mediante el teorema de Gottesman-Knill, las subrutinas aritméticas aprovechan esta estructura computacional para comprimir vectores de características de alta dimensión en representaciones de operadores de Pauli de bajo peso, logrando una reducción exponencial en la profundidad del circuito. Esta estrategia de compresión explota el hecho de que la mayoría de los conjuntos de datos del mundo real exhiben estructura estabilizadora aproximada en sus matrices de covarianza.",
    "C": "Codifican combinaciones no lineales de características de entrada directamente en parámetros de estado cuántico mediante operaciones como multiplicación modular y rotaciones de fase controladas, permitiendo que el circuito represente límites de decisión complejos y no separables que de otro modo requerirían exponencialmente muchos parámetros clásicos. Las incrustaciones lineales solo pueden capturar separaciones de hiperplano, mientras que los circuitos aritméticos crean interacciones de características en el espacio de Hilbert, habilitando transformaciones tipo kernel polinomiales y trascendentales eficientemente.",
    "D": "Implementan la estimación de fase cuántica como el oráculo de gradiente",
    "solution": "C"
  },
  {
    "id": 390,
    "question": "Dado un unitario U con descomposición espectral que contiene valores propios cerca de ±1, y asumiendo que estás trabajando con un circuito QPE donde el número de qubits de conteo está fijado en n=8, ¿cuál se convierte en el principal desafío de implementación cuando U describe un operador de evolución hamiltoniana de muchos cuerpos complejo exp(-iHt) con términos no locales?",
    "A": "Las puertas controlled-U^(2^j) requieren descomposiciones de Trotter exponencialmente profundas para j grandes, haciendo las estimaciones de fase de alta precisión prohibitivamente costosas incluso con sobrecarga moderada de ancillas. El conteo de puertas escala como O(2^j · poly(tamaño del sistema)) por operación controlada, y los errores de Trotter acumulados destruyen los patrones de interferencia necesarios para resolver diferencias de fase finas. Para sistemas de muchos cuerpos con interacciones no locales, cada paso de Trotter involucra puertas a través de qubits distantes, agravando tanto la profundidad del circuito como la sensibilidad a la decoherencia.",
    "B": "Cuando los valores propios se agrupan cerca de ±1, las fases correspondientes se concentran cerca de 0 y π, donde el núcleo de Dirichlet de la transformada cuántica de Fourier exhibe sensibilidad máxima a errores de discretización de Trotter que escalan como O(τ²) por paso de tiempo. Para operaciones controlled-U^(2^j) con j grande, el tiempo de evolución efectivo 2^j·t empuja al sistema al régimen no perturbativo donde las aproximaciones estándar de fórmula de producto se descomponen, requiriendo extrapolación de Richardson o integradores de orden superior que multiplican la profundidad del circuito por factores de 4-8. Los términos hamiltonianos no locales exacerban esto porque cada corrección de orden superior involucra conmutadores que acoplan subsistemas cada vez más distantes, requiriendo redes SWAP que crecen polinomialmente con el tamaño del sistema.",
    "C": "El registro de conteo finito con n=8 qubits proporciona resolución de fase de 2π/256 ≈ 0.0245 radianes, pero cuando las operaciones controlled-U^(2^j) con j grande se implementan mediante división de Trotter de hamiltonianos no locales, el error de Trotter acumulado σ_Trotter crece como O(2^j·m·τ³) donde m es el número de términos hamiltonianos y τ es el tamaño del paso de Trotter. Para sistemas de muchos cuerpos, m escala con el tamaño del sistema y las interacciones no locales requieren enrutamiento basado en SWAP que añade profundidad lineal en el diámetro de conectividad de qubits, causando que el error total de Trotter exceda 2π/256 para j ≥ 6, saturando así la resolución QPE y produciendo estimaciones de fase indistinguibles para valores propios distintos.",
    "D": "Los valores propios cerca de ±1 se mapean a fases θ ≈ 0 y θ ≈ π, pero el algoritmo QPE mide fases módulo 2π, creando una ambigüedad cuando |θ| < 2π/2^(n+1) porque tanto las fases positivas como negativas cerca de cero producen resultados de cadena de bits idénticos en el registro de conteo. Para n=8, esta región de ambigüedad abarca aproximadamente ±0.0122 radianes. Cuando U = exp(-iHt) describe un hamiltoniano de muchos cuerpos con términos no locales, implementar controlled-U^(2^j) para j grande requiere descomposiciones de Trotter cuya acumulación de error empuja las estimaciones de fase a esta zona de ambigüedad, necesitando protocolos auxiliares de resolución de signo que miden proyecciones ⟨ψ|U|ψ⟩ para desambiguar θ de -θ, duplicando efectivamente la profundidad del circuito cuántico requerida.",
    "solution": "A"
  },
  {
    "id": 391,
    "question": "Los qubits ancilla se agregan frecuentemente a circuitos variacionales durante tareas de aprendizaje supervisado para:",
    "A": "Servir como grados de libertad auxiliares que efectivamente duplican la profundidad de procesamiento coherente del circuito mientras mantienen tasas constantes de error de puerta física, porque las descomposiciones de puertas mediadas por ancilla distribuyen los errores de una única puerta de dos qubits a través de múltiples interacciones ancilla-datos, diluyendo estadísticamente la acumulación de error por capa. Este mecanismo de dispersión de errores permite circuitos parametrizados más profundos sin cruzar el umbral de decoherencia, habilitando la exploración de ansätze variacionales más expresivos para límites de clasificación complejos.",
    "B": "Imponer convexidad estricta en el paisaje de costo variacional al restringir el espacio de parámetros a un subconjunto donde la matriz Hessiana de la función de pérdida permanece definida positiva, lo cual es alcanzable porque los qubits ancilla introducen libertades de gauge adicionales que regularizan la trayectoria de optimización. Esta garantía de convexidad previene mesetas áridas y asegura que los optimizadores basados en gradiente converjan al mínimo global en tiempo polinomial, independientemente de la inicialización aleatoria de parámetros o las elecciones de arquitectura del circuito.",
    "C": "Proporcionar registros cuánticos adicionales donde la información de etiquetas supervisadas puede codificarse directamente como estados de la base computacional mediante operaciones controladas condicionadas a mediciones de qubits de datos, creando efectivamente una representación entrelazada que acopla las características de entrada con sus clasificaciones objetivo. Al medir los qubits ancilla en la base computacional después de la evaluación del circuito, el resultado de clasificación se extrae como un resultado discreto sin requerir post-procesamiento complejo de valores de expectación continuos, simplificando así el procedimiento de inferencia y reduciendo la sobrecarga clásica en el bucle de aprendizaje híbrido cuántico-clásico.",
    "D": "Habilitar el reinicio y reutilización en medio del circuito para reducir el conteo total de qubits exponencialmente en comparación con los requisitos de escalado de profundidad del circuito.",
    "solution": "C"
  },
  {
    "id": 392,
    "question": "¿Qué paso de preprocesamiento ayuda a reducir el entrelazamiento antes del corte?",
    "A": "Técnicas de resíntesis de puertas que descomponen bloques de circuito de alto entrelazamiento en patrones equivalentes más superficiales con profundidad reducida de puertas de dos qubits, a menudo identificando identidades algebraicas o explotando relaciones de conmutación que permiten reorganizar operaciones de múltiples qubits en formas donde menos qubits están entrelazados simultáneamente. Esta reorganización disminuye la entropía de entrelazamiento a través de los límites de corte potenciales, reduciendo la sobrecarga de muestreo en la reconstrucción de cuasiprobabilidad subsiguiente",
    "B": "Aplicar reglas de optimización de circuito que identifican secuencias de puertas que crean y luego destruyen inmediatamente el entrelazamiento a través de límites de corte potenciales—por ejemplo, CX(i,j) seguido poco después por CX(i,j)—y eliminar estas operaciones de entrelazamiento redundantes o conmutarlas hacia regiones del circuito alejadas de los cortes. Al reducir la generación innecesaria de entrelazamiento mediante simplificación algebraica y cancelación de puertas, el rango de Schmidt a través de los cables cortados disminuye, reduciendo directamente el número de términos en la descomposición de cuasiprobabilidad y por tanto el costo de muestreo asociado para la reconstrucción del circuito",
    "C": "Emplear algoritmos de ordenamiento de contracción de redes tensoriales para identificar regiones del circuito donde el entrelazamiento temporal puede resolverse mediante mediciones parciales intermedias antes de alcanzar el límite de corte. Al inserir mediciones proyectivas estratégicamente colocadas sobre grados de libertad auxiliares que han cumplido su función computacional, la dimensión efectiva de entrelazamiento transmitida a través de los cortes disminuye de 2ᵏ (para k cables cortados) a 2ᵏ⁻ᵐ (después de m colapsos inducidos por medición). Esta estrategia de pre-medición requiere un análisis cuidadoso para asegurar que los qubits medidos no participen en puertas subsiguientes, pero reduce exitosamente los conteos de términos de cuasiprobabilidad exponencialmente en m",
    "D": "Usar algoritmos de reescritura de circuitos basados en ZX-calculus u otros formalismos gráficos para identificar y eliminar puertas de entrelazamiento redundantes que crean estados de rango de Schmidt-2 a través de ubicaciones de corte potenciales cuando existen representaciones de rango menor. La reescritura busca subgrafos correspondientes a productos de Pauli de peso alto que generan entrelazamiento innecesariamente—como escaleras de CNOT en cascada que podrían reemplazarse por descomposiciones Clifford+T más superficiales con menos correlaciones simultáneas de múltiples qubits. Al simplificar algebraicamente la estructura de entrelazamiento antes del corte, el número de mediciones en la base de Pauli requeridas para la reconstrucción de cables cortados disminuye de 4ⁿ a ~2ⁿ por corte, donde n es el número de cables cortados",
    "solution": "A"
  },
  {
    "id": 393,
    "question": "¿Por qué se podría usar corte híbrido tensorial/cuasiprobabilidad en una única ejecución?",
    "A": "Para validar la convergencia estadística del muestreo de cuasiprobabilidad comparando estimaciones intermedias contra resultados de contracción tensorial determinística para subcircuitos donde la dimensión de enlace permanece clásicamente tratable, proporcionando un diagnóstico en tiempo real que señala conteos de muestras insuficientes antes de que se complete el cómputo completo. El componente de red tensorial calcula probabilidades marginales exactas para fragmentos de circuito superficiales, que sirven como variantes de control que reducen la varianza del estimador de cuasiprobabilidad aplicado a regiones más profundas y más entrelazadas. Al anclar la reconstrucción estocástica a estos puntos de control determinísticos, el método híbrido logra convergencia más rápida en distancia de variación total mientras también habilita la detección de errores mediante pruebas de consistencia entre las dos ramas computacionales.",
    "B": "Los cortes tensoriales funcionan bien donde el entrelazamiento es bajo y la dimensión de enlace permanece manejable para la contracción clásica, mientras que los métodos de cuasiprobabilidad manejan mejor las regiones de alta negatividad al absorber la intratabilidad clásica en sobrecarga de muestreo en lugar de costo de contracción exponencial. Combinar estas dos técnicas complementarias dentro de un único circuito permite optimizar la sobrecarga computacional general al enrutar diferentes subcircuitos a través de la estrategia de descomposición más eficiente. El enfoque híbrido minimiza simultáneamente tanto los requisitos de memoria clásica como la complejidad de muestreo cuántico.",
    "C": "Para descomponer la varianza total de medición en contribuciones clásicas y cuánticas, donde la contracción de red tensorial aísla el ruido de disparo irreducible que surge de mediciones proyectivas mientras el muestreo de cuasiprobabilidad captura las fluctuaciones adicionales introducidas por la fragmentación del circuito y el postprocesamiento clásico. Al ejecutar ambos métodos en paralelo sobre la misma partición de circuito, se puede restar la varianza de referencia derivada del tensor de la varianza total del estimador de cuasiprobabilidad para cuantificar el costo de sobrecarga del procedimiento de corte mismo. Esta descomposición de varianza informa estrategias adaptativas que ajustan dinámicamente la colocación de cortes para minimizar la complejidad de muestreo mientras respetan las restricciones de memoria clásica.",
    "D": "Para eliminar la sobrecarga exponencial de cuasiprobabilidades negativas en ciertas regiones del circuito preprocesando esos fragmentos con contracción tensorial, que efectivamente calcula y almacena la distribución de probabilidad completa sobre los resultados de medición de modo que la reconstrucción de cuasiprobabilidad posterior pueda muestrear de estas distribuciones precomputadas sin introducir negatividad. La red tensorial absorbe el costo de simulación clásica solo para subcircuitos cuya negatividad de otro modo requeriría sobremuestreo prohibitivo, mientras el componente de cuasiprobabilidad maneja las capas de circuito restantes donde la negatividad es leve. Esta estrategia de partición asegura que la sobrecarga de muestreo general crezca solo polinomialmente con la profundidad del circuito al confinar el costo exponencial a una fase de preprocesamiento clásicamente tratable.",
    "solution": "B"
  },
  {
    "id": 394,
    "question": "En pipelines híbridos cuántico-clásicos, realizar reducción de dimensionalidad con un autoencoder clásico antes del procesamiento cuántico tiene como objetivo principal:",
    "A": "Los autoencoders clásicos comprimen datos de entrada de alta dimensión en una representación latente de baja dimensión mediante redes encodificadoras entrenadas por retropropagación. Cuando esta representación comprimida se alimenta a un circuito variacional cuántico posterior, la dimensionalidad reducida elimina la necesidad de calcular gradientes con respecto a parámetros cuánticos durante el entrenamiento.",
    "B": "Al entrenar un autoencoder clásico para proyectar características de entrada sobre una variedad unidimensional, el circuito cuántico subsecuente hereda esta restricción geométrica y opera completamente dentro de un subespacio computacional abarcado por un único qubit, eliminando puertas de entrelazamiento y permitiendo que todos los parámetros variacionales se optimicen usando optimización convexa clásica.",
    "C": "Menores requisitos de qubits mientras se preserva información relevante para la tarea al comprimir vectores de características clásicas de alta dimensión en representaciones latentes compactas que pueden codificarse eficientemente en estados cuánticos usando menos operaciones de codificación de amplitud o codificación de base, reduciendo así los recursos de hardware necesarios para la preparación de estados mientras se retiene la estructura esencial necesaria para tareas de aprendizaje automático cuántico posteriores.",
    "D": "Un autoencoder clásico impone un cuello de botella de información fijo sobre los datos de entrada, comprimiendo representaciones en un espacio latente con entropía controlada. Cuando esta representación latente se codifica posteriormente en un estado cuántico y se procesa a través de capas cuánticas variacionales, la baja entropía inicial restringe la evolución del entrelazamiento dentro del circuito.",
    "solution": "C"
  },
  {
    "id": 395,
    "question": "En el contexto de simulación cuántica de sistemas de muchos cuerpos usando solucionadores de valores propios variacionales cuánticos, ¿cuál es la principal ventaja de usar ansätze específicos del problema (como el ansatz Unitary Coupled Cluster para sistemas moleculares) en comparación con ansätze eficientes en hardware con puertas parametrizadas arbitrarias? Considere tanto la precisión de la preparación del estado fundamental como el paisaje de optimización clásica al formular su respuesta.",
    "A": "Los ansätze específicos del problema eliminan la optimización clásica por completo por construcción: la estructura del ansatz UCC codifica directamente la función de onda exacta de muchos cuerpos mediante sus amplitudes de coupled-cluster, que pueden determinarse analíticamente a partir de los elementos de matriz del hamiltoniano sin ninguna búsqueda iterativa de parámetros. Los ansätze eficientes en hardware, por el contrario, requieren optimización de tiempo exponencial porque deben explorar el espacio de Hilbert completo de 2ⁿ dimensiones sin ninguna guía física, haciéndolos fundamentalmente inadecuados para la preparación del estado fundamental más allá de sistemas triviales a pesar de sus implementaciones de circuito superficial.",
    "B": "La ventaja fundamental proviene de la eficiencia de medición más que de la optimización: los ansätze específicos del problema como UCC generan estados cuyos valores de expectación de energía requieren exponencialmente menos mediciones de términos de Pauli porque los operadores UCC conmutan con grandes subconjuntos de la descomposición de Pauli del hamiltoniano molecular. Esta propiedad de conmutación permite la medición simultánea de términos correlacionados, reduciendo la sobrecarga de medición de O(N⁴) a O(N²) para un sistema de N orbitales, mientras que los ansätze eficientes en hardware deben medir cada término del hamiltoniano independientemente debido a su estructura de puerta arbitraria que destruye esta conmutatividad.",
    "C": "Los ansätze eficientes en hardware logran convergencia rápida al explotar operaciones de puerta nativas que minimizan la profundidad del circuito, pero su verdadera limitación radica en su incapacidad para capturar efectos de correlación fuerte. Los ansätze específicos del problema, en contraste, garantizan la preparación exacta del estado fundamental incluso con profundidad de circuito polinomial porque la estructura UCC inherentemente codifica todas las correlaciones electrónicas relevantes mediante su forma de operador exponencial. Las simetrías incorporadas de número de partículas y espín de UCC restringen automáticamente el espacio de búsqueda al subespacio físico, transformando efectivamente el espacio de Hilbert exponencial en un problema de optimización polinomial al que los circuitos eficientes en hardware no pueden acceder.",
    "D": "Los ansätze específicos del problema como UCC incorporan estructura física del hamiltoniano objetivo, lo que reduce dramáticamente la complejidad del paisaje de optimización al evitar mesetas áridas. También proporcionan mejorabilidad sistemática mediante jerarquía (UCCSD, UCCSDT, etc.), aunque al costo de requerir circuitos más profundos con más puertas de dos qubits que pueden no mapearse eficientemente a los grafos de conectividad del hardware.",
    "solution": "D"
  },
  {
    "id": 396,
    "question": "En la decodificación basada en síndromes para códigos de superficie, ¿por qué alimentar al decodificador con datos de síndrome de múltiples rondas consecutivas de medición típicamente mejora las tasas de error lógico en comparación con usar solo la ronda más reciente? Considera un escenario en el que estás ejecutando un código de superficie de distancia 5 en un procesador superconductor con fidelidades de puerta realistas, y tienes la opción de almacenar y procesar solo el síndrome actual o las últimas cuatro rondas de síndromes.",
    "A": "El historial de múltiples rondas distingue errores de medición de errores de datos mediante el seguimiento de la persistencia del síndrome — los errores reales de datos producen patrones consistentes mientras que las fallas de medición crean contradicciones transitorias entre rondas.",
    "B": "Las correlaciones temporales entre defectos del síndrome revelan direcciones de propagación de errores bajo la estructura causal del circuito — específicamente, si los bits de síndrome s₁ y s₂ se activan en rondas consecutivas con s₂ espacialmente adyacente a s₁, el decodificador infiere una cadena de errores en expansión en lugar de fallas independientes. Esta información direccional restringe la hipótesis de error de máxima verosimilitud a trayectorias consistentes con el ordenamiento de puertas, reduciendo la degeneración efectiva del código estabilizador al eliminar configuraciones de error temporalmente imposibles que de otro modo contribuirían con igual peso a la distribución posterior.",
    "C": "Los códigos de repetición de síndrome se concatenan naturalmente con el código de superficie espacial cuando hay datos de múltiples rondas disponibles — la serie temporal de cada bit de síndrome forma un código de repetición clásico que detecta errores de medición mediante votación por mayoría entre rondas. Dado que los errores de medición ocurren a tasas comparables a los errores de puerta (típicamente 0.1-1% por extracción de síndrome), la decodificación de una sola ronda confunde fallas de medición con errores de datos, provocando que el decodificador infiera cadenas de error espurias que desencadenan correcciones innecesarias. El historial de múltiples rondas permite la decodificación separada de los síndromes temporal y espacial, factorizando efectivamente el modelo combinado de error espacio-temporal en subproblemas independientes con umbrales más bajos por ronda.",
    "D": "Los errores de gancho se vuelven identificables a través de su firma característica de múltiples rondas — cuando un error de qubit de datos ocurre durante la extracción de síndrome, crea un par correlacionado de defectos de síndrome que abarca dos rondas consecutivas en un patrón geométrico específico determinado por el calendario de medición del estabilizador. La decodificación de una sola ronda no puede distinguir este error de gancho de dos errores independientes de un solo qubit que requerirían corrección en qubits diferentes, llevando a operaciones de recuperación incorrectas la mitad del tiempo. Los algoritmos de emparejamiento de múltiples rondas detectan estas correlaciones espacio-temporales y asignan pesos apropiados a las hipótesis de error de gancho, mejorando las estimaciones de umbral en aproximadamente 0.3 puntos porcentuales para modelos típicos de ruido a nivel de circuito.",
    "solution": "A"
  },
  {
    "id": 397,
    "question": "¿Por qué la estimación de fase es particularmente desafiante para los dispositivos cuánticos actuales de escala intermedia ruidosa (NISQ)?",
    "A": "Requiere un número exponencial de lecturas para resolver diferencias de fase que están cerca de aproximaciones racionales de π, porque cuando una autofase φ = 2πp/q para enteros pequeños p, q, la expansión en fracciones continuas termina temprano, y los picos de Fourier en la distribución de medición se vuelven irresolublemente estrechos en comparación con el ruido de muestreo. Lograr n bits de precisión en este régimen demanda aproximadamente 2^n disparos de medición para acumular suficientes estadísticas para distinguir el pico de las fluctuaciones de fondo, incluso si el circuito en sí pudiera ejecutarse perfectamente. Cada bit adicional de precisión duplica tanto la profundidad del circuito (para implementar rotaciones controladas de grano más fino) como el conteo de disparos, creando un escalado de recursos doblemente exponencial que excede los presupuestos de coherencia NISQ más allá de 6-8 qubits de precisión cuando las autofases se agrupan cerca de puntos degenerados del círculo unitario, donde los efectos de aliasing del muestreo finito exacerban los requisitos de sensibilidad y causan que la fase extraída oscile entre bins de discretización adyacentes.",
    "B": "Requiere circuitos profundos con muchas operaciones controladas que exceden los tiempos de coherencia, ya que lograr una resolución de fase precisa demanda secuencias largas de puertas unitarias controladas aplicadas condicionalmente basándose en estados de qubits auxiliares. Cada bit adicional de precisión duplica la profundidad del circuito, con precisión de n bits requiriendo operaciones controladas de la forma U^(2^k) para k hasta n-1, creando circuitos exponencialmente profundos que rápidamente superan los límites de decoherencia del hardware actual. Los errores acumulativos de puerta a través de estos circuitos profundos hacen que la información de fase extraída se vuelva poco confiable más allá de aproximadamente 6-8 qubits de precisión en los dispositivos NISQ actuales.",
    "C": "Sufre de contaminación del autoestado cuando el estado inicial se superpone con múltiples autovectores del unitario que se está analizando, porque el paso de transformada de Fourier cuántica de la estimación de fase interfiere coherentemente la retroalimentación de fase de todos los autoestados contribuyentes, y si los coeficientes de superposición tienen magnitudes similares, la distribución de medición resultante se convierte en una suma de picos tipo sinc que se superponen y crean patrones de interferencia destructiva. El diseño del algoritmo asume una entrada de autoestado puro o al menos una distribución fuertemente ponderada hacia un autovalor, pero los protocolos de preparación de estado NISQ rara vez logran mejor que 85% de fidelidad con el autoestado objetivo, lo que significa que 15% de fuga a otros autoestados contamina la lectura de fase. Para un registro de n qubits, esta fuga introduce picos espurios en el espacio de resultados de medición de dimensión 2^n que no pueden filtrarse post-hoc porque son cuánticamente indistinguibles de la señal verdadera, forzando tiempos de promediado más largos que colisionan con los límites de decoherencia o aceptación de resultados de fase ambiguos que requieren heurísticas de post-procesamiento clásico para desambiguar.",
    "D": "Encuentra errores de inversión de bit en la transformada de Fourier cuántica inversa que se propagan no linealmente a través de la red de mariposa de puertas de fase controladas, específicamente porque la arquitectura en capas de la QFT aplica rotaciones de fase progresivamente más finas (puertas R_k con ángulos 2π/2^k) que se vuelven cada vez más sensibles a errores de control a medida que k crece. Un solo bit invertido en el registro auxiliar durante la k-ésima capa causa que todas las puertas R_m subsiguientes con m > k apliquen ángulos de fase incorrectos, y debido a que estos errores se componen multiplicativamente en el plano complejo en lugar de aditivamente, la distribución de medición final exhibe sensibilidad exponencial a la fidelidad de puerta en las etapas posteriores de la QFT. En hardware NISQ con errores de puerta de dos qubits alrededor de 0.5%, un circuito de estimación de fase de 8 qubits acumula aproximadamente 10% de error total solo en la QFT inversa, que se manifiesta como un ensanchamiento de los picos espectrales que degrada la resolución de fase por debajo del límite teórico establecido por el número de qubits auxiliares, y este canal de error es distinto de la decoherencia porque persiste incluso en el límite de temperatura cero donde los tiempos T₁ y T₂ son efectivamente infinitos.",
    "solution": "B"
  },
  {
    "id": 398,
    "question": "¿Qué limita la practicidad del algoritmo de Grover para atacar sistemas criptográficos del mundo real?",
    "A": "Construir oráculos cuánticos que implementen fielmente primitivas criptográficas complejas como AES o RSA requiere descomponer todo el cifrado en puertas cuánticas reversibles, demandando profundidades de circuito que pueden exceder millones de operaciones. Cada puerta elemental en el oráculo debe sintetizarse a partir de un conjunto de puertas tolerante a fallas, y los requisitos acumulados de coherencia significan que incluso tamaños de clave moderados necesitan implementaciones corregidas de errores con miles de qubits físicos por qubit lógico.",
    "B": "El algoritmo de Grover exhibe pobre escalabilidad cuando se distribuye entre múltiples procesadores cuánticos porque el mecanismo de amplificación de amplitud depende de patrones de interferencia globales que deben mantenerse coherentemente a través de todos los qubits involucrados en la búsqueda.",
    "C": "La implementación práctica requiere computadoras cuánticas tolerantes a fallas con suficientes qubits lógicos para manejar espacios de claves criptográficamente relevantes (128-256 bits), además de implementaciones eficientes de circuito cuántico de oráculos de funciones criptográficas. El desafío de construcción del oráculo es particularmente severo: descomponer AES o SHA en puertas reversibles produce circuitos con millones de operaciones, cada una requiriendo corrección de errores que multiplica los requisitos de qubits físicos por factores de 1000 o más, haciendo que el hardware cuántico actual y de corto plazo sea inadecuado para ataques criptográficos significativos.",
    "D": "Todos estos factores presentan desafíos significativos",
    "solution": "D"
  },
  {
    "id": 399,
    "question": "¿Cuál es la idea clave detrás de la computación cuántica de reservorio?",
    "A": "Solo la capa de lectura requiere entrenamiento, lo que reduce drásticamente la carga de optimización ya que la gran mayoría de los parámetros de la red permanecen fijos durante todo el proceso de aprendizaje, evitando gradientes que desaparecen y retropropagación a través de puertas de muchos qubits.",
    "B": "Los reservorios cuánticos procesan secuencias temporales mediante evolución unitaria, donde cada paso temporal corresponde a aplicar un hamiltoniano fijo que rota el estado del reservorio en el espacio de Hilbert. La trayectoria resultante a través del espacio de estados de muchos qubits codifica naturalmente dependencias temporales y correlaciones entre elementos de la secuencia, transformando la serie temporal de entrada en un estado cuántico de alta dimensión cuyas estadísticas de medición capturan patrones de largo alcance. Debido a que la evolución unitaria es reversible y determinista, la dinámica del reservorio preserva información sobre entradas tempranas incluso cuando llegan nuevos datos, permitiendo un modelado efectivo de secuencias sin conexiones recurrentes explícitas.",
    "C": "La dinámica cuántica no controlada de los qubits del reservorio mapea automáticamente las entradas a espacios de características de alta dimensión mediante evolución e interacción naturales, eliminando la necesidad de entrenar la mayor parte de los parámetros de la red. Al dejar que el sistema cuántico evolucione bajo su hamiltoniano intrínseco sin una ingeniería cuidadosa, generas transformaciones no lineales complejas gratuitamente, y solo necesitas ajustar una simple capa de lectura lineal clásica al final para extraer predicciones de las mediciones del estado cuántico.",
    "D": "Los circuitos cuánticos aleatorios generan mapas de características gratuitamente explotando el hecho de que las puertas unitarias típicas extraídas de la medida de Haar rápidamente mezclan los datos de entrada a través de todos los qubits, creando una transformación pseudoaleatoria pero determinista en un espacio de características exponencialmente grande. Dado que los circuitos aleatorios aproximan 2-diseños unitarios después de solo profundidad polinómica, no necesitas diseñar cuidadosamente la arquitectura del reservorio — las capas entrelazantes genéricas son suficientes para producir incrustaciones expresivas cuya complejidad rivaliza con la de redes entrenadas, efectivamente subcontratando el aprendizaje de características a la complejidad natural de la dinámica cuántica de muchos cuerpos.",
    "solution": "C"
  },
  {
    "id": 400,
    "question": "¿De qué manera específica los códigos de superficie optimizados para entornos de ruido sesgado — donde la defasamiento domina sobre las inversiones de bit por órdenes de magnitud — difieren estructuralmente de los códigos de superficie estándar de red cuadrada para explotar esta asimetría y lograr umbrales de error sustancialmente más altos?",
    "A": "Modifican la longitud relativa de los operadores lógicos X versus Z, intercambiando algo de protección contra inversión de bits por protección mejorada contra inversión de fase, dado que los errores de defasamiento son mucho más comunes en el modelo de ruido asumido. Al alargar el operador Z lógico y acortar el operador X lógico dentro de la geometría de la red, el código asigna más recursos de detección de síndrome a errores de fase mientras acepta mayor vulnerabilidad a eventos raros de inversión de bits. Este diseño asimétrico permite que el umbral de error aumente sustancialmente cuando la relación de sesgo de ruido excede un valor crítico, emparejando efectivamente la estructura del código con las características de ruido operacional del hardware.",
    "B": "La conectividad de los qubits físicos se reconfigura para formar una red rectangular en lugar de cuadrada, donde la relación de aspecto entre los espaciamientos de estabilizadores horizontales y verticales se ajusta para igualar la raíz cuadrada de la relación de sesgo de ruido, equilibrando así las tasas efectivas de error lógico para fallas tipo X y Z a pesar de la asimetría física subyacente. Al estirar la red a lo largo de una dirección espacial, el código aumenta el peso de los estabilizadores Z en relación con los estabilizadores X, lo que compensa la mayor tasa de defasamiento al requerir más errores de fase simultáneos para producir una falla lógica. Esta deformación geométrica ajusta la distancia del código asimétricamente, elevando el umbral cuando η = p_defasamiento / p_inversionbit excede aproximadamente 10, y la relación de aspecto óptima escala logarítmicamente con η para equilibrar los dos canales de falla.",
    "C": "El calendario de extracción de síndrome se modifica para medir estabilizadores Z a una tasa de repetición más alta que los estabilizadores X, explotando la asimetría temporal en los tiempos de llegada de errores para dedicar más de los ciclos disponibles de corrección de errores cuánticos a detectar los eventos dominantes de defasamiento. Al intercalar múltiples mediciones de síndrome Z entre rondas sucesivas de síndrome X, el decodificador recibe actualizaciones más frecuentes sobre cadenas de inversión de fase antes de que se propaguen a configuraciones incorregibles. Este sesgo temporal aumenta efectivamente la distancia del código contra errores de defasamiento al reducir la latencia entre ocurrencia y detección de errores, mientras acepta brechas más largas en el monitoreo de inversión de bits ya que esos eventos se acumulan a una tasa insignificante en comparación con el período de extracción de síndrome.",
    "D": "Los qubits auxiliares se asignan estrategias asimétricas de supresión de ruido en reposo, donde los auxiliares de estabilizadores de fase emplean pulsos continuos de desacoplamiento dinámico entre rondas de síndrome para reducir aún más su susceptibilidad al defasamiento, mientras que los auxiliares de inversión de bits permanecen desprotegidos ya que la tasa ambiental de inversión de bits ya está órdenes de magnitud por debajo del umbral donde la sobrecarga adicional produciría ganancias significativas. Esta estrategia de protección selectiva concentra los recursos de control disponibles en mitigar el canal de error dominante, efectivamente reduciendo la tasa de defasamiento lógico sin aumentar el número total de operaciones físicas por ciclo de síndrome. El desajuste resultante en las fidelidades de los auxiliares crea un sesgo de ruido efectivo a nivel de medición del estabilizador que refleja y amplifica el sesgo en los errores de qubits de datos.",
    "solution": "A"
  },
  {
    "id": 401,
    "question": "Los algoritmos de caminata cuántica a veces utilizan una moneda reflectante en los vértices marcados para que:",
    "A": "El operador de reflexión crea un desplazamiento de fase de π específicamente para la componente del vértice marcado, implementando el mecanismo de phase kickback que invierte el signo de la amplitud mientras preserva la magnitud. Esta inversión selectiva de fase en las soluciones causa interferencia destructiva a lo largo de los bordes salientes cuando se combina con el operador de difusión estándar, atrapando efectivamente la amplitud en los estados marcados mediante el mismo mecanismo de interferencia subyacente al algoritmo de Grover.",
    "B": "La amplitud no se filtra una vez que alcanza un estado marcado, manteniendo al caminante localizado allí para que la probabilidad se acumule en la solución mediante interferencia constructiva en lugar de dispersarse de vuelta en la estructura del grafo donde continuaría explorando vértices que no son solución.",
    "C": "La reflexión de la moneda en los vértices marcados implementa una condición de frontera que invierte el vector de momento del caminante, creando un patrón de onda estacionaria centrado en el vértice solución. Esta inversión de momento impide que la amplitud se propague lejos mientras permite que la amplitud entrante continúe llegando, estableciendo un equilibrio dinámico donde el flujo de probabilidad hacia los estados marcados excede el flujo saliente, concentrando así la distribución del caminante en las soluciones durante múltiples iteraciones.",
    "D": "Aplicar operadores de reflexión en los vértices marcados modifica el espectro propio del operador de caminata al introducir un defecto localizado que divide los niveles de energía degenerados, creando una brecha de energía entre las variedades de vértices marcados y no marcados. Esta separación espectral hace que el sistema pueble preferentemente los estados propios de los vértices marcados durante la evolución adiabática, con la fuerza de reflexión determinando la magnitud de la brecha y por lo tanto la tasa de transición diabática entre variedades.",
    "solution": "B"
  },
  {
    "id": 402,
    "question": "¿Qué limita la efectividad de la simulación de Trotter-Suzuki a medida que crece el tamaño de la molécula?",
    "A": "Las moléculas grandes mapeadas a hamiltonianos de red para la simulación de Trotter a menudo exhiben estados excitados casi degenerados debido a simetrías en la disposición espacial de los orbitales atómicos, creando regiones espectrales densas en el paisaje de energía. Cuando el tamaño del paso de Trotter se elige para resolver la energía del estado fundamental, aliasa inadvertidamente estos estados excitados degenerados, causando plegamiento de frecuencias en la evolución temporal simulada.",
    "B": "A medida que los sistemas moleculares crecen, la entropía de entrelazamiento entre cualquier subsistema local y el resto se acerca a su valor máximo, saturando la capacidad de información de los qubits individuales durante la medición. Este efecto de saturación introduce sesgo sistemático en las estadísticas de lectura porque los estados altamente entrelazados no pueden proyectarse de manera confiable en estados de base computacional sin pérdida de información de fase.",
    "C": "La estimación de fase cuántica, que proporciona la aceleración exponencial para extraer energías del estado fundamental molecular, sufre degradación de fidelidad a medida que aumenta el número de orbitales moleculares porque los qubits auxiliares usados para phase kickback acumulan errores proporcionalmente al conteo de orbitales. Cada orbital adicional contribuye con canales de ruido independientes que interfieren destructivamente con la información de fase coherente que se está acumulando en el registro de QPE.",
    "D": "La descomposición de Trotter-Suzuki aproxima la evolución temporal bajo un hamiltoniano H = H₁ + H₂ + ... dividiéndolo en productos de exponenciales exp(-iH_k·Δt), donde cada término evoluciona por separado. Para hamiltonianos moleculares, el número de términos no conmutativos escala como N⁴ con el tamaño del sistema N (debido a integrales bielectrónicas), lo que significa que el error de Trotter — que depende de conmutadores anidados como [H_i, [H_j, H_k]] — crece cuárticamente. Esto obliga a que los tamaños de paso de Trotter Δt se reduzcan como ~1/N⁴ para mantener una precisión fija, causando que el número de pasos temporales requeridos (y la profundidad del circuito) explote, haciendo la simulación imprácticable para moléculas grandes.",
    "solution": "D"
  },
  {
    "id": 403,
    "question": "¿Qué estrategias clave permiten la ejecución de puertas cuánticas entre qubits remotos en un sistema cuántico distribuido?",
    "A": "Clonar los estados de los qubits y procesarlos localmente en cada nodo, evitando así la sobrecarga de distribución de entrelazamiento. Esto explota la clonación aproximada para estados mixtos, generando copias con suficiente fidelidad para operaciones de puertas antes de reconciliar los resultados clásicamente.",
    "B": "Transporte físico de qubits a través de redes de fibra usando técnicas clásicas de multiplexación, donde los qubits codificados en modos de guía de onda fotónica se enrutan a través de conmutadores ópticos reconfigurables. La multiplexación por división de tiempo asegura que múltiples qubits atraviesen la misma fibra sin interferencia mutua, manteniendo coherencia a distancias metropolitanas al explotar ventanas de infraestructura de telecomunicaciones de baja pérdida.",
    "C": "Métodos basados en teletransportación como los protocolos telegate y teledata, que aprovechan el entrelazamiento pre-compartido entre nodos remotos para ejecutar puertas cuánticas no locales. El enfoque telegate consume pares de Bell para implementar operaciones de dos qubits entre qubits separados mediante operaciones locales y comunicación clásica, sintetizando efectivamente la interacción de puerta sin transporte físico de qubits. Teledata usa de manera similar el entrelazamiento como recurso para transmitir información cuántica, permitiendo computación distribuida mientras preserva la coherencia a pesar de la separación espacial de los nodos computacionales.",
    "D": "Se aplican códigos de corrección de errores cuánticos para fusionar físicamente los qubits remotos en un único espacio coherente antes de las operaciones de puertas. Esta fusión usa parches de surface code fusionados adiabáticamente mediante mediciones mediadas por ancillas, creando un espacio unificado de qubit lógico que abarca todos los nodos. Solo después de esta fusión pueden ejecutarse puertas de dos qubits con la fidelidad requerida, ya que la sobrecarga de corrección de errores estabiliza el estado cuántico extendido contra la decoherencia inducida por la red.",
    "solution": "C"
  },
  {
    "id": 404,
    "question": "¿Por qué las puertas no-Clifford son esenciales para la computación cuántica universal?",
    "A": "Las puertas no-Clifford permiten acceso a ángulos de fase fuera del conjunto discreto {0, π/2, π, 3π/2} que caracterizan las operaciones Clifford, lo cual es necesario porque el teorema de Solovay-Kitaev requiere relaciones de fase irracionales para aproximar unitarios arbitrarios. Mientras que las puertas Clifford forman un grupo finito eficientemente simulable por el teorema de Gottesman-Knill, puertas como T (que aplica fase e^(iπ/4)) introducen ángulos trascendentales que rompen esta simulabilidad. Sin tales fases, el conjunto de puertas permanece dentro de un subconjunto contable de SU(2^n) y no puede cubrir densamente el espacio de transformación continuo requerido para la computación universal.",
    "B": "Las operaciones Clifford por sí solas pueden simularse eficientemente clásicamente mediante el teorema de Gottesman-Knill, lo que significa que no pueden proporcionar ventaja computacional más allá de lo que logran las computadoras convencionales. Las puertas no-Clifford como la puerta T introducen la complejidad necesaria para escapar de esta restricción de simulabilidad clásica, permitiendo acceso al espacio de Hilbert completo y haciendo posible la computación cuántica universal. Sin al menos una puerta no-Clifford en tu conjunto de puertas, cualquier circuito cuántico permanece atrapado dentro del formalismo estabilizador eficientemente simulable.",
    "C": "Las puertas no-Clifford son requeridas porque las operaciones Clifford preservan la estructura discreta de los estados estabilizadores, que forman un subconjunto de medida cero del espacio de Hilbert completo. Aunque los circuitos Clifford pueden generar entrelazamiento máximo (ej., estados GHZ y cluster), no pueden crear superposiciones con relaciones de amplitud continuas arbitrarias necesarias para algoritmos como la factorización de Shor. La distinción clave es que los estados estabilizadores tienen solo matrices de densidad reducidas de valores reales cuando se miden en ciertas bases, mientras que las puertas no-Clifford permiten patrones de interferencia complejos con relaciones de fase irracionales que son esenciales para la ventaja computacional cuántica más allá de tareas de muestreo.",
    "D": "Las puertas no-Clifford rompen la garantía de simulación clásica en tiempo polinomial del teorema de Gottesman-Knill al introducir estados mágicos — estados recurso cuya función de Wigner exhibe valores negativos, significando comportamiento cuántico genuino. Mientras que las puertas Clifford solas pueden preparar eficientemente todos los estados de grafo y realizar extracción de síndrome para corrección de errores, generan solo fases discretas que corresponden a transformaciones simplécticas sobre campos finitos. Puertas como T inyectan la complejidad de fase continua necesaria para abarcar SU(2^n), como lo prueba el hecho de que Clifford+T forma un conjunto de puertas universal mediante la construcción de Solovay-Kitaev requiriendo O(log^c(1/ε)) puertas para ε-aproximación.",
    "solution": "B"
  },
  {
    "id": 405,
    "question": "¿Qué significa NISQ?",
    "A": "Near-Intermediate-Scale Quantum, caracterizando dispositivos en el régimen transicional entre qubits lógicos completamente corregidos de errores y experimentos de prueba de principio a pequeña escala, típicamente presentando 50-500 qubits físicos con fidelidades de puerta acercándose pero sin exceder el umbral del surface code del 99%. Estos sistemas demuestran ventaja cuántica en problemas especializados como muestreo de circuitos aleatorios mientras permanecen demasiado ruidosos para algoritmos prácticos que requieren circuitos profundos, ocupando la escala donde la simulación clásica se vuelve intratable en supercomputadoras convencionales pero la tolerancia a fallos permanece inalcanzable, impulsando investigación en algoritmos variacionales y técnicas de mitigación de errores que extraen valor computacional a pesar de la decoherencia que limita la profundidad del circuito a 100-1000 puertas.",
    "B": "Noisy Intermediate-Scale Quantum (Cuántico de Escala Intermedia Ruidoso), el término que caracteriza los procesadores cuánticos de generación actual que operan con 50-1000 qubits, fidelidades de puerta moderadas (típicamente 99-99.9%), y tiempos de coherencia limitados insuficientes para corrección de errores tolerante a fallos completa. Estos dispositivos ocupan el régimen entre experimentos de prueba de principio pequeños y futuras computadoras cuánticas corregidas de errores, permitiendo exploración de ventaja cuántica en dominios específicos como optimización, muestreo y simulación cuántica a pesar de operaciones de puerta imperfectas y decoherencia ambiental que impiden ejecutar algoritmos arbitrariamente largos.",
    "C": "Non-Idealized Scalable Quantum, denotando arquitecturas donde los rendimientos de fabricación de qubits varían a través del chip, requiriendo post-selección y caracterización para identificar subconjuntos de alta fidelidad adecuados para computación. Estas plataformas logran escalabilidad no mediante arreglos uniformes de qubits de alta calidad sino fabricando grandes números de qubits (100-10,000) y mapeando algoritmos en los subgrafos de mejor rendimiento identificados mediante calibración tomográfica, aceptando que el 20-40% de los qubits físicos pueden exhibir fidelidades bajo el umbral. Esta nomenclatura surgió de esfuerzos industriales de computación cuántica enfocados en maximizar el conteo útil de qubits a pesar de imperfecciones de fabricación inherentes a tecnologías de qubits superconductores y basados en semiconductores.",
    "D": "Noise-Intensive Subthreshold Quantum, describiendo sistemas operando en el régimen donde las tasas de error de puertas de dos qubits exceden el umbral de tolerancia a fallos (típicamente >1%) pero permanecen bajo el umbral de simulación clásica donde el comportamiento cuántico se vuelve intratable de verificar. Estos procesadores presentan 50-200 qubits con tiempos de coherencia de 10-100 microsegundos, permitiendo 50-500 operaciones de puerta antes de que la decoherencia domine. La terminología enfatiza que aunque las tasas de error previenen corrección de errores cuántica completa, los dispositivos aún exhiben fenómenos cuánticos genuinos como entrelazamiento a través de decenas de qubits, haciéndolos valiosos para benchmarking de protocolos de mitigación de errores y estudio de diseño de algoritmos resilientes al ruido en la era pre-tolerante a fallos.",
    "solution": "B"
  },
  {
    "id": 406,
    "question": "¿Cuál es el propósito de la compilación iterativa en el diseño de circuitos cuánticos?",
    "A": "Refinar progresivamente las implementaciones de circuitos utilizando retroalimentación de intentos de compilación previos o resultados de ejecución en hardware, mejorando el conteo de puertas, la profundidad o la fidelidad a través de ciclos sucesivos de optimización.",
    "B": "Incorporar datos de calibración en tiempo real provenientes de ejecuciones de caracterización de hardware en pasadas de compilación sucesivas, donde cada iteración actualiza los pesos de la función de coste basándose en fidelidades de puertas medidas, matrices de diafonía (crosstalk) y tiempos de coherencia de los resultados de ejecución de la compilación anterior. Esta optimización de bucle cerrado adapta progresivamente la topología del circuito a las características variables del hardware en el tiempo, mejorando la fidelidad efectiva mediante la programación de puertas consciente del hardware y la asignación de qubits que responde a la deriva de los parámetros del dispositivo entre ciclos de calibración.",
    "C": "Explorar sistemáticamente el espacio de representaciones de circuitos equivalentes aplicando rondas sucesivas de reglas de conmutación, cancelación y síntesis de puertas, donde cada iteración genera múltiples circuitos candidatos que se evalúan contra métricas de profundidad, conteo de puertas y error estimado. La compilación termina cuando iteraciones consecutivas no logran producir mejoras más allá de un umbral, asegurando convergencia a un óptimo local en el paisaje de coste del circuito mediante búsqueda de ascenso de colina (hill-climbing) que refina secuencias de puertas sin requerir ejecución en hardware entre pasadas.",
    "D": "Descomponer puertas complejas de múltiples qubits en operaciones nativas del hardware a través de pasos secuenciales de trotterización, donde cada iteración de compilación aumenta el orden de Trotter para reducir el error de aproximación proveniente de términos hamiltonianos no conmutativos. Comenzar con descomposición de Trotter de primer orden y refinar progresivamente a órdenes superiores permite al compilador equilibrar el conteo de puertas contra la precisión de simulación, terminando cuando la mejora marginal en la fidelidad del operador proveniente de pasos adicionales de Trotter cae por debajo de la tasa de error por puerta de la plataforma de hardware objetivo.",
    "solution": "A"
  },
  {
    "id": 407,
    "question": "¿Los computadores cuánticos actualmente accesibles soportan todas las puertas unitarias posibles?",
    "A": "No, pero los procesadores contemporáneos soportan unitarias arbitrarias de un solo qubit y una puerta universal de dos qubits de forma nativa, lo cual matemáticamente es suficiente para aproximar cualquier unitaria de n qubits con precisión arbitraria mediante el teorema de Solovay-Kitaev. El desafío es que cada capa adicional de descomposición compone errores de puertas de manera multiplicativa, por lo que aunque el conjunto de puertas es formalmente universal, el techo de fidelidad práctica significa que las unitarias complejas exceden los presupuestos de error antes de que la compilación se complete, limitando qué operaciones permanecen experimentalmente viables en hardware NISQ.",
    "B": "No, porque aunque la conectividad de todos con todos (all-to-all) en iones atrapados permite puertas directas de múltiples qubits, el hamiltoniano de interacción Mølmer-Sørensen restringe las operaciones alcanzables al subespacio simétrico de los modos fonónicos colectivos de la cadena de iones. Esta restricción geométrica significa que ciertas unitarias antisimétricas —particularmente aquellas que requieren control de fase independiente de factores tensoriales no conmutativos— no pueden implementarse sin descomponerse en puertas secuenciales que rompen la operación nativa en subgrupos direccionables, reintroduciendo la sobrecarga de compilación que se suponía que la conectividad debía eliminar.",
    "C": "No — el hardware cuántico proporciona solo un conjunto finito de puertas nativas, típicamente consistente en rotaciones de un solo qubit y una o dos puertas entrelazantes de dos qubits como CNOT o CZ. Cualquier operación unitaria arbitraria debe compilarse en una secuencia de estas puertas primitivas mediante algoritmos de descomposición, lo cual introduce profundidad adicional al circuito y acumula errores con cada capa de aproximación.",
    "D": "No, porque incluso con corrección de errores operacional, el conjunto de puertas transversales implementables en qubits lógicos forma un subgrupo discreto (típicamente el grupo de Clifford para códigos estabilizadores) que carece de universalidad. Lograr unitarias lógicas arbitrarias requiere puertas no transversales como la puerta T, que deben implementarse mediante destilación de estados mágicos (magic state distillation) —un protocolo intensivo en recursos que consume muchos qubits físicos por operación lógica. Hasta que se puedan integrar suficientes fábricas de estados mágicos, los programadores permanecen limitados a conjuntos de puertas aproximados.",
    "solution": "C"
  },
  {
    "id": 408,
    "question": "Estás implementando el algoritmo HHL para resolver sistemas lineales. La matriz con la que trabajas tiene valores propios distribuidos a través de tres órdenes de magnitud — el valor propio no nulo más pequeño es alrededor de 0.001 y el más grande está cerca de 1.0. Tu colega te advierte sobre un problema fundamental de escalamiento que dominará tus requerimientos de recursos. ¿De dónde proviene exactamente este cuello de botella y cómo crece el parámetro relevante en función de la estructura de valores propios?",
    "A": "El cuello de botella de recursos emerge de los requerimientos de precisión de la subrutina de estimación de fase, que debe resolver diferencias de valores propios hasta la escala del valor propio más pequeño λ_min ≈ 0.001 para evitar aliasing durante el paso de inversión mediante rotación controlada. La estimación de fase cuántica estándar logra precisión ε usando O(1/ε) aplicaciones del unitario controlado e^(iAt), pero aquí necesitas ε ≪ λ_min, demandando Θ(1/λ_min) ≈ 1000 aplicaciones de unitario controlado solo para resolver la estructura espectral. La inversión de valor propio subsiguiente |λ⟩ → sin(λ̃/λ)|λ⟩ requiere síntesis de ángulos de rotación precisos hasta δθ ≈ λ_min/λ_max para evitar introducir errores que corrompan los componentes de valor propio pequeño del vector solución, y el teorema de Solovay-Kitaev garantiza que lograr precisión de rotación de un solo qubit δθ requiere profundidad de puerta Θ(log^c(1/δθ)) donde c ≈ 2 para implementaciones óptimas. Combinando estos: la profundidad total escala como Θ((κ/λ_min)·log²(κ)) donde κ = λ_max/λ_min ≈ 1000, produciendo el término de coste dominante que crece peor que lineal en el número de condición, aunque no exactamente cuadrático como sugieren algunos modelos simplificados.",
    "B": "El cuello de botella crítico de escalamiento surge durante el paso de amplificación de amplitud que sigue a la inversión de valor propio, donde la probabilidad de éxito para extraer el estado final |x⟩ = A⁻¹|b⟩ escala como P_success ∝ ||A⁻¹||²||b||² / (número de condición)². Con tu número de condición κ ≈ 1000, la amplitud de éxito para medir el registro objetivo en el estado deseado se vuelve ≈ 10⁻⁶, requiriendo O(√(1/P_success)) ≈ 10³ iteraciones de amplificación de amplitud para incrementar la probabilidad a casi unidad. Cada iteración de amplificación demanda un ciclo completo de estimación de fase más inversión controlada, creando una estructura de bucle anidado donde el conteo total de puertas crece como Θ(κ·log(κ)·√κ) ≈ Θ(κ^(3/2)·log κ). El factor log(κ) proviene del tamaño del registro ancila necesario para estimación de fase (necesitas ⌈log₂(κ·poly(n))⌉ qubits para resolver valores propios), el factor √κ de las repeticiones de amplificación de amplitud, y el factor lineal κ de los requerimientos de precisión de inversión de valor propio. Este escalamiento superlineal en κ domina el conteo de recursos y representa la barrera fundamental para aplicar HHL a sistemas mal condicionados.",
    "C": "El desafío de escalamiento se origina en los requerimientos de reconstrucción tomográfica para extraer el vector solución: mientras HHL produce el estado cuántico |x⟩ ∝ A⁻¹|b⟩, medir valores esperados de observables ⟨x|M|x⟩ para operadores arbitrarios M requiere estimar amplitudes de estados base individuales |xᵢ⟩, lo cual necesita Ω(2^n/ε²) disparos de medición para sistemas de n qubits cuando se busca precisión ε mediante tomografía cuántica estándar. Sin embargo, para matrices mal condicionadas, los componentes del vector solución correspondientes a valores propios pequeños dominan la norma L²: ||x||² ≈ Σᵢ |⟨b|vᵢ⟩|²/λᵢ² donde |vᵢ⟩ son vectores propios y el término i-ésimo contribuye ∝ 1/λᵢ². Con λ_min ≈ 0.001, estos componentes llevan pesos ∼ 10⁶ veces mayores que componentes de λ_max ≈ 1.0, creando rango dinámico extremo en la distribución de amplitud. La medición limitada por ruido de disparo entonces requiere N_shots ∝ (λ_max/λ_min)² ∝ κ² muestras para resolver los componentes de solución más pequeños por encima del piso de ruido de medición, produciendo escalamiento cuadrático en número de condición que domina el tiempo de ejecución general a pesar de la complejidad polinomial de puertas de HHL.",
    "D": "El cuello de botella fundamental proviene del paso de rotación controlada que realiza la inversión de valor propio, donde debes aplicar una rotación proporcional a 1/λ para cada valor propio λ. Para distinguir tu valor propio más pequeño (λ_min ≈ 0.001) de cero con suficiente precisión para inversión precisa, la subrutina de estimación de fase requiere precisión del registro ancila escalando como Ω(log(κ)) qubits, pero más críticamente, la precisión de inversión demanda que los ángulos de rotación resuelvan diferencias del orden de 1/λ_min. Dado que la síntesis de puertas cuánticas con precisión ε requiere profundidad de circuito Θ(log(1/ε)), y necesitas ε ≪ λ_min para evitar inundar las contribuciones de valor propio pequeño, la complejidad temporal escala como Θ(κ log(κ)) donde κ = λ_max/λ_min ≈ 1000 es el número de condición — esta dependencia cuadrático-logarítmica en la razón de valores propios se convierte en el coste dominante, no meramente lineal como sugieren algunos análisis simplificados.",
    "solution": "D"
  },
  {
    "id": 409,
    "question": "¿Cuál de las siguientes afirmaciones es más precisa respecto al rendimiento de los clasificadores cuánticos actuales comparados con modelos clásicos?",
    "A": "Ambos tipos de clasificadores logran rendimiento esencialmente equivalente en la mayoría de los benchmarks estándar, con los modelos cuánticos ofreciendo ventajas marginales solo en dominios altamente especializados donde el espacio de características admite naturalmente una inmersión en espacio de Hilbert que se alinea con la estructura del problema. En la práctica, factores como el ruido de disparo de medición y la conectividad limitada de qubits compensan los beneficios teóricos de los métodos de kernel cuántico, resultando en una paridad de rendimiento que sugiere que los enfoques cuántico y clásico son fundamentalmente comparables en hardware de corto plazo cuando se evalúan en conjuntos de datos como MNIST, IRIS o tareas estándar del repositorio UCI.",
    "B": "Los clasificadores cuánticos superan confiablemente a los modelos clásicos en diversos dominios de tareas incluyendo reconocimiento de imágenes, procesamiento de lenguaje natural y pronóstico de series temporales, principalmente debido a su capacidad para explorar espacios de características exponencialmente grandes mediante superposición y entrelazamiento.",
    "C": "Los modelos clásicos típicamente superan a los clasificadores cuánticos en la mayoría de los benchmarks contemporáneos debido a algoritmos de optimización maduros, mejor robustez al ruido y los conteos limitados de qubits disponibles en los dispositivos NISQ actuales. Aunque los enfoques cuánticos muestran promesa teórica para ciertos métodos basados en kernel, las implementaciones prácticas sufren de ruido de disparo, profundidad limitada de circuito y efectos de meseta estéril (barren plateau) que impiden el entrenamiento efectivo, resultando en precisiones de prueba que generalmente caen por debajo de las logradas por técnicas de aprendizaje automático clásico optimizadas.",
    "D": "Solo cuando se proporcionan conjuntos de datos masivos conteniendo millones de ejemplos etiquetados los clasificadores cuánticos comienzan a mostrar ventaja medible sobre los enfoques clásicos, ya que la expresividad del kernel cuántico se vuelve estadísticamente significativa solo en el régimen de muestra grande donde las desigualdades de concentración garantizan que los mapas de características cuánticas exploran direcciones ortogonales en el espacio de Hilbert que los kernels clásicos no pueden acceder eficientemente. Por debajo de aproximadamente 10⁶ muestras de entrenamiento, los modelos clásicos mantienen rendimiento superior debido a sus paisajes de optimización maduros y mejor eficiencia de muestras, pero más allá de este umbral, los circuitos cuánticos aprovechan el escalamiento dimensional para lograr supremacía asintótica en error de generalización.",
    "solution": "C"
  },
  {
    "id": 410,
    "question": "En el aprendizaje de circuitos cuánticos, el compromiso expresividad-entrenabilidad captura la observación de que:",
    "A": "Los circuitos altamente expresivos con estructuras de ansatz profundas y amplios conjuntos de puertas pueden sufrir de gradientes que se desvanecen durante la optimización porque el paisaje de coste se concentra exponencialmente alrededor de su media a medida que aumenta la profundidad del circuito, un fenómeno conocido como mesetas estériles (barren plateaus) que hace inefectivas las actualizaciones de parámetros sin inicialización especializada o arquitecturas estructuradas.",
    "B": "Los circuitos que logran alta cobertura del espacio de estados mediante diseños unitarios aleatorios se vuelven más difíciles de entrenar porque sus gradientes de función de coste se concentran exponencialmente alrededor de cero con profundidad creciente según el lema de Lévy sobre medida concentrada en altas dimensiones. Sin embargo, esta concentración ocurre solo para funciones de coste globales; los observables locales que miden subsistemas de pocos qubits mantienen gradientes entrenables incluso a grandes profundidades, sugiriendo que la expresividad medida por pureza de subsistema en lugar de entropía de entrelazamiento global proporciona un predictor más preciso del comportamiento de escalamiento de gradiente.",
    "C": "A medida que la expresividad del ansatz aumenta mediante la adición de capas, el paisaje de parámetros desarrolla un número exponencialmente creciente de mínimos locales cuyas tallas de cuenca siguen una distribución log-normal, haciendo que el descenso de gradiente sea cada vez más propenso a terminar en configuraciones subóptimas. Esta proliferación de mínimos casi degenerados ocurre porque los circuitos altamente expresivos pueden representar exponencialmente muchos estados aproximadamente ortogonales, cada uno correspondiendo a un óptimo local distinto, mientras que los circuitos poco profundos con expresividad limitada tienen mínimos escasos y bien separados que los optimizadores estándar pueden localizar confiablemente.",
    "D": "Los circuitos expresivos con profundidad de capa entrelazante que excede la longitud de coherencia desarrollan escalamiento de gradiente gobernado por la transición desde el comportamiento unitario aleatorio de Haar a profundidad polinomial hacia medida exponencialmente concentrada a profundidad superpolinomial. Específicamente, para ansätze eficientes en hardware con capas alternantes de rotación y entrelazamiento, la entrenabilidad se mantiene cuando la profundidad d satisface d < O(n^(2/3)) qubits pero entra en el régimen de meseta estéril cuando d > O(n), creando una ventana donde la expresividad medida por poder entrelazante crece polinomialmente mientras que la varianza del gradiente permanece inversamente polinomialmente grande.",
    "solution": "A"
  },
  {
    "id": 411,
    "question": "¿Qué metodología de ataque avanzado puede comprometer la distribución cuántica de claves basada en sistemas de variables continuas?",
    "A": "Al explotar el emparejamiento de modo imperfecto entre la señal y el oscilador local en el receptor, un adversario puede introducir un modo auxiliar débil que es ortogonal al OL pero se acopla a la señal a través de efectos no lineales en los fotodiodos del detector homodino. Este modo auxiliar transporta información parcial sobre los valores de cuadratura que se están midiendo, pero no se contabiliza en la calibración de ruido de disparo porque se encuentra fuera del ancho de banda del modo OL. El adversario puede así extraer información de la clave desde este modo no monitoreado sin aumentar el ruido en el modo de señal monitoreado, evadiendo las comprobaciones de parámetros de seguridad que acotan la información de Eve basándose en el exceso de ruido en el canal homodino primario.",
    "B": "Un adversario puede explotar el ancho de banda finito del detector enviando luz comprimida de banda ancha en frecuencias fuera del rango de detección del receptor, lo que crea anti-compresión en la cuadratura de señal medida mediante conversión paramétrica descendente en la trayectoria óptica. Esta compresión dependiente de la frecuencia reduce la varianza efectiva de la señal dentro del ancho de banda de detección mientras permite simultáneamente la extracción de información mediante detección heterodina de los modos anti-comprimidos fuera de banda. Como las pruebas de seguridad de CV-QKD asumen que la varianza de señal medida dentro del ancho de banda del detector refleja el ruido total del sistema, este ataque permite a Eve obtener información parcial mientras las partes legítimas subestiman la pérdida del canal y sobreestiman las tasas de clave segura basándose en la varianza reducida artificialmente en banda.",
    "C": "Manipulación del haz de referencia del oscilador local, permitiendo al adversario controlar los resultados de las mediciones homodinas introduciendo desplazamientos de fase controlados o modulaciones de amplitud.",
    "D": "El adversario explota la eficiencia cuántica imperfecta en detectores homodinos realizando una interacción unitaria entre el modo de señal y un modo auxiliar antes de la detección, con la intensidad de interacción calibrada de tal manera que la información se transfiere al auxiliar en proporción a (1 - η), donde η es la eficiencia del detector. Como los límites de seguridad de CV-QKD consideran la pérdida atribuyendo todos los fotones perdidos a Eve, pero no modelan explícitamente la ineficiencia del detector como un canal separado de la pérdida, este ataque extrae información adicional más allá de lo que la prueba de seguridad asigna a Eve. El auxiliar del adversario contiene información de cuadratura que se habría perdido por ineficiencia, dando efectivamente a Eve acceso a información que el protocolo asume simplemente descartada.",
    "solution": "C"
  },
  {
    "id": 412,
    "question": "¿Qué técnica específica puede detectar modificaciones maliciosas en secuencias de pulsos cuánticos?",
    "A": "Tomografía de proceso—caracterizar completamente el canal implementado preparando un conjunto completo de estados de entrada que abarcan el espacio de operadores, ejecutando la secuencia de pulsos en cada uno, y realizando tomografía de estado en todas las salidas. Al reconstruir la representación completa de matriz χ o matriz de transferencia de Pauli de la operación cuántica realizada, se puede verificar que la fidelidad del proceso con el unitario pretendido excede los umbrales de seguridad.",
    "B": "La huella digital de calibración establece una firma de referencia de secuencias de pulsos legítimas caracterizando los patrones de error nativos del dispositivo bajo operación honesta, luego detecta desviaciones de esta firma que indican manipulación. Al medir correlaciones observables específicas—como patrones de diafonía entre qubits adyacentes, acumulación de fase dependiente de frecuencia en períodos de inactividad, o inclinaciones sistemáticas del eje de rotación en puertas de un solo qubit—se crea una huella digital de alta dimensión de cómo los pulsos auténticos afectan el estado cuántico. Las modificaciones maliciosas de pulsos, incluso si implementan la puerta correcta en promedio, alterarán estas correlaciones de error sutiles de manera detectable. El análisis estadístico de desviaciones de huella digital a través de múltiples ejecuciones de circuito revela anomalías que distinguen la manipulación adversaria de la deriva natural de calibración.",
    "C": "Los protocolos estándar de randomized benchmarking pueden detectar modificaciones maliciosas de pulsos midiendo la fidelidad de puerta promedio sobre elementos del grupo de Clifford muestreados uniformemente al azar. Si un atacante ha inyectado operaciones de puerta trasera en el compilador de pulsos, la tasa de decaimiento exponencial de la polarización bajo secuencias aleatorias se desviará de la tasa de error de hardware esperada de manera estadísticamente significativa.",
    "D": "La discriminación de estados cuánticos proporciona seguridad contra la manipulación de pulsos preparando pares de estados cuánticos no ortogonales que son óptimamente distinguibles bajo la implementación de pulsos honestos asumida, luego midiendo la fidelidad de discriminación lograda para detectar desviaciones.",
    "solution": "B"
  },
  {
    "id": 413,
    "question": "¿Qué vulnerabilidad específica existe en los procedimientos de calibración para puertas de dos qubits?",
    "A": "La acumulación de fase condicional durante la ejecución de la puerta proviene del acoplamiento longitudinal siempre activo entre qubits, que hace que el estado del qubit de control imprima una fase en el qubit objetivo incluso cuando la puerta está nominalmente inactiva, pero esta fase es invisible en mediciones de base Z, creando puntos ciegos de calibración.",
    "B": "Los errores de calibración de acoplamiento paramétrico se acumulan porque la modulación dependiente del tiempo de la frecuencia del acoplador debe ajustarse con precisión para evitar interacciones ZZ residuales, y cuando ocurren desviaciones de calibración debido a fluctuaciones de temperatura o ruido de flujo, la intensidad de acoplamiento efectiva se desvía del valor objetivo de una manera que introduce fases condicionales no deseadas que degradan la fidelidad de la puerta, particularmente en arquitecturas que usan acopladores sintonizables donde la amplitud de impulso paramétrico controla directamente el hamiltoniano de interacción e incluso pequeñas descalibraciones pueden causar fuga a estados no computacionales.",
    "C": "La dependencia de amplitud de cross-resonance crea errores sistemáticos de puerta porque la amplitud de impulso óptima para el qubit de control depende no linealmente del desintonizado y del estado del qubit objetivo, y los protocolos de calibración estándar que barren amplitud a un desintonizado fijo no logran considerar cómo los desplazamientos dispersivos de qubits vecinos alteran la condición de resonancia dinámicamente.",
    "D": "La sensibilidad al moldeado de pulsos de flujo se vuelve crítica porque incluso distorsiones menores en los bordes ascendentes y descendentes de los pulsos de flujo utilizados para sintonizar frecuencias de qubit pueden introducir transiciones no adiabáticas que pueblan estados de fuga fuera del subespacio computacional, y estas imperfecciones de pulso son difíciles de caracterizar sistemáticamente ya que dependen de la respuesta de ancho de banda completo de la electrónica de control y el cableado criogénico, lo que lleva a una deriva de calibración que se agrava con el ruido ambiental.",
    "solution": "D"
  },
  {
    "id": 414,
    "question": "¿Qué tipo de medición puede implementarse usando una puerta CNOT y un qubit auxiliar inicializado en |0⟩?",
    "A": "Medición no destructiva de la paridad del qubit de control cuando el control es parte de un estado entrelazado mayor—el CNOT con objetivo auxiliar crea una correlación que revela el componente de base computacional del control (|0⟩ o |1⟩) mediante la medición del auxiliar, mientras preserva la superposición del control relativa a otros qubits en el sistema. La medición es no destructiva para el estado reducido del control proyectado sobre la base computacional, aunque la información de fase global relativa a otros qubits entrelazados puede verse perturbada. Esta técnica permite la lectura repetida del mismo observable sin colapsar completamente el estado cuántico de múltiples qubits.",
    "B": "Medición no destructiva que preserva el estado del qubit de control—el CNOT correlaciona el estado del auxiliar con el valor de base computacional del control sin perturbar la superposición o información de fase del control. Medir el auxiliar entonces revela si el control estaba en |0⟩ o |1⟩ dejando el control en su estado original, permitiendo mediciones repetidas u operaciones cuánticas subsiguientes.",
    "C": "Medición proyectiva sobre la base computacional que extrae un bit de información clásica del qubit de control mediante un proceso de dos pasos: el CNOT primero entrelaza el control y el auxiliar en un estado tipo Bell donde el resultado del auxiliar está perfectamente correlacionado con el componente de base del control, luego la medición del auxiliar realiza la proyección. Aunque esto parece perturbar el estado del control, la correlación creada por CNOT asegura que el control permanezca en el autoestado correspondiente al resultado de medición del auxiliar, implementando efectivamente una medición de base computacional con el auxiliar sirviendo como proxy de lectura en lugar de preservar verdaderamente la superposición del control.",
    "D": "Medición no destructiva de la información de fase del qubit de control mediante una técnica llamada \"retroceso de fase\" donde el CNOT transfiere la fase del control al auxiliar sin perturbar la población del control en los estados de base |0⟩ y |1⟩. Medir el auxiliar en la base X (|+⟩/|−⟩) entonces revela la fase relativa entre las amplitudes de base computacional del control dejando el vector de estado del qubit de control sin cambios. Este protocolo de medición sensible a fase es esencial para códigos de corrección de errores cuánticos que necesitan extraer información de síndrome sobre errores de fase sin colapsar estados de qubit lógicos, razón por la cual las construcciones CNOT-auxiliar forman la base de mediciones de estabilizador en códigos de superficie.",
    "solution": "B"
  },
  {
    "id": 415,
    "question": "Considere una implementación de variational quantum eigensolver (VQE) en hardware NISQ actual donde está intentando encontrar el estado fundamental de un hamiltoniano molecular. Su colega propone usar 50 capas de puertas parametrizadas para aumentar la expresividad. ¿Por qué la profundidad del circuito sigue siendo una métrica crítica incluso en algoritmos que emplean capas variacionales poco profundas?",
    "A": "En dispositivos cuánticos ruidosos de escala intermedia, los errores de puertas de dos qubits se acumulan multiplicativamente con la profundidad del circuito, y cada capa adicional agrava los efectos de decoherencia del acoplamiento ambiental. Dado que el hardware actual típicamente exhibe tiempos de coherencia de solo 50-200 microsegundos y fidelidades de puerta alrededor del 99-99.5%, un circuito de 50 capas acumularía tasas de error prohibitivas que abrumarían cualquier señal de la energía del estado fundamental molecular, haciendo crucial permanecer dentro del presupuesto de coherencia incluso si sacrifica algo de expresividad en su ansatz.",
    "B": "La profundidad del circuito controla directamente la variedad alcanzable dentro del espacio de Hilbert a través del álgebra de Lie generada por las puertas parametrizadas, y aunque los circuitos más profundos abarcan subespacios más grandes, cada capa introduce ruido despolarizante que escala como ε^D donde ε ≈ 0.005 es el error de puerta promedio y D es la profundidad. Para 50 capas con puertas de dos qubits dominando el presupuesto de error, la infidelidad acumulada alcanza 1-(1-ε)^(50m) donde m es puertas por capa, típicamente produciendo fidelidades efectivas por debajo de 0.1. Este piso de ruido excede las diferencias de energía del estado fundamental de sistemas moleculares (típicamente miliHartrees), haciendo las señales de optimización indetectables bajo fluctuaciones estocásticas de imperfecciones de hardware.",
    "C": "Los circuitos parametrizados profundos con D capas generan paisajes de optimización que exhiben mesetas estériles donde los gradientes desaparecen exponencialmente como O(2^(-n)) para sistemas de n qubits, un fenómeno demostrado por McClean et al. que afecta ansätze eficientes en hardware independientemente del ruido. Aunque 50 capas aumentan dramáticamente la expresividad en principio, el espacio de parámetros se vuelve exponencialmente plano, causando que los optimizadores basados en gradiente se estanquen. Combinado con ruido de muestreo de muestreo finito (requiriendo O(1/ε²) mediciones por estimación de gradiente con precisión ε), la optimización se vuelve computacionalmente intratable incluso si el hardware fuera sin ruido, haciendo la profundidad un cuello de botella fundamental a través de la geometría del paisaje más que la decoherencia sola.",
    "D": "Los algoritmos variacionales requieren medir valores esperados del hamiltoniano descompuesto en operadores de cadenas de Pauli, y los circuitos más profundos aumentan la repetición de circuito necesaria para lograr precisión objetivo. Cada capa adicional aumenta la varianza del estimador de energía por un factor proporcional al número de condición del unitario parametrizado, causando que el número de disparos requeridos para lograr precisión ε escale como O(D²/ε²). Para 50 capas, esta sobrecarga de disparos se vuelve prohibitiva incluso en simuladores, y combinado con tiempos de coherencia finitos en hardware (limitando el rendimiento total de medición por ventana de coherencia), el tiempo efectivo hasta la solución crece insosteniblemente a pesar de la naturaleza variacional del algoritmo.",
    "solution": "A"
  },
  {
    "id": 416,
    "question": "¿Cómo maneja el modelo sp-QCNN las simetrías más allá de la simetría traslacional?",
    "A": "Implementando capas cuánticas equivariantes mediante generadores del álgebra de Lie que conmutan con los operadores de simetría del hamiltoniano, permitiendo que el circuito variacional preserve las restricciones de teoría de grupos durante la optimización. Sin embargo, esto restringe la arquitectura a simetrías continuas (SO(n), SU(n)) ya que las simetrías discretas requieren representaciones proyectivas incompatibles con las descomposiciones estándar de puertas parametrizadas.",
    "B": "Codificando simetrías generales mediante un enfoque de teoría de grupos que mapea elementos del grupo a transformaciones unitarias en el espacio de estados cuánticos, permitiendo que la arquitectura del circuito respete grupos de simetría finitos arbitrarios más allá de simples traslaciones mediante la elección apropiada de puertas paramétricas.",
    "C": "A través de operaciones de pooling conscientes de simetría que aplican unitarios controlados mapeando órbitas de simetría a estados de la base computacional, permitiendo a la red eliminar grados de libertad redundantes. Este pooling geométrico reduce la dimensión efectiva del espacio de Hilbert por un factor igual al orden del grupo de simetría, pero requiere que la simetría sea abeliana para que los representantes de las órbitas puedan identificarse de forma única.",
    "D": "Aumentando el conjunto de datos de entrenamiento con copias transformadas por el grupo de los estados de entrada y promediando la función de pérdida sobre la órbita de simetría durante la retropropagación, forzando efectivamente que el circuito cuántico aprendido conmute con todas las operaciones del grupo. Esta estrategia de aumento de datos funciona para cualquier grupo de simetría finito pero introduce un coste computacional que escala como |G|², limitando la aplicabilidad práctica a grupos pequeños.",
    "solution": "B"
  },
  {
    "id": 417,
    "question": "¿Qué propiedad del encapsulamiento de claves basado en retículos lo convierte en un reemplazo directo para etiquetas de autenticación clásicas en tuberías de post-procesamiento QKD?",
    "A": "Los criptosistemas ring-learning-with-errors están específicamente diseñados con conjuntos de parámetros que exhiben características de tolerancia al ruido que coinciden con las tasas típicas de error de bits cuánticos del 1-5% observadas en canales prácticos de distribución cuántica de claves. A diferencia de los esquemas de autenticación tradicionales que requieren comunicación clásica libre de errores, los MACs basados en RLWE pueden verificar la integridad del mensaje incluso cuando el canal subyacente introduce inversiones estocásticas de bits, haciéndolos únicamente adecuados para el canal lateral clásico ruidoso que acompaña a QKD. Esta resiliencia al error incorporada elimina la necesidad de corrección de errores separada antes de la autenticación, simplificando la tubería de post-procesamiento.",
    "B": "La estructura algebraica hash-and-sign subyacente al encapsulamiento de claves basado en retículos permite la generación determinista de etiquetas de autenticación mediante el hash del material de clave reconciliado y su firma con la clave privada del retículo. Este determinismo es esencial para el post-procesamiento QKD porque ambas partes deben calcular independientemente etiquetas idénticas a partir de sus claves brutas correlacionadas sin rondas de comunicación adicionales. A diferencia de los esquemas de firma probabilísticos que requieren aleatoriedad fresca y sincronización, la propiedad determinista asegura que las etiquetas de Alice y Bob coincidirán siempre que sus claves corregidas de errores coincidan, proporcionando verificación de autenticación inmediata sin protocolos interactivos.",
    "C": "Semillas cortas uniformemente aleatorias producen MACs cuyo coste de verificación es cuasilineal en la longitud de la clave, permitiendo la autenticación eficiente de las largas cadenas de bits generadas durante el post-procesamiento QKD sin el sobrecosto computacional cuadrático que de otro modo dominaría el tiempo de procesamiento. Esta eficiencia proviene de las operaciones estructuradas de retículos que permiten la expansión de semillas en etiquetas de autenticación completas mediante aritmética polinómica rápida en anillos.",
    "D": "Los mecanismos de encapsulamiento basados en retículos NTRU generan textos cifrados con representaciones notablemente compactas, típicamente 700-800 bytes para seguridad de 128 bits, que es significativamente menor que las firmas de 1-2 KB producidas por esquemas de curvas elípticas que de otro modo serían vulnerables al algoritmo de Shor. Esta ventaja de tamaño se vuelve crítica en el post-procesamiento QKD donde miles de etiquetas de autenticación deben intercambiarse durante la amplificación de privacidad, y el consumo reducido de ancho de banda de los textos cifrados NTRU permite que el sobrecosto de autenticación permanezca por debajo del 5% del material de clave bruto. La compacidad proviene de la estructura de anillo de NTRU que permite un empaquetado más denso de información de seguridad en comparación con esquemas de retículos genéricos.",
    "solution": "C"
  },
  {
    "id": 418,
    "question": "En el contexto de arquitecturas cuánticas tolerantes a fallos con conectividad limitada de qubits, explique por qué la teletransportación de puertas representa un compromiso de recursos fundamentalmente diferente en comparación con el enrutamiento convencional basado en SWAP. Considere tanto el papel del entrelazamiento predistribuido como la tolerancia a la aleatoriedad inducida por mediciones en su respuesta.",
    "A": "La teletransportación de puertas aprovecha pares entrelazados precompartidos y comunicación clásica feedforward para implementar puertas de dos qubits no locales sin requerir conectividad física, intercambiando recursos de entrelazamiento preparados y tolerancia a la aleatoriedad inducida por mediciones contra profundidad de circuito coherente reducida, mientras que el enrutamiento basado en SWAP añade capas de puertas coherentes que acumulan decoherencia pero evita consumir estados entrelazados auxiliares o introducir resultados de medición estocásticos hasta la lectura final.",
    "B": "La teletransportación de puertas utiliza pares de Bell predistribuidos para reemplazar cadenas SWAP de múltiples saltos con operaciones no locales de un solo paso, intercambiando estados de recursos entrelazados y latencia de comunicación clásica por profundidad coherente reducida; sin embargo, el colapso inducido por la medición introduce ruido de proyección fundamentalmente irreversible que se propaga a través de puertas subsiguientes como errores de desfase, mientras que el enrutamiento SWAP mantiene coherencia cuántica completa aplicando solo transformaciones unitarias, haciendo que la teletransportación no sea adecuada para circuitos corregidos de errores donde la extracción de síndromes depende de mediciones reversibles de estabilizadores.",
    "C": "La distinción clave es que la teletransportación de puertas consume pares ancilla entrelazados pregenerados para realizar operaciones no locales mediante mediciones locales y correcciones de Pauli, reduciendo la profundidad del circuito a costa de sobrecosto de ancillas y aceptando aleatoriedad inducida por mediciones en los ángulos de corrección, mientras que el enrutamiento basado en SWAP preserva secuencias de puertas deterministas moviendo físicamente qubits a través de cadenas de pulsos coherentes; pero la teletransportación asume incorrectamente que el retroceso de fase inducido por medición del par de Bell no afecta puertas subsiguientes, lo cual solo es válido cuando la puerta teletransportada conmuta con todas las operaciones descendentes en el grafo de dependencias.",
    "D": "La teletransportación de puertas explota pares EPR precompartidos para ejecutar unitarios no locales mediante mediciones locales y correcciones condicionales, aceptando resultados de medición estocásticos que deben ser rastreados clásicamente y compensados mediante actualizaciones del marco de Pauli, intercambiando así consumo de entrelazamiento y sobrecosto de feedforward clásico por profundidad de circuito reducida, mientras que el enrutamiento SWAP aplica secuencias de puertas coherentes deterministas que reubican físicamente qubits a lo largo del grafo de conectividad, acumulando decoherencia proporcional a la distancia de enrutamiento pero sin requerir qubits ancilla o correcciones de medición hasta la lectura final.",
    "solution": "A"
  },
  {
    "id": 419,
    "question": "¿Cuál es el fundamento teórico de la potencial ventaja cuántica en métodos de aprendizaje automático basados en kernels?",
    "A": "La combinación sinérgica de cómputo eficiente de kernels y espacios de características exponencialmente grandes trabaja en conjunto para superar a los métodos clásicos tanto en tiempo de ejecución como en poder representacional simultáneamente.",
    "B": "El entrelazamiento crea espacios de características que son fundamentalmente más ricos y expresivos que cualquier kernel clásico pueda acceder, porque los qubits entrelazados abarcan dimensiones del espacio de Hilbert que crecen exponencialmente con el tamaño del sistema. Incluso si una computadora clásica pudiera de alguna manera calcular entradas individuales del kernel rápidamente, la capacidad representacional del mapa de características cuántico en sí mismo —determinada por cómo los puntos de datos se correlacionan a través de estados base entrelazados— excede lo que las características clásicas separables pueden codificar, dando a los kernels cuánticos una ventaja inherente de expresividad independientemente del tiempo de ejecución computacional.",
    "C": "Evaluar múltiples entradas del kernel a la vez mediante superposición permite la construcción de la matriz de Gram completa en tiempo polilogarítmico, ya que cada par de puntos de datos puede compararse en paralelo a través de todos los qubits simultáneamente. Al codificar el conjunto de datos en un registro cuántico y aplicar un unitario global que calcula productos internos coherentemente, se evita el escalamiento cuadrático del ensamblaje clásico de la matriz del kernel, extrayendo todas las similitudes por pares a través de un solo proceso de medición que muestrea toda la estructura a la vez.",
    "D": "Computar eficientemente funciones kernel que son exponencialmente difíciles clásicamente, donde los circuitos cuánticos evalúan productos internos en espacios de características cuánticos mediante interferencia y entrelazamiento en tiempo polinomial en el número de qubits, mientras que cualquier algoritmo clásico que intente el mismo cálculo requeriría recursos exponenciales para simular las correlaciones del espacio de Hilbert de alta dimensión.",
    "solution": "D"
  },
  {
    "id": 420,
    "question": "En el contexto de la computación cuántica tolerante a fallos, considere un código Steane [[7,1,3]] sometido a dos modelos de error diferentes: uno donde las mediciones de síndrome son perfectas pero los qubits físicos experimentan ruido despolarizante entre rondas de corrección, y otro donde la extracción del síndrome en sí tiene un 1% de probabilidad de producir un resultado erróneo mientras que los errores de puertas físicas permanecen idénticos. Los experimentalistas a menudo reportan dos valores de umbral distintos al caracterizar tales escenarios. ¿Por qué los umbrales de \"capacidad de código\" son distintos de los umbrales \"fenomenológicos\", y qué suposición fundamental separa estos dos puntos de referencia en el análisis del rendimiento de corrección de errores cuánticos?",
    "A": "El análisis de capacidad de código trata las mediciones como perfectas y considera solo errores de almacenamiento en qubits de datos, dando un límite superior al umbral alcanzable; los modelos fenomenológicos añaden mediciones de síndrome defectuosas, que introducen cadenas de errores correlacionados que se propagan a través de rondas de corrección y reducen el umbral práctico que observará en hardware real donde la preparación de ancillas, las puertas de dos qubits durante la extracción de síndromes y la lectura fallan todas a tasas distintas de cero.",
    "B": "Los umbrales de capacidad de código asumen extracción de síndrome instantánea con mediciones perfectas, modelando solo la decoherencia de qubits de datos entre rondas, mientras que los umbrales fenomenológicos incorporan errores de medición que crean ambigüedad de síndrome requiriendo correlación temporal a través de múltiples rondas para decodificar correctamente. Sin embargo, los modelos fenomenológicos aún tratan la preparación de ancillas y las puertas de síndrome de dos qubits como perfectas, contabilizando solo errores clásicos de inversión de bits en los resultados de medición mismos, lo que significa que el umbral fenomenológico en realidad excede la capacidad de código en la práctica cuando los errores de puertas durante la extracción cancelan parcialmente los errores de almacenamiento mediante correlaciones de error fortuitas.",
    "C": "Los umbrales difieren porque los modelos de capacidad de código asumen canales de error de Pauli que preservan la estructura del estabilizador, generando límites de umbral ajustados mediante programación lineal sobre el espacio de síndromes, mientras que los umbrales fenomenológicos deben contabilizar errores no-Pauli introducidos por mediciones imperfectas, específicamente amortiguamiento de amplitud durante la lectura que descoherencia parcialmente la información del síndrome. Esto hace que el análisis fenomenológico requiera evolución completa de la matriz de densidad, pero ambos modelos convergen cuando la fidelidad de medición del síndrome excede el 99% ya que los errores de lectura entonces contribuyen correcciones subdominantes al cálculo del umbral.",
    "D": "Los umbrales de capacidad de código se aplican cuando la lectura del síndrome es instantánea y libre de ruido, capturando solo errores de datos entre rondas, mientras que los modelos fenomenológicos añaden fallos de medición del síndrome que causan errores del decodificador pero aún asumen que el circuito de extracción del síndrome en sí —puertas ancilla, operaciones CNOT y mediciones— se ejecuta perfectamente aparte de que el resultado binario sea incorrecto. La brecha entre umbrales emerge porque las mediciones de síndrome repetidas bajo el modelo fenomenológico acumulan errores correlacionados a través de rondas que el decodificador debe rastrear temporalmente, reduciendo la distancia efectiva del código en comparación con la suposición de capacidad de código de detección de errores inmediata.",
    "solution": "A"
  },
  {
    "id": 421,
    "question": "¿Por qué resulta particularmente difícil adaptar las técnicas clásicas de corrección de errores a la computación cuántica?",
    "A": "Los códigos de corrección de errores cuánticos requieren que los qubits físicos mantengan tiempos de coherencia que se extiendan más allá de la duración del ciclo de corrección de errores, lo cual las implementaciones experimentales actuales solo pueden lograr a temperaturas cercanas al cero absoluto donde las fluctuaciones térmicas se vuelven despreciables. A temperaturas más altas, las excitaciones térmicas introducen errores más rápido de lo que los códigos de corrección pueden detectarlos y corregirlos, creando una barrera de temperatura fundamental que hace la corrección de errores cuánticos a temperatura ambiente teóricamente imposible según el principio de Landauer aplicado a la información cuántica.",
    "B": "Los qubits existen en estados de superposición donde representan simultáneamente múltiples patrones de error clásicos, lo que significa que la medición convencional del síndrome de error colapsaría el estado cuántico y destruiría la información misma que estamos intentando proteger. Además, la naturaleza continua de los errores cuánticos (rotaciones arbitrarias en la esfera de Bloch) contrasta marcadamente con los errores discretos de inversión de bits en sistemas clásicos, requiriendo estrategias de detección y corrección fundamentalmente diferentes que deben tener en cuenta infinitas orientaciones posibles de error en lugar de solo dos.",
    "C": "El teorema de no clonación impide la duplicación directa de la información cuántica, haciendo inviable la corrección de errores tradicional basada en redundancia. A diferencia de los sistemas clásicos donde los bits pueden copiarse libremente para crear codificaciones redundantes, los estados cuánticos no pueden clonarse, requiriendo enfoques fundamentalmente diferentes como la medición de síndromes y códigos estabilizadores que extraen información de error sin destruir la superposición cuántica que se está protegiendo.",
    "D": "El proceso de corrección de errores cuánticos debe abordar simultáneamente todos los tipos de error posibles —inversiones de bit, inversiones de fase y sus combinaciones— en un solo paso de corrección, porque la corrección secuencial de diferentes tipos de error requeriría múltiples operaciones de medición que cada una colapsan el estado cuántico. Esta imposibilidad teórica surge del postulado de medición de la mecánica cuántica, que prohíbe extraer información sobre múltiples observables no conmutantes sin perturbar fundamentalmente el sistema, haciendo de la corrección paralela de todos los tipos de error una barrera insuperable.",
    "solution": "C"
  },
  {
    "id": 422,
    "question": "¿Qué enfoque técnico proporciona las garantías de seguridad más sólidas para el intercambio de claves autenticado por contraseña resistente a ataques cuánticos?",
    "A": "Los protocolos PAKE basados en retículos logran reducciones de seguridad ajustadas a problemas difíciles como Learning With Errors, proporcionando resistencia demostrable contra adversarios cuánticos con pérdida mínima de seguridad en la reducción, y admitiendo implementaciones eficientes mediante retículos estructurados en anillos.",
    "B": "Los protocolos de transferencia inconsciente basados en códigos aprovechan la dificultad de la decodificación de síndromes en códigos lineales aleatorios para habilitar el intercambio de claves autenticado por contraseña con garantías de seguridad teórico-informativas. Al codificar la contraseña como un síndrome y requerir que ambas partes resuelvan un problema de decodificación de distancia acotada, estos esquemas aseguran que incluso un adversario cuántico con poder computacional ilimitado no puede extraer la clave compartida sin conocimiento de la contraseña, haciéndolos superiores a los supuestos de dureza computacional.",
    "C": "Las pruebas de conocimiento cero con supuestos de dureza post-cuánticos permiten la verificación de contraseñas sin revelar la contraseña misma, permitiendo a ambas partes autenticarse y establecer claves mientras mantienen la seguridad incluso contra adversarios cuánticos que pueden romper los supuestos tradicionales del logaritmo discreto.",
    "D": "Los esquemas de compromiso mediante hash combinados con funciones de extracción de entropía resistentes a ataques cuánticos proporcionan la seguridad PAKE más fuerte al obligar a ambas partes a comprometerse con sus hashes de contraseña antes de que se intercambie cualquier material de clave.",
    "solution": "C"
  },
  {
    "id": 423,
    "question": "¿Qué metodología de ataque específica amenaza las extensiones de DNS seguras post-cuánticas?",
    "A": "El envenenamiento de caché cuántico mediante algoritmos de predicción de respuestas explota el hecho de que los resolutores DNS deben aceptar respuestas dentro de una ventana de tiempo limitada, y las computadoras cuánticas pueden usar amplificación de amplitud para probar todos los IDs de transacción y números de puerto posibles simultáneamente, encontrando una falsificación válida en tiempo proporcional a la raíz cuarta del espacio de búsqueda en lugar de requerir una búsqueda clásica de fuerza bruta.",
    "B": "La enumeración de zonas acelerada por búsqueda de Grover permite a los atacantes descubrir todos los nombres de host dentro de una zona DNS exponencialmente más rápido que los ataques clásicos de recorrido, consultando una superposición de posibles nombres de subdominio y midiendo cuáles devuelven registros NSEC o NSEC3 válidos. Incluso cuando NSEC3 usa funciones hash post-cuánticas, la aceleración cuadrática del algoritmo de Grover reduce la seguridad efectiva en bits.",
    "C": "Las colisiones de hash NSEC3 encontradas usando algoritmos cuánticos como la búsqueda de Grover, que puede encontrar preimágenes o segundas preimágenes con aceleración cuadrática, potencialmente comprometiendo el mecanismo de negación de existencia autenticada incluso cuando esquemas de firma post-cuánticos protegen los registros de zona mismos.",
    "D": "El compromiso de claves DNSSEC mediante ataques de reducción de retículos puede romper los supuestos criptográficos subyacentes incluso en esquemas post-cuánticos si los parámetros se eligen incorrectamente, particularmente cuando los implementadores subestiman el nivel de seguridad concreto necesario para resistir la reducción de base de retículos mejorada cuánticamente. Específicamente, si las claves DNSSEC se generan usando firmas basadas en retículos con módulo q y distribución de ruido σ elegidos para proporcionar solo 128 bits de seguridad clásica.",
    "solution": "C"
  },
  {
    "id": 424,
    "question": "¿Cuál es la estrategia central detrás de los pases de transpilación adaptativos al ruido?",
    "A": "Consultar datos de calibración para preferir qubits y acopladores de alta fidelidad durante el mapeo. Al examinar mediciones recientes de caracterización del dispositivo —incluyendo tasas de error de puertas, tiempos de coherencia y fidelidades de lectura— el transpilador asigna dinámicamente qubits lógicos a qubits físicos y selecciona rutas de acoplamiento que minimizan el error esperado del circuito, adaptando la estrategia de compilación al estado actual del dispositivo en lugar de tratar todos los recursos de hardware como equivalentes.",
    "B": "Aprovechar métricas de calibración en tiempo real para construir un grafo de conectividad ponderado donde los costos de aristas reflejan errores actuales de puertas de dos qubits y los costos de nodos codifican límites de coherencia de un solo qubit. El transpilador entonces resuelve un problema de enrutamiento de peso mínimo que asigna qubits lógicos a ubicaciones físicas minimizando el error total esperado, mientras simultáneamente optimiza la inserción de SWAP para evitar acopladores de alto error. Este mapeo consciente del dispositivo usa directamente datos medidos de T1, T2 y fidelidad de puertas en lugar de asumir homogeneidad del hardware.",
    "C": "Incorporar datos de caracterización del dispositivo en un modelo de ruido bayesiano que predice la fidelidad esperada del circuito bajo diferentes asignaciones de qubits, luego usar recocido simulado para explorar el espacio de mapeo y converger en colocaciones de qubits que maximizan la probabilidad de éxito general. Al tratar errores de puertas, errores de lectura y tiempos de coherencia como variables aleatorias correlacionadas aprendidas de ejecuciones de calibración, el transpilador se adapta a la deriva temporal en el rendimiento del dispositivo y enruta preferentemente a través de regiones de hardware actualmente de alto rendimiento.",
    "D": "Utilizar barridos de calibración recientes para identificar qubits y puertas actualmente operando por encima de sus umbrales de error especificados, luego remapear dinámicamente el circuito para excluir estos recursos degradados del objetivo de compilación. El transpilador consulta métricas del dispositivo en vivo durante la fase de mapeo y aplica un algoritmo de satisfacción de restricciones asegurando que ningún qubit lógico sea asignado a un qubit físico con T1 por debajo de 50 μs o error de puerta por encima de 0.5%, creando efectivamente una máscara de hardware adaptativa que refleja la salud instantánea del dispositivo.",
    "solution": "A"
  },
  {
    "id": 425,
    "question": "¿Qué protocolo avanzado proporciona la seguridad más sólida para esquemas de compromiso cuántico?",
    "A": "El compromiso de cadenas cuánticas bajo modelos de almacenamiento acotado aprovecha la restricción física de que un adversario no puede almacenar estados cuánticos arbitrariamente grandes de forma coherente, típicamente acotados por estimaciones realistas de capacidad de memoria cuántica alcanzable (por ejemplo, 10^9 qubits mantenidos coherentemente durante la duración del protocolo). El protocolo transmite un flujo de alta tasa de estados cuánticos —superando ampliamente la capacidad de almacenamiento del adversario— que codifican la cadena comprometida mediante un código de corrección de errores cuántico. El receptor debe realizar mediciones sensibles al tiempo y almacenar solo síndromes clásicos, mientras el comprometedor retiene suficiente información cuántica para revelar posteriormente la cadena. La seguridad deriva de argumentos teórico-informativos que muestran que cualquier adversario con almacenamiento por debajo del umbral del protocolo no puede distinguir la cadena comprometida de datos aleatorios.",
    "B": "Los protocolos estándar de compromiso de bit cuántico logran seguridad incondicional cuando se aumentan con una fase de configuración confiable, específicamente mediante entrelazamiento compartido previamente entre el comprometedor y el receptor que ha sido verificado a través de múltiples rondas de pruebas de desigualdad de Bell. Los pares entrelazados, típicamente distribuidos como singuletes EPR, sirven como recurso criptográfico que vincula el compromiso mientras previene tanto la revelación prematura como los cambios posteriores al compromiso. Al realizar mediciones locales en sus respectivas mitades según un protocolo previamente acordado, el comprometedor puede codificar el valor del bit de manera que queda bloqueado teórico-informativamente una vez que se hacen las elecciones de medición. El supuesto de configuración confiable se considera aceptable en entornos criptográficos prácticos.",
    "C": "El compromiso de bit cuántico sensible a trampas representa un cambio de paradigma al reconocer que la seguridad perfecta contra todas las estrategias de trampa es imposible debido al teorema de imposibilidad de Mayers-Lo-Chau, pero en su lugar diseña protocolos donde cualquier intento de trampa necesariamente deja rastros detectables en el canal cuántico. El protocolo codifica el bit comprometido en un estado cuántico que ocupa un subespacio específico del espacio de Hilbert conjunto de múltiples qubits, de modo que cualquier intento de extraer información prematuramente o cambiar el compromiso retroactivamente requiere mediciones o transformaciones unitarias que inevitablemente perturban cantidades observables. El análisis estadístico de tasas de error en rondas de verificación subsiguientes puede entonces revelar intentos de trampa con alta confianza.",
    "D": "Compromiso de bit relativista que explota la restricción fundamental de que la información no puede viajar más rápido que la luz para hacer cumplir las propiedades de vinculación y ocultación.",
    "solution": "D"
  },
  {
    "id": 426,
    "question": "¿Qué protocolo avanzado proporciona la seguridad más sólida para la distribución cuántica de claves a distancias extremadamente largas?",
    "A": "La distribución cuántica de claves de campo gemelo logra una fuerte seguridad a larga distancia haciendo que tanto Alice como Bob envíen pulsos coherentes débiles a una estación repetidora no confiable posicionada en el punto medio, donde se mide la interferencia de fotones individuales sin revelar qué parte envió qué fotón.",
    "B": "La distribución cuántica de claves basada en satélites proporciona una seguridad superior a larga distancia al aprovechar el casi vacío del espacio para minimizar la pérdida de fotones y la decoherencia a distancias de miles de kilómetros. Al establecer enlaces ópticos entre estaciones terrestres y satélites en órbita terrestre baja durante breves pasos por encima, este enfoque evita la atenuación exponencial que afecta a los sistemas basados en fibra, con el beneficio adicional de que la turbulencia atmosférica solo afecta los últimos kilómetros de transmisión. Los canales cuánticos de espacio libre a través del espacio logran tasas de pérdida efectivas por debajo de 5 dB incluso para distancias intercontinentales.",
    "C": "La distribución cuántica de claves independiente del dispositivo de medición (MDI-QKD) ofrece una fuerte seguridad para largas distancias porque elimina todos los canales laterales del detector y los ataques de caballo de Troya al colocar el aparato de medición en una ubicación no confiable. Ambas partes comunicantes preparan pares de fotones entrelazados y envían un fotón de cada par a una estación de medición central, que realiza mediciones de estado de Bell sin aprender nada sobre la clave.",
    "D": "Los repetidores cuánticos basados en entrelazamiento proporcionan la seguridad más sólida al establecer entrelazamiento entre nodos distantes mediante intercambio y purificación de entrelazamiento, permitiendo una distribución de claves que escala favorablemente con la distancia mientras mantiene seguridad incondicional a través de la monogamia del entrelazamiento, asegurando que ningún espía pueda compartir las correlaciones cuánticas.",
    "solution": "D"
  },
  {
    "id": 427,
    "question": "¿Por qué es crucial la estimación de la varianza antes de comprometerse con una estrategia de corte?",
    "A": "La estimación de la varianza permite que el protocolo de mitigación de errores asigne recursos óptimos de qubits a través de fragmentos de circuito, porque la fidelidad de reconstrucción de cada fragmento depende de la ponderación de varianza inversa de sus observables muestreados; si la varianza se subestima, la recombinación ponderada amplificará el ruido de fragmentos de alta varianza, corrompiendo el valor de expectativa final y anulando los beneficios de reducción de profundidad del corte.",
    "B": "La predicción precisa de la varianza permite calcular el coste total de muestreo para el protocolo de reconstrucción de cortes, asegurando que el experimento permanezca factible dentro del presupuesto de disparos disponible y las restricciones de tiempo de ejecución antes de invertir recursos en la descomposición de circuitos y la ejecución de fragmentos.",
    "C": "El preanálisis de la varianza determina el número mínimo de bases de medición paralelas requeridas para cada fragmento, porque la descomposición observable en sumas de Pauli debe tener en cuenta la propagación de la varianza a través de las distribuciones de cuasi-probabilidad utilizadas en la reconstrucción de cortes; subestimar la varianza conduce a una cobertura de base insuficiente, causando sesgo sistemático en el valor de expectativa reconstruido que no puede corregirse después de la medición.",
    "D": "La caracterización de la varianza establece los hiperparámetros para el planificador de fragmentos, ya que los observables de alta varianza requieren ranuras de ejecución priorizadas con decoherencia inducida por cola mínima para mantener la eficiencia de disparos; si la varianza se juzga mal, el planificador asigna ranuras de baja prioridad a fragmentos críticos, aumentando exponencialmente la sobrecarga de muestreo necesaria para lograr la precisión objetivo en el observable reconstruido final.",
    "solution": "B"
  },
  {
    "id": 428,
    "question": "Un paseo cuántico de tiempo continuo a veces puede simularse más rápido que un paseo discreto porque el tiempo continuo:",
    "A": "Permite atajos de diagonalización analítica mediante descomposición espectral de hamiltonianos de grafos dispersos, porque cuando el hamiltoniano del paseo H es la matriz laplaciana o de adyacencia del grafo de un grafo altamente simétrico (como hipercubos, grafos completos o grafos circulantes), sus valores propios y vectores propios pueden calcularse en forma cerrada usando teoría de representación del grupo de automorfismos del grafo, evitando por completo la necesidad de aproximación numérica de Trotter. Específicamente, para un grafo de n vértices con una representación irreducible de dimensión d, el operador de evolución e^(-iHt) se descompone en d exponenciales independientes de bloques diagonales que pueden implementarse usando solo O(d log n) puertas en lugar de las O(n²) puertas requeridas para simulación hamiltoniana general, y estos bloques corresponden a paseos en grafos cociente obtenidos por reducción de simetría. La formulación continua es esencial aquí porque los paseos de tiempo discreto introducen un operador de moneda que rompe las simetrías del grafo, forzando el uso de trotterización completa y eliminando la estructura de bloques diagonales que habilita los atajos analíticos, requiriendo en última instancia circuitos cuya profundidad escala linealmente con el tiempo de simulación en lugar de logarítmicamente.",
    "B": "Evita ramificación estocástica en las trayectorias de evolución de amplitud, porque los paseos cuánticos de tiempo discreto requieren en cada paso una elección de operador de moneda (moneda Hadamard, Grover o DFT) cuya acción crea una superposición de ramificación sobre todas las posibles direcciones del siguiente paso ponderadas por los elementos de matriz de la moneda, simulando efectivamente un árbol de exponencialmente muchas trayectorias de amplitud que deben rastrearse coherentemente. En contraste, la evolución de tiempo continuo bajo el hamiltoniano del grafo H genera una única trayectoria determinista en el espacio de Hilbert gobernada por la ecuación de Schrödinger i ℏ(d|ψ⟩/dt) = H|ψ⟩, que puede discretizarse en pasos de Trotter de tamaño uniforme sin introducir ramificación, porque cada rebanada de tiempo infinitesimal avanza el estado por un unitario fijo e^(-iHδt) que aplica las mismas operaciones locales a todos los vértices simultáneamente. Esta eliminación de ramificación reduce la profundidad del circuito de O(T·d) para un paseo discreto en un grafo de grado d durante T pasos a O(T·polylog(n)) para simulación de tiempo continuo mediante descomposición de Trotter-Suzuki en un grafo de n vértices, ya que la estructura de localidad del hamiltoniano (cada vértice se acopla a como máximo d vecinos) puede explotarse para paralelizar las capas de Trotter, aunque esta ventaja solo se materializa cuando d = o(n^(1/3)) debido a restricciones de enrutamiento en arreglos de qubits planares.",
    "C": "Explota la dispersión hamiltoniana para la eficiencia de descomposición de Trotter, donde el hamiltoniano del grafo H se descompone naturalmente en una suma de términos locales conmutantes o casi conmutantes correspondientes a aristas individuales, habilitando fórmulas de Trotter de primer orden como e^(-iHt) ≈ ∏ⱼ e^(-iHⱼt) con error escalando como O(t²||[Hⱼ, Hⱼ′]||), lo que para grafos de grado acotado produce implementaciones de circuito altamente paralelizables. A diferencia de los paseos discretos que requieren un operador de moneda global que entrelaza el registro de posición con un qubit auxiliar en cada paso de tiempo (creando profundidad de circuito lineal tanto en el número de pasos como en el grado del grafo), los paseos de tiempo continuo permiten una descomposición local donde el término hamiltoniano de cada arista Hⱼ = |u⟩⟨v| + |v⟩⟨u| se implementa mediante una única puerta de dos qubits entre qubits adyacentes en el registro, y los términos correspondientes a aristas disjuntas pueden aplicarse en paralelo. Para un grafo con grado máximo d y n vértices, un paseo discreto de T pasos requiere profundidad O(T·d) con Ω(T·n) puertas totales, mientras que la simulación de tiempo continuo mediante Trotter logra profundidad O((dt/ε)·log(n)) con sobrecarga de enrutamiento, donde ε es la precisión deseada y el factor logarítmico surge de redes SWAP en arquitecturas con conectividad restringida.",
    "D": "No requiere registro de moneda, reduciendo sustancialmente el ancho del circuito al eliminar el espacio de qubit auxiliar necesario para implementar el operador de moneda que gobierna las probabilidades de transición en cada paso en las formulaciones de tiempo discreto. Esta simplificación arquitectónica reduce el número total de qubits de n+log(d) a solo n para un grafo de n vértices con grado máximo d, lo que se traduce directamente en circuitos de menor profundidad porque se necesitan menos operaciones SWAP para el enrutamiento de qubits en hardware con conectividad limitada, y la ausencia de puertas de entrelazamiento de lanzamiento de moneda significa que el circuito general comprende principalmente términos de evolución hamiltoniana local que pueden paralelizarse más eficientemente durante la compilación.",
    "solution": "D"
  },
  {
    "id": 429,
    "question": "¿Qué vector de ataque apunta específicamente al dominio de frecuencia de las señales de control cuántico?",
    "A": "La explotación de fuga de bandas laterales aprovecha el filtrado imperfecto en el hardware de control, donde el proceso de modulación utilizado para generar pulsos conformados necesariamente crea componentes de frecuencia fuera de la banda portadora prevista, y estas bandas laterales pueden acoplarse a transiciones no deseadas en el qubit o su entorno. Un adversario que puede inyectar una señal a una frecuencia de banda lateral puede efectivamente aprovecharse del sistema de control, induciendo errores de puerta que están correlacionados entre qubits porque comparten osciladores comunes, creando así un camino tanto para la fuga de estado como para la amplificación de diafonía que no sería visible en simulaciones que asumen filtros de pared de ladrillo ideales.",
    "B": "La manipulación de deriva de frecuencia opera desplazando sutilmente la condición de resonancia de los qubits objetivo a través de perturbaciones ambientales, como variaciones de campo magnético o gradientes de temperatura, de modo que los pulsos de control, que están sintonizados a una frecuencia fija, se vuelven progresivamente desintonizados con el tiempo. Al inducir derivas lentas del orden de decenas de kHz por hora, un atacante puede causar que las puertas calibradas acumulen errores de fase que se componen multiplicativamente a través de un cálculo, y debido a que la deriva a menudo se confunde con ruido ambiental benigno, puede permanecer sin detectar hasta que la fidelidad se degrada por debajo del umbral, momento en el cual el cálculo ya está comprometido y los esfuerzos de recalibración pueden ser inútiles si la fuente de deriva está controlada externamente.",
    "C": "La inyección espectral implica introducir señales electromagnéticas cuidadosamente diseñadas a frecuencias que se superponen o se encuentran adyacentes al espectro de pulso de control, permitiendo a un atacante interferir con las operaciones de qubit ya sea amplificando tonos de control existentes o introduciendo impulsos espurios que causan rotaciones no deseadas o transferencias de población dentro del subespacio computacional.",
    "D": "Los ataques de corrupción de resonancia explotan el hecho de que los sistemas cuánticos tienen espectros de escalera con múltiples frecuencias de transición, y si un adversario puede inyectar una señal cerca de un estado excitado superior o la transición de un qubit auxiliar, pueden poblar esos niveles incluso cuando el subespacio computacional está nominalmente protegido por grandes desintonías. Una vez que la población se filtra a estos estados no deseados, no regresa inmediatamente porque los tiempos de relajación son largos, y los pulsos de control posteriores diseñados para dinámica de dos niveles tendrán efectos impredecibles en la matriz de densidad contaminada. El ataque es particularmente insidioso en sistemas con espectros abarrotados, como qubits transmon o iones atrapados con cadenas largas, donde incluso un impulso débil fuera de resonancia puede sembrar errores que se propagan coherentemente a través del sistema mediante interacciones de intercambio o modos de movimiento compartidos.",
    "solution": "C"
  },
  {
    "id": 430,
    "question": "¿Cómo ayuda la degeneración de los códigos estabilizadores a suprimir errores lógicos más allá de las predicciones basadas únicamente en la distancia?",
    "A": "En códigos estabilizadores con operadores lógicos degenerados, cada subespacio degenerado distinto aloja un qubit lógico independiente codificado con su propia tabla de síndromes separada, multiplicando efectivamente la capacidad del código. Debido a que los errores dentro de un sector degenerado no pueden propagarse para afectar qubits en otros sectores, el código logra una reducción exponencial en errores de diafonía proporcional al número de subespacios degenerados. Esta compartimentación significa que incluso si ocurren múltiples errores simultáneamente, están aislados dentro de sus respectivos sectores y pueden corregirse independientemente sin interferencia.",
    "B": "Cuando un código estabilizador exhibe degeneración, la medición repetida de generadores estabilizadores se vuelve innecesaria porque la estructura degenerada misma proporciona información de error implícita a través de la superposición de subespacios de error. Esta redundancia permite que el código opere sin extracción activa de síndromes, confiando en cambio en el colapso natural del estado cuántico en el espacio fundamental degenerado del grupo estabilizador. Al eliminar la sobrecarga de medición, la degeneración reduce las oportunidades para que errores inducidos por medición corrompan la información codificada, mejorando así las tasas de error lógico más allá de lo que la distancia sola predice.",
    "C": "La degeneración permite que múltiples cadenas de error distintas produzcan síndromes idénticos, dando al decodificador flexibilidad para elegir una corrección que podría mapear el error real a un resultado trivial o lógicamente equivalente en lugar de uno dañino, aumentando efectivamente el número de patrones de error corregibles más allá de lo que la distancia sola sugiere.",
    "D": "Los códigos estabilizadores degenerados explotan grados de libertad de gauge al incrustar información de síndrome directamente en qubits de gauge auxiliares que viven dentro del espacio del código pero fuera del subespacio lógico. Cuando ocurren errores, el síndrome se manifiesta como excitaciones de estos qubits de gauge, que luego pueden reiniciarse directamente al estado fundamental sin requerir medición o procesamiento clásico. Este mecanismo de reinicio inmediato, habilitado por la estructura degenerada que mapea síndromes de error a violaciones de gauge locales, suprime errores lógicos a través de un proceso de purificación continua que opera más rápido que el tiempo de ciclo de corrección de errores predicho por límites basados en distancia.",
    "solution": "C"
  },
  {
    "id": 431,
    "question": "¿Qué modificación al algoritmo de Shor permite resolver el problema del logaritmo discreto?",
    "A": "Implementar un paseo cuántico sobre la estructura de grupo cíclico permite al algoritmo recorrer todos los candidatos posibles de logaritmo en superposición, aprovechando la propiedad de clausura del grupo para identificar el logaritmo discreto mediante interferencia destructiva de caminos incorrectos. Este enfoque explota la reversibilidad de los paseos cuánticos para amplificar la amplitud de probabilidad del exponente correcto mientras suprime todos los demás, reemplazando efectivamente la subrutina de búsqueda de período con una búsqueda sobre las potencias del generador del grupo.",
    "B": "Añadir un registro adicional dedicado a almacenar valores intermedios del logaritmo permite al algoritmo realizar comparaciones paralelas entre exponentes candidatos, usando operaciones controladas para verificar si g^x = h en el grupo cíclico. Este tercer registro mantiene la coherencia durante todo el cálculo y se mide al final para colapsar la superposición sobre el logaritmo discreto correcto.",
    "C": "Usar una transformada de Fourier doble sobre ambos registros de entrada en lugar de solo uno",
    "D": "Cambiar la operación de exponenciación modular por una operación de grupo diferente, específicamente reemplazando la multiplicación módulo N con la operación de grupo del grupo multiplicativo cíclico directamente, transforma el problema de búsqueda de período en un problema de búsqueda de logaritmo al explotar propiedades homomórficas entre las estructuras aditiva y multiplicativa. La función modificada f(x) = g^x (mod p) se convierte en f(x,y) = g^x · h^y (mod p), donde el algoritmo cuántico busca soluciones enteras que satisfacen la relación de grupo mediante exponenciación simultánea en ambos registros antes de aplicar la QFT estándar para extraer el logaritmo discreto.",
    "solution": "C"
  },
  {
    "id": 432,
    "question": "En una arquitectura tolerante a fallos que implementa códigos de superficie con un parche de distancia 7, observas que las tasas de error lógico se estabilizan a pesar de aumentar el número de rondas de extracción de síndrome. Tu diagnóstico revela errores correlacionados que aparecen en grupos de 3-4 qubits de datos adyacentes después de cada ciclo de medición de estabilizadores. El agrupamiento de errores persiste incluso después de optimizar las fidelidades de puertas de un solo qubit al 99.99%. ¿Qué vulnerabilidad de seguridad específica emerge en los protocolos de intercambio de claves autenticado post-cuántico?",
    "A": "La susceptibilidad de la confirmación de clave a ataques de temporización de medición surge porque los errores correlacionados en el hardware cuántico crean retrasos detectables en el proceso de verificación de firma post-cuántica que concluye el protocolo de intercambio de claves. Específicamente, cuando los errores agrupados afectan qubits involucrados en esquemas de firma basados en retículos como Falcon o Dilithium, la sobrecarga de corrección de errores introduce variaciones de temporización a escala de microsegundos correlacionadas con el peso de Hamming del material de clave privada.",
    "B": "El secreto hacia adelante se ve comprometido mediante técnicas de restauración de estado cuántico que explotan la reversibilidad de las operaciones unitarias aplicadas durante la generación de claves, permitiendo a un adversario con acceso al circuito cuántico que implementa el intercambio de claves reconstruir retroactivamente las claves de sesión invirtiendo temporalmente el cálculo.",
    "C": "La reutilización de claves efímeras se vuelve detectable mediante algoritmos cuánticos de búsqueda de período aplicados a la estructura de retículo subyacente a esquemas post-cuánticos como Kyber o Dilithium. Cuando los errores correlacionados afectan el proceso de muestreo polinomial usado para generar claves efímeras, introducen una periodicidad débil en el espacio de claves que algoritmos tipo Shor pueden explotar para factorizar el módulo efectivo de la función de generación de claves. Un adversario que observa múltiples sesiones puede procesar en lote los textos cifrados capturados usando transformadas de Fourier cuánticas para extraer el período oculto, luego reconstruir claves de sesiones anteriores mediante reducción de base de retículo incluso si esas sesiones parecían usar aleatoriedad fresca de un generador cuántico de números aleatorios.",
    "D": "Ocurre un error de vinculación de identidad durante el establecimiento de sesión multi-parte cuando el protocolo no logra vincular criptográficamente las identidades de los participantes a las claves efímeras en el protocolo inicial, permitiendo la sustitución de intermediario. Las soluciones clásicas como Diffie-Hellman firmado se extienden naturalmente a entornos post-cuánticos, pero las firmas basadas en retículos requieren una integración cuidadosa para evitar crear nuevos canales de temporización en la fase de confirmación de clave. Cuando errores cuánticos correlacionados afectan el proceso de generación de firma, pueden introducir patrones detectables en el compromiso de vinculación que un adversario explota para sustituir identidades durante el protocolo sin ser detectado.",
    "solution": "D"
  },
  {
    "id": 433,
    "question": "¿Por qué fijar el qubit de control de una puerta CNOT a |1⟩ produce una operación Pauli X?",
    "A": "Porque la teletransportación del estado de control introduce el eigenoperador X, que actúa como una transformación de base sobre el subespacio objetivo y genera la dinámica de inversión mediante un protocolo coherente de medición-retroalimentación. Específicamente, cuando el qubit de control se prepara en el eigenestado |1⟩ de X, el unitario condicional de la CNOT se reduce a un canal maximalmente mezclado sobre el objetivo que, tras la traza parcial sobre el control, induce la transformación Pauli X como su operación efectiva de un solo qubit. Este mecanismo depende de que el qubit de control funcione como ancilla que media la propagación de información de fase a través del entrelazamiento controlado.",
    "B": "El qubit objetivo actúa como un espejo para registrar información de paridad del espacio de producto tensorial control-objetivo, de modo que fijar el control a |1⟩ establece una restricción de paridad persistente que fuerza al objetivo a evolucionar bajo una permutación impar de la base computacional. Este efecto de espejo surge porque la tabla de verdad de CNOT implementa una operación XOR reversible, y cuando una entrada se fija al 1 lógico, el dispositivo funciona efectivamente como un reflector inversor. El estado de control fijo así programa la estructura de la puerta para producir el complemento de cualquier estado que entre al canal objetivo, que es precisamente la acción definitoria de Pauli X.",
    "C": "CNOT implementa NOT controlado, lo que significa que el objetivo se invierte si y solo si el control es |1⟩. Cuando el control se fija a |1⟩, la condición de inversión siempre se satisface, por lo que la puerta aplica determinísticamente X al objetivo independientemente de su estado de entrada. Esto se sigue directamente de la tabla de verdad de CNOT donde control=1 produce comportamiento XOR sobre el qubit objetivo.",
    "D": "La propagación de retroceso de fase ocurre cuando el estado de base computacional del qubit de control modula la fase relativa entre amplitudes objetivo a través del operador entrelazador de CNOT, creando patrones de interferencia que efectivamente rotan el vector de Bloch del objetivo π radianes alrededor del eje X, lo cual es matemáticamente equivalente a aplicar la puerta Pauli X.",
    "solution": "C"
  },
  {
    "id": 434,
    "question": "¿Cuál es la limitación principal de la simulación clásica directa de algoritmos de aprendizaje automático cuántico?",
    "A": "La memoria escala exponencialmente con el número de qubits, requiriendo 2^n amplitudes complejas para un estado de n qubits. Aunque los métodos de red tensorial pueden comprimir ciertos estados, la entropía de entrelazamiento típica en circuitos de entrenamiento de QML crece linealmente con la profundidad, forzando que las dimensiones de enlace escalen exponencialmente y eliminando las ventajas de compresión más allá de ~40 qubits incluso con representaciones de estado de producto matricial.",
    "B": "La memoria escala exponencialmente con el número de qubits, requiriendo 2^n amplitudes complejas para representar un estado de n qubits, lo cual rápidamente excede los recursos computacionales disponibles incluso para tamaños modestos de sistema alrededor de 50 qubits.",
    "C": "La memoria escala exponencialmente con el número de parámetros en circuitos variacionales, requiriendo almacenamiento de 2^p componentes de gradiente para p parámetros. Aunque cada estado de n qubits necesita solo 2^n amplitudes, la retropropagación a través de capas cuánticas demanda mantener valores de activación intermedios de O(2^n × p), y las arquitecturas modernas de QML con p > n crean cuellos de botella de memoria que dominan los requisitos de almacenamiento de estado.",
    "D": "Los requisitos de memoria escalan como 2^n, pero el cuello de botella dominante es el costo de operación de puertas: calcular unitarios de dos qubits requiere O(2^(2n)) operaciones por puerta debido a la expansión del producto de Kronecker sobre todo el espacio de Hilbert. Como los circuitos QML contienen O(poly(n)) puertas, el tiempo de ejecución total en lugar de la memoria se convierte en el factor limitante para la simulación clásica de sistemas que exceden 30 qubits.",
    "solution": "B"
  },
  {
    "id": 435,
    "question": "¿Cuál es el propósito de la técnica de descomposición diagonal en la síntesis de circuitos cuánticos?",
    "A": "La descomposición diagonal permite la verificación de la corrección unitaria al explotar el hecho de que las matrices diagonales en la base computacional tienen autovalores iguales a sus entradas diagonales, que pueden extraerse eficientemente mediante una sola capa de puertas Hadamard seguidas de mediciones en base computacional. Al insertar periódicamente estos pasos de verificación durante la síntesis, el algoritmo puede detectar desviaciones del unitario objetivo mediante comparación de autovalores, asegurando que los errores numéricos acumulados por aritmética de punto flotante o aproximaciones de compilación de puertas permanezcan bajo umbrales de tolerancia especificados a lo largo del proceso de descomposición, particularmente al sintetizar unitarios de alta precisión para implementaciones tolerantes a fallos.",
    "B": "Las matrices diagonales corresponden a unitarios que solo aplican desplazamientos de fase sin cambiar las poblaciones de estados de base computacional, haciéndolos implementables usando solo rotaciones Z de un qubit y puertas de fase controlada sin sobrecarga de SWAP. Al descomponer un unitario arbitrario en un producto de matrices diagonales y unitarios estructurados simples (como rotaciones de Givens o matrices de permutación), el algoritmo de síntesis puede implementar los componentes diagonales eficientemente con circuitos superficiales de puertas de fase con T-count óptimo. Esta descomposición reduce el número total de puertas porque las operaciones diagonales requieren menos recursos que puertas generales de dos qubits.",
    "C": "La técnica de descomposición diagonal explota el teorema de Descomposición Coseno-Seno (CSD), que expresa cualquier unitario de n qubits como un producto de rotaciones multiplexadas de un solo qubit intercaladas con puertas uniformemente controladas que actúan sobre subconjuntos disjuntos de qubits. Al factorizar recursivamente el unitario en forma bloque-diagonal donde cada bloque corresponde a una configuración fija de qubits de control, el algoritmo de síntesis reduce unitarios generales a una secuencia de rotaciones condicionales. Esta estructura de bloques diagonales es particularmente ventajosa porque los circuitos resultantes se mapean naturalmente a topologías de vecino más cercano lineal sin requerir puertas SWAP adicionales, ya que cada rotación multiplexada actúa sobre un subconjunto de qubits geométricamente localizado alineado con la conectividad del hardware.",
    "D": "La descomposición diagonal separa la matriz de densidad objetivo ρ en su componente diagonal D (representando poblaciones clásicas) y coherencias fuera de diagonal C, explotando el hecho de que D y C pueden prepararse independientemente mediante primitivos de circuito diferentes. Los elementos diagonales especifican las probabilidades requeridas de estados base y pueden prepararse eficientemente usando circuitos de codificación de amplitud con profundidad logarítmica en la dimensión del estado. Al implementar primero el componente diagonal mediante rotaciones controladas basadas en estructuras de árbol binario, luego generar separadamente las coherencias necesarias mediante técnicas de retroceso de fase aplicadas a qubits auxiliares, el método minimiza la profundidad de entrelazamiento necesaria para la preparación de estado mixto en comparación con enfoques directos de isomorfismo de Choi-Jamio³kowski.",
    "solution": "B"
  },
  {
    "id": 436,
    "question": "¿En qué se diferencia el concepto de tasa de error lógica de la tasa de error física en la corrección de errores cuánticos?",
    "A": "Las tasas de error lógicas miden la probabilidad de fallo de la información cuántica codificada después de aplicar el ciclo completo de QEC, incluyendo la extracción de síndrome, decodificación clásica e interpretación de errores, representando el ruido efectivo experimentado por el qubit lógico protegido. Las tasas de error físicas parametrizan las probabilidades elementales de fallo (errores de puerta, errores de medición, decoherencia en reposo) de los componentes individuales del hardware antes de cualquier corrección de errores, con tasas físicas típicas de 10⁻³ siendo suprimidas a tasas lógicas por debajo de 10⁻⁶ mediante mediciones de síndrome repetidas y decodificación.",
    "B": "Las tasas de error lógicas miden la probabilidad residual de fallo de la información cuántica codificada después de que se ha aplicado la corrección de errores, representando con qué frecuencia el protocolo QEC falla en proteger el qubit lógico. Las tasas de error físicas cuantifican las probabilidades brutas de fallo por puerta, por medición o por paso temporal de los componentes individuales del hardware antes de aplicar cualquier corrección de errores: estos son los parámetros fundamentales de ruido del sustrato físico.",
    "C": "Las tasas de error lógicas caracterizan la probabilidad neta de error del estado del qubit protegido después de que la decodificación del síndrome haya identificado y corregido errores detectables dentro del espacio del código, pero antes de aplicar retroalimentación activa: cuantifican la precisión de inferencia del decodificador en lugar de la fidelidad última de la información codificada. Las tasas de error físicas miden el ruido no corregido del hardware, incluyendo tanto canales de Pauli estocásticos como errores de control coherentes, estableciendo el teorema del umbral que las tasas lógicas escalan como (p_phys/p_th)^((d+1)/2) para distancia de código d, donde p_th es el umbral más allá del cual la codificación no proporciona ventaja.",
    "D": "Las tasas de error lógicas representan la amplitud residual de error coherente, principalmente ángulos de sobre-rotación y errores sistemáticos de calibración de control, que se propagan a través del formalismo estabilizador sin activar banderas de síndrome, evadiendo así la detección por el protocolo de corrección de errores. Las tasas de error físicas cuantifican solo los procesos de ruido estocástico incoherente (canales despolarizantes, amortiguamiento de amplitud) que afectan a qubits individuales antes de la codificación. La distinción es operacionalmente crítica porque los errores lógicos requieren aprendizaje hamiltoniano y control óptimo para suprimirse, mientras que los errores físicos se mitigan mediante códigos QEC estándar con suficiente distancia de código.",
    "solution": "B"
  },
  {
    "id": 437,
    "question": "¿Cómo se relaciona el problema de intercambio de tokens con la planificación de SWAP cuántico?",
    "A": "El marco de intercambio de tokens modela las permutaciones de qubits lógicos como reetiquetado de vértices en el grafo de conectividad, pero críticamente asume que cada operación SWAP actúa simétricamente sobre ambos qubits: esto funciona perfectamente para puertas iSWAP y √SWAP donde la matriz unitaria es simétrica, pero falla en arquitecturas heterogéneas donde la fidelidad de SWAP depende de qué qubit físico inicia la secuencia de puertas.",
    "B": "La abstracción de intercambio de tokens proporciona un marco combinatorio donde minimizar el número de intercambios de aristas necesarios para reorganizar tokens en los vértices del grafo corresponde directamente a minimizar el recuento de puertas SWAP para alinear qubits lógicos sobre acopladores físicos en el problema de compilación de circuitos cuánticos.",
    "C": "La optimización de intercambio de tokens produce la secuencia mínima de intercambio de aristas bajo el supuesto de que todas las aristas del grafo tienen costo uniforme, lo cual modela correctamente arquitecturas superconductoras donde las puertas CNOT e iSWAP tienen fidelidades comparables, pero falla en sistemas de trampa de iones donde la fidelidad de puerta varía con la distancia inter-iónica: la solución de tokens minimiza el recuento de intercambios pero puede seleccionar acopladores de largo alcance de baja fidelidad sobre trayectorias más cortas de alta fidelidad.",
    "D": "El modelo de tokens mapea el enrutamiento de qubits lógicos a físicos en un problema de automorfismo de grafos donde la secuencia mínima de intercambios corresponde a la trayectoria más corta del grupo de permutaciones entre configuraciones inicial y objetivo; sin embargo, esta formulación clásica ignora la conmutatividad de puertas: los circuitos cuánticos a menudo permiten que puertas conmutativas se ejecuten simultáneamente, habilitando que las operaciones SWAP se paralelicen sobre aristas disjuntas, mientras que el modelo de tokens serializa estrictamente todos los intercambios.",
    "solution": "B"
  },
  {
    "id": 438,
    "question": "Considere un escenario de computación cuántica distribuida donde desea ejecutar la Transformada Cuántica de Fourier a través de múltiples procesadores cuánticos más pequeños conectados por canales de comunicación clásica. ¿Por qué la QFT presenta dificultades fundamentales en este entorno distribuido, más allá del desafío técnico de mantener la coherencia?",
    "A": "El algoritmo carece fundamentalmente de mecanismos de corrección de errores porque la estructura matemática de la QFT, específicamente su dependencia de rotaciones de fase precisas con ángulos irracionales, no puede codificarse en códigos estabilizadores ni protegerse mediante arquitecturas estándar de código de superficie. La implementación tolerante a fallos requeriría destilación de estados mágicos para cada puerta de fase controlada.",
    "B": "Las mediciones frecuentes en medio del circuito son inherentes a la estructura de la QFT, creando decoherencia inducida por medición que se propaga catastróficamente cuando la ejecución abarca múltiples procesadores. Cada rotación controlada en la base QFT mide implícitamente información de fase relativa entre pares de qubits, y distribuir estas mediciones entre procesadores rompe el marco de referencia de fase global.",
    "C": "La QFT es esencialmente monolítica en su estructura computacional porque opera sobre un número de qubits que excede lo que los procesadores pequeños típicos pueden acomodar, requiriendo estrategias de particionamiento que introducen sobrecarga significativa. La profundidad del circuito del algoritmo crece super-linealmente cuando se distribuye, ya que la comunicación entre procesadores domina la línea temporal de ejecución incluso cuando la distribución de entrelazamiento tiene éxito.",
    "D": "La QFT requiere conectividad total entre qubits mediante puertas de fase controladas, y distribuir estas operaciones requeriría exponencialmente muchos pasos de teletransportación para transportar información cuántica entre procesadores. Cada teletransportación consume pares entrelazados e introduce tanto latencia como fuentes adicionales de error que escalan desfavorablemente con el tamaño del sistema. Las rondas de comunicación clásica necesarias para la transmisión de resultados de medición y correcciones feed-forward crean cuellos de botella que destruyen la estructura paralela, mientras que la pérdida acumulativa de fidelidad de protocolos repetidos de teletransportación se compone multiplicativamente a través de la profundidad del circuito.",
    "solution": "D"
  },
  {
    "id": 439,
    "question": "¿Qué ventaja clave ofrece la naturaleza reversible de las puertas cuánticas sobre las puertas clásicas irreversibles?",
    "A": "Las puertas cuánticas reversibles eliminan completamente la producción de entropía termodinámica porque las transformaciones unitarias preservan la entropía de von Neumann, satisfaciendo el principio de Landauer en su formulación cuántica donde la preservación de información implica disipación de calor cero. Dado que las puertas cuánticas implementan transformaciones biyectivas sin borrar grados de libertad, alcanzan el límite termodinámico fundamental de computación, evitando el costo energético kT ln(2) por borrado de bit que la lógica clásica irreversible debe pagar, permitiendo que circuitos cuánticos arbitrariamente complejos operen sin sobrecarga termodinámica.",
    "B": "Preserva la información mediante mapeos biyectivos entrada-salida, lo cual es exigido por la evolución unitaria en mecánica cuántica. Esta preservación de información habilita efectos de interferencia cuántica esenciales para algoritmos cuánticos y asegura que los estados cuánticos puedan manipularse coherentemente sin el aumento de entropía que acompaña a las operaciones de puertas clásicas irreversibles, manteniendo la reversibilidad computacional a lo largo de la ejecución del circuito.",
    "C": "Las puertas cuánticas reversibles mantienen la coherencia de fase a través de pasos computacionales mediante su estructura unitaria, lo que previene mecanismos de decoherencia que surgen en puertas clásicas irreversibles por pérdida acumulada de información. Al preservar el vector de estado cuántico completo incluyendo fases relativas, las puertas reversibles permiten patrones de interferencia controlados que la computación clásica no puede lograr, ya que las puertas irreversibles destruyen relaciones de fase mediante sus mapeos muchos-a-uno que colapsan el espacio de estado computacional durante la ejecución.",
    "D": "La restricción de reversibilidad asegura que las puertas cuánticas preserven la pureza de los estados cuánticos manteniendo trace(ρ²) = 1 a lo largo de la ejecución del circuito, mientras que las puertas clásicas irreversibles introducen mezcla que aumenta la entropía de von Neumann. Esta preservación de pureza permite que los códigos de corrección de errores cuánticos funcionen, ya que las operaciones reversibles mantienen los errores dentro de subespacios corregibles, mientras que las puertas clásicas irreversibles causarían fuga de información hacia reservorios de entropía irrecuperables, impidiendo fundamentalmente arquitecturas de computación clásica tolerantes a fallos.",
    "solution": "B"
  },
  {
    "id": 440,
    "question": "En el algoritmo de Shor, ¿qué sucede si la base elegida a comparte factores con N?",
    "A": "La verificación clásica preliminar revela inmediatamente el factor compartido mediante el cálculo de gcd(a,N), que devuelve un divisor no trivial de N sin requerir ninguna computación cuántica. Este descubrimiento fortuito ocurre antes de que la costosa subrutina cuántica de búsqueda de período siquiera se inicialice, proporcionando un atajo que resuelve directamente el problema de factorización identificando al menos un factor primo del número objetivo mediante operaciones básicas del algoritmo euclidiano en hardware clásico únicamente.",
    "B": "La subrutina cuántica de búsqueda de período se ejecuta normalmente y devuelve un período significativo r, pero este período corresponde al orden multiplicativo de a en el grupo cociente Z*_{N/gcd(a,N)} en lugar de Z*_N, causando que el paso de postprocesamiento clásico subsiguiente que calcula gcd(a^{r/2}±1, N) falle sistemáticamente ya que el período extraído satisface restricciones de divisibilidad diferentes. La transformada cuántica de Fourier mide exitosamente una estructura periódica, pero la periodicidad refleja el sistema de módulo reducido donde los factores comunes han sido implícitamente factorizados, produciendo un período matemáticamente válido pero criptográficamente inútil que no puede aprovecharse para factorizar el N original mediante el procedimiento estándar de extracción de fracciones continuas.",
    "C": "Cuando gcd(a,N) > 1, la función de exponenciación modular exhibe pseudo-periodicidad donde ciclos aparentes emergen de la proyección de estructura de grupo de dimensión superior sobre la base computacional accesible, pero estos ciclos carecen de las propiedades algebraicas requeridas para factorización. El estado cuántico después de la búsqueda de período muestra picos de medición fuertes en intervalos específicos, pero estos picos corresponden a resonancias en el grafo de Cayley del subgrupo multiplicativo no cíclico en lugar de periodicidad verdadera, causando que el algoritmo de fracciones continuas extraiga divisores espurios que son siempre 1 o N, nunca revelando factores no triviales a pesar de que el circuito cuántico se ejecuta sin errores detectables.",
    "D": "El oráculo que implementa f(x) = a^x mod N devuelve cero para todas las entradas cuando gcd(a,N) divide a^x para cada x en la superposición, causando interferencia destructiva total a través de todos los estados de la base computacional y colapsando el registro cuántico a un vector de estado indefinido fuera del espacio de Hilbert válido. Esta condición patológica activa el sistema de detección de errores del hardware cuántico, que reconoce la distribución anómala de amplitud toda-cero y detiene la ejecución con una bandera de diagnóstico, previniendo desperdicio de recursos cuánticos en casos degenerados aunque requiere preprocesamiento clásico para identificar tales situaciones por adelantado mediante exponenciación de prueba de bases candidatas.",
    "solution": "A"
  },
  {
    "id": 441,
    "question": "En los protocolos de transmisión anónima cuántica, los adversarios pueden explotar correlaciones entre los tiempos de llegada de las señales cuánticas en diferentes nodos de la red, incluso cuando los estados cuánticos en sí mismos son perfectamente seguros. Este canal lateral temporal se vuelve particularmente problemático en implementaciones prácticas donde la latencia de red varía. ¿Cuál es la vulnerabilidad principal que esto crea?",
    "A": "Durante las operaciones de intercambio de entrelazamiento que establecen canales cuánticos anónimos entre nodos distantes, el patrón específico de qué pares entrelazados se intercambian y en qué secuencia crea una firma única que se correlaciona con la identidad del emisor. Un adversario que monitorea la distribución del entrelazamiento puede rastrear cómo las mediciones de estados de Bell se propagan a través del grafo de entrelazamiento de la red, y al analizar la evolución temporal de la conectividad del entrelazamiento, reconstruir el nodo fuente más probable mediante inferencia bayesiana sobre la topología de intercambio, ya que diferentes emisores típicamente generan patrones de intercambio distinguibles basados en su posición en la red",
    "B": "Cuando múltiples nodos de mezcla cuántica coludidos comparan las marcas temporales y los metadatos de enrutamiento de los paquetes cuánticos que pasan por sus respectivas posiciones en la cadena de anonimización, pueden reconstruir mapeos parciales emisor-receptor incluso sin acceder a información del estado cuántico. Esta colusión se vuelve particularmente efectiva cuando los nodos adversarios controlan posiciones consecutivas en la red de mezcla, ya que las correlaciones temporales entre paquetes de entrada y salida en nodos adyacentes reducen dramáticamente el conjunto de anonimato mediante ataques de intersección sobre el espacio de permutaciones",
    "C": "Los generadores de números pseudoaleatorios de selección de rutas exhiben sesgos explotables para el análisis de determinación de rutas",
    "D": "El adversario puede usar análisis estadístico de patrones temporales para inferir qué nodos se están comunicando, rompiendo así el anonimato sin nunca medir los estados cuánticos ni romper ninguna suposición criptográfica. Esto funciona porque las señales cuánticas aún se propagan a través de canales físicos con retardos medibles que se correlacionan con los pares emisor-receptor y la topología de la red.",
    "solution": "D"
  },
  {
    "id": 442,
    "question": "¿Por qué la corrección de errores cuánticos (QEC) debe detectar y corregir errores sin medir directamente los qubits?",
    "A": "Medir qubits aumenta su tiempo de coherencia al forzar el sistema a un autoestado de energía definido, lo que estabiliza la función de onda y crea una barrera protectora contra la decoherencia ambiental. Este efecto de estabilización inducido por medición ha sido verificado experimentalmente para extender los tiempos T1 y T2 hasta en un 40% en arquitecturas de transmon superconductoras, haciendo de las operaciones de medición repetidas una piedra angular de las estrategias modernas de supresión de errores.",
    "B": "La información de fase se borra por las operaciones de medición, pero las poblaciones de los estados de la base computacional permanecen perfectamente intactas y recuperables, lo que significa que cualquier algoritmo cuántico puede tolerar mediciones frecuentes siempre que opere exclusivamente en la base Z.",
    "C": "La medición directa colapsa la función de onda cuántica, proyectando irreversiblemente los estados de superposición sobre estados de la base computacional definidos y destruyendo la información cuántica codificada. QEC evita esto midiendo información de síndrome a través de qubits ancilla, extrayendo firmas de error sin aprender nada sobre el estado lógico en sí, preservando así la superposición que lleva el cómputo.",
    "D": "Los estados cuánticos son fundamentalmente demasiado frágiles para ser medidos directamente bajo ninguna circunstancia, incluso mediante extracción de síndrome indirecta o técnicas mediadas por ancilla, por lo que la corrección de errores cuánticos debe depender exclusivamente de patrones de interferencia cuántica cuidadosamente diseñados y secuencias de desacoplamiento dinámico para detectar y corregir errores sin ninguna forma de medición. Este paradigma libre de medición opera dirigiendo los errores hacia trayectorias de interferencia destructiva mediante secuencias de compuertas temporizadas con precisión, cancelando efectivamente los errores únicamente a través de control coherente.",
    "solution": "C"
  },
  {
    "id": 443,
    "question": "Considere un clasificador cuántico que logra buenos resultados en un conjunto de datos simple y linealmente separable con solo 4 características y 100 ejemplos de entrenamiento. El modelo usa un ansatz básico con entrelazamiento mínimo y converge rápidamente durante el entrenamiento. Sin embargo, está intentando evaluar si este éxito indica una ventaja cuántica genuina o si un modelo clásico podría lograr un rendimiento similar o mejor. ¿Qué se debe hacer a continuación para evaluar su potencial ventaja cuántica?",
    "A": "Probar la misma arquitectura del modelo en datos significativamente más complejos o de alta dimensión donde los métodos clásicos tienen dificultades, como conjuntos de datos con correlaciones de características intrincadas, límites de decisión no lineales o espacios de características exponencialmente grandes que podrían aprovechar el espacio de estados cuánticos más efectivamente, revelando así si el enfoque cuántico proporciona beneficios computacionales más allá de lo que los algoritmos clásicos simples pueden lograr",
    "B": "Probar el clasificador en conjuntos de datos cuya dimensión de características escala exponencialmente con el tamaño del problema, como espacios de configuración de química cuántica o tareas de descomposición tensorial de alto orden donde la dimensionalidad del espacio de estados cuánticos 2^n coincide naturalmente con la estructura del problema, evaluando así si el modelo explota un paralelismo cuántico genuino en la codificación de características en lugar de simplemente reimplementar métodos de kernel clásicos a través de circuitos variacionales que podrían ser eficientemente simulados",
    "C": "Comparar las tasas de convergencia del entrenamiento contra redes neuronales clásicas con recuentos de parámetros equivalentes en conjuntos de datos progresivamente de mayor dimensión donde los fenómenos de mesetas áridas dominarían la optimización basada en gradientes, ya que la ventaja cuántica se manifiesta específicamente cuando el descenso de gradiente clásico falla mientras que el gradiente natural cuántico o las reglas de desplazamiento de parámetros permiten un entrenamiento eficiente a través de la estructura geométrica de las variedades de estados cuánticos a pesar de la maldición de la dimensionalidad que afecta tanto a los enfoques clásicos como cuánticos",
    "D": "Reducir sistemáticamente la profundidad del ansatz mientras se monitorea la degradación de la precisión de clasificación, ya que la ventaja cuántica genuina requiere demostrar que los circuitos cuánticos superficiales con recuento de compuertas polinomial superan las redes clásicas profundas, probando que la interferencia cuántica permite la aproximación eficiente de límites de decisión complejos sin la sobrecarga de profundidad que las arquitecturas clásicas requieren para lograr una expresividad comparable a través de la composición jerárquica de funciones de activación no lineales en múltiples capas",
    "solution": "A"
  },
  {
    "id": 444,
    "question": "¿Qué propiedad del QKD de variables continuas con estados comprimidos lo hace más resistente a ataques de canal lateral dependientes de la longitud de onda?",
    "A": "La modulación gaussiana aleatoriza el número de fotones a través de los marcos de señal, lo que distribuye el contenido de información uniformemente y evita que el sondeo específico de longitud de onda extraiga correlaciones significativas con los datos codificados.",
    "B": "La simetría del espacio de fases elimina la necesidad de seguimiento activo de polarización al garantizar que las codificaciones de cuadratura permanezcan invariantes bajo rotaciones dependientes de longitud de onda de la esfera de Poincaré.",
    "C": "La detección homodina filtra intrínsecamente los tonos ópticos parásitos fuera de banda. El oscilador local actúa como un filtro espectral estrecho centrado en la longitud de onda de la señal, de modo que cualquier fotón de canal lateral introducido a diferentes longitudes de onda no interferirá con el haz de referencia y por lo tanto permanecerá sin medir en las estadísticas de cuadratura. Esta selectividad de longitud de onda incorporada significa que los ataques que explotan la dispersión cromática o el sondeo multiplexado por longitud de onda son automáticamente rechazados por el propio aparato de medición.",
    "D": "La estructura del protocolo de reconciliación inversa descorrelaciona inherentemente las dependencias de longitud de onda al realizar la corrección de errores en el lado de Alice después de que Bob anuncie su información de síndrome, lo que significa que cualquier manipulación específica de longitud de onda durante la transmisión se desordena.",
    "solution": "C"
  },
  {
    "id": 445,
    "question": "¿Cuál es la diferencia principal en las implementaciones del algoritmo de Shor para factorización versus logaritmo discreto?",
    "A": "En el algoritmo de factorización de Shor, la transformada de Fourier cuántica debe aplicarse a un registro cuyo tamaño escala como 2n qubits donde n es la longitud en bits del número a factorizar, porque la subrutina de búsqueda de periodo requiere muestrear la función sobre un dominio lo suficientemente grande como para capturar al menos un periodo completo con alta probabilidad.",
    "B": "Aunque ambas variantes del algoritmo de Shor dependen de la búsqueda de periodo cuántica seguida de post-procesamiento clásico, el número de disparos de medición requeridos difiere aproximadamente por un factor de log(N) donde N es el tamaño del espacio de búsqueda. La factorización requiere medir el registro de salida solo una vez por ejecución ya que el algoritmo de fracciones continuas puede extraer periodos candidatos de un único valor muestreado con alta probabilidad de éxito.",
    "C": "El oráculo implementa una operación de grupo diferente: la factorización emplea exponenciación modular con respecto a una base elegida aleatoriamente dentro del grupo multiplicativo de enteros módulo N, mientras que el logaritmo discreto realiza exponenciación modular con bases restringidas por el generador conocido y el exponente desconocido que se busca, requiriendo arquitecturas de circuitos de multiplicación controlada distintas a pesar de utilizar subrutinas idénticas de transformada de Fourier cuántica.",
    "D": "Las porciones cuánticas de los algoritmos de factorización y logaritmo discreto de Shor son estructuralmente idénticas: ambos realizan exponenciación modular mediante operaciones repetidas de multiplicación controlada y aplican la transformada de Fourier cuántica para extraer información del periodo. Sin embargo, el post-procesamiento clásico diverge sustancialmente en enfoque computacional y complejidad.",
    "solution": "C"
  },
  {
    "id": 446,
    "question": "¿Qué metodología de ataque avanzada puede comprometer la seguridad de las firmas digitales cuánticas?",
    "A": "La falsificación mediante exploits de medición selectiva aprovecha la dependencia de los esquemas de firma cuántica en selecciones de base aleatorias anunciadas después de la distribución de estados, permitiendo a los adversarios con memoria cuántica almacenar los estados de firma recibidos, esperar el anuncio de la base y luego realizar mediciones exclusivamente en bases complementarias sobre subconjuntos cuidadosamente elegidos de qubits de firma para obtener información clásica parcial que puede recombinarse a través de múltiples rondas de firma para reconstruir suficiente estructura de clave privada como para generar firmas válidas para mensajes no autorizados",
    "B": "La intercepción y reenvío durante la distribución aprovecha el teorema de no clonación a la inversa: un atacante intercepta estados de firma, realiza clonación óptima con fidelidad cercana a 5/6 para qubits, reenvía copias imperfectas a los destinatarios mientras retiene los originales para análisis, y aunque la fidelidad individual del clon está degradada, la agregación estadística sobre muchas instancias de firma permite extraer suficiente información sobre la distribución de base de firma para predecir patrones futuros",
    "C": "Los ataques de fidelidad mediante swap test se dirigen al protocolo de verificación donde los destinatarios comparan estados cuánticos recibidos contra copias de referencia mantenidas por otras partes; al explotar fidelidades de puerta finitas en operaciones controlled-SWAP, los adversarios fabrican estados cuánticos que parecen coincidir con firmas legítimas bajo swap tests ruidosos mientras codifican mensajes clásicos diferentes",
    "D": "La tomografía de estados sobre la clave pública permite la reconstrucción completa de parámetros de estado cuántico mediante mediciones sistemáticas en múltiples bases, permitiendo a los adversarios extraer descripciones clásicas completas de estados de firma públicos.",
    "solution": "D"
  },
  {
    "id": 447,
    "question": "En el contexto de las máquinas de vectores de soporte cuánticas, la alineación de kernel se refiere a optimizar qué característica durante el ajuste de hiperparámetros?",
    "A": "Qué tan bien el kernel cuántico se correlaciona con el kernel objetivo ideal",
    "B": "Qué tan bien el espectro de autovalores de la matriz de Gram del kernel cuántico coincide con la estructura de covarianza del conjunto de datos",
    "C": "Qué tan bien la métrica de distancia del kernel cuántico se alinea con la geometría del espacio de características clásico",
    "D": "Qué tan bien la dimensión de embedding del kernel cuántico coincide con la curvatura intrínseca de la variedad del problema",
    "solution": "A"
  },
  {
    "id": 448,
    "question": "Considere un compilador cuántico que necesita ejecutar un circuito en hardware con topología de qubit restringida—por ejemplo, un chip donde el qubit 0 se conecta solo al qubit 1, el qubit 1 a los qubits 0 y 2, y así sucesivamente en una cadena lineal. Su circuito requiere un CNOT entre los qubits 0 y 3. ¿Cuál es el propósito del compilador BRIDGE en el diseño de circuitos cuánticos?",
    "A": "BRIDGE es un paso de transpilación que sintetiza puertas de dos qubits de largo alcance insertando secuencias de operaciones SWAP o explotando descomposiciones alternativas de puertas que respetan el grafo de conectividad nativa. Esencialmente construye un 'puente' de operaciones a través de qubits intermedios para implementar puertas entre qubits no adyacentes, lo cual es crítico cuando el circuito lógico asume conectividad total pero el hardware no la proporciona.",
    "B": "BRIDGE es un paso de transpilación que sintetiza puertas de dos qubits de largo alcance descomponiéndolas en secuencias de puertas nativas que respetan el grafo de conectividad del hardware, usando qubits ancilla en posiciones intermedias como recursos temporales de enrutamiento. Construye protocolos 'puente' basados en medición donde el estado se teleporta a través de qubits no adyacentes mediante distribución de entrelazamiento—este enfoque es crítico cuando el circuito lógico asume conectividad total pero la topología física restringe interacciones directas, aunque requiere sobrecarga adicional de ancilla y latencia de feedforward clásico.",
    "C": "BRIDGE es un paso de transpilación que sintetiza puertas de dos qubits de largo alcance insertando secuencias conmutativas de rotaciones de un solo qubit y CNOTs de vecinos más cercanos que respetan el grafo de conectividad nativa, explotando el hecho de que cualquier unitaria de dos qubits puede descomponerse en como máximo tres puertas CNOT más rotaciones locales. Construye un 'puente' reescribiendo la puerta objetivo en una descomposición de Cartan donde los componentes no locales se factorizan—esto es esencial cuando el circuito lógico asume conectividad total, aunque los circuitos resultantes tienen sobrecarga de profundidad que escala logarítmicamente con la distancia entre qubits.",
    "D": "BRIDGE es un paso de transpilación que sintetiza puertas de dos qubits de largo alcance programándolas como operaciones multiplexadas en tiempo sobre hardware de acoplamiento compartido que se reconfigura dinámicamente entre ciclos de reloj—los chips superconductores modernos usan acopladores ajustables cuyos hamiltonianos de interacción pueden modularse adiabáticamente para crear puertas de entrelazamiento efectivas multi-salto. Construye un 'puente' coordinando la secuencia de activación de acopladores intermedios para propagar información cuántica a través de qubits no adyacentes sin sobrecarga física de SWAP, lo cual es crítico cuando los circuitos lógicos asumen conectividad total pero la topología de acoplamiento fija limita la implementación directa de puertas.",
    "solution": "A"
  },
  {
    "id": 449,
    "question": "¿Qué valor matemático busca encontrar el algoritmo de Shor como parte del proceso de factorización?",
    "A": "El periodo de la función de exponenciación modular f(x) = a^x mod N, que codifica el orden multiplicativo y conduce a la extracción de factores mediante cálculos de máximo común divisor.",
    "B": "El logaritmo discreto de un elemento generador en el grupo multiplicativo Z*_N, cuyo cómputo mediante transformada de Fourier cuántica revela la estructura del grupo necesaria para extraer factores a través de reducción de Pohlig-Hellman.",
    "C": "La expansión en fracciones continuas del cociente k/r donde k se mide del registro de salida de la QFT y r es el periodo desconocido, cuyos convergentes aproximan r con probabilidad superior a 1/2.",
    "D": "El autovalor más pequeño del operador de permutación cíclica U definido por U|x⟩ = |ax mod N⟩, cuya fase codifica el orden r mediante la relación λ = e^(2πis/r) para algún entero s coprimo con r.",
    "solution": "A"
  },
  {
    "id": 450,
    "question": "¿Cómo se integran los componentes clásicos y cuánticos durante el entrenamiento de una HQNN?",
    "A": "El entrenamiento usa descenso de coordenadas por bloques alternantes: los parámetros cuánticos se optimizan para pesos clásicos fijos usando estimaciones de gradiente parameter-shift computadas en hardware cuántico, luego los pesos clásicos se actualizan para parámetros cuánticos fijos usando retropropagación estándar en hardware clásico. Esta alternancia continúa hasta la convergencia. La pérdida híbrida combina valores de expectación cuánticos con salidas de capa clásica, pero el flujo de gradiente está particionado en lugar de ser simultáneo, evitando la necesidad de diferenciación automática conjunta a través de la frontera cuántico-clásica.",
    "B": "Entrenamiento conjunto end-to-end donde los gradientes fluyen simultáneamente a través de capas cuánticas y clásicas mediante la regla parameter-shift para puertas cuánticas y retropropagación estándar para pesos clásicos. La función de pérdida abarca toda la arquitectura híbrida, permitiendo que los parámetros del circuito cuántico y los pesos de la red neuronal clásica se co-optimicen usando métodos unificados basados en gradiente como Adam o SGD.",
    "C": "Los parámetros del circuito cuántico se entrenan usando aprendizaje por refuerzo con gradiente de política con la salida de la red clásica como señal de recompensa, mientras que los pesos clásicos se optimizan mediante retropropagación supervisada usando resultados de medición cuántica como características. Esta estrategia de entrenamiento asimétrica evita calcular gradientes cuánticos directamente, tratando en cambio la capa cuántica como una política estocástica que muestrea resultados de medición, y usando estimadores de gradiente estilo REINFORCE para actualizar parámetros de puerta basándose en qué tan bien las capas clásicas posteriores se desempeñan en la tarea.",
    "D": "Los parámetros de la capa cuántica se inicializan aleatoriamente y luego se refinan usando optimización bayesiana guiada por el rendimiento de la capa clásica, muestreando diferentes configuraciones de parámetros cuánticos y ajustando un modelo sustituto de proceso gaussiano al paisaje de pérdida de validación. Los pesos clásicos se entrenan normalmente mediante retropropagación en cada punto de prueba. Esta optimización híbrida evita calcular gradientes cuánticos por completo mientras permite el ajuste conjunto, con adquisición de mejora esperada guiando la búsqueda de parámetros cuánticos basándose en la superficie de pérdida diferenciable de la red clásica.",
    "solution": "B"
  },
  {
    "id": 451,
    "question": "¿Cuál es el papel del diseño del ansatz en los circuitos cuánticos variacionales?",
    "A": "El diseño del ansatz define la arquitectura parametrizada del circuito que representa soluciones candidatas al problema de optimización. La estructura del ansatz determina qué regiones del espacio de Hilbert pueden explorarse eficientemente durante el entrenamiento variacional, equilibrando la expresividad para capturar buenas soluciones contra la entrenabilidad para evitar mesetas estériles. Elegir un ansatz apropiado—ya sea eficiente en hardware, inspirado en el problema o motivado químicamente—impacta directamente la capacidad del algoritmo para converger a parámetros óptimos y aproximar el estado cuántico deseado.",
    "B": "El diseño del ansatz especifica la estructura parametrizada del circuito que restringe el flujo de gradiente durante la optimización. La arquitectura del ansatz determina qué paisajes de función de coste pueden navegarse eficientemente durante la retropropagación, equilibrando la profundidad para alcanzar suficiente expresividad contra la anchura para prevenir el desvanecimiento del gradiente. Elegir un ansatz apropiado—ya sea alternante por capas, con patrón de pared de ladrillos o que preserve simetrías—impacta directamente la capacidad del algoritmo para converger a mínimos locales y representar el autoespacio objetivo, aunque las mesetas estériles surgen del ruido de disparo en las mediciones más que de la topología del circuito.",
    "C": "El diseño del ansatz establece la secuencia fija de puertas que mapea parámetros clásicos a amplitudes cuánticas representando candidatos de solución. La topología del ansatz determina qué sectores de simetría del espacio de Hilbert pueden muestrearse eficientemente durante las actualizaciones de parámetros, equilibrando la profundidad del circuito para lograr expresividad contra la fidelidad de las puertas para mantener coherencia. Elegir un ansatz apropiado—ya sea agnóstico al problema, inspirado en el hamiltoniano o motivado por redes tensoriales—impacta directamente la convergencia a estados fundamentales, aunque la entrenabilidad depende principalmente de la elección del optimizador clásico más que de la estructura del circuito cuántico en sí.",
    "D": "El diseño del ansatz determina la familia unitaria parametrizada que codifica variables de optimización en circuitos de preparación de estados cuánticos. El marco del ansatz define qué variedades del espacio de Hilbert pueden alcanzarse eficientemente durante el refinamiento iterativo, equilibrando la expresividad para aproximar estados objetivo contra la profundidad del circuito para evitar decoherencia. Elegir un ansatz apropiado—ya sea rico en entrelazamiento, adaptado al problema o motivado adiabáticamente—impacta directamente el rendimiento algorítmico, aunque las mesetas estériles surgen fundamentalmente de gradientes exponencialmente pequeños en funciones de coste globales más que de la estructura local del circuito, haciendo que la elección del ansatz sea secundaria al diseño de la función objetivo.",
    "solution": "A"
  },
  {
    "id": 452,
    "question": "¿Cómo crea el algoritmo cuántico para el problema del subgrupo oculto abeliano los estados de coset?",
    "A": "Aplicar la función de ocultamiento a una superposición uniforme de elementos del grupo crea estados de coset al mapear todos los elementos dentro de cada coset al mismo valor de salida. La función actúa como un operador de proyección que colapsa elementos que comparten la misma relación de coset en trayectorias computacionales indistinguibles. Esta agrupación natural a través de la evaluación de la función produce la estructura de superposición deseada donde las amplitudes se distribuyen uniformemente entre los miembros del coset, permitiendo que el análisis de Fourier posterior revele el subgrupo oculto.",
    "B": "Aplicar la función de ocultamiento a una superposición uniforme crea estados de coset al mapear todos los elementos en el mismo coset derecho a valores de salida ortogonales mientras se preserva la estructura de coset izquierdo. La periodicidad de la función con respecto al subgrupo oculto asegura que los elementos separados por elementos del subgrupo reciban relaciones de fase que codifican la descomposición de cosets. Esta agrupación a través de la evaluación de la función produce una estructura de superposición donde las fases relativas entre cosets permiten que el análisis de Fourier posterior revele generadores del subgrupo a través de patrones de interferencia constructiva.",
    "C": "Preparar una superposición uniforme sobre el grupo cociente seguida de multiplicaciones controladas que elevan representantes a superposiciones completas de coset dentro del espacio del grupo original. La evaluación de la función proyecta entonces este estado elevado sobre la base computacional midiendo el registro auxiliar, que contiene la salida de la función. Este colapso inducido por medición crea superposiciones de peso igual sobre cada coset con alta probabilidad, aunque el procedimiento puede requerir múltiples rondas de preparación de estados cuando la función exhibe etiquetado irregular de cosets.",
    "D": "Aplicar secuencialmente operaciones de grupo controladas que generan sistemáticamente representantes de coset a través de acumulación multiplicativa de elementos del subgrupo en un registro auxiliar. Cada operación de control multiplica condicionalmente el estado actual por un generador diferente del subgrupo, construyendo la estructura de coset capa por capa a través del paralelismo cuántico. La evaluación de la función se aplica después de la construcción del coset para verificar la membresía, produciendo la estructura de superposición requerida donde las amplitudes se concentran en elementos de coset válidos que satisfacen la condición de periodicidad del subgrupo.",
    "solution": "A"
  },
  {
    "id": 453,
    "question": "¿Qué limita la precisión del conteo cuántico?",
    "A": "La precisión de las operaciones de Grover controladas determina la fidelidad con la que se acumula el retroceso de fase durante los pasos iterativos de amplificación de amplitud. Dado que el conteo cuántico se basa en aplicar operadores de Grover controlados con números variables de iteraciones, cualquier error sistemático en la implementación de estos unitarios se propaga a través del circuito de estimación de fase, causando que la fase medida se desvíe de su valor ideal.",
    "B": "El principio de incertidumbre fundamental cuando se aplica a procedimientos de estimación de fase, que establece un compromiso intrínseco entre la varianza en la fase medida y el número de consultas al oráculo consumidas por el algoritmo. Este límite mecánico-cuántico surge de la no conmutatividad del operador de Grover con el observable de medición de fase, impidiendo la determinación precisa simultánea de las propiedades de autovalor y autoestado. El límite de Heisenberg dicta que reducir la incertidumbre de fase por debajo de un umbral requiere aumentar cuadráticamente la profundidad del circuito, haciendo imposible el conteo arbitrariamente preciso con recursos polinomiales independientemente del postprocesamiento clásico empleado.",
    "C": "El número de qubits en el registro de estimación de fase, que determina directamente la resolución con la que las fases propias pueden distinguirse durante el paso de la transformada cuántica de Fourier. Usar más qubits proporciona una discriminación de fase más fina, permitiendo una estimación más precisa de los autovalores del operador de Grover y, por tanto, un conteo más preciso de los elementos marcados en el espacio de búsqueda.",
    "D": "La tasa de error del oráculo se acumula a lo largo del número polinomial de consultas requeridas para la estimación de fase.",
    "solution": "C"
  },
  {
    "id": 454,
    "question": "¿Cuál es la relación entre las redes neuronales cuánticas y el aprendizaje de circuitos cuánticos?",
    "A": "Las redes neuronales cuánticas requieren arquitecturas de puertas específicas que imitan explícitamente neuronas biológicas—como puertas parametrizadas dispuestas en capas feedforward con no linealidades tipo activación implementadas a través de mediciones durante el circuito—mientras que el aprendizaje de circuitos cuánticos se refiere a cualquier optimización de unitarios parametrizados genéricos sin esta restricción estructural. La distinción clave es que las QNN imponen una topología por capas inspirada en el aprendizaje profundo clásico, mientras que QCL permite topologías de circuito arbitrarias incluyendo ansätze altamente entrelazados que no se descomponen en transformaciones capa por capa.",
    "B": "Estos términos surgieron de grupos de investigación competidores pero se refieren esencialmente al mismo concepto: optimizar circuitos cuánticos parametrizados usando retroalimentación clásica. La división terminológica surgió principalmente de un accidente histórico más que de una distinción técnica, con 'redes neuronales cuánticas' favorecido por investigadores de aprendizaje automático y 'aprendizaje de circuitos cuánticos' preferido por teóricos de información cuántica. Ambos describen marcos matemáticos idénticos que involucran optimización variacional de parámetros de puertas cuánticas para minimizar una función de coste.",
    "C": "El aprendizaje de circuitos cuánticos es un marco más amplio que incluye las redes neuronales cuánticas",
    "D": "La diferencia fundamental radica en sus funciones objetivo: las redes neuronales cuánticas se dirigen específicamente a tareas de clasificación supervisada donde se mapean estados cuánticos de entrada a etiquetas discretas a través de la minimización de entropía cruzada. El aprendizaje de circuitos cuánticos, por el contrario, se centra en optimización continua como encontrar estados fundamentales o resolver problemas variacionales de autovalores, donde las funciones de pérdida son valores esperados de observables hermitianos en lugar de errores de clasificación. Esto los convierte en marcos fundamentalmente distintos que abordan diferentes clases de problemas.",
    "solution": "C"
  },
  {
    "id": 455,
    "question": "El algoritmo HHL asume que la matriz de entrada es dispersa, lo que significa que cada fila:",
    "A": "Contiene como máximo un número polilogarítmico de entradas no nulas en relación con la dimensión total de la matriz.",
    "B": "Tiene entradas no nulas que pueden evaluarse en tiempo polinomial en log(N), permitiendo que el paso de simulación hamiltoniana implemente la exponencial de la matriz con complejidad de puertas polilogarítmica en la dimensión.",
    "C": "Contiene como máximo s entradas no nulas donde s escala polinomialmente con log(N), permitiendo consultas eficientes al oráculo para la subrutina de estimación de fase cuántica que domina el tiempo de ejecución de HHL.",
    "D": "Puede codificarse en bloques en una matriz unitaria usando qubits auxiliares con sobrecarga proporcional al número de elementos no nulos por fila, que debe permanecer polilogarítmico para que se mantenga la complejidad general del algoritmo.",
    "solution": "A"
  },
  {
    "id": 456,
    "question": "El algoritmo de element distinctness basado en caminatas cuánticas utiliza una estructura de datos que almacena qué parte de la lista dentro del estado cuántico?",
    "A": "La lista completa nunca se almacena — la estructura de caminata consulta bajo demanda y mantiene únicamente información de fase sobre la probabilidad de colisión.",
    "B": "Todos los pares de elementos de la lista ordenados lexicográficamente junto con sus valores hash criptográficos correspondientes para facilitar la detección paralela de colisiones en todo el espacio de búsqueda. Esta representación exhaustiva permite que la caminata cuántica recorra el grafo completo de comparaciones por pares en superposición, verificando todas las combinaciones posibles de elementos simultáneamente. Los valores hash proporcionan un mecanismo de comparación compacto que reduce la complejidad de puertas de las pruebas de igualdad, mientras que el ordenamiento lexicográfico garantiza una cobertura sistemática del espacio de colisiones sin verificaciones redundantes, aunque mantener esta estructura completa requiere memoria cuántica O(N²).",
    "C": "Únicamente un hash criptográfico compacto de los elementos muestreados para minimizar la sobrecarga de registros y reducir la decoherencia derivada de mantener estados cuánticos grandes.",
    "D": "Un subconjunto de índices junto con sus valores consultados, mantenidos en registros cuánticos durante toda la evolución de la caminata. Esto permite al algoritmo verificar colisiones comparando elementos recién muestreados contra el subconjunto almacenado, permitiendo la detección de valores duplicados sin requerir el almacenamiento de la lista completa. El tamaño del subconjunto se elige para equilibrar los requisitos de memoria contra la probabilidad de detección de colisiones.",
    "solution": "D"
  },
  {
    "id": 457,
    "question": "La poda de parámetros variacionales basada en la magnitud del gradiente se emplea principalmente para:",
    "A": "Producir circuitos con menos parámetros que mantienen el rendimiento mientras son más factibles de ejecutar en dispositivos a corto plazo, reduciendo tanto el número de puertas como la sensibilidad al ruido",
    "B": "Identificar parámetros que contribuyen mínimamente a la variación de la función de coste, permitiendo la eliminación de puertas cuyos gradientes caen por debajo de umbrales adaptativos mientras se preserva la expresividad del circuito mediante la retención selectiva de parámetros de alta sensibilidad",
    "C": "Comprimir circuitos eliminando parámetros de bajo gradiente que exhiben débil correlación con cambios en la función de coste, manteniendo así el rendimiento algorítmico en ansätze más superficiales desplegables en hardware actual",
    "D": "Reducir el número de parámetros eliminando puertas con magnitudes de gradiente pequeñas relativas a la mediana, creando circuitos más dispersos que retienen el rendimiento de optimización mientras disminuyen la susceptibilidad a fenómenos de mesetas áridas (barren plateau)",
    "solution": "A"
  },
  {
    "id": 458,
    "question": "¿Qué propiedad de los orbitales Hartree-Fock ayuda a optimizar el ordenamiento de términos?",
    "A": "La descomposición de Pauli simétrica elimina subtérminos de orden superior, reduciendo la profundidad total del circuito requerida para la simulación al consolidar operadores conmutantes en bloques simultáneamente diagonalizables que pueden exponenciarse en paralelo sin acumulación adicional de error de Trotter, particularmente cuando se combina con heurísticas de ordenamiento energético específicas de orbital.",
    "B": "La aproximación de campo medio Hartree-Fock vuelve diagonales todos los términos de interacción de dos cuerpos en la base de orbitales moleculares, permitiendo que se expresen como sumas de coeficientes escalares reales independientes que pueden agruparse y factorizarse en productos tensoriales de operadores de Pauli de un solo qubit.",
    "C": "Como los orbitales Hartree-Fock diagonizan el operador de Fock, cada orbital corresponde a un autoestado de energía de una sola partícula definido, lo que significa que la evolución temporal bajo el hamiltoniano puede implementarse completamente mediante puertas de fase de una sola partícula aplicadas independientemente a cada qubit.",
    "D": "Los términos efectivos de salto (hopping) que surgen de las contribuciones de energía cinética de una sola partícula exhiben patrones sistemáticos de cancelación cuando se evalúan en la base de orbitales Hartree-Fock, ya que la aproximación de campo medio asegura que las ocupaciones orbitales se alineen con la configuración electrónica dominante, reduciendo así elementos de matriz extradiagonales y permitiendo una agrupación más eficiente de cadenas de Pauli conmutantes en la representación hamiltoniana mapeada a qubits.",
    "solution": "D"
  },
  {
    "id": 459,
    "question": "En el contexto de redes neuronales cuánticas, considera un escenario donde estás entrenando un circuito cuántico variacional con 10 qubits y 50 puertas de rotación parametrizadas para clasificar un conjunto de datos con desequilibrio significativo de clases (95% clase A, 5% clase B). El circuito usa codificación de amplitud para los datos de entrada y mide todos los qubits en la base computacional para extraer características. Después de 100 épocas de entrenamiento con un optimizador de descenso de gradiente estándar, observas que el modelo alcanza 95% de precisión pero predice la clase A para casi todas las muestras. ¿En qué se diferencian las máquinas de Boltzmann cuánticas de las máquinas de Boltzmann clásicas en términos de su poder representacional, y cuál de las siguientes opciones sería más relevante para abordar el desafío de entrenamiento descrito anteriormente?",
    "A": "Todas las características mencionadas—exploración basada en tunelamiento, capacidad representacional exponencial y correlaciones mediadas por entrelazamiento—son teóricamente relevantes, pero el problema fundamental es el conjunto de entrenamiento desequilibrado en lugar de limitaciones de la arquitectura del modelo. La precisión del 95% al predecir únicamente la clase A indica convergencia a la solución trivial de clase mayoritaria debido a que las funciones de pérdida estándar ignoran las frecuencias de clase. Abordar esto requiere funciones de pérdida ponderadas, sobremuestreo de la minoría o umbrales de decisión ajustados, ninguno específico de arquitecturas cuánticas versus clásicas.",
    "B": "Los efectos de tunelamiento cuántico permiten el recorrido probabilístico de barreras energéticas insuperables en recocido térmico clásico, permitiendo la exploración de regiones distantes del espacio de parámetros sin tiempos de mezcla exponencialmente largos, lo que ayuda a descubrir fronteras de decisión de clases raras con menos muestras de entrenamiento ya que la dinámica cuántica muestrea la distribución de Boltzmann más eficientemente que los métodos MCMC clásicos con amplitud de tunelamiento que escala favorablemente comparado con la activación térmica clásica.",
    "C": "Representan ciertas distribuciones de probabilidad con eficiencia exponencial porque la dimensión del espacio de estados cuánticos crece como 2^n para n qubits mientras que las máquinas de Boltzmann clásicas están limitadas a escalamiento polinomial, lo que significa que las versiones cuánticas capturan correlaciones de alto orden con menos unidades ocultas—abordando directamente la clasificación desequilibrada al aprender patrones discriminativos intrincados en clases minoritarias sin datos de entrenamiento proporcionales mediante la codificación simultánea por superposición de todas las configuraciones 2^n.",
    "D": "Las máquinas de Boltzmann cuánticas aprovechan correlaciones no locales mediante entrelazamiento para capturar dependencias multivariadas complejas en distribuciones de datos, permitiéndoles modelar patrones de clases raras más efectivamente que enfoques clásicos, mientras que el tunelamiento cuántico durante el aprendizaje ayuda a escapar de mínimos locales pobres.",
    "solution": "D"
  },
  {
    "id": 460,
    "question": "Los dispositivos NISQ se caracterizan por atributos como puertas base y topología. El conjunto de puertas base, comúnmente denominado ISA en computación clásica, está definido por el hardware y determina cómo se traducen los circuitos cuánticos del usuario. ¿Qué significa ISA en este contexto?",
    "A": "Integrated System Algorithm (Algoritmo de Sistema Integrado), que se refiere al marco algorítmico nativo que el hardware cuántico utiliza para descomponer operaciones cuánticas de alto nivel en instrucciones primitivas específicas del dispositivo. Esta terminología surgió de la investigación temprana en computación cuántica donde el enfoque estaba en integrar algoritmos de software directamente con capacidades de hardware, particularmente en sistemas que usan computación cuántica adiabática donde el algoritmo y el sistema son co-diseñados.",
    "B": "Initial State Assignment (Asignación de Estado Inicial), el protocolo mediante el cual los dispositivos cuánticos asignan estados de la base computacional a qubits físicos antes de que comience la ejecución del circuito. Este proceso de asignación determina qué qubits comienzan en |0⟩ versus |1⟩ y establece el mapeo entre índices de qubits lógicos y físicos.",
    "C": "Instruction Set Architecture (Arquitectura de Conjunto de Instrucciones), el término estándar de ciencias de la computación adoptado por la computación cuántica para describir las operaciones de puerta nativas que el hardware cuántico puede implementar directamente sin mayor descomposición.",
    "D": "Intermediate State Approximation (Aproximación de Estado Intermedio), una técnica usada durante la compilación de circuitos cuánticos donde los estados cuánticos intermedios se aproximan para reducir la profundidad del circuito. El marco ISA define qué métodos de aproximación son permisibles basándose en tasas de error del hardware y tiempos de decoherencia.",
    "solution": "C"
  },
  {
    "id": 461,
    "question": "¿Por qué aparecen los \"gadgets hamiltonianos\" en las reducciones de complejidad hamiltoniana?",
    "A": "Mapean términos de interacción de k cuerpos a acoplamientos de 2 y 3 cuerpos introduciendo grados de libertad de espín auxiliares cuyo subespacio de baja energía impone las restricciones k-locales originales a través de procesos virtuales perturbativos, pero a diferencia de las construcciones estándar preservan la conmutatividad de todos los términos, permitiendo la verificación clásica en tiempo polinómico de los estados fundamentales mediante minimización voraz de energía sobre proyectores conmutativos.",
    "B": "Replican sistemáticamente términos de interacción de k cuerpos usando únicamente operadores de acoplamiento de 2 cuerpos al introducir qubits auxiliares cuyas penalizaciones energéticas imponen las correlaciones multipartita deseadas, permitiendo reducciones a clases hamiltonianas más simples mientras preservan la dureza computacional de encontrar estados fundamentales.",
    "C": "Los gadgets codifican restricciones k-locales en hamiltonianos de penalización 2-locales añadiendo qubits mediadores cuyos niveles de energía intermedios se ajustan de modo que los estados de baja energía del sistema aumentado correspondan a asignaciones satisfactorias de la restricción original, pero introducen brechas espectrales que escalan inversamente con la fuerza de penalización, requiriendo un análisis perturbativo cuidadoso para mantener la equivalencia de complejidad computacional.",
    "D": "Reducen términos de k cuerpos a interacciones de 2 cuerpos mediante qubits auxiliares con energías de penalización diagonales que suprimen estados no deseados de la base computacional, asegurando que el hamiltoniano reducido permanezca estocástico cuando el original estaba libre del problema de signo, preservando así tanto la energía del estado fundamental como la simulabilidad clásica a través de métodos quantum Monte Carlo sin sobrecarga exponencial debida a oscilaciones de signo fermiónico.",
    "solution": "B"
  },
  {
    "id": 462,
    "question": "En redes multiplexadas por división de tiempo, ¿por qué podrían cambiar las decisiones de enrutamiento en cada intervalo temporal?",
    "A": "La disponibilidad y fidelidad de los enlaces varía con el tiempo debido a efectos de decoherencia y fluctuaciones ambientales",
    "B": "Las probabilidades de éxito en la generación de entrelazamiento fluctúan debido a variaciones en la eficiencia de detectores y resultados probabilísticos de señalización.",
    "C": "La topología de red se reconfigura dinámicamente mientras los nodos repetidores cuánticos alternan entre fases de intercambio de entrelazamiento y purificación.",
    "D": "Los protocolos adaptativos optimizan para capacidades de canal variables en el tiempo causadas por demandas competitivas de usuarios y prioridades de asignación de recursos.",
    "solution": "A"
  },
  {
    "id": 463,
    "question": "El código heavy-hexagon de IBM modifica las aristas de la red del código de superficie. ¿Qué desafío plantea esto para los decodificadores estándar de emparejamiento perfecto de peso mínimo?",
    "A": "Los operadores lógicos dejan de conmutar una vez que se introducen las modificaciones de aristas heavy-hexagon, porque el número de coordinación reducido en ciertos vértices cambia la clase de homología de los bucles no contráctiles en la red. Esto significa que los operadores lógicos X y Z, que deben anticonmutar para un código válido, pueden en realidad conmutar en ciertos bordes de la red modificada, destruyendo la capacidad del código de proteger información cuántica. Los decodificadores MWPM estándar asumen que los operadores lógicos mantienen sus relaciones de anticonmutación a lo largo del grafo de decodificación, por lo que este fallo invalida las garantías de corrección del decodificador.",
    "B": "La fidelidad de lectura en arquitecturas heavy-hexagon introduce errores de medición correlacionados que violan el supuesto de independencia subyacente a la extracción de síndrome binaria, causando que el decodificador malinterprete fallos de lectura multiqubit como violaciones reales de estabilizadores. Esta correlación de ruido significa que los propios bits de síndrome contienen errores que no están distribuidos uniformemente, y los decodificadores MWPM estándar que tratan cada bit de síndrome como una variable aleatoria de Bernoulli independiente subestimarán sistemáticamente las tasas de error reales, llevando a una degradación significativa en la supresión de errores lógicos.",
    "C": "Las mediciones ocurren en tiempos diferentes por lo que no se puede construir el grafo 3D usual. En topologías heavy-hexagon, la programación temporal de mediciones de síndrome está escalonada entre diferentes tipos de estabilizadores debido a restricciones de hardware, lo que impide la construcción de un grafo espaciotemporal uniforme donde todos los síndromes estén alineados en una red regular. Debido a que el decodificador MWPM se basa en incrustar errores en un grafo 3D donde el eje temporal está discretizado uniformemente, este desalineamiento temporal rompe el marco de decodificación estándar y requiere ajustes ad-hoc que no están bien soportados por implementaciones de decodificadores existentes.",
    "D": "Los vértices de grado irregular requieren construcciones de grafos no bipartitos, complicando las asignaciones de pesos de aristas para el decodificador",
    "solution": "D"
  },
  {
    "id": 464,
    "question": "¿Cómo mejoran las Redes Neuronales Recurrentes Cuánticas (QRNNs) los modelos de aprendizaje automático?",
    "A": "Las Redes Neuronales Recurrentes Cuánticas emplean puertas de fase controlada entre registros de estados cuánticos temporalmente adyacentes para codificar correlaciones secuenciales a través de relaciones de fase de entrelazamiento, donde el estado oculto de cada paso temporal se entrelaza con un registro de memoria persistente que acumula información de fase a lo largo de toda la secuencia. Este mecanismo de memoria codificada en fase permite a la red representar dependencias de largo alcance sin degradación de amplitud, aunque el enfoque requiere protocolos cuidadosos de estimación de fase durante la lectura ya que la información temporal reside en fases relativas en lugar de amplitudes de probabilidad, necesitando esquemas de medición interferométricos que extraen estructura de correlación mediante procedimientos de tomografía de estados cuánticos de múltiples tiempos.",
    "B": "Aprovechan el paralelismo cuántico para procesar múltiples elementos de secuencia simultáneamente en superposición, mientras explotan el entrelazamiento para crear representaciones de estados ocultos más expresivas que pueden capturar dependencias temporales complejas. Las conexiones recurrentes cuánticas permiten a la red mantener y propagar información a lo largo de horizontes temporales más largos en comparación con las RNNs clásicas, potencialmente mitigando problemas de gradientes evanescentes mediante la preservación de amplitudes cuánticas en los estados ocultos evolucionados unitariamente.",
    "C": "Las QRNNs utilizan circuitos cuánticos parametrizados con operadores de evolución unitaria fijos aplicados recurrentemente en cada paso temporal, donde la dinámica temporal emerge de la aplicación repetida de la misma secuencia de puertas cuánticas en lugar de hamiltonianos dependientes del tiempo. El estado cuántico oculto acumula información a través de bucles de retroalimentación coherentes donde los resultados de medición del tiempo t condicionan la codificación de entrada en t+1, creando una recurrencia híbrida clásica-cuántica que elude limitaciones puras de decoherencia. Esta arquitectura sobresale en capturar patrones temporales porque la recurrencia unitaria preserva correlaciones cuánticas entre pasos temporales no adyacentes a través de entrelazamiento multiqubit que persiste a lo largo de intervalos de evolución sin medición que abarcan la longitud de la secuencia.",
    "D": "Estas redes implementan convolución temporal mediante operadores de caminata cuántica en estructuras de grafos donde los nodos representan pasos temporales secuenciales y las aristas codifican dependencias causales, permitiendo que la información se propague bidireccionalmente a través de la secuencia mediante tunelización cuántica simétrica entre estados temporalmente no locales. La dinámica de caminata cuántica genera superposiciones sobre múltiples trayectorias temporales posibles simultáneamente, con interferencia destructiva suprimiendo naturalmente estados históricos irrelevantes mientras amplifica constructivamente patrones causalmente significativos. Este enfoque proporciona aceleración cuadrática en longitud de secuencia para tareas de reconocimiento de patrones porque la amplitud de caminata se extiende a lo largo de O(√T) pasos temporales en T iteraciones, aunque requiere post-selección para extraer el estado oculto final.",
    "solution": "B"
  },
  {
    "id": 465,
    "question": "¿Cómo funciona típicamente una Máquina de Vectores de Soporte mejorada cuánticamente?",
    "A": "Los solvers variacionales de autovalores optimizan parámetros de margen mediante circuitos cuánticos.",
    "B": "Kernels cuánticos — básicamente solo mapear datos al espacio de Hilbert implícitamente.",
    "C": "La estimación de amplitud cuántica acelera los cálculos de matriz de kernel eficientemente.",
    "D": "El muestreo cuántico genera datos de entrenamiento en espacios de características exponencialmente grandes.",
    "solution": "B"
  },
  {
    "id": 466,
    "question": "¿Cuál es un desafío conocido en la implementación del Algoritmo de Shor en hardware cuántico real?",
    "A": "Se necesitan muy pocos qubits, creando una paradoja donde números que requieren solo 10-20 qubits pueden resolverse más rápido clásicamente, dejando sin instancias de problemas lo suficientemente grandes para demostrar ventaja cuántica pero lo suficientemente pequeñas para el hardware disponible.",
    "B": "Los fundamentos teóricos del Algoritmo de Shor permanecen sin demostrar para la factorización de primos a pesar de una revisión por pares exhaustiva, con el paso de la transformada cuántica de Fourier aún careciendo de verificación matemática rigurosa en el contexto de exponenciación modular. Esto crea incertidumbre sobre si el algoritmo puede factorizar confiablemente grandes semiprimos incluso con hardware cuántico perfecto, ya que las amplitudes de probabilidad pueden no concentrarse correctamente alrededor del período de la función modular.",
    "C": "El posprocesamiento clásico de los resultados de medición cuántica no puede realizarse eficientemente porque el algoritmo de fracciones continuas requerido para extraer el período de la fase medida falla para números con más de tres factores primos.",
    "D": "Los altos requisitos de cantidad de qubits combinados con tasas de error elevadas presentan barreras significativas, ya que factorizar números criptográficamente relevantes exige miles de qubits lógicos mientras que los sistemas actuales luchan por mantener coherencia incluso en cientos de qubits físicos. Las fidelidades de puerta deben superar el 99.9% para que la corrección de errores sea efectiva, un umbral que muchas plataformas no han alcanzado consistentemente.",
    "solution": "D"
  },
  {
    "id": 467,
    "question": "En los códigos estabilizadores, ¿qué mide específicamente la \"distancia\"?",
    "A": "El número mínimo de errores físicos de un solo qubit requeridos para producir un error lógico que no pueda ser detectado por ninguna medición de estabilizador. Esta definición basada en peso captura la robustez del código: un código de distancia d puede detectar hasta d-1 errores y corregir hasta floor((d-1)/2) errores, haciendo de la distancia el parámetro fundamental que gobierna la capacidad de corrección de errores del código.",
    "B": "El peso mínimo de cualquier operador lógico no trivial que conmuta con todos los estabilizadores pero no es en sí mismo un elemento estabilizador. Esta definición basada en operadores captura la robustez del código: un código de distancia d protege contra d-1 errores físicos y corrige hasta floor((d-1)/2) errores, haciendo de la distancia el parámetro fundamental que gobierna la capacidad de corrección de errores a través de la estructura de soporte del operador lógico.",
    "C": "La separación euclidiana mínima entre regiones de soporte de operadores lógicos en la red, particularmente en códigos topológicos como el código de superficie donde la distancia geométrica entre excitaciones anyónicas determina la detectabilidad. Un código de distancia d detecta hasta d-1 errores localizados y corrige floor((d-1)/2) errores midiendo la separación mínima de anyones durante ciclos de extracción de síndrome.",
    "D": "El peso de síndrome mínimo producible por errores lógicos incorregibles, cuantificando cuántas mediciones de estabilizador deben fallar simultáneamente antes de que ocurra daño lógico indetectable. Un código de distancia d tolera d-1 fallos de extracción de síndrome y corrige floor((d-1)/2) errores de medición, estableciendo la distancia en el espacio de síndromes como el parámetro fundamental de corrección de errores que gobierna el rendimiento del decodificador.",
    "solution": "A"
  },
  {
    "id": 468,
    "question": "¿Qué es el funcional de influencia de Feynman-Vernon en el contexto de la computación cuántica?",
    "A": "Un formalismo de integral de camino que describe cómo la dinámica de un sistema cuántico es moldeada por su acoplamiento a campos clásicos externos, capturando la influencia completa dependiente del historial de Hamiltonianos de control dependientes del tiempo sobre la evolución del sistema. Este marco matemático es esencial para entender rigurosamente la teoría de control óptimo, ya que codifica todos los efectos no adiabáticos y la acumulación de fase dinámica que causan que las operaciones de puerta se desvíen de sus unitarias objetivo con el tiempo.",
    "B": "Un formalismo de integral de camino que describe cómo los grados de libertad ambientales se acoplan al sistema cuántico, capturando la influencia completa dependiente del historial del baño sobre la dinámica reducida del sistema. Este marco matemático es esencial para entender rigurosamente los procesos de decoherencia, ya que codifica todos los efectos de memoria no markovianos e interacciones disipativas que causan que estados cuánticos puros evolucionen hacia estados mixtos con el tiempo.",
    "C": "Un formalismo basado en propagadores que describe cómo la retroacción de medición de qubits auxiliares influye en la evolución condicional de qubits de datos en protocolos de corrección de errores cuánticos, capturando las correlaciones completas dependientes del historial entre resultados de síndrome e información lógica protegida. Este marco matemático es esencial para entender rigurosamente los umbrales tolerantes a fallos, ya que codifica todas las correlaciones de múltiples rondas y dinámicas inducidas por medición que causan que los códigos cuánticos acumulen síndromes de error con el tiempo.",
    "D": "Un formalismo de integral de camino que describe cómo los procesos de ruido estocástico se acoplan al sistema cuántico a través de inserciones de operadores ordenadas temporalmente, capturando la influencia completa dependiente del historial de imperfecciones de puerta sobre trayectorias computacionales. Este marco matemático es esencial para entender rigurosamente el rendimiento promedio de algoritmos, ya que codifica todos los mecanismos de error correlacionados y procesos de fuga coherente que causan que dispositivos cuánticos de escala intermedia ruidosos se desvíen de la evolución unitaria ideal con el tiempo.",
    "solution": "B"
  },
  {
    "id": 469,
    "question": "¿Cuál de los siguientes es un desafío en la integración de embeddings cuánticos con modelos clásicos?",
    "A": "Las altas cantidades de qubits se vuelven prohibitivas cuando los embeddings cuánticos se usan como capas de entrada para redes neuronales clásicas, porque mantener suficiente expresividad en el mapa de características requiere que la dimensión de embedding escale exponencialmente con el número de características de entrada.",
    "B": "La incompatibilidad con modelos lineales surge de la naturaleza no lineal de la medición cuántica, que colapsa estados de superposición en cadenas de bits clásicas a través de un proceso estocástico que viola el principio de superposición requerido por regresión ridge y máquinas de vectores de soporte. Los clasificadores lineales clásicos asumen características de valor continuo que se combinan aditivamente, pero los embeddings cuánticos producen resultados discretos muestreados de distribuciones de la regla de Born, necesitando soluciones alternativas con el truco del kernel que reintroducen sobrecarga computacional equivalente a métodos clásicos de expansión de características.",
    "C": "Dificultad para interpretar mapas de características no clásicos, ya que los embeddings cuánticos operan en espacios de Hilbert de alta dimensión donde las intuiciones geométricas sobre la importancia de características y límites de decisión se desmoronan. Las representaciones transformadas carecen de visualización o explicación directa en términos de características de entrada originales, complicando la depuración del modelo, comunicación con stakeholders y cumplimiento de requisitos de interpretabilidad en dominios regulados.",
    "D": "Ausencia de compiladores de circuitos cuánticos estándar capaces de interconectar salidas de embeddings cuánticos con operaciones tensoriales clásicas, ya que las estadísticas de medición producidas por circuitos variacionales existen en el espacio simplex de probabilidad en lugar de espacios vectoriales euclidianos donde los gradientes clásicos están bien definidos. Los frameworks actuales de diferenciación automática como TensorFlow y PyTorch carecen de soporte nativo para retropropagar a través de observables cuánticos, requiriendo bibliotecas puente personalizadas que introducen inestabilidades numéricas cuando las actualizaciones de parámetros cruzan regiones donde las probabilidades de Born se aproximan a cero, particularmente en arquitecturas híbridas que mezclan capas convolucionales cuánticas con operaciones de pooling clásicas.",
    "solution": "C"
  },
  {
    "id": 470,
    "question": "¿Cuál es el propósito de las secuencias de desacoplamiento dinámico en el diseño de circuitos cuánticos?",
    "A": "Suprimir la decoherencia aplicando secuencias de pulsos cuidadosamente temporizadas que promedian el ruido ambiental sobre múltiples períodos, efectivamente desacoplando el qubit de fluctuaciones del baño que varían lentamente mediante técnicas de reenfoque análogas a protocolos CPMG en espectroscopía de RMN, donde pulsos π se insertan a intervalos que coinciden con el tiempo de correlación del espectro de ruido para proyectar componentes de alta frecuencia mientras se preserva información de la base computacional, extendiendo así los tiempos T₂ de microsegundos a milisegundos en sistemas dominados por ruido de carga 1/f a través de funciones de filtro que remodelan la susceptibilidad del qubit para coincidir con brechas espectrales en la densidad de potencia ambiental",
    "B": "Suprimir la decoherencia aplicando secuencias de pulsos cuidadosamente temporizadas que crean estados vestidos inmunes al acoplamiento ambiental mediante expansión de Magnus del Hamiltoniano en el marco de conmutación, efectivamente desacoplando el qubit de fluctuaciones del baño que varían lentamente al inducir dinámica de Zeno donde la retroacción de medición continua congela los grados de libertad ambientales, análogo a protocolos de bombeo óptico donde el rebombeo rápido previene pérdida de población, con pulsos π insertados a intervalos específicos que exceden el tiempo de correlación del baño para prevenir seguimiento adiabático de la manipulación del qubit, extendiendo así los tiempos de coherencia de microsegundos a milisegundos en sistemas donde la decoherencia surge del acoplamiento cuasi-estático a baños de espín nuclear",
    "C": "Suprimir la decoherencia aplicando secuencias de pulsos cuidadosamente temporizadas que promedian el ruido ambiental sobre múltiples períodos, efectivamente desacoplando el qubit de fluctuaciones del baño que varían lentamente mediante técnicas de reenfoque análogas a protocolos de eco de espín en espectroscopía de RMN, donde pulsos π se insertan a intervalos específicos para revertir los errores de fase acumulados causados por inhomogeneidades de campo magnético cuasi-estáticas o ruido de carga, extendiendo así los tiempos de coherencia de microsegundos a milisegundos en sistemas limitados por ruido de baja frecuencia",
    "D": "Suprimir la decoherencia aplicando secuencias de pulsos cuidadosamente temporizadas que impulsan paramétricamente la interacción qubit-baño hacia el régimen de acoplamiento ultrafuerte, efectivamente desacoplando estados computacionales de fluctuaciones del baño que varían lentamente mediante ingeniería de onda contra-rotante análoga a protocolos de Casimir dinámico, donde pulsos π modulan el Hamiltoniano de interacción al doble de la frecuencia de Rabi para abrir brechas espectrales que previenen intercambio de energía con modos ambientales, extendiendo así los tiempos de desfase de microsegundos a milisegundos en sistemas donde T₂ está limitado por ruido Johnson de baja frecuencia de elementos resistivos en la circuitería de control",
    "solution": "C"
  },
  {
    "id": 471,
    "question": "¿Qué vulnerabilidad sofisticada existe en la implementación de la comparación privada cuántica?",
    "A": "Fuga de elección de base a través de canales laterales de temporización de fotones.",
    "B": "Distinguibilidad de estados entrelazados mediante caídas de Hong-Ou-Mandel.",
    "C": "Las dependencias de orden de medición revelan la paridad de bits de entrada.",
    "D": "Análisis de correlación de resultados de medición.",
    "solution": "D"
  },
  {
    "id": 472,
    "question": "¿Por qué se consideran sigilosas las modificaciones a nivel de pulsos?",
    "A": "Estos ataques desencadenan fallos de ejecución inmediatos y catastróficos que detienen la compilación de circuitos antes de que se aplique cualquier puerta a los qubits físicos, haciéndolos instantáneamente detectables por sistemas de monitoreo automatizados pero impidiendo simultáneamente cualquier computación cuántica coherente. La terminación abrupta ocurre porque la manipulación a nivel de pulsos interrumpe las tablas de calibración que mapean puertas lógicas a formas de onda de control, causando que el procesador cuántico rechace el flujo de instrucciones malformado durante la fase de validación previa a la ejecución, lo que paradójicamente hace visible el ataque mientras vuelve inoperable el circuito.",
    "B": "Los ataques a nivel de pulsos funcionan exclusivamente en entornos idealizados libres de ruido donde las tasas de decoherencia son despreciables y las fidelidades de las puertas se aproximan a la unidad, ya que cualquier ruido ambiental enmascararía inmediatamente las sutiles modificaciones de amplitud o fase introducidas en la capa de control. En sistemas realistas con tiempos finitos T₁ y T₂, las fluctuaciones ambientales dominan sobre las distorsiones de pulso intencionadas, causando que las modificaciones adversarias sean absorbidas en la tasa de error de fondo y por tanto se vuelvan operacionalmente indistinguibles de las imperfecciones naturales del hardware, lo que limita su despliegue práctico a entornos de laboratorio con aislamiento extremo.",
    "C": "Las modificaciones a nivel de pulsos operan por debajo de la capa de abstracción de puertas donde típicamente se aplican mecanismos de verificación de integridad como el hashing criptográfico y las sumas de verificación. Dado que estas comprobaciones de seguridad validan secuencias de puertas a nivel de circuito lógico en lugar de inspeccionar las formas de onda de control subyacentes, los adversarios pueden introducir desplazamientos de fase sutiles, distorsiones de amplitud o perturbaciones de temporización en los pulsos analógicos que implementan cada puerta, mientras dejan sin cambios la descripción del circuito de alto nivel y pasan todos los protocolos de verificación estándar sin ser detectados.",
    "D": "Implementar modificaciones a nivel de pulsos exige acceso físico directo al refrigerador de dilución que alberga el procesador cuántico, ya que las formas de onda de control deben inyectarse a temperaturas criogénicas a través de líneas coaxiales dedicadas que terminan en el empaquetado del chip. Los adversarios remotos no pueden ejecutar estos ataques a través de interfaces en la nube porque la programación de pulsos ocurre en matrices de puertas programables en campo ubicadas físicamente dentro del recinto blindado, debajo de la etapa de la cámara de mezcla. Este aislamiento de brecha de aire entre la electrónica de control a temperatura ambiente y el hardware de generación de pulsos asegura que solo el personal in situ con credenciales de sala limpia pueda manipular las señales analógicas que impulsan las transiciones de qubits.",
    "solution": "C"
  },
  {
    "id": 473,
    "question": "En el aprendizaje automático cuántico, te encuentras con afirmaciones sobre ventajas universales sobre los métodos clásicos. Sin embargo, los científicos informáticos teóricos a menudo invocan el 'teorema no free lunch' al evaluar estas afirmaciones. Considera un escenario donde estás diseñando un algoritmo cuántico variacional para un conjunto de datos específico y te preguntas si los enfoques cuánticos siempre superarán a los clásicos. ¿Cuál es la limitación fundamental que el 'teorema no free lunch' impone a los algoritmos de aprendizaje automático cuántico?",
    "A": "Ningún algoritmo cuántico puede lograr un rendimiento mejor que el clásico en todas las tareas posibles de aprendizaje automático—cualquier ventaja cuántica debe ser dependiente del problema y específica de la tarea, con el teorema demostrando que promediando sobre todas las funciones objetivo posibles, los aprendices cuánticos y clásicos se desempeñan idénticamente, lo que significa que cada dominio donde lo cuántico destaca está equilibrado por otros donde no proporciona beneficio alguno",
    "B": "Los aprendices cuánticos no pueden lograr simultáneamente un rendimiento óptimo tanto en distribuciones de entrenamiento como de prueba porque el teorema no free lunch se extiende a la generalización: cualquier modelo cuántico que se ajuste perfectamente a datos de entrenamiento extraídos de una clase de distribución necesariamente sobreajustará cuando se pruebe en distribuciones de una clase complementaria. Este compromiso fundamental es más pronunciado que en el aprendizaje clásico porque los modelos cuánticos codifican distribuciones de probabilidad a través de amplitudes en lugar de probabilidades directas, lo que significa que la restricción de amplitud al cuadrado |⟨ψ|φ⟩|² impone restricciones geométricas en el espacio de hipótesis que los modelos clásicos evitan, forzando a los algoritmos cuánticos a especializarse más estrechamente para beneficiarse de su capacidad representacional exponencial.",
    "C": "El teorema no free lunch demuestra que la ventaja cuántica en el aprendizaje automático solo puede surgir explotando conocimiento previo sobre la estructura del problema codificado en el ansatz del circuito, pero este requisito crea una paradoja: diseñar un ansatz apropiado requiere trabajo computacional clásico equivalente a resolver una gran fracción del problema de aprendizaje original. Específicamente, encontrar la forma variacional óptima requiere buscar en un espacio exponencialmente grande de posibles arquitecturas de circuitos, y aunque las computadoras cuánticas podrían entrenar más rápido una vez que el ansatz está fijado, el costo clásico de preprocesamiento de la selección de ansatz niega la aceleración cuántica cuando se amortiza sobre el flujo de trabajo completo.",
    "D": "Los algoritmos cuánticos deben pagar una penalización de profundidad de circuito para lograr menor complejidad muestral que los métodos clásicos, creando un compromiso fundamental de recursos gobernado por el principio no free lunch: cualquier reducción en el número de ejemplos de entrenamiento requeridos debe compensarse con un mayor conteo de puertas en el circuito cuántico. Esta relación se deriva de límites teóricos de información cuántica que muestran que extraer k bits de información sobre una función desconocida requiere consultarla k veces clásicamente o implementar un circuito cuántico de profundidad-Ω(k), lo que significa que los aprendices cuánticos no pueden minimizar simultáneamente tanto los requisitos de datos de entrenamiento como los recursos computacionales.",
    "solution": "A"
  },
  {
    "id": 474,
    "question": "En el contexto de la teoría de canales cuánticos, cuando decimos que un canal es \"doblemente estocástico\" —satisfaciendo tanto la unitalidad como la preservación de traza— ¿qué propiedad garantiza esto? Este es un análogo cuántico directo de las matrices biestocásticas clásicas que aparecen en la teoría de cadenas de Markov.",
    "A": "El estado completamente mezclado se mapea a sí mismo: el estado completamente mezclado ρ_mixed = I/d (donde d es la dimensión del espacio de Hilbert) es un punto fijo de cualquier canal doblemente estocástico. Esto se deriva porque la unitalidad asegura Φ(I) = I, y dado que ρ_mixed es proporcional al operador identidad, tenemos Φ(ρ_mixed) = Φ(I/d) = I/d = ρ_mixed. Esta propiedad de punto fijo refleja el resultado clásico de que las distribuciones de probabilidad uniformes permanecen uniformes bajo mapeos biestocásticos.",
    "B": "Preserva el volumen de la esfera de Bloch: bajo un canal doblemente estocástico, la imagen de cualquier operador de densidad permanece dentro de la esfera de Bloch con radio sin cambios, porque la unitalidad fuerza Φ(I) = I, asegurando que el estado completamente mezclado I/d permanezca fijo, mientras que la preservación de traza Tr[Φ(ρ)] = Tr[ρ] asegura que no escape probabilidad fuera del espacio de estados. Juntas, estas restricciones significan que el canal preserva el volumen en el cuerpo convexo de matrices de densidad, análogo a cómo los mapeos biestocásticos clásicos preservan la norma L¹ de vectores de probabilidad, aunque esto no impide la distorsión de coordenadas angulares.",
    "C": "Mayorización de autovalores de matrices de densidad: para cualquier estado de entrada ρ, la salida Φ(ρ) tiene un espectro de autovalores λ(Φ(ρ)) que es mayorizado por λ(ρ), lo que significa que la salida está más mezclada (mayor entropía) a menos que ρ ya esté completamente mezclado. La unitalidad garantiza que el espectro de la identidad {1/d, ..., 1/d} se preserve, mientras que la preservación de traza asegura que este espectro uniforme sirva como el límite superior de mayorización. Esta propiedad de mayorización generaliza el teorema de Perron-Frobenius para matrices biestocásticas clásicas, donde los vectores de probabilidad se vuelven más uniformes bajo aplicación repetida.",
    "D": "Contractividad bajo norma de operador: los canales doblemente estocásticos satisfacen ||Φ(ρ) - Φ(σ)||₁ ≤ ||ρ - σ||₁ para todos los operadores de densidad ρ, σ, lo que significa que acercan estados distintos en distancia de traza. La unitalidad fuerza el punto fijo en I/d, mientras que la preservación de traza asegura que se respete la estructura de combinación convexa, juntas implicando que Φ actúa como un mapeo de contracción en el espacio de matrices de densidad. Esto garantiza convergencia al estado completamente mezclado bajo aplicación iterada, reflejando cómo las matrices biestocásticas clásicas impulsan distribuciones de probabilidad hacia la uniformidad.",
    "solution": "A"
  },
  {
    "id": 475,
    "question": "En muchas implementaciones experimentales de algoritmos cuánticos variacionales, los investigadores han encontrado necesario modificar sus procedimientos de entrenamiento para tener en cuenta estadísticas de medición finitas. Un enfoque común implica introducir funciones de pérdida balanceadas en bucles de entrenamiento basados en disparos. La motivación principal para esta modificación es:",
    "A": "Al construir cuidadosamente funciones de pérdida que incorporan información de curvatura de la matriz Hessiana, los investigadores pueden garantizar matemáticamente la convexidad estricta del paisaje objetivo a través de profundidades arbitrarias de circuito, eliminando todos los mínimos locales y puntos de silla. Esta garantía de convexidad asegura que el simple descenso de gradiente, sin ninguna programación adaptativa de tasa de aprendizaje o términos de momento, convergerá demostrablemente al óptimo global único independientemente de la inicialización. La formulación balanceada remodela la superficie de energía en un cuenco perfectamente suave, aprovechando patrones de interferencia cuántica para suprimir puntos críticos espurios que de otro modo atraparían optimizadores clásicos en regiones de parámetros subóptimas.",
    "B": "La formulación de pérdida balanceada elimina completamente el bucle de optimización clásico del marco del algoritmo cuántico variacional, permitiendo al procesador cuántico realizar actualizaciones autónomas de parámetros a través de retroalimentación de medición autorreferencial sin ninguna supervisión computacional externa. Al codificar la información de gradiente directamente en los resultados de medición mediante observables de Pauli especialmente diseñados, el circuito cuántico mismo implementa la dinámica de optimización a través de ciclos repetidos de preparación-medición. Esto elimina completamente el cuello de botella clásico, transformando el paradigma híbrido cuántico-clásico en un procedimiento iterativo puramente cuántico donde la evolución de parámetros ocurre nativamente dentro del espacio de estados cuánticos.",
    "C": "Abordar sesgos sistemáticos que surgen al estimar valores esperados a partir de muestras de medición finitas, particularmente cuando diferentes términos en la función de pérdida tienen magnitudes o presupuestos de disparos de medición vastamente diferentes. Sin balanceo, los términos de alta varianza pueden dominar la señal de gradiente, causando inestabilidad de optimización y pobre convergencia. Las formulaciones balanceadas normalizan o ponderan los componentes de pérdida para asegurar que todos los términos contribuyan proporcionalmente a las actualizaciones de parámetros a pesar del ruido de muestreo.",
    "D": "Las funciones de pérdida balanceadas permiten programaciones agresivas de tasa de aprendizaje que sistemáticamente duplican el tamaño del paso después de cada iteración de entrenamiento, explotando el escalado exponencial de los espacios de estados cuánticos para acelerar la convergencia hacia parámetros óptimos. Esta estrategia de duplicación aprovecha el principio de superposición para explorar exponencialmente muchas configuraciones de parámetros simultáneamente durante cada evaluación de gradiente, logrando efectivamente ventaja cuántica en el proceso de optimización mismo. La formulación balanceada asegura estabilidad numérica a pesar de los tamaños de paso crecientes al normalizar gradientes según la varianza del ruido de disparos, permitiendo al algoritmo atravesar el paisaje de parámetros a velocidades exponencialmente crecientes sin sobrepasar mínimos o encontrar inestabilidades de divergencia.",
    "solution": "C"
  },
  {
    "id": 476,
    "question": "¿Por qué las técnicas convencionales de planificación de computación de alto rendimiento (HPC) requieren modificación para la computación cuántica distribuida?",
    "A": "El consumo de energía por operación exhibe un comportamiento de escalado fundamentalmente diferente en los sistemas cuánticos, donde cada operación de puerta disipa energía de maneras que violan las suposiciones subyacentes a la gestión térmica clásica y las heurísticas de balanceo de carga.",
    "B": "Las computaciones cuánticas requieren tiempos de ejecución más largos que las clásicas, lo que hace que los algoritmos de planificación existentes sean ineficientes debido a recursos inactivos y crea cuellos de botella que los planificadores por lotes convencionales no fueron diseñados para manejar eficazmente.",
    "C": "El entrelazamiento y el teorema de no clonación rompen fundamentalmente las suposiciones clásicas de asignación de recursos. Los planificadores HPC tradicionales asumen que los datos pueden copiarse libremente entre nodos para balanceo de carga y tolerancia a fallos, pero la información cuántica no puede clonarse debido a leyes físicas fundamentales. Además, los estados entrelazados crean correlaciones no locales que no pueden particionarse independientemente entre recursos computacionales de la manera en que las cargas de trabajo clásicas pueden distribuirse.",
    "D": "Los planificadores clásicos ya asumen que la copia de qubits funciona bien para la migración de estados entre nodos, tratando los datos cuánticos como páginas de memoria clásicas que pueden replicarse libremente para balanceo de carga o tolerancia a fallos.",
    "solution": "C"
  },
  {
    "id": 477,
    "question": "¿Cuál es una consecuencia probable de ejecutar múltiples intercambios de entrelazamiento a lo largo de una cadena larga de nodos intermedios sin purificación?",
    "A": "La fidelidad se degrada progresivamente con cada salto sucesivo a lo largo de la cadena debido al ruido acumulado y las mediciones de Bell imperfectas en los nodos intermedios, eventualmente haciendo que el estado compartido final sea demasiado mezclado para soportar operaciones cuánticas remotas confiables o violaciones significativas de las desigualdades de Bell",
    "B": "La fidelidad disminuye aproximadamente de manera multiplicativa con cada operación de intercambio según F_n ≈ (F_0)^n para n intercambios con fidelidad inicial F_0, pero esta degradación sigue un patrón de oscilación amortiguada característico donde los intercambios de número impar sufren peor degradación que los de número par debido a bases de medición alternadas en nodos sucesivos. Esta acumulación de error dependiente de la paridad proviene de operadores de medición de Bell no conmutantes que crean coherencia de fase entre intercambios adyacentes que parcialmente cancela el ruido en cada segundo salto.",
    "C": "La fidelidad se degrada a través de un mecanismo fundamentalmente diferente al que a menudo se asume: aunque los errores de medición de Bell sí contribuyen, la fuente dominante de infidelidad se convierte en la decoherencia durante el tiempo de espera obligatorio en cada nodo intermedio para que el siguiente intercambio se complete en la cadena. Dado que los protocolos distribuidos imponen ordenamiento temporal de las operaciones de intercambio para prevenir violaciones de causalidad, los qubits en las posiciones tempranas de la cadena deben permanecer en memoria mientras se ejecutan intercambios posteriores, y este tiempo de almacenamiento escala linealmente con la longitud de la cadena, haciendo que la degradación T2 sea el cuello de botella principal en lugar de la infidelidad de la operación de intercambio en sí.",
    "D": "La fidelidad cae por debajo del umbral clásico (F < 0.5) después de aproximadamente log(n) intercambios para una cadena de n nodos debido al ruido despolarizante acumulado, pero la degradación es autolimitante porque una vez que la fidelidad alcanza el estado máximamente mezclado, intercambios adicionales no pueden aumentar la entropía más allá del máximo. Este efecto de saturación significa que cadenas muy largas (n > 20) en realidad muestran fidelidades finales similares independientemente de la longitud exacta, aunque todas caen por debajo del régimen útil para ventaja cuántica, haciendo que la curva de degradación precisa sea menos crítica que la cuestión binaria de si se usó purificación.",
    "solution": "A"
  },
  {
    "id": 478,
    "question": "¿Qué técnica ayuda a evitar bucles de enrutamiento en redes cuánticas dinámicas?",
    "A": "Los protocolos de vector de distancia con reglas de horizonte dividido previenen bucles de enrutamiento al asegurar que los nodos nunca anuncien rutas de entrelazamiento de vuelta al vecino del cual las aprendieron. Cada nodo de enrutamiento mantiene costos de ruta medidos en degradación de fidelidad esperada y conteo de saltos, actualizando estos vectores al recibir anuncios de estado de enlace. La modificación de horizonte dividido previene problemas de conteo hasta infinito en topologías cíclicas al bloquear anuncios de ruta a lo largo de rutas inversas. Cuando se combina con envenenamiento de ruta—donde enlaces fallidos se anuncian con costo infinito—esto crea enrutamiento libre de bucles que converge dentro de tiempo acotado, asegurando que las solicitudes de distribución de entrelazamiento alcancen destinos sin circular indefinidamente a través de la topología de red.",
    "B": "Los contadores de tiempo de vida (TTL) en solicitudes de entrelazamiento virtual proporcionan un mecanismo para prevenir bucles de enrutamiento infinitos al imponer un límite máximo de saltos en intentos de distribución de entrelazamiento. Cada nodo de enrutamiento decrementa el campo TTL al reenviar una solicitud de entrelazamiento; si el contador llega a cero antes de que la solicitud alcance su destino, la solicitud se descarta y se notifica a la fuente. Esto previene que las solicitudes circulen indefinidamente a través de topologías de red cíclicas, asegurando que los fallos de enrutamiento se detecten dentro de tiempo acotado y que los recursos de red no se agoten por tráfico en bucle, similar a cómo los campos TTL de paquetes IP previenen bucles de enrutamiento clásicos.",
    "C": "Los protocolos de enrutamiento de vector de ruta que rastrean explícitamente la secuencia de sistemas autónomos atravesados durante la distribución de entrelazamiento previenen bucles mediante filtrado de rutas. Cada solicitud de distribución de entrelazamiento lleva una lista completa de nodos repetidores cuánticos ya visitados a lo largo de su ruta. Cuando un nodo recibe una solicitud, examina este vector de ruta—si su propio identificador aparece en cualquier lugar de la lista, la ruta crearía un ciclo y se rechaza inmediatamente. Esta detección explícita de bucles asegura que las solicitudes nunca revisiten el mismo nodo dos veces, previniendo patrones de enrutamiento circular mientras permite rutas alternativas legítimas. La técnica refleja el mecanismo de ruta AS de BGP pero opera sobre topologías de red cuántica en lugar de interredes clásicas.",
    "D": "Los protocolos de árbol de expansión que construyen topologías lógicas libres de bucles sobre la red cuántica física previenen ciclos de enrutamiento mediante algoritmos de grafos distribuidos. Los nodos de red intercambian unidades de datos de protocolo de puente conteniendo prioridades de fidelidad e información topológica para elegir un nodo raíz y deshabilitar enlaces de entrelazamiento redundantes que crearían ciclos. Al construir una estructura de árbol donde existe exactamente una ruta entre cualquier par de nodos, el protocolo elimina bucles de enrutamiento a nivel de topología. Los enlaces deshabilitados permanecen disponibles como rutas de respaldo que se activan solo cuando fallan las rutas primarias, asegurando reenvío libre de bucles mientras se mantiene redundancia. Este enfoque es paralelo al STP clásico de Ethernet pero opera sobre topología de canal cuántico en lugar de estructuras de conmutación.",
    "solution": "B"
  },
  {
    "id": 479,
    "question": "En el contexto de la computación cuántica distribuida, ¿por qué son significativos los protocolos de \"computación cuántica no local instantánea\"? Estos protocolos involucran partes espacialmente separadas que comparten estados entrelazados y desean implementar una operación unitaria conjunta sin mover físicamente qubits. La pregunta es fundamentalmente sobre qué recursos (entrelazamiento vs. comunicación clásica) son suficientes para simular puertas multiparte arbitrarias, y qué nos dice esto sobre la estructura de las correlaciones cuánticas.",
    "A": "Demuestran un intercambio fundamental de recursos en sistemas cuánticos distribuidos: el entrelazamiento pre-compartido combinado con un número limitado de rondas de comunicación clásica puede simular cualquier operación unitaria no local que de otro modo requeriría transportar físicamente estados cuánticos entre ubicaciones. Esto revela conexiones matemáticas profundas entre tasas de consumo de entrelazamiento, jerarquías de complejidad de comunicación y la estructura de localidad de operaciones cuánticas en arquitecturas distribuidas. Los protocolos iluminan qué computaciones cuánticas multiparte pueden realizarse usando solo operaciones locales y comunicación clásica (LOCC) aumentadas con entrelazamiento compartido, estableciendo límites rigurosos sobre el costo de comunicación clásica requerido para implementar varias clases de puertas distribuidas e informando así el diseño de redes cuánticas prácticas.",
    "B": "Estos protocolos establecen que ciertas clases de operaciones unitarias no locales sobre sistemas cuánticos bipartitos pueden implementarse usando solo operaciones locales y comunicación clásica (LOCC) cuando las partes comparten suficiente entrelazamiento previo, pero con una restricción sutil: las unitarias alcanzables deben preservar el rango de Schmidt bipartito de cualquier estado de entrada. Esta restricción surge porque las operaciones LOCC no pueden aumentar el entrelazamiento entre subsistemas, incluso cuando se consumen recursos entrelazados pre-compartidos. La significancia radica en identificar qué algoritmos cuánticos distribuidos pueden ejecutarse sin canales de comunicación cuántica—específicamente, aquellos correspondientes a unitarias compatibles con LOCC. Sin embargo, puertas no locales generales que aumentan el rango de Schmidt requieren teleportación cuántica (consumiendo entrelazamiento más dos bits clásicos por qubit) o transferencia directa de estado cuántico, haciendo estos protocolos fundamentales para entender los límites de la computación cuántica distribuida bajo restricciones de localidad.",
    "C": "Los protocolos demuestran que al combinar entrelazamiento pre-compartido con rondas de comunicación clásica cuidadosamente estructuradas, partes espacialmente separadas pueden implementar mediciones en bases entrelazadas sin requerir canales cuánticos—específicamente, pueden realizar mediciones proyectivas no locales correspondientes a análisis de estados de Bell. La intuición clave es que mientras las operaciones unitarias sobre qubits espacialmente separados generalmente requieren comunicación cuántica o transporte físico de qubits, las operaciones de medición pueden simularse mediante una estrategia alternativa: Alice realiza mediciones locales y envía sus resultados clásicos a Bob, quien luego aplica correcciones unitarias adaptativas condicionadas a esos resultados. Este enfoque de computación no local basado en mediciones revela que el entrelazamiento y la comunicación clásica juntos forman una teoría de recursos completa para protocolos cuánticos distribuidos, estableciendo límites inferiores sobre la complejidad de comunicación requerida para simular varias clases de mediciones cuánticas multiparte.",
    "D": "Los protocolos de computación cuántica no local instantánea prueban que el entrelazamiento máximo entre partes espacialmente separadas, combinado con una sola ronda de comunicación clásica, es suficiente para implementar cualquier puerta unitaria multiparte invariante bajo permutaciones sin requerir teleportación cuántica o transferencia directa de estado cuántico. Los protocolos funcionan explotando la estructura del subespacio simétrico: cuando todas las partes comparten un estado GHZ y realizan mediciones locales idénticas seguidas de correcciones dependientes de resultados, pueden rotar colectivamente su estado cuántico compartido dentro del subespacio simétrico. La significancia para la computación cuántica distribuida es establecer que circuitos cuánticos simétricos—que incluyen primitivas algorítmicas importantes como transformadas de Fourier cuánticas sobre entradas simétricas bajo permutación—pueden ejecutarse sin el costo de canales de comunicación cuántica completos, reduciendo la tasa de consumo de entrelazamiento de O(n²) a O(n) ebits por puerta para operaciones de n partes sobre estados simétricos.",
    "solution": "A"
  },
  {
    "id": 480,
    "question": "¿De qué manera el uso de puertas XOR facilita la medición de operadores de Pauli multi-qubit?",
    "A": "Las puertas XOR propagan información de paridad desde qubits de datos hacia qubits de medición auxiliares mediante operaciones controladas que preservan el estado del sistema mientras extraen información de valores propios, pero esto funciona solo para mediciones de estabilizadores donde el operador de Pauli anticonmuta con al menos un observable de la base computacional. Para operadores que conmutan con Z en todos los qubits, el mecanismo XOR no logra distinguir subespacios propios, requiriendo en cambio una transformación de base antes de que la extracción de paridad pueda codificar exitosamente la estructura conjunta de valores propios en la lectura de la ancilla sin perturbar el estado medido.",
    "B": "Las puertas XOR permiten que los valores propios conjuntos de operadores de Pauli multi-qubit se codifiquen sistemáticamente en un solo qubit ancilla mediante propagación de paridad, todo sin perturbar el estado cuántico de los qubits del sistema. Esta propiedad permite la medición simultánea de todos los términos en un conjunto conmutante mediante una sola lectura colectiva, extrayendo la información necesaria de valores propios de manera eficiente.",
    "C": "Las operaciones XOR acumulan información de paridad multi-qubit en qubits de medición designados mediante cascadas secuenciales de CNOT controladas que extraen datos de valores propios sin colapsar el estado del sistema, habilitando lectura conjunta de términos de Pauli conmutantes. Sin embargo, la preservación de coherencia durante esta extracción depende de que la ancilla se inicialice en el estado |+⟩ en lugar de |0⟩, ya que solo la superposición simétrica permite codificación de paridad reversible mediante el mecanismo XOR—la inicialización en base computacional causaría retroacción inmediata que proyectaría los qubits del sistema en estados propios definidos prematuramente.",
    "D": "Las puertas XOR facilitan mediciones de Pauli multi-qubit al crear entrelazamiento entre qubits del sistema y ancillas de medición de manera que mapea información de paridad conjunta a observables de un solo qubit, pero la ventaja crítica proviene de la supresión de errores en lugar de eficiencia de medición: cada operación XOR implementa una extracción de síndrome que detecta errores de bit-flip en los qubits de datos mediante verificaciones de paridad de ancilla, que deben realizarse antes de la medición del observable de Pauli para asegurar lectura precisa de valores propios, reduciendo tasas de error de medición de O(ε) a O(ε²) para probabilidad de error de un solo qubit ε mediante esta verificación de paridad redundante.",
    "solution": "B"
  },
  {
    "id": 481,
    "question": "¿Qué técnica sofisticada proporciona la reconciliación de claves más eficiente en la distribución cuántica de claves con una filtración mínima de información?",
    "A": "El protocolo Cascade con permutaciones aleatorias identifica y corrige iterativamente las discrepancias de bits entre Alice y Bob realizando múltiples pasadas con tamaños de bloque progresivamente mayores, explotando verificaciones de paridad en subconjuntos mezclados aleatoriamente para reducir exponencialmente la tasa de error mientras minimiza la sobrecarga de comunicación clásica — este enfoque logra una eficiencia casi óptima refinando adaptativamente la estructura de bloques basándose en las discrepancias detectadas en rondas anteriores.",
    "B": "Los códigos LDPC de tasa adaptativa ajustan dinámicamente su tasa de codificación según la tasa de error cuántico de bits medida, permitiendo que la eficiencia de reconciliación se aproxime al límite de Shannon mediante la actualización iterativa del algoritmo de propagación de creencias a medida que se intercambian más síndromes — la estructura dispersa de la matriz de verificación de paridad asegura que cada ronda de reconciliación revele información mínima a un espía mientras mantiene una complejidad de decodificación lineal en la longitud del bloque.",
    "C": "Los códigos polares con información lateral cuántica explotan el fenómeno de polarización de canal para lograr una eficiencia de reconciliación arbitrariamente cercana al límite de Shannon mediante la división recursiva de canales cuánticos en subcanales casi perfectos y casi inútiles, permitiendo a Alice y Bob transmitir información selectivamente solo a través de los canales confiables mientras congelan bits en los no confiables — esta construcción alcanza demostrablemente la capacidad con límites de rendimiento explícitos de longitud finita y complejidad polinomial de codificación/decodificación, haciéndola teóricamente óptima para escenarios QKD donde las mediciones cuánticas proporcionan información lateral natural que puede incorporarse al decodificador de cancelación sucesiva para mejorar aún más la eficiencia efectiva de reconciliación más allá de lo que los códigos polares clásicos logran por sí solos.",
    "D": "Los códigos cuánticos de corrección de errores para destilación de claves realizan mediciones de síndrome en qubits auxiliares entrelazados para identificar y revertir errores de fase y de inversión de bit sin colapsar el estado de clave secreta compartida.",
    "solution": "C"
  },
  {
    "id": 482,
    "question": "¿Qué técnica sofisticada de criptoanálisis podría comprometer esquemas criptográficos post-cuánticos basados en retículos?",
    "A": "Usar el algoritmo de Grover para acelerar métodos de enumeración clásicos por un factor de raíz cuadrada, convirtiendo la reducción de retículos de tiempo exponencial en búsqueda de tiempo polinomial mediante amplificación de amplitud aplicada a la enumeración por fuerza bruta de vectores cortos de retículo, reduciendo efectivamente los parámetros de seguridad de 256 bits a 128 bits contra adversarios cuánticos.",
    "B": "Explotar la filtración por canal lateral en implementaciones de hardware, particularmente durante operaciones de muestreo por rechazo o muestreo gaussiano, donde las variaciones de tiempo o patrones de consumo de energía pueden revelar información sobre vectores de base de retículo secretos o términos de error.",
    "C": "Algoritmos cuánticos de tamizado que logran aceleraciones exponenciales sobre enfoques clásicos para resolver problemas de vector más corto en retículos de alta dimensión, usando técnicas de paseo aleatorio cuántico y amplificación de amplitud para buscar en el espacio exponencialmente grande de vectores candidatos de manera más eficiente que métodos clásicos de tamizado como el algoritmo GaussSieve, potencialmente reduciendo la seguridad efectiva de esquemas basados en retículos como Kyber y Dilithium mediante la explotación del paralelismo cuántico en el proceso de enumeración de vectores mientras se mantienen requisitos de memoria cuántica polinomiales.",
    "D": "Ataques estadísticos sobre distribuciones de ruido LWE que explotan desviaciones sutiles del muestreo gaussiano discreto ideal, permitiendo a los adversarios distinguir muestras LWE de uniformes acumulando evidencia a través de miles de muestras mediante pruebas de chi-cuadrado u otras técnicas de ajuste de momentos que revelan estructura en lo que debería ser pseudoaleatorio.",
    "solution": "C"
  },
  {
    "id": 483,
    "question": "Los protocolos de destilación de estados mágicos dispersos tipo H reducen el conteo de T principalmente explotando qué propiedad del código?",
    "A": "La ventaja definitoria de la destilación de estados mágicos dispersos tipo H es que el protocolo opera confiablemente a temperatura ambiente con requisitos mínimos de enfriamiento, a diferencia de la destilación estándar de código de superficie que exige temperaturas de milikelvin para suprimir excitaciones térmicas. Al explotar el modelo de error particular de qubits a mayor temperatura — donde la amortiguación de fase domina sobre los errores de inversión de bit — la estructura dispersa se alinea naturalmente con los síndromes de error que ocurren en ambientes más cálidos. Esta tolerancia a la temperatura significa que el protocolo puede usar electrónica de control más económica y menos precisa que no necesita blindaje criogénico, reduciendo indirectamente el conteo de T al permitir velocidades de reloj más rápidas y fidelidades de puerta más altas a nivel de hardware.",
    "B": "El protocolo disperso tipo H incorpora una etapa de post-procesamiento clásico que convierte algebraicamente errores tipo Z detectados en errores tipo X mediante una transformación de base ingeniosa aplicada después de la medición pero antes de la decodificación. Dado que los errores X en estados mágicos son dramáticamente más fáciles de suprimir que los errores Z — requiriendo solo votación simple por mayoría en lugar de rondas complejas de destilación — esta conversión efectivamente transmuta errores de fase difíciles de corregir en errores triviales de inversión de bit. Esta asimetría en el costo de corrección de errores significa que al cambiar el tipo de error mediante computación clásica, el protocolo reduce el número de puertas T necesarias en capas subsiguientes de destilación aproximadamente por un factor de dos por ronda.",
    "C": "Los protocolos dispersos tipo H logran su reducción de conteo de T eliminando completamente la necesidad de mediciones de estabilizadores durante el proceso de destilación, dependiendo en cambio puramente de aplicaciones de puertas Clifford transversales y seguimiento determinista de errores a través de la estructura del código. La destilación tradicional requiere costosos circuitos de extracción de síndrome que consumen puertas T adicionales para el aparato de medición mismo, pero al omitir estas mediciones y aplicar directamente correcciones Clifford predeterminadas basadas en la estructura dispersa del grupo estabilizador, el protocolo evita esta sobrecarga. Este enfoque sin medición reduce el conteo de T eliminando el costo recursivo de medir y corregir dentro del circuito de destilación, aunque requiere asumir tasas de error iniciales más bajas para mantener la fidelidad.",
    "D": "Mayor rendimiento por ronda debido a restricciones de estabilizadores superpuestos",
    "solution": "D"
  },
  {
    "id": 484,
    "question": "¿Qué beneficios tiene una Máquina de Boltzmann Restringida Cuántica (QRBM) sobre su contraparte clásica?",
    "A": "Las QRBM aprovechan efectos de interferencia cuántica y amplificación de amplitud para lograr una extracción de características mejorada mediante muestreo en tiempo polinomial de distribuciones que requieren recursos clásicos exponenciales, estimación de gradiente mejorada mediante estimación de fase cuántica que permite actualizaciones de parámetros con cuadráticamente menos muestras de entrenamiento, aprendizaje de representación mejorado que captura correlaciones multiescala mediante estructuras de entrelazamiento jerárquicas, y compatibilidad nativa con conjuntos de datos cuánticos donde el preprocesamiento clásico destruiría la coherencia cuántica.",
    "B": "Las QRBM explotan el tunelamiento cuántico entre mínimos de energía locales durante la fase de entrenamiento, permitiendo convergencia garantizada a óptimos globales en paisajes de pérdida no convexos mediante actualizaciones adiabáticas de parámetros que el descenso de gradiente clásico no puede lograr, mientras que la coherencia cuántica mantiene distribuciones de probabilidad exactas sobre configuraciones de capa oculta exponencialmente grandes que requerirían una sobrecarga de muestreo prohibitiva en métodos clásicos de Monte Carlo con cadenas de Markov.",
    "C": "Las QRBM aprovechan propiedades cuánticas incluyendo superposición y entrelazamiento para lograr una captura de características mejorada mediante capacidad representacional exponencialmente grande, entrenamiento e inferencia más rápidos mediante paralelismo cuántico que explora múltiples configuraciones simultáneamente, aprendizaje de representación mejorado que captura correlaciones complejas entre unidades visibles y ocultas, y procesamiento cuántico nativo que maneja eficientemente estructuras de datos de alta dimensión.",
    "D": "Las QRBM utilizan la contextualidad cuántica para construir representaciones de capa oculta que violan las desigualdades de Bell clásicas, permitiendo la extracción de correlaciones de características no locales que son demostrablemente inaccesibles para cualquier arquitectura clásica de máquina de Boltzmann independientemente de su profundidad o anchura, mientras que la retroacción de medición cuántica durante el muestreo implementa naturalmente una forma de regularización dropout que previene el sobreajuste sin requerir procedimientos de entrenamiento estocásticos explícitos.",
    "solution": "C"
  },
  {
    "id": 485,
    "question": "¿Qué mecanismo intenta reducir la diafonía colocando qubits no utilizados entre programas?",
    "A": "La expansión de memoria cuántica explota la capacidad de qubits no utilizados del procesador intercalando qubits inactivos como zonas de amortiguamiento entre programas ejecutándose concurrentemente, basándose en el principio de que los efectos de diafonía decaen exponencialmente con la distancia física en el chip, aunque esto requiere calibración cuidadosa para asegurar que los qubits de amortiguamiento permanezcan en equilibrio térmico y no introduzcan ruido adicional mediante procesos de relajación.",
    "B": "La programación de realineación de fase de pulso coordina los pulsos de control de microondas entre diferentes programas insertando qubits inactivos bloqueados en fase entre ellos.",
    "C": "La asignación de qubits consciente de diafonía posiciona estratégicamente qubits computacionales con zonas de amortiguamiento de qubits no utilizados entre programas cuánticos ejecutándose simultáneamente, explotando la separación espacial para minimizar interacciones de acoplamiento no deseadas que de otro modo corromperían las fidelidades de puerta mediante términos ZZ parásitos o excitación de qubits espectadores.",
    "D": "La ofuscación de difusión Grover despliega patrones de interferencia controlados en qubits de amortiguamiento inactivos posicionados entre programas para cancelar activamente vías de diafonía.",
    "solution": "C"
  },
  {
    "id": 486,
    "question": "Las redes neuronales cuánticas de bajo costo paramétrico representan una dirección de investigación crítica para dispositivos cuánticos de corto plazo, donde la fidelidad de las puertas y la coherencia de los qubits limitan severamente la profundidad de los circuitos. Estas arquitecturas intentan maximizar la expresividad de los circuitos cuánticos variacionales mientras minimizan la sobrecarga. ¿Qué requisito de recursos buscan reducir específicamente manteniendo la capacidad del modelo para representar funciones complejas?",
    "A": "El número acumulado de disparos requeridos en todas las mediciones de valores esperados, que determina directamente el tiempo total de ejecución en los procesadores cuánticos actuales y escala mal con la complejidad del circuito.",
    "B": "El tiempo de coherencia del hardware medido en microsegundos, que limita fundamentalmente cuánto tiempo la información cuántica puede almacenarse y manipularse de manera confiable antes de que la decoherencia ambiental destruya el cómputo.",
    "C": "El número total de ángulos de rotación entrenables en el circuito cuántico parametrizado, que impacta directamente tanto la carga de optimización clásica durante el entrenamiento como la profundidad del circuito requerida para implementar todas las puertas parametrizadas. Al reducir este número mediante esquemas de compartición de pesos, ansätze estructurados o técnicas de reducción de dimensionalidad, estas arquitecturas mantienen la expresividad mientras reducen la demanda sobre los recursos de estimación de gradientes e implementación de puertas.",
    "D": "El número de núcleos de CPU clásicos asignados a tareas de post-procesamiento, incluyendo el cálculo de gradientes y los pasos de optimización de parámetros que ocurren después de que se completa la ejecución del circuito cuántico.",
    "solution": "C"
  },
  {
    "id": 487,
    "question": "¿Qué es el problema de navegación de Zermelo cuántico?",
    "A": "El problema de dirigir un sistema cuántico a lo largo de una geodésica en la variedad de operadores de densidad bajo evolución de Lindblad con Hamiltonianos de control acotados, donde se minimiza la distancia métrica de Bures recorrida por unidad de tiempo sujeta a restricciones de decoherencia, tratando la disipación como un término de deriva análogo a las corrientes oceánicas en la navegación clásica de Zermelo. Este enfoque captura el control óptimo en tiempo para sistemas cuánticos abiertos, pero identifica incorrectamente la métrica de Bures como la estructura geométrica relevante en lugar de usar la geometría de Finsler en el grupo unitario.",
    "B": "Encontrar la forma óptima en tiempo de implementar una transformación unitaria objetivo bajo un Hamiltoniano restringido, donde se debe dirigir el sistema cuántico desde un estado inicial a un estado final deseado en tiempo mínimo eligiendo campos de control que satisfagan limitaciones físicas como amplitud acotada o restricciones de energía, directamente análogo a los problemas clásicos de navegación de Zermelo en geometría diferencial.",
    "C": "Determinar el protocolo de tiempo mínimo para evolucionar un estado cuántico desde |ψ₀⟩ hasta |ψf⟩ bajo un Hamiltoniano independiente del tiempo H = H₀ + u(t)H₁ donde |u(t)| ≤ uₘₐₓ, resuelto aplicando el principio del máximo de Pontryagin para encontrar curvas de conmutación de control bang-bang en la representación de la esfera de Bloch. Si bien esto captura el control óptimo en tiempo, se restringe a una forma específica de Hamiltoniano de control y método de solución en lugar de la formulación geométrica general que caracteriza todos estos problemas.",
    "D": "La tarea de minimizar el tiempo de braquistocrona cuántica—la duración mínima absoluta para transformar un estado puro en otro bajo evolución hamiltoniana arbitraria—donde el límite está establecido por el teorema de Margolus-Levitin ΔE·Tₘᵢₙ ≥ πℏ/2, independientemente de la estrategia de control. Aunque esto proporciona un límite de tiempo fundamental para la transformación de estados cuánticos, especifica el límite inferior en lugar del problema de navegación en sí, que concierne la construcción de protocolos explícitos óptimos en tiempo bajo restricciones de control realistas, no solo calcular el límite de velocidad cuántica último.",
    "solution": "B"
  },
  {
    "id": 488,
    "question": "En el contexto de la seguridad de la computación cuántica, considere un escenario donde un adversario tiene acceso a la infraestructura de nube que aloja un procesador cuántico pero no al hardware cuántico en sí. Pueden potencialmente inyectar código malicioso en varios puntos del pipeline de compilación y ejecución. Dado que los circuitos cuánticos típicamente se compilan desde descripciones de alto nivel hasta secuencias de pulsos específicas del hardware, y que los parámetros de calibración deben actualizarse regularmente para tener en cuenta la deriva del hardware, ¿qué componente en esta cadena de suministro representa la vulnerabilidad más crítica para un atacante que busca corromper sutilmente los cómputos cuánticos?",
    "A": "El compilador cuántico en sí, particularmente los pasos de optimización intermedios que realizan la síntesis de circuitos, descomposición de puertas y enrutamiento de qubits, representa la vulnerabilidad más crítica porque cualquier modificación a estas etapas de transformación inyectaría sistemáticamente errores en todos los circuitos enviados por los usuarios sin requerir intervención por ejecución.",
    "B": "Los operadores de medición definidos en el conjunto de instrucciones cuánticas, específicamente las transformaciones de la base de Pauli y las medidas de operador con valores positivos (POVMs) que mapean estados cuánticos a cadenas de bits clásicas, representan la superficie de ataque más explotable porque corromper estas definiciones de operadores permite a un adversario extraer información incorrecta de estados cuánticos que evolucionaron correctamente. Al rotar sutilmente la base de medición lejos de la base computacional prevista o inyectar sesgo en los elementos POVM usados para mediciones generalizadas, el atacante puede invertir bits de salida, suprimir resultados específicos para sesgar distribuciones de probabilidad, o introducir correlaciones que filtren información sobre el estado cuántico sin activar protocolos de corrección de errores.",
    "C": "Los datos de calibración de pulsos almacenados en archivos de configuración o bases de datos, ya que estos parámetros de control de bajo nivel determinan directamente las implementaciones físicas de puertas y típicamente se confía en ellos sin verificación criptográfica, permitiendo la manipulación dirigida de operaciones específicas. Al alterar la amplitud, frecuencia, duración o fase de los pulsos de microondas o láser usados para implementar puertas cuánticas, un adversario puede introducir errores sistemáticos que corrompen los cómputos mientras permanecen difíciles de detectar mediante pruebas estándar. A diferencia de ataques de nivel superior que podrían activar la detección de anomalías, la corrupción a nivel de pulso aparece como deriva de calibración y afecta a todos los circuitos que usan las puertas comprometidas.",
    "D": "Las definiciones de puertas cuánticas en la arquitectura del conjunto de instrucciones, particularmente los ángulos de rotación parametrizados (θ, φ, λ) en puertas de un solo qubit y las fuerzas de acoplamiento en operaciones de entrelazamiento de dos qubits, constituyen el componente más vulnerable porque estas definiciones establecen la correspondencia matemática entre operaciones cuánticas abstractas y sus transformaciones unitarias previstas. Si un atacante modifica la biblioteca de puertas nativas—por ejemplo, alterando el ángulo de rotación en una puerta Rx(π/2) a Rx(π/2 + ε) o ajustando los parámetros del Hamiltoniano de interacción en una implementación de CNOT—introducen errores coherentes que se acumulan predeciblemente a través de la profundidad del circuito mientras permanecen invisibles para la verificación estándar a nivel de puerta. Estas definiciones de puertas corruptas se propagan a través de toda la pila de compilación ya que todas las operaciones de nivel superior se descomponen en puertas nativas, y porque los procedimientos de calibración miden y corrigen desviaciones de las puertas definidas, el ataque persiste incluso cuando el hardware se recalibra periódicamente para coincidir con las especificaciones de puerta (ahora maliciosas).",
    "solution": "C"
  },
  {
    "id": 489,
    "question": "En una arquitectura típica de código bosónico como el código de gato o el código GKP implementado en circuitos superconductores, ¿qué papel juegan los transmons auxiliares en el esquema de corrección de errores, y qué tipos específicos de errores están diseñados para ayudar a identificar?",
    "A": "Estabilizan dinámicamente la paridad del número de fotones a través de bucles de retroalimentación de medición débil continua que rastrean el estado de la cavidad sin colapsar la información lógica codificada, implementando efectivamente un protocolo de ingeniería de reservorio donde el transmon auxiliar media interacciones disipativas.",
    "B": "Suprimen la retroacción de la medición durante las operaciones de puertas actuando como un amortiguador entre el qubit de datos y el resonador de lectura, lo cual es particularmente importante cuando se requieren mediciones de alta fidelidad.",
    "C": "Miden síndromes de error para detectar errores de cambio de fase en el estado del oscilador realizando mediciones de paridad conjuntas entre el transmon auxiliar y el modo de la cavidad. El auxiliar se acopla dispersivamente al modo bosónico, permitiendo rotaciones condicionales que mapean información de fase de la cavidad al estado del transmon, que luego puede leerse destructivamente. Este esquema de medición indirecta preserva la información cuántica codificada en el oscilador mientras extrae datos de síndrome sobre saltos de fase no deseados o eventos de pérdida de fotones que corrompen el qubit lógico, permitiendo que los protocolos de corrección de errores identifiquen qué operaciones de recuperación deben aplicarse para restaurar el estado del código sin medir directamente la amplitud del campo de la cavidad.",
    "D": "Implementan retroalimentación autónoma de corrección de errores mediante impulsos de desplazamiento coherente aplicados al doble de la frecuencia de la cavidad, que interfieren constructivamente con los procesos de error para dirigir el estado del oscilador de vuelta hacia la variedad del código.",
    "solution": "C"
  },
  {
    "id": 490,
    "question": "¿Qué simula un paseo cuántico en tiempo continuo en la computación cuántica?",
    "A": "Evolución bajo el operador Laplaciano del grafo, donde el Hamiltoniano del sistema es proporcional a la matriz Laplaciana discreta del grafo y el estado cuántico experimenta evolución unitaria temporal que propaga amplitud a través de los vértices según la estructura de conectividad del grafo.",
    "B": "Evolución bajo el operador de adyacencia del grafo, donde el Hamiltoniano del sistema es proporcional a la matriz de adyacencia del grafo y el estado cuántico experimenta evolución unitaria temporal que propaga amplitud a través de los vértices según la estructura de conectividad del grafo, implementando dinámicas idénticas a los paseos con moneda de tiempo discreto en el límite continuo.",
    "C": "Evolución bajo el operador de incidencia del grafo, donde el Hamiltoniano del sistema es proporcional a la matriz de incidencia arista-vértice con signo y el estado cuántico experimenta evolución unitaria temporal que propaga amplitud a través de los vértices según flujos de aristas direccionales, codificando naturalmente información de orientación que distingue conexiones entrantes de salientes.",
    "D": "Evolución bajo el operador estocástico del grafo, donde el Hamiltoniano del sistema es proporcional a la matriz de probabilidad de transición del grafo y el estado cuántico experimenta evolución unitaria temporal que propaga amplitud a través de los vértices según la estructura de conectividad del grafo, preservando tanto la conservación de probabilidad como el balance detallado mediante simetrización hermítica.",
    "solution": "A"
  },
  {
    "id": 491,
    "question": "¿Qué desafío surge al teleportar puertas no-Clifford entre procesadores cuánticos remotos?",
    "A": "La teleportación de puertas no-Clifford requiere operaciones de corrección con retroalimentación que dependen de resultados de medición clásicos comunicados entre los procesadores emisor y receptor, introduciendo cuellos de botella de latencia debido a las velocidades finitas de propagación de señales a lo largo del canal de comunicación clásico que conecta los módulos. Dado que las operaciones no-Clifford tienen ángulos de rotación continuos que deben corregirse con una precisión que exceda el umbral de infidelidad de la puerta, los datos de corrección clásicos requieren mayor profundidad de bits que los resultados de un solo bit suficientes para la teleportación Clifford, aumentando tanto la sobrecarga de comunicación como la probabilidad de errores de transmisión que se propagan a través de capas subsiguientes del circuito cuántico.",
    "B": "La teleportación no-Clifford exige estados ancilla entrelazados más sofisticados que simples pares de Bell, específicamente estados mágicos cuya preparación consume muchos recursos y es propensa a errores. Como estas puertas están fuera del grupo Clifford, no pueden corregirse usando solo operaciones de Pauli después de la medición, requiriendo recursos cuánticos adicionales y propagando los errores de manera más severa a través del circuito de teleportación en comparación con las puertas Clifford, que preservan la estructura estabilizadora y permiten protocolos de corrección clásica eficientes.",
    "C": "Las puertas no-Clifford se transforman bajo teleportación mediante conjugación no lineal por los operadores de medición de Bell, causando que los parámetros de la puerta se mezclen con los resultados de medición clásicos de una manera que requiere computación clásica en tiempo real para determinar las operaciones correctas de retroalimentación. Dado que estos cálculos involucran funciones trascendentes de los ángulos de rotación y no pueden pre-compilarse en tablas de búsqueda como las correcciones Clifford, el co-procesador clásico debe resolver ecuaciones no lineales dentro de la ventana de coherencia del qubit, creando un cuello de botella computacional que limita la velocidad a la que las operaciones no-Clifford pueden distribuirse entre módulos remotos.",
    "D": "Teleportar puertas no-Clifford requiere destilar estados mágicos cuya fidelidad debe exceder el umbral determinado por la distancia de la puerta del grupo Clifford medida por su rango estabilizador, consumiendo entrelazamiento a tasas que escalan exponencialmente con la precisión deseada del ángulo de rotación. Como la teleportación Clifford usa solo pares de Bell de base computacional y aplica correcciones de Pauli dictadas por resultados de medición sin estados de recurso adicionales, la sobrecarga para teleportación no-Clifford de alta fidelidad crece prohibitivamente grande para ángulos de rotación que requieren más de unos pocos bits de precisión, creando el cuello de botella dominante en arquitecturas modulares tolerantes a fallos donde la inyección de puertas T domina el costo de recursos.",
    "solution": "B"
  },
  {
    "id": 492,
    "question": "¿Por qué se prefiere el ansatz unitario de cúmulos acoplados (uCC) en simulaciones cuánticas sobre las clásicas?",
    "A": "La forma unitaria exp(T - T†) garantiza consistencia de tamaño para la disociación molecular, una propiedad que los métodos CC truncados clásicos logran solo aproximadamente mediante una elección cuidadosa de operadores de excitación. Mientras que el CCSD clásico es consistente en tamaño para fragmentos bien separados, la formulación unitaria asegura factorización exacta de la función de onda en componentes de subsistemas no interactuantes incluso con conjuntos de bases finitos, haciendo uCC superior para escaneo de coordenadas de reacción. Sin embargo, esta ventaja proviene de la estructura algebraica más que de compatibilidad de hardware: tanto las implementaciones clásicas como cuánticas enfrentan escalado computacional similar.",
    "B": "La evolución unitaria a través del operador exponencial exp(T - T†) se mapea naturalmente a secuencias de puertas cuánticas que preservan la coherencia cuántica, a diferencia de los operadores truncados de cúmulos acoplados no unitarios usados en simulaciones clásicas que no pueden implementarse directamente en hardware cuántico. La estructura anti-hermítica asegura preservación de norma y reversibilidad, propiedades esenciales para algoritmos cuánticos variacionales pero ausentes en métodos CC truncados clásicos.",
    "C": "El generador anti-hermítico (T - T†) en cúmulos acoplados unitarios conmuta naturalmente con el hamiltoniano electrónico para sistemas de capa cerrada en geometrías de equilibrio, permitiendo implementación directa mediante una única puerta de rotación parametrizada por operador de excitación en lugar de requerir descomposición de Trotter. Esta conmutatividad surge porque la referencia Hartree-Fock elimina todos los términos de un cuerpo en el hamiltoniano de segunda cuantización, dejando solo interacciones de dos cuerpos que comparten la misma estructura de Pauli que los operadores de cúmulo. Consecuentemente, los circuitos uCC evitan la explosión de profundidad característica de la simulación hamiltoniana general mientras mantienen exactitud para preparación de estados fundamentales.",
    "D": "Implementar uCC mediante la forma exponencial incorpora automáticamente efectos de correlación de orden infinito dentro de cada paso de Trotter, mientras que el CC truncado clásico requiere construcción explícita de operadores de excitación superiores (T₃, T₄, etc.) para capturar la misma física. El exponencial genera una rotación unitaria en el espacio de Fock que incluye implícitamente todas las potencias del operador de cúmulo, sumando efectivamente una serie infinita que sería intratable clásicamente. Esta resuma incorporada asegura que uCC mejore sistemáticamente la precisión a medida que aumenta la profundidad del circuito, convergiendo a autoestados exactos sin incluir manualmente excitaciones de rango superior.",
    "solution": "B"
  },
  {
    "id": 493,
    "question": "¿Cuál es la ventaja principal de los códigos cuánticos concatenados sobre los códigos de un solo nivel?",
    "A": "Los códigos cuánticos concatenados logran supresión de errores mediante codificación recursiva donde cada qubit lógico en el nivel k se convierte en el bloque de construcción para el nivel k+1, pero esta estructura jerárquica tiene el beneficio contraintuitivo de reducir realmente el número total de qubits físicos requeridos en comparación con códigos de un solo nivel. Mientras que un código [[7,1,3]] requiere 7 qubits físicos por qubit lógico, concatenarlo dos veces solo requiere 7 + 7 = 14 qubits físicos en lugar de 49, porque la codificación recursiva permite que los qubits se reutilicen entre niveles de concatenación mediante un esquema inteligente de multiplexación temporal.",
    "B": "Los códigos concatenados emplean una estrategia de codificación jerárquica donde cada nivel envuelve el anterior en capas protectoras adicionales, y esta estructura anidada permite medición directa de observables de qubits lógicos sin primero decodificar de vuelta al nivel físico. Al medir estabilizadores en el nivel más externo de concatenación, se pueden extraer resultados computacionales mientras se dejan los estados codificados internos en superposición, lo cual es esencial para mantener coherencia cuántica durante operaciones de lectura a mitad de circuito. Esta capacidad de medición sin colapso es única de arquitecturas concatenadas y no puede replicarse en códigos de superficie u otras construcciones topológicas.",
    "C": "La estructura recursiva de la corrección de errores cuánticos concatenada crea un mecanismo de detección de errores auto-reforzante donde los errores se empujan hacia afuera a través de capas de codificación sucesivas hasta que eventualmente se manifiestan como síndromes detectables en el límite del espacio de código. Esta migración de errores hacia afuera significa que la extracción de síndrome se vuelve obsoleta después del tercer o cuarto nivel de concatenación, ya que los errores se revelan naturalmente a través de efectos de frontera en lugar de requerir mediciones activas de estabilizador. Al eliminar ciclos repetidos de medición de síndrome, los códigos concatenados reducen la sobrecarga de circuito en aproximadamente 60% comparado con códigos de superficie mientras mantienen umbrales comparables de supresión de errores.",
    "D": "Supresión exponencial de errores con solo costo polinomial de recursos mediante codificación recursiva que amplifica la protección en cada nivel.",
    "solution": "D"
  },
  {
    "id": 494,
    "question": "¿Por qué son importantes las puertas base en computación cuántica?",
    "A": "Determinan la resolución de la aproximación Solovay-Kitaev: cualquier unitario de n qubits puede aproximarse con precisión ε usando O(log^c(1/ε)) puertas base de un conjunto universal como {H, T, CNOT}, donde c ≈ 3.97 es el exponente de Solovay-Kitaev. La elección específica de puertas base afecta esta constante c y por tanto la sobrecarga de profundidad de circuito requerida para lograr una precisión objetivo. Dado que todos los algoritmos cuánticos deben finalmente descomponerse en secuencias aproximadas de puertas físicas, el conjunto de puertas base restringe fundamentalmente tanto la complejidad de compilación como la fidelidad alcanzable de los cálculos cuánticos.",
    "B": "Definen las operaciones nativas fundamentales que están físicamente disponibles y directamente implementables en una arquitectura de procesador cuántico dada. Todos los algoritmos cuánticos de nivel superior deben compilarse en secuencias de estas puertas base, y la elección específica de puertas base determina la eficiencia y fidelidad con la cual pueden ejecutarse circuitos cuánticos complejos.",
    "C": "Establecen las propiedades de clausura algebraica del conjunto de puertas implementables, asegurando que cualquier secuencia de aplicaciones de puertas base permanezca dentro del mismo grupo de Lie. Por ejemplo, las puertas Clifford forman un grupo finito que puede simularse eficientemente de manera clásica, mientras que agregar la puerta T extiende esto a un conjunto universal al hacer que el grupo de puertas sea denso en SU(2^n). La elección de puertas base determina así si la computadora cuántica puede generar operaciones fuera de subgrupos eficientemente simulables, lo cual es esencial para lograr ventaja computacional. Sin puertas base cuidadosamente elegidas que satisfagan condiciones específicas de clausura de teoría de grupos, los circuitos compilados podrían carecer de universalidad.",
    "D": "Sirven como operaciones primitivas para las cuales las tasas de error se caracterizan y optimizan experimentalmente mediante ingeniería de pulsos de control. Cada puerta base corresponde a una secuencia de control calibrada (pulsos de microondas, pulsos láser, etc.) con fidelidades medidas de uno y dos qubits. Las puertas de nivel superior deben descomponerse en estas primitivas calibradas, y el error total del circuito se acumula según las tasas de error de puertas base y la profundidad de descomposición. La elección de puertas base por tanto impacta directamente la fidelidad de circuito alcanzable, ya que la longitud de descomposición de puertas y las tasas de error por puerta determinan conjuntamente la precisión general del cálculo.",
    "solution": "B"
  },
  {
    "id": 495,
    "question": "Considere una matriz singular H usada en una simulación cuántica mediante la operación exp(-iHt). Aunque H tiene autovalores cero y no es invertible, los simuladores cuánticos aún pueden procesarla sin que surjan problemas matemáticos fundamentales durante la evolución del estado. Un estudiante que estudia simulación hamiltoniana pregunta por qué es este el caso, dado que la singularidad típicamente causa problemas en métodos numéricos clásicos. ¿Cuál es la razón subyacente por la que las matrices singulares permanecen viables en este contexto cuántico?",
    "A": "Los componentes del espacio nulo evolucionan con autofase exp(0·t) = 1, permaneciendo estacionarios y proyectándose sobre subespacios no medibles inaccesibles a observables físicos, no contribuyendo así errores numéricos.",
    "B": "La restricción unitaria de la mecánica cuántica aplica automáticamente regularización espectral durante la exponenciación, reemplazando autovalores cero con pequeños valores positivos cercanos a la precisión de máquina para prevenir divergencias de estilo clásico.",
    "C": "La exponencial matricial exp(-iHt) está matemáticamente bien definida para hamiltonianos singulares porque la función exponencial converge para todas las matrices cuadradas independientemente de su invertibilidad, y los autovalores cero en H simplemente contribuyen términos exp(-i·0·t) = 1 al operador unitario resultante. Estas contribuciones similares a la identidad dejan los componentes de autovectores correspondientes sin cambios durante la evolución temporal: permanecen estacionarios en lugar de rotar en el plano complejo. Dado que el mapeo exponencial siempre produce un operador unitario válido que preserva la normalización del estado cuántico y genera distribuciones de probabilidad legítimas tras la medición, la simulación procede sin encontrar las inestabilidades numéricas u operaciones indefinidas que afligen a métodos clásicos que intentan invertir o descomponer matrices singulares. La invertibilidad simplemente no es requerida para la exponenciación.",
    "D": "Parámetros de tiempo grandes causan que las amplitudes de autovalores cero excedan la unidad mediante errores de Trotter, pero la renormalización automática después de cada paso proyecta los estados de vuelta al espacio de Hilbert válido, previniendo distribuciones no físicas.",
    "solution": "C"
  },
  {
    "id": 496,
    "question": "¿Qué técnica avanzada permite extraer material de clave secreta de redes de distribución cuántica de claves con nodos de confianza?",
    "A": "Al analizar los patrones de temporización precisos a nivel de microsegundos de las operaciones de retransmisión de claves a través de nodos de confianza, un adversario puede reconstruir correlaciones entre segmentos secuenciales de clave que revelan información parcial sobre la estructura XOR del material de clave sin procesar mediante variaciones de latencia dependientes de los datos.",
    "B": "Dado que las arquitecturas de nodos de confianza dependen de canales autenticados clásicos para la identificación de nodos antes de que comience el establecimiento de claves cuánticas, comprometer los certificados PKI utilizados en la autenticación permite a un atacante hacerse pasar por nodos legítimos, solicitar material de clave mediante operaciones normales del protocolo y explotar vulnerabilidades de autenticación para obtener estado de confianza sin requerir acceso físico al canal cuántico.",
    "C": "Sondeo de registros de claves intermedias",
    "D": "Los nodos de confianza almacenan temporalmente bits de clave derivados cuánticamente en búferes DRAM o SRAM antes de reenviarlos a nodos adyacentes, y estas celdas de memoria exhiben emanaciones electromagnéticas cuando el contenido cambia de estado durante operaciones de lectura/escritura, permitiendo la reconstrucción del material de clave transitorio a partir de emisiones RF de canal lateral capturadas por receptores sensibles posicionados cerca del hardware.",
    "solution": "C"
  },
  {
    "id": 497,
    "question": "Las estrategias de arranque en caliente (warm-start) tomadas de QAOA pueden beneficiar a los clasificadores variacionales mediante:",
    "A": "En la primera iteración del entrenamiento, los protocolos de arranque en caliente configuran el clasificador variacional para usar únicamente rotaciones parametrizadas de un solo qubit (puertas RX, RY, RZ) mientras posponen todas las operaciones de entrelazamiento de dos qubits a épocas subsiguientes, reflejando la estrategia de QAOA de construir gradualmente la estructura del problema a través de capas. Este enfoque por fases asegura que el paisaje de parámetros inicial sea convexo—porque los unitarios de un solo qubit forman una variedad de baja dimensión sin mesetas estériles—permitiendo que optimizadores clásicos como COBYLA o L-BFGS converjan rápidamente a un estado separable casi óptimo antes de introducir entrelazamiento. Una vez completada esta fase de arranque en caliente, el clasificador introduce puertas CNOT una a la vez, usando la solución separable como punto de anclaje para evitar puntos de silla en el espacio de parámetros entrelazados completo.",
    "B": "El arranque en caliente permite al clasificador variacional asignar el doble de qubits físicos para codificar el espacio de características sin aumentar la profundidad del circuito, porque la configuración inicial de parámetros pre-entrelaza qubits auxiliares con qubits de datos en un estado producto que efectivamente duplica la dimensión del espacio de Hilbert. Esta técnica aprovecha la observación de QAOA de que circuitos más profundos con más parámetros exploran naturalmente variedades de mayor dimensión.",
    "C": "Al inicializar los parámetros variacionales del clasificador según la trayectoria adiabática derivada de los hamiltonianos mezclador y de costo de QAOA, el sistema permanece confinado a un subespacio libre de decoherencia (DFS) durante todas las iteraciones de descenso de gradiente, porque el DFS se preserva bajo actualizaciones continuas de parámetros siempre que el hamiltoniano conmute con el operador de momento angular total J². El arranque en caliente establece específicamente los ángulos iniciales θ₀ y β₀ de modo que el estado evolucionado en el tiempo se encuentre completamente dentro del subespacio simétrico del registro de qubits, que es inmune al defasamiento colectivo y ciertos procesos de amortiguamiento de amplitud. Esto permite al clasificador variacional mantener coherencia durante un número arbitrario de pasos de optimización sin requerir corrección de errores.",
    "D": "Inicializar los parámetros variacionales cerca de soluciones casi óptimas usando heurísticas específicas del dominio o aproximaciones clásicas derivadas de la estructura del problema, posicionando así al optimizador en una región favorable del paisaje de parámetros donde los gradientes apuntan hacia mínimos de alta fidelidad y evitando mesetas estériles u óptimos locales pobres que afectan a la inicialización aleatoria.",
    "solution": "D"
  },
  {
    "id": 498,
    "question": "¿Cuál es el efecto de la ubicación de puertas SWAP en el rendimiento del circuit knitting?",
    "A": "Una mala ubicación de SWAP aumenta dramáticamente la profundidad del circuito y destruye la fidelidad al forzar a los qubits a través de cadenas de interacción innecesariamente largas.",
    "B": "La ubicación subóptima de SWAP aumenta exponencialmente la sobrecarga de muestreo porque cada SWAP mal ubicado introduce ramas adicionales de cuasi-probabilidad en la descomposición del circuit knitting. Cuando los qubits se enrutan ineficientemente a través de límites de dispositivos, la expansión de suma de productos tensoriales resultante adquiere más términos con coeficientes mayores, inflando directamente el número de muestras de circuito necesarias para reconstruir con precisión los valores de expectación dentro de límites de error fijos.",
    "C": "Un mal ordenamiento de SWAP rompe la conmutatividad entre particiones de subcircuitos al introducir dependencias espurias que impiden la ejecución paralela de fragmentos independientes. Cuando los SWAP se posicionan mal, crean falsas dependencias de datos que fuerzan la programación secuencial de operaciones que de otro modo podrían ejecutarse concurrentemente en procesadores cuánticos separados. Este cuello de botella de serialización destruye el paralelismo que el circuit knitting está diseñado para explotar, limitando directamente la escalabilidad.",
    "D": "La ubicación ineficiente de SWAP infla el costo de post-procesamiento clásico al expandir el número de cortes de cables requeridos para particionar el circuito en dispositivos disponibles. Cada SWAP adicional cerca de un límite de corte necesita pares extra de medición-preparación, aumentando multiplicativamente la carga computacional de reconstruir la función de onda completa a partir de fragmentos distribuidos. La sobrecarga escala combinatoriamente con el número de SWAP mal ubicados que cruzan límites de partición.",
    "solution": "A"
  },
  {
    "id": 499,
    "question": "¿Cuál es la limitación principal de usar la métrica de información de Fisher cuántica para el descenso de gradiente natural cuántico?",
    "A": "Calcular la métrica de información de Fisher cuántica requiere medir momentos estadísticos de orden superior de observables cuánticos, lo que demanda circuitos significativamente más profundos que la estimación estándar de gradiente. Cada elemento de matriz implica preparar extensiones asistidas por ancillas del estado parametrizado y realizar operaciones unitarias controladas condicionadas en registros de parámetros, aumentando la profundidad del circuito por un factor proporcional al conteo de parámetros. En hardware NISQ, esta profundidad adicional causa que los errores de puerta se acumulen más allá de umbrales aceptables, degradando la precisión de estimación de la métrica y socavando los beneficios de convergencia de la optimización de gradiente natural.",
    "B": "Calcular el tensor de métrica de información de Fisher cuántica cuesta exponencialmente en el conteo de parámetros, requiriendo recursos que escalan como O(p²) mediciones para p parámetros. Este escalamiento cuadrático en evaluaciones de circuito hace que el enfoque sea computacionalmente prohibitivo para algoritmos variacionales con cientos o miles de parámetros, eliminando la ventaja práctica sobre métodos estándar de descenso de gradiente.",
    "C": "La métrica de información de Fisher cuántica se vuelve mal condicionada cerca de puntos críticos en el paisaje de optimización donde el estado cuántico exhibe simetrías aproximadas, causando que los autovalores del tensor métrico abarquen muchos órdenes de magnitud. Invertir esta matriz mal condicionada para calcular el gradiente natural amplifica errores numéricos y produce actualizaciones de parámetros inestables que oscilan entre regiones distantes del espacio de parámetros. Las técnicas estándar de regularización como agregar desplazamientos diagonales al tensor métrico destruyen la geometría riemanniana en la que se basan los gradientes naturales, reintroduciendo las patologías de convergencia que el método fue diseñado para resolver.",
    "D": "La métrica de información de Fisher cuántica se aplica estrictamente a estados cuánticos puros preparados por unitarios parametrizados actuando sobre estados iniciales fijos, pero los algoritmos variacionales prácticos en dispositivos NISQ producen estados mixtos debido a la decoherencia y el defasamiento inducido por medición durante operaciones a mitad de circuito. Cuando el estado cuántico exhibe una mezcla clásica genuina en lugar de una superposición coherente pura, la información de Fisher cuántica ya no captura la estructura geométrica correcta de la variedad de parámetros, y las actualizaciones de gradiente natural resultantes incorporan contribuciones tanto de información de Fisher cuántica como clásica de maneras que no pueden desenredarse sin tomografía completa del estado cuántico, lo que reintroduce escalamiento exponencial.",
    "solution": "B"
  },
  {
    "id": 500,
    "question": "Considere una arquitectura cuántica a gran escala ejecutando el algoritmo de Shor con millones de qubits lógicos codificados usando surface codes. A medida que el algoritmo escala, la sobrecarga computacional se vuelve dominada por un único cuello de botella arquitectónico. El sistema de control clásico puede mantenerse al día con las mediciones de síndrome, y la destilación de estados mágicos ha sido optimizada. ¿Qué limitación fundamental hace que los surface codes sean ineficientes a esta escala?",
    "A": "El número de qubits físicos por qubit lógico crece cuadráticamente con la distancia del código. Para umbrales tolerantes a fallos relevantes para algoritmos grandes, se necesita distancia d ≈ 20-30, lo que significa que cada qubit lógico requiere 800-1800 qubits físicos. Esta sobrecarga se multiplica a través de millones de qubits lógicos, haciendo que el conteo total de recursos físicos sea astronómico—potencialmente miles de millones o decenas de miles de millones de qubits físicos—incluso cuando las tasas de error son relativamente bajas, creando una carga de hardware enorme que domina todas las demás consideraciones de recursos.",
    "B": "Los surface codes imponen un circuito de extracción de síndrome de profundidad constante independiente de la distancia del código, lo que paradójicamente se vuelve problemático a escala. Cada ronda de medición de estabilizador requiere que qubits auxiliares interactúen con qubits de datos mediante puertas CNOT, y a distancias d ≈ 20-30 necesarias para algoritmos de millones de puertas, el arreglo espacial fuerza que las fuerzas de acoplamiento ancilla-dato se debiliten debido a restricciones geométricas. Aunque se necesitan menos rondas por puerta lógica, la fidelidad reducida de acoplamiento por ronda compensa exactamente, causando que las tasas de error lógico se estabilicen alrededor de 10^-5 independientemente de la distancia—insuficiente para algoritmos que requieren 10^8 operaciones.",
    "C": "La conectividad de qubits lógicos en surface codes es fundamentalmente no local: implementar un CNOT entre qubits lógicos distantes requiere operaciones de lattice surgery que consumen tiempo proporcional a su distancia de separación. Para el algoritmo de Shor con millones de qubits lógicos dispuestos en un arreglo 2D, la puerta lógica promedio debe comunicarse a través de distancias que escalan como √N, donde N es el conteo de qubits lógicos. Esto crea un cuello de botella de latencia donde la profundidad del circuito crece superlinealmente con el tamaño del problema—no por sobrecarga de corrección de errores, sino por el tiempo de tránsito de protocolos de lattice surgery propagando defectos topológicos a través de la red física para fusionar parches de código distantes.",
    "D": "Los surface codes requieren que los qubits auxiliares para extracción de síndrome sean actualizados cada ciclo mediante mediciones proyectivas, que colapsan irreversiblemente su estado. A distancias d ≈ 20-30, cada qubit lógico demanda aproximadamente 50-100 ancillas medidas simultáneamente por ronda. Para millones de qubits lógicos, el aparato de medición debe ejecutar 10^8 a 10^9 lecturas de disparo único en microsegundos para mantener corrección de errores en tiempo real. Incluso con procesamiento clásico perfecto, la circuitería de lectura física no puede paralelizarse más allá de ~10^6 canales por criostato debido a límites de densidad de cableado, forzando multiplexado temporal que introduce latencia proporcional al conteo de qubits y detiene el cómputo.",
    "solution": "A"
  }
]